,abstract,id,journal,keywords,title,year
0,"We consider discrete-time observations of a continuous martingale under measurement error. This serves as a fundamental model for high-frequency data in finance, where an efficient price process is observed under microstructure noise. It is shown that this nonparametric model is in Le Cam's sense asymptotically equivalent to a Gaussian shift experiment in terms of the square root of the volatility function a and a nonstandard noise level. As an application, new rate-optimal estimators of the volatility function and simple efficient estimators of the integrated volatility are constructed.",WOS:000291183300004,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'MICROSTRUCTURE NOISE', 'EFFICIENT ESTIMATION', 'WHITE-NOISE']",ASYMPTOTIC EQUIVALENCE FOR INFERENCE ON THE VOLATILITY FROM NOISY OBSERVATIONS,2011
1,"Many statistical analyses (e. g., in econometrics, biostatistics and experimental design) are based on models containing systems of structurally related equations. The systemfit package provides the capability to estimate systems of linear equations within the R programming environment. For instance, this package can be used for ""ordinary least squares"" (OLS), ""seemingly unrelated regression"" (SUR), and the instrumental variable (IV) methods ""two-stage least squares"" (2SLS) and ""three-stage least squares"" (3SLS), where SUR and 3SLS estimations can optionally be iterated. Furthermore, the systemfit package provides tools for several statistical tests. It has been tested on a variety of datasets and its reliability is demonstrated.",WOS:000252431400001,JOURNAL OF STATISTICAL SOFTWARE,"['3-STAGE LEAST-SQUARES', 'REGRESSION EQUATIONS', 'TESTS']",systemfit: A package for estimating systems of simultaneous equations in R,2007
2,"It has been recently shown that, under the margin (or low noise) assumption, there exist classifiers attaining fast rates of convergence of the excess Bayes risk, that is, rates faster than n(-1/2). The work on this subject has suggested the following two conjectures: (i) the best achievable fast rate is of the order n(-1), and (ii) the plug-in classifiers generally converge more slowly than the classifiers based on empirical risk minimization. We show that both conjectures are not correct. In particular, we construct plug-in classifiers that can achieve not only fast, but also super-fast rates, that is, rates faster than n-1. We establish minimax lower bounds showing that the obtained rates cannot be improved.",WOS:000248987600006,ANNALS OF STATISTICS,"['RISK BOUNDS', 'CONVERGENCE', 'CLASSIFICATION']",Fast learning rates for plug-in classifiers,2007
3,"Reference analysis produces objective Bayesian inference, in the sense that inferential statements depend only on the assumed model and the available data, and the prior distribution used to make an inference is least informative in a certain information-theoretic sense. Reference priors have been rigorously defined in specific contexts and heuristically defined in general, but a rigorous general definition has been lacking. We produce a rigorous general definition here and then show how an explicit expression for the reference prior can be obtained under very weak regularity conditions. The explicit expression can be used to derive new reference priors both analytically and numerically.",WOS:000265500500013,ANNALS OF STATISTICS,"['POSTERIOR DISTRIBUTIONS', 'MATHEMATICAL-THEORY', 'BAYESIAN-ANALYSIS', 'PROBABILITY', 'INFORMATION', 'INFERENCE', 'COMMUNICATION', 'CONVERGENCE', 'ENTROPY']",THE FORMAL DEFINITION OF REFERENCE PRIORS,2009
4,"We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.",WOS:000399022900001,JOURNAL OF STATISTICAL SOFTWARE,"['MACHINE LEARNING-METHODS', 'PROBABILITY ESTIMATION', 'ASSOCIATION']",ranger: A Fast Implementation of Random Forests for High Dimensional Data in C plus plus and R,2017
5,"We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of l(1) and l(2) penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.",WOS:000288204000001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED LINEAR-MODELS', 'GENE-EXPRESSION DATA', 'VARIABLE SELECTION', 'REGRESSION', 'LASSO']",Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent,2011
6,"Semisupervised methods are techniques for using labeled data (X-1, Y-1), ..., (X-n, Y-n) together with unlabeled data Xn+1, ..., X-N to make predictions. These methods invoke some assumptions that link the marginal distribution P-X of X to the regression function f(x). For example, it is common to assume that f is very smooth over high density regions of P-X. Many of the methods are ad-hoc and have been shown to work in specific examples but are lacking a theoretical foundation. We provide a minimax framework for analyzing semisupervised methods. In particular, we study methods based on metrics that are sensitive to the distribution P-X. Our model includes a parameter alpha that controls the strength of the semisupervised assumption. We then use the data to adapt to alpha.",WOS:000320488200013,ANNALS OF STATISTICS,['SAMPLES'],DENSITY-SENSITIVE SEMISUPERVISED INFERENCE,2013
7,"This paper describes the core features of the R package geepack, which implements the generalized estimating equations (GEE) approach for fitting marginal generalized linear models to clustered data. Clustered data arise in many applications such as longitudinal data and repeated measures. The GEE approach focuses on models for the mean of the correlated observations within clusters without fully specifying the joint distribution of the observations. It has been widely used in statistical practice. This paper illustrates the application of the GEE approach with geepack through an example of clustered binary data.",WOS:000235180600001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR-MODELS', 'REGRESSION-MODELS', 'INFERENCE']",The R Package geepack for Generalized Estimating Equations,2006
8,"Matrix completion and quantum tomography are two unrelated research areas with great current interest in many modern scientific studies. This paper investigates the statistical relationship between trace regression in matrix completion and quantum state tomography in quantum physics and quantum information science. As quantum state tomography and trace regression share the common goal of recovering an unknown matrix, it is nature to put them in the Le Cam paradigm for statistical comparison. Regarding the two types of matrix inference problems as two statistical experiments, we establish their asymptotic equivalence in terms of deficiency distance. The equivalence study motivates us to introduce a new trace regression model. The asymptotic equivalence provides a sound statistical foundation for applying matrix completion methods to quantum state tomography. We investigate the asymptotic equivalence for sparse density matrices and low rank density matrices and demonstrate that sparsity and low rank are not necessarily helpful for achieving the asymptotic equivalence of quantum state tomography and trace regression. In particular, we show that popular Pauli measurements are bad for establishing the asymptotic equivalence for sparse density matrices and low rank density matrices.",WOS:000327746100007,ANNALS OF STATISTICS,"['LOW-RANK MATRICES', 'HIGH-DIMENSIONAL MATRICES', 'PENALIZATION', 'ESTIMATORS']",ASYMPTOTIC EQUIVALENCE OF QUANTUM STATE TOMOGRAPHY AND NOISY MATRIX COMPLETION,2013
9,"In a multiple testing problem where one is willing to tolerate a few false rejections, procedure controlling the familywise error rate (FWER) can potentially be improved in terms of its ability to detect false null hypotheses by generalizing it to control the k-FWER, the probability of falsely rejecting at least k null hypotheses, for some fixed k > 1. Simes' test for testing the intersection null hypothesis is generalized to control the k-FWER weakly, that is, under the intersection null hypothesis, and Hochberg's stepup procedure for simultaneous testing of the individual null hypotheses is generalized to control the k-FWER strongly, that is, under any configuration of the true and false null hypotheses. The proposed generalizations are developed utilizing joint null distributions of the k-dimensional subsets of the p-values, assumed to be identical. The generalized Simes' test is proved to control the k-FWER weakly under the multivariate totally positive of order two (MTP2) condition V. Multivariate Analysis 10 (1980) 467-498] of the joint null distribution of the P-values by generalizing the original Simes' inequality. It is more powerful to detect k or more false null hypotheses than the original Simes' test when the p-values are independent. A stepdown procedure strongly controlling the k-FWER, a version of generalized Holm's procedure that is different from and more powerful than [Ann. Statist. 33 (2005) 1138-1154] with independent p-values, is derived before proposing the generalized Hochberg's procedure. The strong control of the k-FWER for the generalized Hochberg's procedure is established in situations where the generalized Simes' test is known to control its k-FWER weakly.",WOS:000253390000013,ANNALS OF STATISTICS,"['FAMILYWISE ERROR RATE', 'FALSE DISCOVERY RATE', 'BONFERRONI PROCEDURE', 'RANDOM-VARIABLES', 'MULTIPLE TESTS', 'INEQUALITIES']",Generalizing Simes' test and Hochberg's stepup procedure,2008
10,"In order to facilitate teaching complex topics in an interactive way, the authors developed a computer-assisted teaching system, a graphical user interface named TGUI (Teaching Graphical User Interface). TGUI was introduced at the beginning of 2009 in the Austrian Journal of Statistics (Dinges and Templ 2009) as being an effective instrument to train and teach staff on mathematical and statistical topics. While the fundamental principles were retained, the current TGUI system has been undergone a complete redesign. The ultimate goal behind the reimplementation was to share the advantages of TGUI and provide teachers and people who need to hold training courses with a strong tool that can enrich their lectures with interactive features. The idea was to go a step beyond the current modular blended-learning systems (see, e.g., Da Rin 2003) or the related teaching techniques of classroom-voting (see, e.g., Cline 2006). In this paper the authors have attempted to exemplify basic idea and concept of TGUI by means of statistics seminars held at Statistics Austria. The powerful open source software R (R Development Core Team 2010a) is the backend for TGUI which can therefore be used to process even complex statistical contents. However, with specifically created contents the interactive TGUI system can be used to support a wide range of courses and topics. The open source R packages TGUI Core and TGUI Teaching are freely available from the Comprehensive R Archive Network at http://CRAN.R-project.org/",WOS:000288204700001,JOURNAL OF STATISTICAL SOFTWARE,,An Open Source Approach for Modern Teaching Methods: The Interactive TGUI System,2011
11,"Response-adaptive randomization hits recently attracted a lot of attention in the literature. In this paper, we propose a new and simple family of response-adaptive randomization procedures that attain the cramer-Rao lower bounds on the allocation variances for any allocation proportions including optimal allocation proportions. The allocation Probability functions of proposed procedures are discontinuous. The existing large sample theory for adaptive designs relies on Taylor expansions of the allocation probability functions. which do not apply to nondifferentiable cases. In the present paper, we Study stopping times of stochastic processes to establish the asymptotic efficiency results. Furthermore, we demonstrate our proposal through examples, simulations and a discussion on the relationship with earlier works, including Efron's biased coin design.",WOS:000268604900017,ANNALS OF STATISTICS,"['BIASED COIN DESIGNS', 'SEQUENTIAL CLINICAL-TRIALS', 'PLAY-WINNER RULE', 'ASYMPTOTIC PROPERTIES', 'PROGNOSTIC FACTORS', 'URN MODELS', 'VARIABILITY', 'ALLOCATION']",EFFICIENT RANDOMIZED-ADAPTIVE DESIGNS,2009
12,"Covariance matrix plays a central role in multivariate statistical analysis. Significant advances have been made recently on developing both theory and methodology for estimating large covariance matrices. However, a minimax theory has yet been developed. In this paper we establish the optimal rates of convergence for estimating the covariance matrix under both the operator norm and Frobenius norm. It is shown that optimal procedures under the two norms are different and consequently matrix estimation under the operator norm is fundamentally different from vector estimation. The minimax upper bound is obtained by constructing a special class of tapering estimators and by studying their risk properties. A key step in obtaining the optimal rate of convergence is the derivation of the minimax lower bound. The technical analysis requires new ideas that are quite different from those used in the more conventional function/sequence estimation problems.",WOS:000280359400007,ANNALS OF STATISTICS,"['SELECTION', 'SPARSITY']",OPTIMAL RATES OF CONVERGENCE FOR COVARIANCE MATRIX ESTIMATION,2010
13,"The notion of probability density for a random function is not as straightforward as in finite-dimensional cases. While a probability density function generally does not exist for functional data, we show that it is possible to develop the notion of density when functional data are considered in the space determined by the eigenfunctions of principal component analysis. This leads to a transparent and meaningful surrogate for density defined in terms of the average value of the logarithms of the densities of the distributions of principal components for a given dimension. This density approximation is estimable readily from data. It accurately represents, in a monotone way, key features of small-ball approximations to density. Our results on estimators of the densities of principal component scores are also of independent interest; they reveal interesting shape differences that have not previously been considered. The statistical implications of these results and properties are identified and discussed, and practical ramifications are illustrated in numerical work.",WOS:000275510800020,ANNALS OF STATISTICS,"['PRINCIPAL-COMPONENTS-ANALYSIS', 'NONPARAMETRIC MODEL', 'RANDOM-VARIABLES', 'SCALE-SPACE', 'TIME-SERIES', 'CURVES', 'NUMBER', 'DISCRIMINATION', 'CLASSIFICATION', 'REGRESSION']",DEFINING PROBABILITY DENSITY FOR A DISTRIBUTION OF RANDOM FUNCTIONS,2010
14,"This paper illustrates asymptotic properties for a response-adaptive design generated by a two-color, randomly reinforced urn model. The design considered is optimal in the sense that it assigns patients to the best treatment, with probability converging to one. An approach to show the joint asymptotic normality of the estimators of the mean responses to the treatments is provided in spite of the fact that allocation proportions converge to zero and one. Results on the rate of convergence of the number of patients assigned to each treatment are also obtained. Finally, we study the asymptotic behavior of a suitable test statistic.",WOS:000265500500019,ANNALS OF STATISTICS,"['BIASED-COIN DESIGNS', 'CLINICAL-TRIALS', 'ALLOCATION', 'THEOREMS', 'MODELS', 'RANDOMIZATION', 'PROPORTION']","ASYMPTOTICS IN RESPONSE-ADAPTIVE DESIGNS GENERATED BY A TWO-COLOR, RANDOMLY REINFORCED URN",2009
15,"We aim at estimating a function lambda : [0, 1] -> R, subject to the constraint that it is decreasing (or increasing). We provide a unified approach for studying the L-p-loss of an estimator defined as the slope of a concave (or convex) approximation of an estimator of a primitive of., based on n observations. Our main task is to prove that the Lp-loss is asymptotically Gaussian with explicit (though unknown) asymptotic mean and variance. We also prove that the local L-p-risk at a fixed point and the global Lp-risk are of order n(-p/3). Applying the results to the density and regression models, we recover and generalize known results about Grenander and Brunk estimators. Also, we obtain new results for the Huang-Wellner estimator of a monotone failure rate in the random censorship model, and for an estimator of the monotone intensity function of an inhomogeneous Poisson process.",WOS:000248692700007,ANNALS OF STATISTICS,"['ASYMPTOTIC NORMALITY', 'GRENANDER-ESTIMATOR', 'DENSITY']",On the L-P-error of monotonicity constrained estimators,2007
16,"The generalized estimating equations (GEE) method is popular for analyzing clustered and longitudinal data. It is important to determine a proper working correlation matrix when applying the GEE method since an improper selection sometimes results in inefficient parameter estimates. In this paper, we provide the CriteriaWorkCorr macro in SAS to calculate the criteria proposed by Pan (2001), Hin, Carey, andWang (2007), Hin andWang (2009), and Gosho, Hamada, and Yoshimura (2011) for selecting the working correlation structure when the GEE method is applied. We illustrate the implementation and an example of the macro.",WOS:000334019700001,JOURNAL OF STATISTICAL SOFTWARE,['EFFICIENCY'],Criteria to Select a Working Correlation Structure for the Generalized Estimating Equations Method in SAS,2014
17,"We present a minimax optimal solution to the problem of estimating a compact, convex set from finitely many noisy measurements of its support function. The solution is based on appropriate regularizations of the least squares estimator. Both fixed and random designs are considered.",WOS:000304684900015,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'LOG-CONCAVE DENSITY', 'LIMIT DISTRIBUTION-THEORY', 'LINE MEASUREMENTS', 'RECONSTRUCTION', 'REGRESSION', 'ENTROPY']",OPTIMAL RATES OF CONVERGENCE FOR CONVEX SET ESTIMATION FROM SUPPORT FUNCTIONS,2012
18,"Community detection is one of the fundamental problems of network analysis, for which a number of methods have been proposed. Most model-based or criteria-based methods have to solve an optimization problem over a discrete set of labels to find communities, which is computationally infeasible. Some fast spectral algorithms have been proposed for specific methods or models, but only on a case-by-case basis. Here, we propose a general approach for maximizing a function of a network adjacency matrix over discrete labels by projecting the set of labels onto a subspace approximating the leading eigenvectors of the expected adjacency matrix. This projection onto a low-dimensional space makes the feasible set of labels much smaller and the optimization problem much easier. We prove a general result about this method and show how to apply it to several previously proposed community detection criteria, establishing its consistency for label estimation in each case and demonstrating the fundamental connection between spectral properties of the network and various model-based approaches to community detection. Simulations and applications to real-world data are included to demonstrate our method performs well for multiple problems over a wide range of parameters.",WOS:000368022000013,ANNALS OF STATISTICS,"['STOCHASTIC BLOCK-MODELS', 'SOCIAL NETWORKS', 'MINKOWSKI ADDITION', 'BLOCKMODELS', 'CONSISTENCY', 'PREDICTION', 'POLYTOPES', 'GRAPHS']",OPTIMIZATION VIA LOW-RANK APPROXIMATION FOR COMMUNITY DETECTION IN NETWORKS,2016
19,"The problem of ranking/ordering instances, instead of simply classifying them, has recently gained much attention in machine learning. In this paper we formulate the ranking problem in a rigorous statistical framework. The goal is to learn a ranking rule for deciding, among two instances, which one is ""better,"" with minimum ranking risk. Since the natural estimates of the risk are of the form of a U-statistic, results of the theory of U-processes are required for investigating the consistency of empirical risk minimizers. We establish, in particular, a tail inequality for degenerate U-processes, and apply it for showing that fast rates of convergence may be achieved under specific noise assumptions, just like in classification. Convex risk minimization methods are also studied.",WOS:000254502700013,ANNALS OF STATISTICS,"['MOMENT INEQUALITIES', 'RISK MINIMIZATION', 'RANDOM-VARIABLES', 'LIMIT-THEOREMS', 'CONSISTENCY', 'BOUNDS', 'ERROR', 'CLASSIFICATION', 'CLASSIFIERS']",Ranking and empirical minimization of U-statistics,2008
20,"This paper describes an R package, rpartOrdinal, that implements alternative splitting functions for fitting a classification tree when interest lies in predicting an ordinal response. This includes the generalized Gini impurity function, which was introduced as a method for predicting an ordinal response by including costs of misclassification into the impurity function, as well as an alternative ordinal impurity function due to Piccarreta (2008) that does not require the assignment of misclassification costs. The ordered twoing splitting method, which is not defined as a decrease in node impurity, is also included in the package. Since, in the ordinal response setting, misclassifying observations to adjacent categories is a less egregious error than misclassifying observations to distant categories, this pack a geal so includes a function for estimating an ordinal measure of association, the gamma statistic.",WOS:000276954700001,JOURNAL OF STATISTICAL SOFTWARE,,rpartOrdinal: An R Package for Deriving a Classification Tree for Predicting an Ordinal Response,2010
21,,WOS:000385276100031,R JOURNAL,,Conference Report: useR! 2016,2016
22,"In spatial statistics the ability to visualize data and models superimposed with their basic social landmarks and geographic context is invaluable. ggmap is a new tool which enables such visualization by combining the spatial information of static maps from Google Maps, OpenStreetMap, Stamen Maps or CloudMade Maps with the layered grammar of graphics implementation of ggplot2. In addition, several new utility functions are introduced which allow the user to access the Google Geocoding, Distance Matrix, and Directions APIs. The result is an easy, consistent and modular framework for spatial graphics with several convenient tools for spatial data analysis.",WOS:000321944400015,R JOURNAL,,ggmap: Spatial Visualization with ggplot2,2013
23,"We present data structures and algorithms for sets and some generalizations thereof (fuzzy sets, multisets, and fuzzy multisets) available for R through the sets package. Fuzzy (multi-)sets are based on dynamically bound fuzzy logic families. Further extensions include user-definable iterators and matching functions.",WOS:000268700400001,JOURNAL OF STATISTICAL SOFTWARE,,Generalized and Customizable Sets in R,2009
24,"Kernel smoothing is one of the most widely used non-parametric data smoothing techniques. We introduce a new R package ks for multivariate kernel smoothing. Currently it contains functionality for kernel density estimation and kernel discriminant analysis. It is a comprehensive package for bandwidth matrix selection, implementing a wide range of data-driven diagonal and unconstrained bandwidth selectors.",WOS:000252428800001,JOURNAL OF STATISTICAL SOFTWARE,"['CROSS-VALIDATION', 'BANDWIDTH SELECTION', 'MATRICES', 'CHOICE']",ks: Kernel density estimation and kernel discriminant analysis for multivariate data in R,2007
25,"There has been much recent interest in Bayesian inference for generalized additive and related models. The increasing popularity of Bayesian methods for these and other model classes is mainly caused by the introduction of Markov chain Monte Carlo (MCMC) simulation techniques which allow realistic modeling of complex problems. This paper describes the capabilities of the free software package BayesX for estimating regression models with structured additive predictor based on MCMC inference. The program extends the capabilities of existing software for semiparametric regression included in S-PLUS, SAS, R or Stata. Many model classes well known from the literature are special cases of the models supported by BayesX. Examples are generalized additive ( mixed) models, dynamic models, varying coefficient models, geoadditive models, geographically weighted regression and models for space-time regression. BayesX supports the most common distributions for the response variable. For univariate responses these are Gaussian, Binomial, Poisson, Gamma, negative Binomial, zero inflated Poisson and zero inflated negative binomial. For multicategorical responses, both multinomial logit and probit models for unordered categories of the response as well as cumulative threshold models for ordered categories can be estimated. Moreover, BayesX allows the estimation of complex continuous time survival and hazard rate models.",WOS:000232927600001,JOURNAL OF STATISTICAL SOFTWARE,['BINARY'],BayesX: Analyzing Bayesian structured additive regression models,2005
26,"In many contexts such as queuing theory, spatial statistics, geostatistics and meteorology, data are observed at irregular spatial positions. One model of this situation involves considering the observation points as generated by a Poisson process. Under this assumption, we study the limit behavior of the partial sums of the marked point process {(t(i), X (t(i)))}, where X(t) is a stationary random field and the points t(i) are generated from an independent Poisson random measure N on R-d. We define the sample mean and sample variance statistics and determine their joint asymptotic behavior in a heavy-tailed setting, thus extending some finite variance results of Karr [Adv. in Appl. Probab. 18 (1986) 406-422]. New results on subsampling in the context of a marked point process are also presented, with the application of forming a confidence interval for the unknown mean under an unknown degree of heavy tails.",WOS:000247498100015,ANNALS OF STATISTICS,['INFERENCE'],Stable marked point processes,2007
27,"This paper presents a new method for spatially adaptive local (constant) likelihood estimation which applies to a broad class of nonparametric models, including the Gaussian, Poisson and binary response models. The main idea of the method is, given a sequence of local likelihood estimates (""weak"" estimates), to construct a new aggregated estimate whose pointwise risk is of order of the smallest risk among all ""weak"" estimates. We also propose a new approach toward selecting the parameters of the procedure by providing the prescribed behavior of the resulting estimate in the simple parametric situation. We establish a number of important theoretical results concerning the optimality of the aggregated estimate. In particular, our ""oracle"" result claims that its risk is, up to some logarithmic multiplier, equal to the smallest risk for the given family of estimates. The performance of the procedure is illustrated by application to the classification problem. A numerical study demonstrates its reasonable performance in simulated and real-life examples.",WOS:000251096100018,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'REGRESSION']",Spatial aggregation of local likelihood estimates with applications to classification,2007
28,"Interval censored outcomes arise when a silent event of interest is known to have occurred within a specific time period determined by the times of the last negative and first positive diagnostic tests. There is a rich literature on parametric and non-parametric approaches for the analysis of interval-censored outcomes. A commonly used strategy is to use a proportional hazards (PH) model with the baseline hazard function parameterized. The proportional hazards assumption can be relaxed in stratified models by allowing the baseline hazard function to vary across strata defined by a subset of explanatory variables. In this paper, we describe and implement a new R package straweib, for fitting a stratified Weibull model appropriate for interval censored outcomes. We illustrate the R package straweib by analyzing data from a longitudinal oral health study on the timing of the emergence of permanent teeth in 4430 children.",WOS:000343788100004,R JOURNAL,['TUTORIAL'],Stratified Weibull Regression Model for Interval-Censored Data,2014
29,"Traditional Rasch estimation of the item and student parameters via marginal maximum likelihood, joint maximum likelihood or conditional maximum likelihood, assume individuals in clustered settings are uncorrelated and items within a test that share a grouping structure are also uncorrelated. These assumptions are often violated, particularly in educational testing situations, in which students are grouped into classrooms and many test items share a common grouping structure, such as a content strand or a reading passage. Consequently, one possible approach is to explicitly recognize the clustered nature of the data and directly incorporate random effects to account for the various dependencies. This article demonstrates how the multilevel Rasch model can be estimated using the functions in R for mixed-effects models with crossed or partially crossed random effects. We demonstrate how to model the following hierarchical data structures: a) individuals clustered in similar settings ( e. g., classrooms, schools), b) items nested within a particular group ( such as a content strand or a reading passage), and c) how to estimate a teacher x content strand interaction.",WOS:000247010700001,JOURNAL OF STATISTICAL SOFTWARE,,Estimating the multilevel Rasch model: with the lme4 package,2007
30,"The estimation of kernel-smoothed relative risk functions is a useful approach to examining the spatial variation of disease risk. Though there exist several options for performing kernel density estimation in statistical software packages, there have been very few contributions to date that have focused on estimation of a relative risk function perse. Use of a variable or adaptive smoothing parameter for estimation of the individual densities has been shown to provide additional benefits in estimating relative risk and specific computational tools for this approach are essentially absent. Furthermore, little attention has been given to providing methods in available software for any kind of subsequent analysis with respect to an estimated risk function. To facilitate analyses in the field, the R package sparr is introduced, providing the ability to construct both fixed and adaptive kernel-smoothed densities and risk functions, identify statistically significant fluctuations in an estimated risk function through the use of asymptotic tolerance contours, and visualize these objects in flexible and attractive ways.",WOS:000287814900001,JOURNAL OF STATISTICAL SOFTWARE,['GEOGRAPHICAL EPIDEMIOLOGY'],sparr: Analyzing Spatial Relative Risk Using Fixed and Adaptive Kernel Density Estimation in R,2011
31,"We derive multiscale statistics for deconvolution in order to detect qualitative features of the unknown density. An important example covered within this framework is to test for local monotonicity on all scales simultaneously. We investigate the moderately ill-posed setting, where the Fourier transform of the error density in the deconvolution model is of polynomial decay. For multiscale testing, we consider a calibration, motivated by the modulus of continuity of Brownian motion. We investigate the performance of our results from both the theoretical and simulation based point of view. A major consequence of our work is that the detection of qualitative features of a density in a deconvolution problem is a doable task, although the minimax rates for pointwise estimation are very slow.",WOS:000321847600010,ANNALS OF STATISTICS,"['KERNEL DENSITY ESTIMATORS', 'NONPARAMETRIC DECONVOLUTION', 'WAVELET DECONVOLUTION', 'CONVERGENCE', 'RATES', 'REGRESSION', 'PROJECTION', 'BANDS', 'SPECT', 'MODE']",MULTISCALE METHODS FOR SHAPE CONSTRAINTS IN DECONVOLUTION: CONFIDENCE STATEMENTS FOR QUALITATIVE FEATURES,2013
32,"Finite mixtures of von Mises-Fisher distributions allow to apply model-based clustering methods to data which is of standardized length, i.e., all data points lie on the unit sphere. The R package movMF contains functionality to draw samples from finite mixtures of von Mises-Fisher distributions and to fit these models using the expectation-maximization algorithm for maximum likelihood estimation. Special features are the possibility to use sparse matrix representations for the input data, different variants of the expectation-maximization algorithm, different methods for determining the concentration parameters in the M-step and to impose constraints on the concentration parameters over the components.
In this paper we describe the main fitting function of the package and illustrate its application. In addition we compare the clustering performance of finite mixtures of von Mises-Fisher distributions to spherical k-means. We also discuss the resolution of several numerical issues which occur for estimating the concentration parameters and for determining the normalizing constant of the von Mises-Fisher distribution.",WOS:000341642400001,JOURNAL OF STATISTICAL SOFTWARE,"['MODIFIED BESSEL-FUNCTIONS', 'FUNCTION RATIOS', 'COMPUTATION', 'PARAMETER', 'SPHERE', 'SIMULATION']",movMF: An R Package for Fitting Mixtures of von Mises-Fisher Distributions,2014
33,"The survPresmooth package for R implements nonparametric presmoothed estimators of the main functions studied in survival analysis (survival, density, hazard and cumulative hazard functions). Presmoothed versions of the classical nonparametric estimators have been shown to increase efficiency if the presmoothing bandwidth is suitably chosen. The survPresmooth package provides plug-in and bootstrap bandwidth selectors, also allowing the possibility of using fixed bandwidths.",WOS:000324372200001,JOURNAL OF STATISTICAL SOFTWARE,"['RANDOM CENSORSHIP MODELS', 'KAPLAN-MEIER', 'DENSITY-FUNCTION', 'NONPARAMETRIC-INFERENCE', 'HAZARD FUNCTION', 'REPRESENTATION', 'KERNELS']",survPresmooth: An R Package for Presmoothed Estimation in Survival Analysis,2013
34,"A group-sequential clinical trial design is one in which interim analyses of the data are conducted after groups of patients are recruited. After each interim analysis, the trial may stop early if the evidence so far shows the new treatment is particularly effective or ineffective. Such designs are ethical and cost-effective, and so are of great interest in practice. An optimal group-sequential design is one which controls the type-I error rate and power at a specified level, but minimizes the expected sample size of the trial when the true treatment effect is equal to some specified value. Searching for an optimal group-sequential design is a significant computational challenge because of the high number of parameters. In this paper the R package OptGS is described. Package OptGS searches for near-optimal and balanced (i.e., one which balances more than one optimality criterion) group-sequential designs for randomized controlled trials with normally distributed outcomes. Package OptGS uses a two-parameter family of functions to determine the stopping boundaries, which improves the speed of the search process whilst still allowing flexibility in the possible shape of stopping boundaries. The resulting package allows optimal designs to be found in a matter of seconds - much faster than a previous approach.",WOS:000365976500001,JOURNAL OF STATISTICAL SOFTWARE,"['CLINICAL-TRIALS', 'CONTINUOUS OUTCOMES', 'SPENDING FUNCTIONS', 'TESTS', 'FAMILY']",OptGS: An R Package for Finding Near-Optimal Group-Sequential Designs,2015
35,"The aim of this paper is to demonstrate the R package conting for the Bayesian analysis of complete and incomplete contingency tables using hierarchical log-linear models. This package allows a user to identify interactions between categorical factors (via complete contingency tables) and to estimate closed population sizes using capture-recapture studies (via incomplete contingency tables). The models are fitted using Markov chain Monte Carlo methods. In particular, implementations of the Metropolis-Hastings and reversible jump algorithms appropriate for log-linear models are employed. The conting package is demonstrated on four real examples.",WOS:000341584600001,JOURNAL OF STATISTICAL SOFTWARE,"['CAPTURE-RECAPTURE METHODS', 'MODEL DETERMINATION', 'PRIOR INFORMATION', 'LINEAR-MODELS', 'SCOTLAND', 'DISTRIBUTIONS', 'POPULATION', 'INJECTORS', 'NUMBER', 'SIZE']",conting: An R Package for Bayesian Analysis of Complete and Incomplete Contingency Tables,2014
36,"We propose a new technique, called wild binary segmentation (WBS), for consistent estimation of the number and locations of multiple change-points in data. We assume that the number of change-points can increase to infinity with the sample size. Due to a certain random localisation mechanism, WBS works even for very short spacings between the change-points and/or very small jump magnitudes, unlike standard binary segmentation. On the other hand, despite its use of localisation, WBS does not require the choice of a window or span parameter, and does not lead to a significant increase in computational complexity. WBS is also easy to code. We propose two stopping criteria for WBS: one based on thresholding and the other based on what we term the 'strengthened Schwarz information criterion'. We provide default recommended values of the parameters of the procedure and show that it offers very good practical performance in comparison with the state of the art. The 'WBS methodology is implemented in the R package wbs, available on CRAN.
In addition, we provide a new proof of consistency of binary segmentation with improved rates of convergence, as well as a corresponding result for WBS.",WOS:000345884900004,ANNALS OF STATISTICS,"['LEAST-SQUARES ESTIMATION', 'TIME-SERIES', 'FUSED LASSO', 'NUMBER', 'CRITERION', 'SELECTION', 'SEQUENCE', 'INFORMATION', 'ALGORITHMS', 'REGRESSION']",WILD BINARY SEGMENTATION FOR MULTIPLE CHANGE-POINT DETECTION,2014
37,"The probability of false discovery proportion (FDP) exceeding gamma is an element of [0, 1), defined as gamma-FDP, has received much attention as a measure of false discoveries in multiple testing. Although this measure has received acceptance due to its relevance under dependency, not much progress has been made yet advancing its theory under such dependency in a nonasymptotic setting, which motivates our research in this article. We provide a larger class of procedures containing the stepup analog of, and hence more powerful than, the stepdown procedure in Lehmann and Romano [Ann. Statist. 33 (2005) 1138-1154] controlling the gamma-FDP under similar positive dependence condition assumed in that paper. We offer better alternatives of the stepdown and stepup procedures in Romano and Shaikh [IMS Lecture Notes Monogr: Ser. 49 (2006a) 33-50, Ann. Statist. 34 (2006b) 1850-1873] using pairwise joint distributions of the null p-values. We generalize the notion of gamma-FDP making it appropriate in situations where one is willing to tolerate a few false rejections or, due to high dependency, some false rejections are inevitable, and provide methods that control this generalized gamma-FDP in two different scenarios: (i) only the marginal p-values are available and (ii) the marginal p-values as well as the common pairwise joint distributions of the null p-values are available, and assuming both positive dependence and arbitrary dependence conditions on the p-values in each scenario. Our theoretical findings are being supported through numerical studies.",WOS:000338477800009,ANNALS OF STATISTICS,"['FAMILYWISE ERROR RATE', 'STEPUP PROCEDURES', 'NUMBER', 'FDR', 'DISTRIBUTIONS', 'INDEPENDENCE', 'INEQUALITIES', 'DEPENDENCE']",FURTHER RESULTS ON CONTROLLING THE FALSE DISCOVERY PROPORTION,2014
38,"The generalized likelihood ratio (GLR) test proposed by Fan, Zhang and Zhang [Ann. Statist. 29 (2001) 153-193] and Fan and Yao [Nonlinear Time Series: Nonparametric and Parametric Methods (2003) Springer] is a generally applicable nonparametric inference procedure. In this paper, we show that although it inherits many advantages of the parametric maximum likelihood ratio (LR) test, the GLR test does not have the optimal power property. We propose a generally applicable test based on loss functions, which measure discrepancies between the null and nonparametric alternative models and are more relevant to decision-making under uncertainty. The new test is asymptotically more powerful than the GLR test in terms of Pitman's efficiency criterion. This efficiency gain holds no matter what smoothing parameter and kernel function are used and even when the true likelihood function is available for the GLR test.",WOS:000321847600005,ANNALS OF STATISTICS,"['LIKELIHOOD RATIO TESTS', 'NONLINEAR TIME-SERIES', 'NONPARAMETRIC REGRESSION', 'CONDITIONAL HETEROSCEDASTICITY', 'BANDWIDTH SELECTION', 'MAXIMUM-LIKELIHOOD', 'ASYMMETRIC LOSS', 'INFERENCES', 'PREDICTION', 'VARIANCE']",A LOSS FUNCTION APPROACH TO MODEL SPECIFICATION TESTING AND ITS RELATIVE EFFICIENCY,2013
39,"R (R Core Team 2014) provides a powerful and flexible system for statistical computations. It has a default-install set of functionality that can be expanded by the use of several thousand add-in packages as well as user-written scripts. While R is itself a programming language, it has proven relatively easy to incorporate programs in other languages, particularly Fortran and C. Success, however, can lead to its own costs:
Users face a confusion of choice when trying to select packages in approaching a problem.
A need to maintain workable examples using early methods may mean some tools offered as a default may be dated.
In an open-source project like R, how to decide what tools offer ""best practice"" choices, and how to implement such a policy, present a serious challenge.
We discuss these issues with reference to the tools in R for nonlinear parameter estimation (NLPE) and optimization, though for the present article 'optimization' will be limited to function minimization of essentially smooth functions with at most bounds constraints on the parameters. We will abbreviate this class of problems as NLPE. We believe that the concepts proposed are transferable to other classes of problems seen by R users.",WOS:000345288500001,JOURNAL OF STATISTICAL SOFTWARE,['ALGORITHMS'],On Best Practice Optimization Methods in R,2014
40,"Exponential-family random graph models are probabilistic network models that are parametrized by sufficient statistics based on structural (i.e., graph-theoretic) properties. The ergm package for the R statistical computing environment is a collection of tools for the analysis of network data within an exponential-family random graph model framework. Many different network properties can be employed as sufficient statistics for exponentialfamily random graph models by using the model terms defined in the ergm package; this functionality can be expanded by the creation of packages that code for additional network statistics. Here, our focus is on the addition of statistics based on graphlets. Graphlets are classes of small, connected, induced subgraphs that can be used to describe the topological structure of a network. We introduce an R package called ergm.graphlets that enables the use of graphlet properties of a network within the ergm package of R. The ergm.graphlets package provides a complete list of model terms that allows to incorporate statistics of any 2-, 3-, 4- and 5-node graphlets into exponential-family random graph models. The new model terms of the ergm.graphlets package enable both exponential-family random graph modeling of global structural properties and investigation of relationships between node attributes (i.e., covariates) and local topologies around nodes.",WOS:000365975400001,JOURNAL OF STATISTICAL SOFTWARE,"['EXPONENTIAL-FAMILY', 'SOCIAL NETWORKS', 'LOGISTIC REGRESSIONS', 'MARKOV GRAPHS', 'LOGIT-MODELS', 'INFERENCE']",ergm.graphlets: A Package for ERG Modeling Based on Graphlet Statistics,2015
41,"Large-scale data are often characterized by some degree of inhomogeneity as data are either recorded in different time regimes or taken from multiple sources. We look at regression models and the effect of randomly changing coefficients, where the change is either smoothly in time or some other dimension or even without any such structure. Fitting varying-coefficient models or mixture models can be appropriate solutions but are computationally very demanding and often return more information than necessary. If we just ask for a model estimator that shows good predictive properties for all regimes of the data, then we are aiming for a simple linear model that is reliable for all possible subsets of the data. We propose the concept of ""maximin effects"" and a suitable estimator and look at its prediction accuracy from a theoretical point of view in a mixture model with known or unknown group structure. Under certain circumstances the estimator can be computed orders of magnitudes faster than standard penalized regression estimators, making computations on large-scale data feasible. Empirical examples complement the novel methodology and theory.",WOS:000357441000019,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'FINITE MIXTURE-MODELS', 'REGRESSION', 'SELECTION', 'REGRET', 'LASSO']",MAXIMIN EFFECTS IN INHOMOGENEOUS LARGE-SCALE DATA,2015
42,"We introduce a shiny web application to facilitate the construction of Item Factor Analysis (a.k.a. Item Response Theory) models using the OpenMx package. The web application assists with importing data, outcome recoding, and model specification. However, the app does not conduct any analysis but, rather, generates an analysis script. Generated Rmarkdown output serves dual purposes: to analyze a data set and demonstrate good programming practices. The app can be used as a teaching tool or as a starting point for custom analysis scripts.",WOS:000385276100014,R JOURNAL,"['RESPONSE THEORY MODEL', 'GOODNESS-OF-FIT', 'EM ALGORITHM', 'PERFORMANCE', 'DEPENDENCE']",Model Builder for Item Factor Analysis with OpenMx,2016
43,"In recent years, composite models based on the lognormal distribution have become popular in actuarial sciences and related areas. In this short note, we present a new R package for computing the probability density function, cumulative density function, and quantile function, and for generating random numbers of any composite model based on the lognormal distribution. The use of the package is illustrated using a real data set.",WOS:000330193300011,R JOURNAL,,CompLognormal: An R Package for Composite Lognormal Distributions,2013
44,"This paper considers the efficient estimation of copula-based semiparametric strictly stationary Markov models. These models are characterized by nonparametric invariant (one-dimensional marginal) distributions and parametric bivariate copula functions where the copulas capture temporal dependence and tail dependence of the processes. The Markov processes generated via tail dependent copulas may look highly persistent and are useful for financial and economic applications. We first show that Markov processes generated via Clayton, Gumbel and Student's t copulas and their Survival copulas are all geometrically ergodic. We then propose a sieve maximum likelihood estimation (MLE) for the copula parameter, the invariant distribution and the conditional quantiles. We show that the sieve MLEs of any smooth functional is root-n consistent, asymptotically normal and efficient and that their sieve likelihood ratio statistics are asymptotically chi-square distributed. Monte Carlo studies indicate that, even for Markov models generated via tail dependent copulas and fat-tailed marginals, our sieve MLEs perform very well.",WOS:000271673700018,ANNALS OF STATISTICS,"['TIME-SERIES', 'LIKELIHOOD', 'INFERENCE', 'DEPENDENCE', 'DISTRIBUTIONS']",EFFICIENT ESTIMATION OF COPULA-BASED SEMIPARAMETRIC MARKOV MODELS,2009
45,"This paper introduces the R package lavaan. survey, a user-friendly interface to designbased complex survey analysis of structural equation models (SEMs). By leveraging existing code in the lavaan and survey packages, the lavaan. survey package allows for SEM analyses of stratified, clustered, and weighted data, as well as multiply imputed complex survey data. lavaan. survey provides several features such as SEMs with replicate weights, a variety of resampling techniques for complex samples, and finite population corrections, features that should prove useful for SEM practitioners faced with the common situation of a sample that is not iid.",WOS:000334019900001,JOURNAL OF STATISTICAL SOFTWARE,"['COVARIANCE STRUCTURE-ANALYSIS', 'TEST STATISTICS', 'SAMPLE WEIGHTS', 'MULTIPLE-IMPUTATION', 'VARIANCE-ESTIMATION', 'INTEGRATION', 'LEVEL']",lavaan.survey: An R Package for Complex Survey Analysis of Structural Equation Models,2014
46,"Statistical tolerance intervals are used for a broad range of applications, such as quality control, engineering design tests, environmental monitoring, and bioequivalence testing. tolerance is the only R package devoted to procedures for tolerance intervals and regions. Perhaps the most commonly-employed functions of the package involve normal tolerance intervals. A number of new procedures for this setting have been included in recent versions of tolerance. In this paper, we discuss and illustrate the functions that implement these normal tolerance interval procedures, one of which is a new, novel type of operating characteristic curve.",WOS:000395669800013,R JOURNAL,['LIMITS'],Normal Tolerance Interval Procedures in the tolerance Package,2016
47,"Two important recent advances in areal modeling are the centered autologistic model and the sparse spatial generalized linear mixed model (SGLMM), both of which are reparameterizations of traditional models. The reparameterizations improve regression inference by alleviating spatial confounding, and the sparse SGLMM also greatly speeds computing by reducing the dimension of the spatial random effects. Package ngspatial ('ng' = non-Gaussian) provides routines for fitting these new models. The package supports composite likelihood and Bayesian inference for the centered autologistic model, and Bayesian inference for the sparse SGLMM.",WOS:000348651700008,R JOURNAL,['CHAIN MONTE-CARLO'],ngspatial: A Package for Fitting the Centered Autologistic and Sparse Spatial Generalized Linear Mixed Models for Areal Data,2014
48,,WOS:000312899000004,ANNALS OF STATISTICS,"['COVARIANCE ESTIMATION', 'MATRIX DECOMPOSITION', 'LASSO']",DISCUSSION: LATENT VARIABLE GRAPHICAL MODEL SELECTION VIA CONVEX OPTIMIZATION,2012
49,"Response-adaptive randomization designs are becoming increasingly popular in clinical trial practice. In this paper, we present RARtool, a user interface software developed in MATLAB for designing response-adaptive randomized comparative clinical trials with censored time-to-event outcomes. The RARtool software can compute different types of optimal treatment allocation designs, and it can simulate response-adaptive randomization procedures targeting selected optimal allocations. Through simulations, an investigator can assess design characteristics under a variety of experimental scenarios and select the best procedure for practical implementation. We illustrate the utility of our RARtool software by redesigning a survival trial from the literature.",WOS:000365976300001,JOURNAL OF STATISTICAL SOFTWARE,"['BIASED COIN DESIGNS', 'SURVIVAL TRIALS', 'PHASE-III', 'UTILITY', 'CANCER']",RARtool: A MATLAB Software Package for Designing Response-Adaptive Randomized Clinical Trials with Time-to-Event Outcomes,2015
50,"We consider the problem of approximating a given element f from a Hilbert space H by means of greedy algorithms and the application of such procedures to the regression problem in statistical learning theory. We improve on the existing theory of convergence rates for both the orthogonal greedy algorithm and the relaxed greedy algorithm, as well as for the forward stepwise projection algorithm. For all these algorithms, we prove convergence results for a variety of function classes and not simply those that are related to the convex hull of the dictionary. We then show how these bounds for convergence rates lead to a new theory for the performance of greedy algorithms in learning. In particular, we build upon the results in [IEEE Trans. Inform. Theory 42 (1996) 2118-2132] to construct learning algorithms based on greedy approximations which are universally consistent and provide provable convergence rates for large classes of functions. The use of greedy algorithms in the context of learning is very appealing since it greatly reduces the computational burden when compared with standard model selection using general dictionaries.",WOS:000253390000003,ANNALS OF STATISTICS,"['NEURAL-NETWORKS', 'REGRESSION']",Approximation and learning by greedy algorithms,2008
51,"In this paper, we propose a covariate-adjusted nonlinear regression model. In this model, both the response and predictors can only be observed after being distorted by some multiplicative factors. Because of nonlinearity, existing methods for the linear setting cannot be directly employed. To attack this problem, we propose estimating the distorting functions by nonparametrically regressing the predictors and response on the distorting covariate; then, nonlinear least squares estimators for the parameters are obtained using the estimated response and predictors. Root n-consistency and asymptotic normality are established. However, the limiting variance has a very complex structure with several unknown components, and confidence regions based on normal approximation are not efficient. Empirical likelihood-based confidence regions are proposed, and their accuracy is also verified due to its self-scale invariance. Furthermore, unlike the common results derived from the profile methods, even when plug-in estimates are used for the infinite-dimensional nuisance parameters (distorting functions), the limit of empirical likelihood ratio is still chi-squared distributed. This property cases the construction of the empirical likelihood-based confidence regions. A simulation study is carried out to assess the finite sample performance of the proposed estimators and confidence regions. We apply our method to study the relationship between glomerular filtration rate and serum creatinine.",WOS:000268113500008,ANNALS OF STATISTICS,"['DIETARY-PROTEIN RESTRICTION', 'GLOMERULAR-FILTRATION-RATE', 'EMPIRICAL LIKELIHOOD', 'CONFIDENCE-REGIONS', 'LONGITUDINAL DATA', 'LINEAR-MODELS', 'EQUATION', 'DISEASE']",COVARIATE-ADJUSTED NONLINEAR REGRESSION,2009
52,"We propose a roughness regularization approach in making nonparametric inference for generalized functional linear models. In a reproducing kernel Hilbert space framework, we construct asymptotically valid confidence intervals for regression mean, prediction intervals for future response and various statistical procedures for hypothesis testing. In particular, one procedure for testing global behaviors of the slope function is adaptive to the smoothness of the slope function and to the structure of the predictors. As a by-product, a new type of Wilks phenomenon [Ann. Math. Stat. 9 (1938) 60-62; Ann. Statist. 29 (2001) 153-193] is discovered when testing the functional linear models. Despite the generality, our inference procedures are easy to implement. Numerical examples are provided to demonstrate the empirical advantages over the competing methods. A collection of technical tools such as integro-differential equation techniques [Trans. Amer Math. Soc. (1927) 29 755-800; Trans. Amer. Math. Soc. (1928) 30 453-471; Trans. Amer Math. Soc. (1930) 32 860-868], Stein's method [Ann. Statist. 41 (2013) 2786- 2819] [Stein, Approximate Computation of Expectations (1986) IMS] and functional Bahadur representation [Ann. Statist. 41 (2013) 2608-2638] are employed in this paper.",WOS:000357441000017,ANNALS OF STATISTICS,"['INTEGRODIFFERENTIAL EQUATIONS', 'EXPONENTIAL-FAMILIES', 'REGRESSION PROBLEMS', 'CORRELATED ERRORS', 'LONGITUDINAL DATA', 'LIKELIHOOD RATIO', 'GREENS-FUNCTION', 'CONVERGENCE', 'PREDICTION', 'MINIMAX']",NONPARAMETRIC INFERENCE IN GENERALIZED FUNCTIONAL LINEAR MODELS,2015
53,"Approximate Bayesian computation (ABC) is a popular family of algorithms which perform approximate parameter inference when numerical evaluation of the likelihood function is not possible but data can be simulated from the model. They return a sample of parameter values which produce simulations close to the observed dataset. A standard approach is to reduce the simulated and observed datasets to vectors of summary statistics and accept when the difference between these is below a specified threshold. ABC can also be adapted to perform model choice.
In this article, we present a new software package for R, abctools which provides methods for tuning ABC algorithms. This includes recent dimension reduction algorithms to tune the choice of summary statistics, and coverage methods to tune the choice of threshold. We provide several illustrations of these routines on applications taken from the ABC literature.",WOS:000368551800015,R JOURNAL,"['LIKELIHOOD-FREE INFERENCE', 'CHAIN MONTE-CARLO', 'POPULATION HISTORY', 'MICROSATELLITE DATA', 'DNA-SEQUENCE', 'MODEL', 'SOFTWARE', 'DISTRIBUTIONS', 'TUBERCULOSIS', 'STATISTICS']",abctools: An R Package for Tuning Approximate Bayesian Computation Analyses,2015
54,"Clinical trials are complex and usually involve multiple objectives such as controlling type I error rate, increasing power to detect treatment difference, assigning more patients to better treatment, and more. In literature, both response-adaptive randomization (RAR) procedures (by changing randomization procedure sequentially) and sequential monitoring (by changing analysis procedure sequentially) have been proposed to achieve these objectives to some degree. In this paper, we propose to sequentially monitor response-adaptive randomized clinical trial and study it's properties. We prove that the sequential test statistics of the new procedure converge to a Brownian motion in distribution. Further, we show that the sequential test statistics asymptotically satisfy the canonical joint distribution defined in Jennison and Turnbull (2000). Therefore, type I error and other objectives can be achieved theoretically by selecting appropriate boundaries. These results open a door to sequentially monitor response-adaptive randomized clinical trials in practice. We can also observe from the simulation studies that, the proposed procedure brings together the advantages of both techniques, in dealing with power, total sample size and total failure numbers, while keeps the type I error. In addition, we illustrate the characteristics of the proposed procedure by redesigning a well-known clinical trial of maternal-infant HIV transmission.",WOS:000280359400010,ANNALS OF STATISTICS,"['BIASED COIN DESIGNS', 'PLAY-WINNER RULE', 'LIMIT-THEOREMS', 'URN MODELS', 'VARIABILITY']",SEQUENTIAL MONITORING OF RESPONSE-ADAPTIVE RANDOMIZED CLINICAL TRIALS,2010
55,"We consider estimating an unknown signal, both blocky and sparse, which is corrupted by additive noise. We study three interrelated least squares procedures and their asymptotic properties. The first procedure is the fused lasso, put forward by Friedman et al. [Ann. Appl. Statist. 1 (2007) 302-332], which we modify into a different estimator, called the fused adaptive lasso, with better properties. The other two estimators we discuss solve least squares problems on sieves; one constrains the maximal e I norm and the maximal total variation seminorm, and the other restricts the number of blocks and the number of nonzero coordinates of the signal. We derive conditions for the recovery of the true block partition and the true sparsity patterns by the fused lasso and the fused adaptive lasso, and we derive convergence rates for the sieve estimators, explicitly in terms of the constraining parameters.",WOS:000268605000012,ANNALS OF STATISTICS,"['LOCAL EXTREMES', 'MULTIRESOLUTION', 'OPTIMIZATION', 'CONVERGENCE', 'SELECTION', 'STRINGS', 'RUNS']",PROPERTIES AND REFINEMENTS OF THE FUSED LASSO,2009
56,"LazySorted is a Python C extension implementing a partially and lazily sorted list data structure. It solves a common problem faced by programmers, in which they need just part of a sorted list, like its middle element (the median), but sort the entire list to get it. LazySorted presents them with the abstraction that they are working with a fully sorted list, while actually only sorting the list partially with quicksort partitions to return the requested sub-elements. This enables programmers to use naive ""sort first"" algorithms but nonetheless attain linear run-times when possible. LazySorted may serve as a drop-in replacement for the built-in sorted function in most cases, and can sometimes achieve run-times more than 7 times faster.",WOS:000365975500001,JOURNAL OF STATISTICAL SOFTWARE,['QUICKSORT'],"LazySorted: A Lazily, Partially Sorted Python List",2015
57,"Given an m-dimensional compact submanifold M of Euclidean space R-s, the concept of mean location of a distribution, related to mean or expected vector, is generalized to more general R-s-valued functionals including median location, which is derived from the spatial median. The asymptotic statistical inference for general functionals of distributions on such submanifolds is elaborated. Convergence properties are studied in relation to the behavior of the underlying distributions with respect to the cutlocus. An application is given in the context of independent, but not identically distributed, samples, in particular, to a multisample setup.",WOS:000247498100006,ANNALS OF STATISTICS,"['SAMPLE-MEAN LOCATION', 'BEHAVIOR']",Asymptotic data analysis on manifolds,2007
58,"We study the conditional distribution of low-dimensional projections from high-dimensional data, where the conditioning is on other low-dimensional projections. To fix ideas, consider a random d-vector Z that has a Lebesgue density and that is standardized so that EZ = 0 and EZZ' = I-d. Moreover, consider two projections defined by unit-vectors alpha and beta, namely a response y = alpha'Z and an explanatory variable x = beta'Z. It has long been known that the conditional mean of y given x is approximately linear in x, under some regularity conditions; cf. Hall and Li [Ann. Statist. 21 (1993) 867-889]. However, a corresponding result for the conditional variance has not been available so far. We here show that the conditional variance of y given x is approximately constant in x (again, under some regularity conditions). These results hold uniformly in alpha and for most beta's, provided only that the dimension of Z is large. In that sense, we see that most linear submodels of a high-dimensional overall model are approximately correct. Our findings provide new insights in a variety of modeling scenarios. We discuss several examples, including sliced inverse regression, sliced average variance estimation, generalized linear models under potential link violation, and sparse linear modeling.",WOS:000320488200003,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'REDUCTION', 'PREDICTORS']",ON THE CONDITIONAL DISTRIBUTIONS OF LOW-DIMENSIONAL PROJECTIONS FROM HIGH-DIMENSIONAL DATA,2013
59,"flexsurv is an R package for fully-parametric modeling of survival data. Any parametric time-to-event distribution may be fitted if the user supplies a probability density or hazard function, and ideally also their cumulative versions. Standard survival distributions are built in, including the three and four-parameter generalized gamma and F distributions. Any parameter of any distribution can be modeled as a linear or log-linear function of covariates. The package also includes the spline model of Royston and Parmar (2002), in which both baseline survival and covariate effects can be arbitrarily flexible parametric functions of time. The main model-fitting function, flexsurvreg, uses the familiar syntax of survreg from the standard survival package (Therneau 2016). Censoring or left-truncation are specified in 'Surv' objects. The models are fitted by maximizing the full log-likelihood, and estimates and confidence intervals for any function of the model parameters can be printed or plotted. flexsurv also provides functions for fitting and predicting from fully-parametric multi-state models, and connects with the mstate package (de Wreede, Fiocco, and Putter 2011). This article explains the methods and design principles of the package, giving several worked examples of its use.",WOS:000384912000001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED ADDITIVE-MODELS', 'MULTISTATE MODELS', 'FRACTIONAL POLYNOMIALS', 'PROPORTIONAL-HAZARDS', 'COMPETING RISKS', 'PANEL-DATA', 'PACKAGE', 'EXTRAPOLATION', 'PREDICTORS', 'REGRESSION']",flexsurv: A Platform for Parametric Survival Modeling in R,2016
60,"Consider the problem of testing s hypotheses simultaneously. The usual approach restricts attention to procedures that control the probability of even one false rejection, the familywise error rate (FWER). If s is large, one might be willing to tolerate more than one false rejection, thereby increasing the ability of the procedure to correctly reject false null hypotheses. One possibility is to replace control of the FWER by control of the probability of k or more false rejections, which is called the k-FWER. We derive both single-step and step-down procedures that control the k-FWER in finite samples or asymptotically, depending on the situation. We also consider the false discovery proportion (FDP) defined as the number of false rejections divided by the total number of rejections (and defined to be 0 if there are no rejections). The false discovery rate proposed by Benjamini and Hochberg [J Roy. Statist. Soc. Ser B 57 (1995) 289-300] controls E(FDP). Here, the goal is to construct methods which satisfy, for a given gamma and alpha, P{FDP > gamma} <= alpha, at least asymptotically. In contrast to the proposals of Lehmann and Romano [Ann. Statist. 33 (2005) 1138-1154], we construct methods that implicitly take into account the dependence structure of the individual test statistics in order to further increase the ability to detect false null hypotheses. This feature is also shared by related work of van der Laan, Dudoit and Pollard [Stat. Appl. Genet. Mol. Biol. 3 (2004) article 15], but our methodology is quite different. Like the work of Pollard and van der Laan [Proc. 2003 International Multi-Conference in Computer Science and Engineering, METMBS'03 Conference (2003) 3-9] and Dudoit, van der Laan and Pollard [Stat. Appl. Genet. Mol. Biol. 3 (2004) article 13], we employ resampling methods to achieve our goals. Some simulations compare finite sample performance to currently available methods.",WOS:000249568000002,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'STEPDOWN']",Control of generalized error rates in multiple testing,2007
61,"Using the asymptotical minimax framework, we examine convergence rates equivalency between a continuous functional deconvolution model and its real-life discrete counterpart over a wide range of Besov balls and for the L-2-risk. For this purpose, all possible models are divided into three groups. For the models in the first group, which we call uniform, the convergence rates in the discrete and the continuous models coincide no matter what the sampling scheme is chosen, and hence the replacement of the discrete model by its continuous counterpart is legitimate. For the models in the second group, to which we refer as regular, one can point out the best sampling strategy in the discrete model, but not every sampling scheme leads to the same convergence rates; there are at least two sampling schemes which deliver different convergence rates in the discrete model (i.e., at least one of the discrete models leads to convergence rates that are different from the convergence rates in the continuous model). The third group consists of models for which, in general, it is impossible to devise the best sampling strategy; we call these models irregular.
We formulate the conditions when each of these situations takes place. In the regular case, we not only point out the number and the selection of sampling points which deliver the fastest convergence rates in the discrete model but also investigate when, in the case of an arbitrary sampling scheme, the convergence rates in the continuous model coincide or do not coincide with the convergence rates in the discrete model. We also study what happens if one chooses a uniform, or a more general pseudo-uniform, sampling scheme which can be viewed as an intuitive replacement of the continuous model. Finally, as a representative of the irregular case, we study functional deconvolution with a boxcar-like blurring function since this model has a number of important applications. All theoretical results presented in the paper are illustrated by numerous examples; many of which are motivated directly by a multitude of inverse problems in mathematical physics where one needs to recover initial or boundary conditions on the basis of observations from a noisy solution of a partial differential equation. The theoretical performance of the suggested estimator in the multichannel deconvolution model with a boxcar-like blurring function is also supplemented by a limited simulation study and compared to an estimator available in the current literature. The paper concludes that in both regular and irregular cases one should be extremely careful when replacing a discrete functional deconvolution model by its continuous counterpart.",WOS:000277471000018,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'ASYMPTOTIC EQUIVALENCE', 'WAVELET DECONVOLUTION', 'INVERSE PROBLEMS', 'RANDOM DESIGN', 'DECOMPOSITION', 'SYSTEMS', 'RISK']",ON CONVERGENCE RATES EQUIVALENCY AND SAMPLING STRATEGIES IN FUNCTIONAL DECONVOLUTION MODELS,2010
62,"This paper describes treeClust, an R package that produces dissimilarities useful for clustering. These dissimilarities arise from a set of classification or regression trees, one with each variable in the data acting in turn as a the response, and all others as predictors. This use of trees produces dissimilarities that are insensitive to scaling, benefit from automatic variable selection, and appear to perform well. The software allows a number of options to be set, affecting the set of objects returned in the call; the user can also specify a clustering algorithm and, optionally, return only the clustering vector. The package can also generate a numeric data set whose inter-point distances relate to the treeClust ones; such a numeric data set can be much smaller than the vector of inter-point dissimilarities, a useful feature in big data sets.",WOS:000368551800017,R JOURNAL,,treeClust: An R Package for Tree-Based Clustering Dissimilarities,2015
63,"We describe an add-on package for the language and environment R which allows simultaneous fitting of several non-linear regression models. The focus is on analysis of dose response curves, but the functionality is applicable to arbitrary non-linear regression models. Features of the package is illustrated in examples.",WOS:000232806700001,JOURNAL OF STATISTICAL SOFTWARE,"['DOSE RESPONSES', 'HORMESIS', 'TOXICOLOGY']",Bioassay analysis using R,2005
64,"We consider repeated measurement designs when a residual or carry-over effect may be present in at most one later period. Since assuming an additive model may be unrealistic for some applications and leads to biased estimation of treatment effects, we consider a model with interactions between carryover and direct treatment effects. When the aim of the experiment is to study the effects of a treatment used alone, we obtain universally optimal approximate designs. We also propose some efficient designs with a reduced number of subjects.",WOS:000345884900005,ANNALS OF STATISTICS,['SELF'],OPTIMAL CROSS-OVER DESIGNS FOR FULL INTERACTION MODELS,2014
65,"We develop inference procedures for longitudinal data where some of the measurements are censored by fixed constants. We consider a semi-parametric quantile regression model that makes no distributional assumptions. Our research is motivated by the lack of proper inference procedures for data from biomedical studies where measurements are censored due to a fixed quantification limit. In such studies the focus is often on testing hypotheses about treatment equality. To this end, we propose a rank score test for large sample inference on a subset of the covariates. We demonstrate the importance of accounting for both censoring and intra-subject dependency and evaluate the performance of our proposed methodology in a simulation study. We then apply the proposed inference procedures to data from an AIDS-related clinical trial. We conclude that our framework and proposed methodology is very valuable for differentiating the influences of predictors at different locations in the conditional distribution of a response variable.",WOS:000265500500008,ANNALS OF STATISTICS,"['MIXED-EFFECTS MODELS', 'MEDIAN REGRESSION', 'HIV-RNA', 'LINEAR-REGRESSION', 'DETECTION LIMITS', 'TIME', 'THERAPY', 'SUBJECT', 'FAILURE']",INFERENCE FOR CENSORED QUANTILE REGRESSION MODELS IN LONGITUDINAL STUDIES,2009
66,"The paper proposes one-to-one transformation of the vector of components {Y-in}(i=1)(m) of Pearson's chi-square statistic,
Y-in = nu(in)-npi/root np(i,) i = l, ... , m,
into another vector {Z(in)}(i=1)(m), which, therefore, contains the same ""statistical information,"" but is asymptotically distribution free. Hence any functional/test statistic based on {Z(in)}(i=1)(m) is also asymptotically distribution free. Natural examples of such test statistics are traditional goodness-of-fit statistics from partial sums Sigma(l <= k) Z(in).
The supplement shows how the approach works in the problem of independent interest: the goodness-of-fit testing of power-law distribution with the Zipf law and the Karlin-Rouault law as particular alternatives.",WOS:000330204900010,ANNALS OF STATISTICS,['MARTINGALES'],NOTE ON DISTRIBUTION FREE TESTING FOR DISCRETE DISTRIBUTIONS,2013
67,"We consider high-dimensional sparse regression problems in which we observe y = X beta + z, where X is an n x p design matrix and z is an n dimensional vector of independent Gaussian errors, each with variance sigma(2). Our focus is on the recently introduced SLOPE estimator [Ann. Appi. Stat. 9 (2015) 1103-1140], which regularizes the least-squares estimates with the rank-dependent penalty Sigma(1 <= i <= p) lambda(i)vertical bar(beta) over cap vertical bar((i)), where vertical bar(beta) over cap vertical bar((i)) is the ith largest magnitude of the fitted coefficients. Under Gaussian designs, where the entries of X are i.i.d. N(0, 1/n), we show that SLOPE, with weights lambda(i) just about equal to sigma center dot Phi(-1) (1 - iq/(2p)) [Phi(-1) (alpha) is the alpha th quantile of a standard normal and q is a fixed number in (0, 1)] achieves a squared error of estimation obeying
sup P(vertical bar vertical bar(beta) over cap (SLOPE) - beta vertical bar vertical bar(2) > (1 + epsilon)2 sigma(2)k log(p/k)) -> 0 vertical bar vertical bar beta vertical bar vertical bar(0)<= k
as the dimension p increases to infinity, and where s > 0 is an arbitrary small constant. This holds under a weak assumption on the l(0)-sparsity level, namely, k/p -> 0 and (k log p)/n -> 0, and is sharp in the sense that this is the best possible error any estimator can achieve. A remarkable feature is that SLOPE does not require any knowledge of the degree of sparsity, and yet automatically adapts to yield optimal total squared errors over a wide range of l(0)-sparsity classes. We are not aware of any other estimator with this property.",WOS:000375175200006,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'MODEL SELECTION', 'VARIABLE SELECTION', 'DANTZIG SELECTOR', 'REGRESSION SHRINKAGE', 'INFLATION CRITERION', 'GAUSSIAN REGRESSION', 'WAVELET SHRINKAGE', 'ORACLE PROPERTIES', 'RANDOM DESIGN']",SLOPE IS ADAPTIVE TO UNKNOWN SPARSITY AND ASYMPTOTICALLY MINIMAX,2016
68,"This paper uses several examples to show how the econometrics program RATS can be used to analyze state space models. It demonstrates Kalman filtering and smoothing, estimation of hyperparameters, unconditional and conditional simulation. It also provides a more complicated example where a dynamic simultaneous equations model is transformed into a proper state space representation and its unknown parameters are estimated.",WOS:000290527600001,JOURNAL OF STATISTICAL SOFTWARE,['MODELS'],State Space Methods in RATS,2011
69,"This paper considers estimation of sparse covariance matrices and establishes the optimal rate of convergence under a range of matrix operator norm and Bregman divergence losses. A major focus is on the derivation of a rate sharp minimax lower bound. The problem exhibits new features that are significantly different from those that occur in the conventional nonparametric function estimation problems. Standard techniques fail to yield good results, and new tools are thus needed.
We first develop a lower bound technique that is particularly well suited for treating ""two-directional"" problems such as estimating sparse covariance matrices. The result can be viewed as a generalization of Le Cam's method in one direction and Assouad's Lemma in another. This lower bound technique is of independent interest and can be used for other matrix estimation problems.
We then establish a rate sharp minimax lower bound for estimating sparse covariance matrices under the spectral norm by applying the general lower bound technique. A thresholding estimator is shown to attain the optimal rate of convergence under the spectral norm. The results are then extended to the general matrix l(w) operator norms for 1 <= w <= infinity. In addition, we give a unified result on the minimax rate of convergence for sparse covariance matrix estimation under a class of Bregman divergence losses.",WOS:000321844300002,ANNALS OF STATISTICS,['DIVERGENCES'],OPTIMAL RATES OF CONVERGENCE FOR SPARSE COVARIANCE MATRIX ESTIMATION,2012
70,"Model formulas are the standard approach for specifying the variables in statistical models in the S language. Although being eminently useful in an extremely wide class of applications, they have certain limitations including being confined to single responses and not providing convenient support for processing formulas with multiple parts. The latter is relevant for models with two or more sets of variables, e. g., different equations for different model parameters (such as mean and dispersion), regressors and instruments in instrumental variable regressions, two-part models such as hurdle models, or alternative-specific and individual-specific variables in choice models among many others. The R package Formula addresses these two problems by providing a new class ""Formula"" (inheriting from ""formula"") that accepts an additional formula operator vertical bar separating multiple parts and by allowing all formula operators (including the new vertical bar) on the left-hand side to support multiple responses.",WOS:000276706700001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION', 'PACKAGE']",Extended Model Formulas in R: Multiple Parts and Multiple Responses,2010
71,"This paper studies the residual empirical process of long- and short-memory time series regression models and establishes its uniform expansion under a general framework. The results are applied to the stochastic regression models and unstable autoregressive models. For the long-memory noise, it is shown that the limit distribution of the Kolmogorov-Smimov test statistic studied in Ho and Hsing [Ann. Statist. 24 (1996) 992-1024] does not hold when the stochastic regression model includes an unknown intercept or when the characteristic polynomial of the unstable autoregressive model has a unit root. To this end, two new statistics are proposed to test for the distribution of the long-memory noises of stochastic regression models and unstable autoregressive models.",WOS:000260554100017,ANNALS OF STATISTICS,"['WEAK-CONVERGENCE', 'RANGE DEPENDENCE', 'LIMITING DISTRIBUTIONS', 'ASYMPTOTIC-EXPANSION', 'MOVING AVERAGES', 'MODELS', 'ESTIMATORS', 'REGRESSION', 'FUNCTIONALS', 'SEQUENCES']",RESIDUAL EMPIRICAL PROCESSES FOR LONG AND SHORT MEMORY TIME SERIES,2008
72,"A linear structural equation model relates random variables of interest and corresponding Gaussian noise terms via a linear equation system. Each such model can be represented by a mixed graph in which directed edges encode the linear equations and bidirected edges indicate possible correlations among noise terms. We study parameter identifiability in these models, that is, we ask for conditions that ensure that the edge coefficients and correlations appearing in a linear structural equation model can be uniquely recovered from the covariance matrix of the associated distribution. We treat the case of generic identifiability, where unique recovery is possible for almost every choice of parameters. We give a new graphical condition that is sufficient for generic identifiability and can be verified in time that is polynomial in the size of the graph. It improves criteria from prior work and does not require the directed part of the graph to be acyclic. We also develop a related necessary condition and examine the ""gap"" between sufficient and necessary conditions through simulations on graphs with 25 or 50 nodes, as well as exhaustive algebraic computations for graphs with up to five nodes.",WOS:000310650900015,ANNALS OF STATISTICS,,HALF-TREK CRITERION FOR GENERIC IDENTIFIABILITY OF LINEAR STRUCTURAL EQUATION MODELS,2012
73,"This article introduces the use of the Amazon Mechanical Turk (MTurk) crowdsourcing platform as a resource for R users to leverage crowdsourced human intelligence for preprocessing ""messy"" data into a form easily analyzed within R. The article first describes MTurk and the MTurkR package, then outlines how to use MTurkR to gather and manage crowdsourced data with MTurk using some of the package's core functionality. Potential applications of MTurkR include construction of manually coded training sets, human transcription and translation, manual data scraping from scanned documents, content analysis, image classification, and the completion of online survey questionnaires, among others. As an example of massive data preprocessing, the article describes an image rating task involving 225 crowdsourced workers and more than 5500 images using just three MTurkR function calls.",WOS:000385276100021,R JOURNAL,,Crowdsourced Data Preprocessing with R and Amazon Mechanical Turk,2016
74,,WOS:000208589700007,R JOURNAL,,expert: Modeling Without Data Using Expert Opinion,2009
75,"We consider a partially linear framework for modeling massive heterogeneous data. The major goal is to extract common features across all subpopulations while exploring heterogeneity of each subpopulation. In particular, we propose an aggregation type estimator for the commonality parameter that possesses the (nonasymptotic) minimax optimal bound and asymptotic distribution as if there were no heterogeneity. This oracle result holds when the number of subpopulations does not grow too fast. A plug-in estimator for the heterogeneity parameter is further constructed, and shown to possess the asymptotic distribution as if the commonality information were available. We also test the heterogeneity among a large number of subpopulations. All the above results require to regularize each subestimation as though it had the entire sample. Our general theory applies to the divide-and-conquer approach that is often used to deal with massive homogeneous data. A technical by-product of this paper is statistical inferences for general kernel ridge regression. Thorough numerical results are also provided to back up our theory.",WOS:000379972900002,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'FINITE MIXTURE-MODELS', 'NONPARAMETRIC REGRESSION', 'SPLINE MODELS', 'INFERENCE']",A PARTIALLY LINEAR FRAMEWORK FOR MASSIVE HETEROGENEOUS DATA,2016
76,"Matching is an R package which provides functions for multivariate and propensity score matching and for finding optimal covariate balance based on a genetic search algorithm. A variety of univariate and multivariate metrics to determine if balance actually has been obtained are provided. The underlying matching algorithm is written in C++, makes extensive use of system BLAS and scales efficiently with dataset size. The genetic algorithm which finds optimal balance is parallelized and can make use of multiple CPUs or a cluster of computers. A large number of options are provided which control exactly how the matching is conducted and how balance is evaluated",WOS:000292097200001,JOURNAL OF STATISTICAL SOFTWARE,"['ASSESSING PARTISAN BIAS', 'TRAINING-PROGRAMS', 'CAUSAL INFERENCE', 'DISTRIBUTIONS', 'ESTIMATORS', 'REDUCTION', 'DESIGNS']",Multivariate and Propensity Score Matching Software with Automated Balance Optimization: The Matching Package for R,2011
77,"For the normal linear model variable selection problem, we propose selection criteria based on a fully Bayes formulation with a generalization of Zellner's g-prior which allows for p > n. A special case of the prior formulation is seen to yield tractable closed forms for marginal densities and Bayes factors which reveal new model evaluation characteristics of potential interest.",WOS:000299186500021,ANNALS OF STATISTICS,"['RIDGE-REGRESSION ESTIMATORS', 'VARIABLE SELECTION', 'MODEL', 'LASSO']",FULLY BAYES FACTORS WITH A GENERALIZED g-PRIOR,2011
78,"We provide several illustrations of Bayesian semiparametric regression analyses in the BRugs package. BRugs facilitates use of the BUGS inference engine from the R computing environment and allows analyses to be managed using scripts. The examples are chosen to represent an array of non-standard situations, for which mixed model software is not viable. The situations include: the response variable being outside of the one-parameter exponential family,data subject to missingness, data subject to measurement error and parameters entering the model via an index.",WOS:000284598000001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR MIXED MODELS', 'ADDITIVE-MODELS', 'MEASUREMENT ERROR', 'T-DISTRIBUTION', 'WINBUGS', 'SPLINES']",Non-Standard Semiparametric Regression via BRugs,2010
79,"In this age of ever-increasing data set sizes, especially in the natural sciences, visualisation becomes more and more important. Self-organizing maps have many features that make them attractive in this respect: they do not rely on distributional assumptions, can handle huge data sets with ease, and have shown their worth in a large number of applications. In this paper, we highlight the kohonen package for R, which implements self-organizing maps as well as some extensions for supervised pattern recognition and data fusion.",WOS:000252428600001,JOURNAL OF STATISTICAL SOFTWARE,,Self- and super-organizing maps in R: The kohonen package,2007
80,"We give an overview of the papers published in this special issue on spatial statistics, of the Journal of Statistical Software. 21 papers address issues covering visualization (micromaps, links to Google Maps or Google Earth), point pattern analysis, geostatistics, analysis of areal aggregated or lattice data, spatio-temporal statistics, Bayesian spatial statistics, and Laplace approximations. We also point to earlier publications in this journal on the same topic.",WOS:000349844700001,JOURNAL OF STATISTICAL SOFTWARE,"['R-PACKAGE', 'POINT PATTERNS', 'MODELS']",Software for Spatial Statistics,2015
81,"gretl is a general-purpose econometric package, whose most important characteristic is being free software. This ensures that its source code is freely available under the general public license (GPL) and, like most GPL software, that it can be used free of charge. As of version 1.8.1 (released in May 2009), it offers a mechanism for handling linear state space models in a reasonably general and efficient way. This article illustrates its main features with two examples.",WOS:000290528000001,JOURNAL OF STATISTICAL SOFTWARE,['MODELS'],State Space Methods in gretl,2011
82,"In sequential change detection, existing performance measures differ significantly in the way they treat the time of change. By modeling this quantity as a random time, we introduce a general framework capable of capturing and better understanding most well-known criteria and also propose new ones. For a specific new criterion that constitutes an extension to Lorden's performance measure, we offer the optimum structure for detecting a change in the constant drift of a Brownian motion and a formula for the corresponding optimum performance.",WOS:000254502700011,ANNALS OF STATISTICS,"['POISSON DISORDER PROBLEM', 'CUSUM PROCEDURE', 'EXPONENTIAL PENALTY', 'CONTINUOUS-TIME', 'OPTIMALITY', 'DELAY']",Sequential change detection revisited,2008
83,"This paper introduces a Monte Carlo method for maximum likelihood inference in the context of discretely observed diffusion processes. The method gives unbiased and a.s. continuous estimators of the likelihood function for a family of diffusion models aid its performance in numerical examples is computationally efficient. It uses a recently developed technique for the exact simulation of diffusions, and involves no discretization error. We show that, under regularity conditions, the Monte Carlo MLE converges a.s. to the true MLE. For datasize n -> infinity, we show that the number of Monte Carlo iterations should be tuned as O (n(1/2)) and we demonstrate the consistency properties of the Monte Carlo MLE as an estimator of the true parameter value.",WOS:000263129000008,ANNALS OF STATISTICS,"['EXACT SIMULATION', 'ERGODIC THEOREM', 'MODELS', 'INFERENCE', 'CONVERGENCE', 'TIME']",MONTE CARLO MAXIMUM LIKELIHOOD ESTIMATION FOR DISCRETELY OBSERVED DIFFUSION PROCESSES,2009
84,"The eigenvector Empirical Spectral Distribution (VESD) is adopted to investigate the limiting behavior of eigenvectors and eigenvalues of covariance matrices. In this paper, we shall show that the Kolmogorov distance between the expected VESD of sample covariance matrix and the Marcenko-Pastur distribution function is of order O(N-1/2). Given that data dimension n to sample size N ratio is bounded between 0 and 1, this convergence rate is established under finite 10th moment condition of the underlying distribution. It is also shown that, for any fixed eta > 0, the convergence rates of VESD are O(N-1/4) in probability and O(N-1/4+eta) almost surely, requiring finite 8th moment of the underlying distribution.",WOS:000327746100010,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'LARGEST EIGENVALUE', 'ASYMPTOTICS', 'LIMIT', 'LAW']",CONVERGENCE RATES OF EIGENVECTOR EMPIRICAL SPECTRAL DISTRIBUTION OF LARGE DIMENSIONAL SAMPLE COVARIANCE MATRIX,2013
85,"Acyclic directed mixed graphs (ADMGs) are graphs that contain directed (->) and bidirected (<->) edges, subject to the constraint that there are no cycles of directed edges. Such graphs may be used to represent the conditional independence structure induced by a DAG model containing hidden variables on its observed margin. The Markovian model associated with an ADMG is simply the set of distributions obeying the global Markov property, given via a simple path criterion (m-separation). We first present a factorization criterion characterizing the Markovian model that generalizes the well-known recursive factorization for DAGs. For the case of finite discrete random variables, we also provide a parameterization of the model in terms of simple conditional probabilities, and characterize its variation dependence. We show that the induced models are smooth. Consequently, Markovian ADMG models for discrete variables are curved exponential families of distributions.",WOS:000342481700008,ANNALS OF STATISTICS,"['CONDITIONAL-INDEPENDENCE', 'MODELS']",MARKOVIAN ACYCLIC DIRECTED MIXED GRAPHS FOR DISCRETE DATA,2014
86,"In this article, we present PCovR, an R package for performing principal covariates regression (PCovR; De Jong and Kiers 1992). PCovR was developed for analyzing regression data with many and/or highly collinear predictor variables. The method simultaneously reduces the predictor variables to a limited number of components and regresses the criterion variables on these components. The flexibility, interpretational advantages, and computational simplicity of PCovR make the method stand out between many other regression methods. The PCovR package offers data preprocessing options, new model selection procedures, and several component rotation strategies, some of which were not available in R up till now. The use and usefulness of the package is illustrated with a real dataset, called psychiatrists",WOS:000365974900001,JOURNAL OF STATISTICAL SOFTWARE,"['PARTIALLY SPECIFIED TARGET', 'ANALYTIC ROTATION', 'OBLIQUE ROTATION', 'CRITERION', 'SELECTION', 'MODELS']",PCovR: An R Package for Principal Covariates Regression,2015
87,"Considering two independent Poisson processes, we address the question of testing equality of their respective intensities. We first propose testing procedures whose test statistics are U -statistics based on single kernel functions. The corresponding critical values are constructed from a nonasymptotic wild bootstrap approach, leading to level alpha tests. Various choices for the kernel functions are possible, including projection, approximation or reproducing kernels. In this last case, we obtain a parametric rate of testing for a weak metric defined in the RKHS associated with the considered reproducing kernel. Then we introduce, in the other cases, aggregated or multiple kernel testing procedures, which allow us to import ideas coming from model selection, thresholding and/or approximation kernels adaptive estimation. These multiple kernel tests are proved to be of level alpha, and to satisfy nonasymptotic oracle-type conditions for the classical L-2-norm. From these conditions, we deduce that they are adaptive in the minimax sense over a large variety of classes of alternatives based on classical and weak Besov bodies in the univariate case, but also Sobolev and anisotropic Nikol'skii-Besov balls in the multivariate case.",WOS:000323271500003,ANNALS OF STATISTICS,"['U-STATISTICS', 'ORACLE INEQUALITIES', 'RISK MINIMIZATION', 'HOMOGENEITY TESTS', 'PERMUTATION TESTS', 'MODEL SELECTION', 'CONSISTENCY', 'HYPOTHESES']",THE TWO-SAMPLE PROBLEM FOR POISSON PROCESSES: ADAPTIVE TESTS WITH A NONASYMPTOTIC WILD BOOTSTRAP APPROACH,2013
88,"In multiple regression models, when there are a large number (p) of explanatory variables which may or may not be relevant for predicting the response, it is useful to be able to reduce the model. To this end, it is necessary to determine the best subset of q (q <= p) predictors which will establish the model with the best prediction capacity. FWD select package introduces a new forward stepwise-based selection procedure to select the best model in different regression frameworks (parametric or nonparametric). The developed methodology, which can be equally applied to linear models, generalized linear models or generalized additive models, aims to introduce solutions to the following two topics: i) selection of the best combination of q variables by using a step-by-step method; and, perhaps, most importantly, ii) search for the number of covariates to be included in the model based on bootstrap resampling techniques. The software is illustrated using real and simulated data.",WOS:000385276100010,R JOURNAL,"['GENERALIZED ADDITIVE-MODELS', 'LINEAR-MODELS', 'NONPARAMETRIC REGRESSION', 'LIKELIHOOD ESTIMATION', 'POLLUTION INCIDENTS', 'BOOTSTRAP', 'PREDICTION', 'JACKKNIFE', 'INFERENCE', 'LASSO']",FWDselect: An R Package for Variable Selection in Regression Models,2016
89,"Code analysis tools are crucial to understand program behavior. Profile tools use the results of time measurements in the execution of a program to gain this understanding and thus help in the optimization of the code. In this paper, we review the different available packages to profile R code and show the advantages and disadvantages of each of them. In additon, we present GUIProfiler, a package that fulfills some unmet needs.
Package GUIProfiler generates an HTML report with the timing for each code line and the relationships between different functions. This package mimics the behavior of the MATLAB profiler. The HTML report includes information on the time spent on each of the lines of the profiled code (the slowest code is highlighted). If the package is used within the RStudio environment, the user can navigate across the bottlenecks in the code and open the editor to modify the lines of code where more time is spent. It is also possible to edit the code using Notepad++ (a free editor for Windows) by simply clicking on the corresponding line. The graphical user interface makes it easy to identify the specific lines which slow down the code.
The integration in RStudio and the generation of an HTML report makes GUIProfiler a very convenient tool to perform code optimization.",WOS:000368551800021,R JOURNAL,,Code Profiling in R: A Review of Existing Methods and an Introduction to Package GUIProfiler,2015
90,"PivotalR is an R package that provides a front-end to PostgreSQL and all PostgreSQLlike databases such as Pivotal Inc.'s Greenplum Database (GPDB), HAWQ. When running on the products of Pivotal Inc., PivotalR utilizes the full power of parallel computation and distributive storage, and thus gives the normal R user access to big data. PivotalR also provides an R wrapper for MADlib. MADlib is an open-source library for scalable in-database analytics. It provides data-parallel implementations of mathematical, statistical and machine-learning algorithms for structured and unstructured data. Thus PivotalR also enables the user to apply machine learning algorithms on big data.",WOS:000343788100007,R JOURNAL,,PivotalR: A Package for Machine Learning on Big Data,2014
91,"In the psychometric literature, item response theory models have been proposed that explicitly take the decision process underlying the responses of subjects to psychometric test items into account. Application of these models is however hampered by the absence of general and flexible software to fit these models. In this paper, we present diffIRT, an R package that can be used to fit item response theory models that are based on a diffusion process. We discuss parameter estimation and model fit assessment, show the viability of the package in a simulation study, and illustrate the use of the package with two datasets pertaining to extraversion and mental rotation. In addition, we illustrate how the package can be used to fit the traditional diffusion model (as it has been originally developed in experimental psychology) to data.",WOS:000365977700001,JOURNAL OF STATISTICAL SOFTWARE,"['LEXICAL-DECISION TASK', 'MEASUREMENT INVARIANCE', 'LIMITED-INFORMATION', 'CONTINGENCY-TABLES', 'FIT', 'FRAMEWORK', 'CHOICE', 'ACCURACY']",Fitting Diffusion Item Response Theory Models for Responses and Response Times Using the R Package diffIRT,2015
92,"Carefully designed Java applications turn out to be efficient and platform independent tools that can compete well with classical implementations of statistical software. The project presented here is an example underlining this statement for random variate generation. An end-user application called RAGE ( Random Variate Generator) is developed to generate random variates from probability distributions. A Java class library called JDiscreteLib has been designed and implemented for the simulation of random variables from the most usual discrete distributions inside RAGE. For each distribution, specific and general algorithms are available for this purpose. RAGE can also be used as an interactive simulation tool for data and data summary visualization.",WOS:000244061900001,JOURNAL OF STATISTICAL SOFTWARE,,RAGE: A Java-implemented visual random generator,2007
93,"In this paper, an over-sampled periodogram higher criticism (OPHC) test is proposed for the global detection of sparse periodic effects in a complex-valued time series. An explicit minimax detection boundary is established between the rareness and weakness of the complex sinusoids hidden in the series. The OPHC test is shown to be asymptotically powerful in the detectable region. Numerical simulations illustrate and verify the effectiveness of the proposed test. Furthermore, the periodogram over-sampled by O(log N) is proven universally optimal in global testing for periodicities under a mild minimum separation condition.",WOS:000379972900003,ANNALS OF STATISTICS,"['LINE SPECTRAL ESTIMATION', 'HIGHER CRITICISM', 'EMPIRICAL PROCESSES', 'SERIES', 'MIXTURES', 'NOISE', 'SINUSOIDS', 'PARAMETER', 'SETS']",GLOBAL TESTING AGAINST SPARSE ALTERNATIVES IN TIME-FREQUENCY ANALYSIS,2016
94,"We consider a general nonlinear regression problem where the predictors contain measurement error. It has been recently discovered that several well-known dimension reduction methods, such as OLS, SIR and pHd, can be performed on the surrogate regression problem to produce consistent estimates for the original regression problem involving the unobserved true predictor. In this paper we establish a general invariance law between the surrogate and the original dimension reduction spaces, which implies that, at least at the population level, the two dimension reduction problems are in fact equivalent. Consequently we can apply all existing dimension reduction methods to measurement error regression problems. The equivalence holds exactly for multivariate normal predictors, and approximately for arbitrary predictors. We also characterize the rate of convergence for the surrogate dimension reduction estimators. Finally, we apply several dimension reduction methods to real and simulated data sets involving measurement error to compare their performances.",WOS:000251096100012,ANNALS OF STATISTICS,"['PRINCIPAL HESSIAN DIRECTIONS', 'SLICED INVERSE REGRESSION', 'DATA VISUALIZATION', 'LINK', 'MOMENTS', 'MODELS']",On surrogate dimension reduction for measurement error regression: An invariance law,2007
95,"In this paper we present the R package deSolve to solve initial value problems (IVP) written as ordinary differential equations (ODE), differential algebraic equations (DAE) of index 0 or 1 and partial differential equations (PDE), the latter solved using the method of lines approach. The differential equations can be represented in R code or as compiled code. In the latter case, R is used as a tool to trigger the integration and post-process the results, which facilitates model development and application, whilst the compiled code significantly increases simulation speed. The methods implemented are efficient, robust, and well documented public-domain Fortran routines. They include four integrators from the ODEPACK package (LSODE, LSODES, LSODA, LSODAR), DVODE and DASPK2.0. In addition, a suite of Runge-Kutta integrators and special-purpose solvers to efficiently integrate 1-, 2- and 3-dimensional partial differential equations are available. The routines solve both stiff and non-stiff systems, and include many options, e. g., to deal in an efficient way with the sparsity of the Jacobian matrix, or finding the root of equations. In this article, our objectives are threefold: (1) to demonstrate the potential of using R for dynamic modeling, (2) to highlight typical uses of the different methods implemented and (3) to compare the performance of models specified in R code and in compiled code for a number of test cases. These comparisons demonstrate that, if the use of loops is avoided, R code can efficiently integrate problems comprising several thousands of state variables. Nevertheless, the same problem may be solved from 2 to more than 50 times faster by using compiled code compared to an implementation using only R code. Still, amongst the bene fits of R are a more flexible and interactive implementation, better readability of the code, and access to R's high-level procedures. deSolve is the successor of packageo desolve which will be deprecated in the future; it is free software and distributed under the GNU General Public License, as part of the R software project.",WOS:000275204000001,JOURNAL OF STATISTICAL SOFTWARE,['SYSTEMS'],Solving Differential Equations in R: Package deSolve,2010
96,"Manufacturers and government agencies frequently use acceptance sampling to decide whether a lot from a supplier or exporting country should be accepted or rejected. International standards on acceptance sampling provide sampling plans for specific circumstances.
The aim of this package is to provide an easy-to-use interface to visualize single, double or multiple sampling plans. In addition, methods have been provided to enable the user to assess sampling plans against pre-specified levels of performance, as measured by the probability of acceptance for a given level of quality in the lot.",WOS:000258180900001,JOURNAL OF STATISTICAL SOFTWARE,,Visualizing and assessing acceptance sampling plans: The R package acceptance sampling,2008
97,"In many applications researchers are typically interested in testing for inequality constraints in the context of linear fixed effects and mixed effects models. Although there exists a large body of literature for performing statistical inference under inequality constraints, user friendly statistical software implementing such methods is lacking, especially in the context of linear fixed and mixed effects models. In this article we introduce CLME, a package in the R language that can be used for testing a broad collection of inequality constraints. It uses residual bootstrap based methodology which is reasonably robust to non-normality as well as heteroscedasticity. The package is illustrated using two data sets. The package also contains a graphical user interface built using the shiny package.",WOS:000392703800001,JOURNAL OF STATISTICAL SOFTWARE,"['LONGITUDINAL DATA', 'DOSE-RESPONSE', 'HEARING-LOSS', 'INFERENCE']",CLME: An R Package for Linear Mixed Effects Models under Inequality Constraints,2016
98,"We consider a time series X = (X-k, k is an element of Z) with memory parameter d(0) is an element of R. This time series is either stationary or can be made stationary after differencing a finite number of times. We study the ""local Whittle wavelet estimator"" of the memory parameter d(0). This is a wavelet-based semiparametric pseudo-likelihood maximum method estimator. The estimator may depend on a given finite range of scales or on a range which becomes infinite with the sample size. We show that the estimator is consistent and rate optimal if X is a linear process, and is asymptotically normal if X is Gaussian.",WOS:000258243000018,ANNALS OF STATISTICS,['LONG-RANGE DEPENDENCE'],A wavelet whittle estimator of the memory parameter of a nonstationary Gaussian time series,2008
99,"Functional data that are nonnegative and have a constrained integral can be considered as samples of one-dimensional density functions. Such data are ubiquitous. Due to the inherent constraints, densities do not live in a vector space and, therefore, commonly used Hilbert space based methods of functional data analysis are not applicable. To address this problem, we introduce a transformation approach, mapping probability densities to a Hilbert space of functions through a continuous and invertible map. Basic methods of functional data analysis, such as the construction of functional modes of variation, functional regression or classification, are then implemented by using representations of the densities in this linear space. Representations of the densities themselves are obtained by applying the inverse map from the linear functional space to the density space. Transformations of interest include log quantile density and log hazard transformations, among others. Rates of convergence are derived for the representations that are obtained for a general class of transformations under certain structural properties. If the subject-specific densities need to be estimated from data, these rates correspond to the optimal rates of convergence for density estimation. The proposed methods are illustrated through simulations and applications in brain imaging.",WOS:000368022000007,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'ALZHEIMERS-DISEASE', 'CONVERGENCE-RATES', 'LINEAR-REGRESSION', 'LONGITUDINAL DATA', 'CORTICAL HUBS', 'CONNECTIVITY', 'ESTIMATORS', 'INFERENCE', 'CURVES']",FUNCTIONAL DATA ANALYSIS FOR DENSITY FUNCTIONS BY TRANSFORMATION TO A HILBERT SPACE,2016
100,"Mathematical models of disease progression predict disease outcomes and are useful epidemiological tools for planners and evaluators of health interventions. The R package gems is a tool that simulates disease progression in patients and predicts the effect of different interventions on patient outcome. Disease progression is represented by a series of events (e.g., diagnosis, treatment and death), displayed in a directed acyclic graph. The vertices correspond to disease states and the directed edges represent events. The package gems allows simulations based on a generalized multistate model that can be described by a directed acyclic graph with continuous transition-specific hazard functions. The user can specify an arbitrary hazard function and its parameters. The model includes parameter uncertainty, does not need to be a Markov model, and may take the history of previous events into account. Applications are not limited to the medical field and extend to other areas where multistate simulation is of interest. We provide a technical explanation of the multistate models used by gems, explain the functions of gems and their arguments, and show a sample application.",WOS:000352915500001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTISTATE MODELS', 'ANTIRETROVIRAL THERAPY', 'HIV TRANSMISSION', 'COMPETING RISKS', 'PROGRAMS']",gems: An R Package for Simulating from Disease Progression Models,2015
101,"We propose a semiparametric approach called the nonparanormal SKEPTIC for efficiently and robustly estimating high-dimensional undirected graphical models. To achieve modeling flexibility, we consider the nonparanormal graphical models proposed by Liu, Lafferty and Wasserman [J. Mach. Learn. Res. 10 (2009) 2295-2328]. To achieve estimation robustness, we exploit nonparametric rank-based correlation coefficient estimators, including Spearman's rho and Kendall's tau. We prove that the nonparanormal SKEPTIC achieves the optimal parametric rates of convergence for both graph recovery and parameter estimation. This result suggests that the nonparanormal graphical models can be used as a safe replacement of the popular Gaussian graphical models, even when the data are truly Gaussian. Besides theoretical analysis, we also conduct thorough numerical simulations to compare the graph recovery performance of different estimators under both ideal and noisy settings. The proposed methods are then applied on a large-scale genomic data set to illustrate their empirical usefulness. The R package huge implementing the proposed methods is available on the Comprehensive R Archive Network: http://cran.r-project.org/.",WOS:000321842400006,ANNALS OF STATISTICS,"['COVARIANCE-MATRIX ESTIMATION', 'SELECTION', 'LASSO', 'MAXIMUM']",HIGH-DIMENSIONAL SEMIPARAMETRIC GAUSSIAN COPULA GRAPHICAL MODELS,2012
102,,WOS:000208589700004,R JOURNAL,,Drawing Diagrams with R,2009
103,"In a wide range of applications, the stochastic properties of the observed time series change over time. The changes often occur gradually rather than abruptly: the properties are (approximately) constant for some time and then slowly start to change. In many cases, it is of interest to locate the time point where the properties start to vary. In contrast to the analysis of abrupt changes, methods for detecting smooth or gradual change points are less developed and often require strong parametric assumptions. In this paper, we develop a fully nonparametric method to estimate a smooth change point in a locally stationary framework. We set up a general procedure which allows us to deal with a wide variety of stochastic properties including the mean, (auto)covariances and higher moments. The theoretical part of the paper establishes the convergence rate of the new estimator. In addition, we examine its finite sample performance by means of a simulation study and illustrate the methodology by two applications to financial return data.",WOS:000352757100009,ANNALS OF STATISTICS,"['VARYING ARCH PROCESSES', 'TIME-SERIES MODELS', 'CHANGE-POINT', 'NONPARAMETRIC REGRESSION', 'STATISTICAL-INFERENCE', 'THRESHOLD ESTIMATION', 'CONTROL CHARTS', 'LINEAR TREND', 'SETTINGS']",DETECTING GRADUAL CHANGES IN LOCALLY STATIONARY PROCESSES,2015
104,"Identifying the optimal number of clusters is a common problem faced by data scientists in various research fields and industry applications. Though many clustering evaluation techniques have been developed to solve this problem, the recently developed algorithm Progeny Clustering is a much faster alternative and one that is relevant to biomedical applications. In this paper, we introduce an R package progenyClust that implements and extends the original Progeny Clustering algorithm for evaluating clustering stability and identifying the optimal cluster number. We illustrate its applicability using two examples: a simulated test dataset for proof-of-concept, and a cell imaging dataset for demonstrating its application potential in biomedical research. The progenyClust package is versatile in that it offers great flexibility for picking methods and tuning parameters. In addition, the default parameter setting as well as the plot and summary methods offered in the package make the application of Progeny Clustering straightforward and coherent.",WOS:000385276100024,R JOURNAL,"['GENE-EXPRESSION PATTERNS', 'DATA SET', 'NUMBER']",progenyClust: an R package for Progeny Clustering,2016
105,"This article describes the BMS (Bayesian model sampling) package for R that implements Bayesian model averaging for linear regression models. The package excels in allowing for a variety of prior structures, among them the ""binomial-beta"" prior on the model space and the so-called ""hyper-g"" specifications for Zellner's g prior. Furthermore, the BMS package allows the user to specify her own model priors and offers a possibility of subjective inference by setting ""prior inclusion probabilities"" according to the researcher's beliefs. Furthermore, graphical analysis of results is provided by numerous built-in plot functions of posterior densities, predictive densities and graphical illustrations to compare results under different prior settings. Finally, the package provides full enumeration of the model space for small scale problems as well as two efficient MCMC (Markov chain Monte Carlo) samplers that sort through the model space when the number of potential covariates is large.",WOS:000366014100001,JOURNAL OF STATISTICAL SOFTWARE,"['WILL DATA TELL', 'GROWTH DETERMINANTS', 'ECONOMIC-GROWTH', 'VARIABLE SELECTION', 'FORECAST COMBINATION', 'FINANCIAL CRISIS', 'REEXAMINATION', 'UNCERTAINTY', 'MIXTURES']",Bayesian Model Averaging Employing Fixed and Flexible Priors: The BMS Package for R,2015
106,The R package DEoptim implements the Differential Evolution algorithm. This algorithm is an evolutionary technique similar to classic genetic algorithms that is useful for the solution of global optimization problems. In this note we provide an introduction to the package and demonstrate its utility for financial applications by solving a non-convex portfolio optimization problem.,WOS:000208590100006,R JOURNAL,,Differential Evolution with DEoptim An Application to Non-Convex Portfolio Optimization,2011
107,"The Apple Xgrid system provides access to groups (or grids) of computers that can be used to facilitate parallel processing. We describe the xgrid package which facilitates access to this system to undertake independent simulations or other long-running jobs that can be divided into replicate runs within R. Detailed examples are provided to demonstrate the interface, along with results from a simulation study of the performance gains using a variety of grids. Use of the grid for ""embarassingly parallel"" independent jobs has the potential for major speedups in time to completion. Appendices provide guidance on setting up the workflow, utilizing add-on packages, and constructing grids using existing machines.",WOS:000313197700007,R JOURNAL,['LATENT CLASS ANALYSIS'],xgrid and R: Parallel Distributed Processing Using Heterogeneous Groups of Apple Computers,2012
108,"This paper considers the problem of clustering a collection of unlabeled data points assumed to lie near a union of lower-dimensional planes. As is common in computer vision or unsupervised learning applications, we do not know in advance how many subspaces there are nor do we have any information about their dimensions. We develop a novel geometric analysis of an algorithm named sparse subspace clustering (SSC) [In IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009 (2009) 2790-2797. IEEE], which significantly broadens the range of problems where it is provably effective. For instance, we show that SSC can recover multiple subspaces, each of dimension comparable to the ambient dimension. We also prove that SSC can correctly cluster data points even when the subspaces of interest intersect. Further, we develop an extension of SSC that succeeds when the data set is corrupted with possibly overwhelmingly many outliers. Underlying our analysis are clear geometric insights, which may bear on other sparse recovery problems. A numerical study complements our theoretical analysis and demonstrates the effectiveness of these methods.",WOS:000321842400003,ANNALS OF STATISTICS,"['MOTION SEGMENTATION', 'COMPRESSION', 'INFORMATION', 'IMAGES', 'MODELS', 'BODIES']",A GEOMETRIC ANALYSIS OF SUBSPACE CLUSTERING WITH OUTLIERS,2012
109,We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes. the distribution of a post-selection estimator conditioned on the selection event. We specialize the approach to model selection by the lasso to form valid confidence intervals for the selected coefficients and test whether all relevant variables have been included in the model.,WOS:000375175200001,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'THE-LOSERS DESIGN', 'CONFIDENCE-INTERVALS', 'MODEL-SELECTION', 'ESTIMATORS', 'PARAMETERS', 'REGRESSION']","EXACT POST-SELECTION INFERENCE, WITH APPLICATION TO THE LASSO",2016
110,"This paper describes the R package VSURF. Based on random forests, and for both regression and classification problems, it returns two subsets of variables. The first is a subset of important variables including some redundancy which can be relevant for interpretation, and the second one is a smaller subset corresponding to a model trying to avoid redundancy focusing more closely on the prediction objective. The two-stage strategy is based on a preliminary ranking of the explanatory variables using the random forests permutation-based score of importance and proceeds using a stepwise forward strategy for variable introduction. The two proposals can be obtained automatically using data-driven default values, good enough to provide interesting results, but strategy can also be tuned by the user. The algorithm is illustrated on a simulated example and its applications to real datasets are presented.",WOS:000368551800003,R JOURNAL,"['FEATURE SUBSET-SELECTION', 'GENE-EXPRESSION DATA', 'CLASSIFICATION', 'PREDICTORS', 'FRAMEWORK']",VSURF: An R Package for Variable Selection Using Random Forests,2015
111,"We propose a two-stage procedure for estimating the location mu and size M of the maximum of a smooth d-variate regression function f. In the first stage, a preliminary estimator of mu obtained from a standard nonparametric smoothing method is used. At the second stage, we ""zoom-in"" near the vicinity of the preliminary estimator and make further observations at some design points in that vicinity. We fit an appropriate polynomial regression model to estimate the location and size of the maximum. We establish that, under suitable smoothness conditions and appropriate choice of the zooming, the second stage estimators have better convergence rates than the corresponding first stage estimators of mu and M. More specifically, for alpha-smooth regression functions, the optimal nonparametric rates n(-(alpha-1)/(2 alpha+d)) and n(-alpha/(2 alpha+d)) at the first stage can be improved to n(-(alpha-1)/(2 alpha)) and n(-1/2), respectively, for alpha > 1 + root 1 + d/2. These rates are optimal in the class of all possible sequential estimators. Interestingly, the two-stage procedure resolves ""the curse of the dimensionality"" problem to some extent, as the dimension d does not control the second stage convergence rates, provided that the function class is sufficiently smooth. We consider a multi-stage generalization of our procedure that attains the optimal rate for any smoothness level alpha > 2 starting with a preliminary estimator with any power-law rate at the first stage.",WOS:000321845400004,ANNALS OF STATISTICS,['MODE'],OPTIMAL TWO-STAGE PROCEDURES FOR ESTIMATING LOCATION AND SIZE OF THE MAXIMUM OF A MULTIVARIATE REGRESSION FUNCTION,2012
112,"Recently, several data analytic techniques based on graph connection Laplacian (GCL) ideas have appeared in the literature. At this point, the properties of these methods are starting to be understood in the setting where the data is observed without noise. We study the impact of additive noise on these methods and show that they are remarkably robust. As a by-product of our analysis, we propose modifications of the standard algorithms that increase their robustness to noise. We illustrate our results in numerical simulations.",WOS:000368022000012,ANNALS OF STATISTICS,"['POINT CLOUD DATA', 'RANDOM MATRICES', 'DIFFUSION MAPS', 'SYNCHRONIZATION', 'REPRESENTATION', 'EIGENVECTORS', 'SURFACES']",GRAPH CONNECTION LAPLACIAN METHODS CAN BE MADE ROBUST TO NOISE,2016
113,"Bayesian and frequentist methods differ in many aspects, but share some basic optimality properties. In practice, there are situations in which one of the methods is more preferred by some criteria. We consider the case of inference about a Set Of Multiple parameters, which can be divided into two disjoint subsets. On one set, a frequentist method may be favored and on the other, the Bayesian. This motivates a joint estimation procedure in which some of the parameters are estimated Bayesian, and the rest by the maximum-likelihood estimator in the same parametric model, and thus keep the strengths of both the methods and avoid their weaknesses. Such a hybrid procedure gives us more flexibility in achieving overall inference advantages. We study the consistency and high-order asymptotic behavior of the proposed estimator, and illustrate its application. Also, the results imply a new method for constructing objective prior.",WOS:000268604900014,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD', 'POSTERIOR DISTRIBUTIONS', 'PROFILE SAMPLER', 'FORMULAS']",BAYESIAN FREQUENTIST HYBRID INFERENCE,2009
114,WhatIf is an R package that implements the methods for evaluating counterfactuals introduced in King and Zeng (2006a) and King and Zeng (2006b). It offers easy-to-use techniques for assessing a counterfactual's model dependence without having to conduct sensitivity testing over specified classes of models. These same methods can be used to approximate the common support of the treatment and control groups in causal inference.,WOS:000235181000001,JOURNAL OF STATISTICAL SOFTWARE,,WhatIf: R software for evaluating counterfactuals,2006
115,"Temporal disaggregation methods are used to disaggregate low frequency time series to higher frequency series, where either the sum, the average, the first or the last value of the resulting high frequency series is consistent with the low frequency series. Temporal disaggregation can be performed with or without one or more high frequency indicator series. The package tempdisagg is a collection of several methods for temporal disaggregation.",WOS:000330193300009,R JOURNAL,,Temporal Disaggregation of Time Series,2013
116,"We describe a parallel implementation in R of the weighted subspace random forest algorithm (Xu, Huang, Williams, Wang, and Ye 2012) available as the wsrf package. A novel variable weighting method is used for variable subspace selection in place of the traditional approach of random variable sampling. This new approach is particularly useful in building models for high dimensional data - often consisting of thousands of variables. Parallel computation is used to take advantage of multi-core machines and clusters of machines to build random forest models from high dimensional data in considerably shorter times. A series of experiments presented in this paper demonstrates that wsrf is faster than existing packages whilst retaining and often improving on the classification performance, particularly for high dimensional data.",WOS:000399023700001,JOURNAL OF STATISTICAL SOFTWARE,['VARIABLE IMPORTANCE'],wsrf: An R Package for Classification with Scalable Weighted Subspace Random Forests,2017
117,"The assessment of dose-response is an integral component of the drug development process. Parallel dose-response studies are conducted, customarily ,in preclinical and phase 1,2 clinical trials for this purpose. Practical constraints on dose range, dose levels and dose proportions are intrinsic issues in the design of dose response studies because of drug toxicity, efficacy, FDA regulations, protocol requirements, clinical trial logistics, and marketing issues.We provide a free on-line software package called Controlled Optimal Design 2.0 for generating controlled optimal designs that can incorporate prior information and multiple objectives, and meet multiple practical constraints at the same time. Researchers can either run the web-based design program or download its stand-alone version to construct the desired multiple-objective controlled Bayesian optimal designs. Because researchers often adopt ad-hoc design schemes such as the equal allocation rules with out knowing how efficient such designs would be for the design problem, the program also evaluates the efficiency of user-supplied design",WOS:000281587700001,JOURNAL OF STATISTICAL SOFTWARE,"['BAYESIAN OPTIMAL DESIGNS', 'COMPOUND OPTIMAL DESIGNS', 'DOUBLE-BLIND', 'RHEUMATOID-ARTHRITIS', 'MISSING OBSERVATIONS', 'EQUIVALENCE', 'EFFICACY', 'TACROLIMUS']",Controlled Optimal Design Program for the Logit Dose Response Mode,2010
118,"This paper provides the requisite information and description of software that perform numerical computations and graphics for the power method polynomial transformation. The software developed is written in the Mathematica 5.2 package PowerMethod. m and is associated with fifth-order polynomials that are used for simulating univariate and multivariate non-normal distributions. The package is flexible enough to allow a user the choice to model theoretical pdfs, empirical data, or a user's own selected distribution(s). The primary functions perform the following ( a) compute standardized cumulants and polynomial coefficients, (b) ensure that polynomial transformations yield valid pdfs, and ( c) graph power method pdfs and cdfs. Other functions compute cumulative probabilities, modes, trimmed means, intermediate correlations, or perform the graphics associated with fitting power method pdfs to either empirical or theoretical distributions. Numerical examples and Monte Carlo results are provided to demonstrate and validate the use of the software package. The notebook Demo. nb is also provided as a guide for user of the power method.",WOS:000246391300001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIVARIATE NONNORMAL DISTRIBUTIONS', 'STRUCTURAL EQUATION MODELS', 'REPEATED-MEASURES DESIGNS', 'RANK TRANSFORMATION', 'FITTING DATA', 'UNIVARIATE', 'HETEROGENEITY', 'COVARIANCE', 'VARIABLES', 'FIT']",Numerical computing and graphics for the power method transformation using Mathematica,2007
119,"This paper presents the R package AdMit which provides flexible functions to approximate a certain target distribution and to efficiently generate a sample of random draws from it, given only a kernel of the target density function. The core algorithm consists of the function AdMit which fits an adaptive mixture of Student-t distributions to the density of interest. Then, importance sampling or the independence chain Metropolis-Hastings algorithm is used to obtain quantities of interest for the target density, using the fitted mixture as the importance or candidate density. The estimation procedure is fully automatic and thus avoids the time-consuming and difficult task of tuning a sampling algorithm. The relevance of the package is shown in two examples. The first aims at illustrating in detail the use of the functions provided by the package in a bivariate bimodal distribution. The second shows the relevance of the adaptive mixture procedure through the Bayesian estimation of a mixture of ARCH model fitted to foreign exchange log-returns data. The methodology is compared to standard cases of importance sampling and the Metropolis-Hastings algorithm using a naive candidate and with the Griddy-Gibbs approach.",WOS:000263105200001,JOURNAL OF STATISTICAL SOFTWARE,"['MONTE-CARLO', 'MODELS', 'INTEGRATION', 'DENSITIES', 'INFERENCE']",Adaptive Mixture of Student-t Distributions as a Flexible Candidate Distribution for Efficient Simulation: The R Package AdMit,2009
120,"Lisp-Stat seems to be quite inert nowadays. However, there is no widespread agreement about what actually killed it, or even whether it has really passed away or not. Here we present the papers included in this special issue of the Journal of Statistical Software about it. Some of the included papers are about what Lisp-Stat was able to do in the past, other give testimony of the current state of health of Lisp-Stat, and other analyze whether there is any chance of its eventual recovery.
We believe that the diagnosis performed here will appeal not only to people interested in Lisp-Stat but to all those involved in the development of statistical languages, as it provides hints as to what elements could make them succeed or fail.",WOS:000232832100001,JOURNAL OF STATISTICAL SOFTWARE,,The health of Lisp-Stat,2005
121,"We study full Bayesian procedures for high-dimensional linear regression under sparsity constraints. The prior is a mixture of point masses at zero and continuous distributions. Under compatibility conditions on the design matrix, the posterior distribution is shown to contract at the optimal rate for recovery of the unknown sparse vector, and to give optimal prediction of the response vector. It is also shown to select the correct sparse model, or at least the coefficients that are significantly different from zero. The asymptotic shape of the posterior distribution is characterized and employed to the construction and study of credible sets for uncertainty quantification.",WOS:000362697700005,ANNALS OF STATISTICS,"['VARIABLE-SELECTION PROBLEM', 'STOCHASTIC SEARCH', 'ORACLE INEQUALITIES', 'MODEL EXPLORATION', 'DANTZIG SELECTOR', 'OPTIMAL RATES', 'LASSO', 'DISTRIBUTIONS', 'SEQUENCES', 'NEEDLES']",BAYESIAN LINEAR REGRESSION WITH SPARSE PRIORS,2015
122,"We present two natural generalizations of the multinomial and multivariate binomial distributions, which arise from the multiplicative binomial distribution of Altham (1978). The resulting two distributions are discussed and we introduce an R package, MM, which includes associated functionality.",WOS:000301231700001,JOURNAL OF STATISTICAL SOFTWARE,"['SOUTHERN NORTH AMERICA', 'CONTINGENCY-TABLES', 'PALYNOLOGY', 'MEXICO']",Multivariate Generalizations of the Multiplicative Binomial Distribution: Introducing the MM Package,2012
123,"The field of thermal comfort generated a number of thermal comfort indices. Their code implementation needs to be done by individual researchers. This paper presents the R package, comf, which includes functions for common and new thermal comfort indices. Additional functions allow comparisons between the predictive performance of these indices. This paper reviews existing thermal comfort indices and available code implementations. This is followed by the description of the R package and an example how to use the R package for the comparison of different thermal comfort indices on data from a thermal comfort study.",WOS:000395669800022,R JOURNAL,"['EXERGY CONSUMPTION RATE', 'MODEL', 'ENVIRONMENTS', 'BUILDINGS', 'PMV']",comf: An R Package for Thermal Comfort Studies,2016
124,"We illustrate how to estimate parameters of linear state-space models using the Stata program sspace. We provide examples of how to use sspace to estimate the parameters of unobserved-component models, vector autoregressive moving-average models, and dynamic-factor models. We also show how to compute one-step, filtered, and smoothed estimates of the series and the states; dynamic forecasts and their confidence intervals; and residuals.",WOS:000290527800001,JOURNAL OF STATISTICAL SOFTWARE,['MODEL'],State Space Methods in Stata,2011
125,"In recent years, data streams have become an increasingly important area of research for the computer science, database and statistics communities. Data streams are ordered and potentially unbounded sequences of data points created by a typically non-stationary data generating process. Common data mining tasks associated with data streams include clustering, classification and frequent pattern mining. New algorithms for these types of data are proposed regularly and it is important to evaluate them thoroughly under standardized conditions.
In this paper we introduce stream, a research tool that includes modeling and simulating data streams as well as an extensible framework for implementing, interfacing and experimenting with algorithms for various data stream mining tasks. The main advantage of stream is that it seamlessly integrates with the large existing infrastructure provided by R. In addition to data handling, plotting and easy scripting capabilities, R also provides many existing algorithms and enables users to interface code written in many programming languages popular among data mining researchers (e.g., C/C++, Java and Python). In this paper we describe the architecture of stream and focus on its use for data stream clustering research. stream was implemented with extensibility in mind and will be extended in the future to cover additional data stream mining tasks like classification and frequent pattern mining.",WOS:000398468200001,JOURNAL OF STATISTICAL SOFTWARE,,Introduction to stream: An Extensible Framework for Data Stream Clustering Research with R,2017
126,"Estimating the leading principal components of data, assuming they are sparse, is a central task in modern high-dimensional statistics. Many algorithms were developed for this sparse PCA problem, from simple diagonal thresholding to sophisticated semidefinite programming (SDP) methods. A key theoretical question is under what conditions can such algorithms recover the sparse principal components? We study this question for a single-spike model with an l(0)-sparse eigenvector, in the asymptotic regime as dimension p and sample size n both tend to infinity. Amini and Wainwright [Ann. Statist. 37 (2009) 2877-2921] proved that for sparsity levels k >= Omega (n/ log p), no algorithm, efficient or not, can reliably recover the sparse eigenvector. In contrast, for k <= O (root n/log p), diagonal thresholding is consistent. It was further conjectured that an SDP approach may close this gap between computational and information limits. We prove that when k >= Omega(root n), the proposed SDP approach, at least in its standard usage, cannot recover the sparse spike. In fact, we conjecture that in the single-spike model, no computationally-efficient algorithm can recover a spike of l(0)-sparsity k >= Omega (root n). Finally, we present empirical results suggesting that up to sparsity levels k = O (root n), recovery is possible by a simple covariance thresholding algorithm.",WOS:000355768700012,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'LARGE HIDDEN CLIQUE', 'COVARIANCE MATRICES', 'HIGH DIMENSIONS', 'APPROXIMATION', 'SELECTION', 'GRAPH']",DO SEMIDEFINITE RELAXATIONS SOLVE SPARSE PCA UP TO THE INFORMATION LIMIT?,2015
127,"Dynamic time warping is a popular technique for comparing time series, providing both a distance measure that is insensitive to local compression and stretches and the warping which optimally deforms one of the two input series onto the other. A variety of algorithms and constraints have been discussed in the literature. The dtw package provides an unification of them; it allows R users to compute time series alignments mixing freely a variety of continuity constraints, restriction windows, endpoints, local distance definitions, and so on. The package also provides functions for visualizing alignments and constraints using several classic diagram types.",WOS:000268992000001,JOURNAL OF STATISTICAL SOFTWARE,"['WORD RECOGNITION', 'ALGORITHMS', 'SERIES']",Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package,2009
128,"Correspondence analysis on generalised aggregated lexical tables (CA-GALT) is a method that generalizes classical CA-ALT to the case of several quantitative, categorical and mixed variables. It aims to establish a typology of the external variables and a typology of the events from their mutual relationships. In order to do so, the influence of external variables on the lexical choices is untangled cancelling the associations among them, and to avoid the instability issued from multicollinearity, they are substituted by their principal components. The CaGalt function, implemented in the FactoMineR package, provides numerous numerical and graphical outputs. Confidence ellipses are also provided to validate and improve the representation of words and variables. Although this methodology was developed mainly to give an answer to the problem of analyzing open-ended questions, it can be applied to any kind of frequency/contingency table with external variables.",WOS:000357431900010,R JOURNAL,,Correspondence Analysis on Generalised Aggregated Lexical Tables (CA-GALT) in the FactoMineR Package,2015
129,"In Bayesian analysis of multi-way contingency tables, the selection of a prior distribution for either the log-linear parameters or the cell probabilities parameters is a major challenge. In this paper, we define a flexible family of conjugate priors for the wide class of discrete hierarchical log-linear models, which includes the class of graphical models. These priors are defined as the Diaconis-Ylvisaker conjugate priors on the log-linear parameters subject to ""baseline constraints"" under multinomial sampling. We also derive the induced prior on the cell probabilities and show that the induced prior is a generalization of the hyper Dirichlet prior. We show that this prior has several desirable properties and illustrate its Usefulness by identifying the most probable decomposable, graphical and hierarchical log-linear models for a six-way contingency table.",WOS:000271673500012,ANNALS OF STATISTICS,"['GRAPHICAL MODELS', 'CONTINGENCY-TABLES', 'EXPONENTIAL-FAMILIES', 'UNCERTAINTY']",A CONJUGATE PRIOR FOR DISCRETE HIERARCHICAL LOG-LINEAR MODELS,2009
130,"We present the RWiener package that provides R functions for the Wiener diffusion model. The core of the package are the four distribution functions dwiener, pwiener, qwiener and rwiener, which use up-to-date methods, implemented in C, and provide fast and accurate computation of the density, distribution, and quantile function, as well as a random number generator for the Wiener diffusion model. We used the typical Wiener diffusion model with four parameters: boundary separation, non-decision time, initial bias and drift rate parameter. Beyond the distribution functions, we provide extended likelihood-based functions that can be used for parameter estimation and model selection. The package can be obtained via CRAN.",WOS:000343788100006,R JOURNAL,['TIME'],The RWiener Package: an R Package Providing Distribution Functions for the Wiener Diffusion Model,2014
131,"The adoption of high-quality tools for collaboration and reproducibile research such as R and Github is becoming more common in many research fields. While Github and other version management systems are excellent resources, they were originally designed to handle code and scale poorly to large text-based or binary datasets. A number of scientific data repositories are coming online and are often focused on dataset archival and publication. To handle collaborative workflows using large scientific datasets, there is increasing need to connect cloud-based online data storage to R. In this article, we describe how the new R package sbtools enables direct access to the advanced online data functionality provided by ScienceBase, the U.S. Geological Survey's online scientific data storage platform.",WOS:000385276100030,R JOURNAL,,sbtools: A Package Connecting R to Cloud-based Data for Collaborative Online Research,2016
132,"Neural networks are important standard machine learning procedures for classifi cation and regression. We describe the R package RSNNS that provides a convenient interface to the popular Stuttgart Neural Network Simulator SNNS. The main features are (a) encapsulation of the relevant SNNS parts in a C++ class, for sequential and parallel usage of different networks, (b) accessibility of all of the SNNS algorithmic functionality from R using a low-level interface, and (c) a high-level interface for convenient, R-style usage of many standard neural network procedures. The package also includes functions for visualization and analysis of the models and the training procedures, as well as functions for data input/output from/to the original SNNS file formats.",WOS:000301071600001,JOURNAL OF STATISTICAL SOFTWARE,"['RECOGNITION', 'TIME', 'CLASSIFICATION', 'ORGANIZATION', 'ARCHITECTURE', 'ALGORITHM']",Neural Networks in R Using the Stuttgart Neural Network Simulator: RSNNS,2012
133,"A new multivariate concept of quantile, based on a directional version of Koenker and Bassett's traditional regression quantiles, is introduced for multivariate location and multiple-output regression problems. In their empirical version, those quantiles can be Computed efficiently via linear programming techniques. Consistency, Bahadur representation and asymptotic normality results are established. Most importantly, the contours generated by those quantiles are shown to coincide with the classical halfspace depth contours associated with the name of Tukey. This relation does not only allow for efficient depth contour computations by means of parametric linear programming, but also for transferring from the quantile to the depth universe such asymptotic results as Bahadur representations. Finally, linear programming duality opens the way to promising developments in depth-related multivariate rank-based inference.",WOS:000275510800001,ANNALS OF STATISTICS,"['LOCATION DEPTH', 'GROWTH CHARTS', 'CONTOURS']",MULTIVARIATE QUANTILES AND MULTIPLE-OUTPUT REGRESSION QUANTILES: FROM L-1 OPTIMIZATION TO HALFSPACE DEPTH,2010
134,"This paper studies statistical aggregation procedures in the regression setting. A motivating factor is the existence of many different methods of estimation, leading to possibly competing estimators. We consider here three different types of aggregation: model selection (MS) aggregation, convex (C) aggregation and linear (L) aggregation. The objective of (MS) is to select the optimal single estimator from the list; that of (C) is to select the optimal convex combination of the given estimators; and that of (L) is to select the optimal linear combination of the given estimators. we are interested in evaluating the rates of convergence of the excess risks of the estimators obtained by these procedures. Our approach is motivated by recently published minimax results [Nemirovski, A. (2000). Topics in non-parametric statistics. Lectures on Probability Theory and Statistics (Saint-Flour, 1998). Lecture Notes in Math. 1738 85-277. Springer, Berlin; Tsybakov, A. B. (2003). Optimal rates of aggregation. Learning Theory and Kernel Machines. Lecture Notes in Artificial Intelligence 2777 303-313. Springer, Heidelberg]. There exist competing aggregation procedures achieving optimal convergence rates for each of the (MS), (C) and (L) cases separately. Since these procedures are not directly comparable with each other, we suggest an alternative solution. We prove that all three optimal rates, as well as those for the newly introduced (S) aggregation (subset selection), are nearly achieved via a single ""universal"" aggregation procedure. The procedure consists of mixing the initial estimators with weights obtained by penalized least squares. Two different penalties are considered: one of them is of the BIC type, the second one is a data-dependent l(1)-type penalty.",WOS:000249568000013,ANNALS OF STATISTICS,"['MODEL SELECTION', 'NONPARAMETRIC REGRESSION', 'ADAPTIVE REGRESSION', 'ORACLE INEQUALITIES', 'COMPLEXITIES', 'ESTIMATORS', 'BOUNDS', 'LASSO']",Aggregation for gaussian regression,2007
135,"Comparing the results obtained by two or more algorithms in a set of problems is a central task in areas such as machine learning or optimization. Drawing conclusions from these comparisons may require the use of statistical tools such as hypothesis testing. There are some interesting papers that cover this topic. In this manuscript we present scmamp, an R package aimed at being a tool that simplifies the whole process of analyzing the results obtained when comparing algorithms, from loading the data to the production of plots and tables.
Comparing the performance of different algorithms is an essential step in many research and practical computational works. When new algorithms are proposed, they have to be compared with the state of the art. Similarly, when an algorithm is used for a particular problem, its performance with different sets of parameters has to be compared, in order to tune them for the best results.
When the differences are very clear (e.g., when an algorithm is the best in all the problems used in the comparison), the direct comparison of the results may be enough. However, this is an unusual situation and, thus, in most situations a direct comparison may be misleading and not enough to draw sound conclusions; in those cases, the statistical assessment of the results is advisable.
The statistical comparison of algorithms in the context of machine learning has been covered in several papers. In particular, the tools implemented in this package are those presented in Demsar (2006); Garcia and Herrera (2008); Garcia et al. (2010). Another good review that covers, among other aspects, the statistical assessment of the results in the context of supervised classification can be found in Santafe et al. (2015).",WOS:000385276100018,R JOURNAL,"['DATA SETS', 'CLASSIFIERS']",scmamp: Statistical Comparison of Multiple Algorithms in Multiple Problems,2016
136,"We present a path algorithm for the generalized lasso problem. This problem penalizes the l(1) norm of a matrix D times the coefficient vector, and has a wide range of applications, dictated by the choice of D. Our algorithm is based on solving the dual of the generalized lasso, which greatly facilitates computation of the path. For D = I (the usual lasso), we draw a connection between our approach and the well-known LARS algorithm. For an arbitrary D, we derive an unbiased estimate of the degrees of freedom of the generalized lasso fit. This estimate turns out to be quite intuitive in many applications.",WOS:000293716500001,ANNALS OF STATISTICS,"['REGRESSION', 'REGULARIZATION', 'SMOOTHNESS', 'SHRINKAGE', 'SELECTION']",THE SOLUTION PATH OF THE GENERALIZED LASSO,2011
137,"This article describes a new class of prior distributions for nonparametric function estimation. The unknown function is modeled as a limit of weighted sums of kernels or generator functions indexed by continuous parameters that control local and global features such as their translation, dilation, modulation and shape. Levy random fields and their stochastic integrals are employed to induce prior distributions for the unknown functions or, equivalently, for the number of kernels and for the parameters governing their features. Scaling, shape, and other features of the generating functions are location-specific to allow quite different function properties in different parts of the space, as with wavelet bases and other methods employing overcomplete dictionaries. We provide conditions under which the stochastic expansions converge in specified Besov or Sobolev norms. Under a Gaussian error model, this may be viewed as a sparse regression problem, with regularization induced via the Levy random field prior distribution. Posterior inference for the unknown functions is based on a reversible jump Markov chain Monte Carlo algorithm. We compare the Levy Adaptive Regression Kernel (LARK) method to wavelet-based methods using some of the standard test functions, and illustrate its flexibility and adaptability in nonstationary applications.",WOS:000296995500004,ANNALS OF STATISTICS,"['BAYESIAN VARIABLE SELECTION', 'DIRICHLET PROCESS PRIOR', 'NONPARAMETRIC PROBLEMS', 'DISTRIBUTIONS', 'SHRINKAGE', 'MIXTURES', 'DENSITY']",STOCHASTIC EXPANSIONS USING CONTINUOUS DICTIONARIES: LEVY ADAPTIVE REGRESSION KERNELS,2011
138,"Starting with the common origins of biometrics and psychometrics at the beginning of the twentieth century, the paper compares and contrasts subsequent developments, informed by the author's 35 years at Rothamsted Experimental Station followed by a period with the data theory group in Leiden and thereafter. Although the methods used by biometricians and psychometricians have much in common, there are important differences arising from the different fields of study. Similar differences arise wherever data are generated and may be regarded as a major driving force in the development of statistical ideas.",WOS:000389126900001,JOURNAL OF STATISTICAL SOFTWARE,"['SYSTEMS', 'POINTS']","Biometrics and Psychometrics: Origins, Commonalities and Differences",2016
139,"In standard fMRI analysis all voxels are tested in a massive univariate approach, that is, each voxel is tested independently. This requires stringent corrections for multiple comparisons to control the number of false positive tests (i.e., marking voxels as active while they are actually not). As a result, fMRI analyses may suffer from low power to detect activation, especially in studies with high levels of noise in the data, for example developmental or single-subject studies. Activated region fitting (ARF) yields a solution by modeling fMRI data by multiple Gaussian shaped regions. ARF only requires a small number of parameters and therefore has increased power to detect activation. If required, the estimated regions can be directly used as regions of interest in a functional connectivity analysis. ARF is implemented in the R package arf3DS4. In this paper ARF and its implementation are described and illustrated with an example.",WOS:000296719900001,JOURNAL OF STATISTICAL SOFTWARE,"['FUNCTIONAL CONNECTIVITY', 'PARAMETERIZED REGIONS', 'IMAGE-ANALYSIS', 'ACTIVATION', 'BRAIN', 'MODEL', 'SYSTEMS', 'NETWORK']",arf3DS4: An Integrated Framework for Localization and Connectivity Analysis of fMRI Data,2011
140,"Generalized linear models and the quasi-likelihood method extend the ordinary regression models to accommodate more general conditional distributions of the response. Nonparametric methods need no explicit parametric specification, and the resulting model is completely determined by the data themselves. However, nonparametric estimation schemes generally have a slower convergence rate such as the local polynomial smoothing estimation of nonparametric generalized linear models studied in Fan, Heckman and Wand [J. Amer Statist. Assoc. 90 (1995) 141-150]. In this work, we propose a unified family of parametrically-guided nonparametric estimation schemes. This combines the merits of both parametric and nonparametric approaches and enables us to incorporate prior knowledge. Asymptotic results and numerical simulations demonstrate the improvement of our new estimation schemes over the original nonparametric counterpart.",WOS:000271673700016,ANNALS OF STATISTICS,"['GENERALIZED LINEAR-MODELS', 'NONPARAMETRIC REGRESSION', 'DENSITY-ESTIMATION', 'REDUCING VARIANCE', 'ESTIMATORS', 'SELECTION']",LOCAL QUASI-LIKELIHOOD WITH A PARAMETRIC GUIDE,2009
141,"We assume i.i.d. data sampled from a mixture distribution with K components along fixed d-dimensional linear subspaces and an additional outlier component. For p > 0, we study the simultaneous recovery of the K fixed subspaces by minimizing the l(p)-averaged distances of the sampled data points from any K subspaces. Under some conditions, we show that if 0 < p <= 1, then all underlying subspaces can be precisely recovered by l(p) minimization with overwhelming probability. On the other hand, if K > 1 and p > 1, then the underlying subspaces cannot be recovered or even nearly recovered by l(p) minimization. The results of this paper partially explain the successes and failures of the basic approach of l(p) energy minimization for modeling data by multiple subspaces.",WOS:000299186500019,ANNALS OF STATISTICS,"['MIXED DATA', 'SEGMENTATION']",ROBUST RECOVERY OF MULTIPLE SUBSPACES BY GEOMETRIC l(p) MINIMIZATION,2011
142,,WOS:000384397200002,ANNALS OF STATISTICS,"['CONFIDENCE-INTERVALS', 'EDGEWORTH EXPANSION', 'STATIONARY OBSERVATIONS', 'EMPIRICAL LIKELIHOOD', 'STATISTICS', 'JACKKNIFE', 'DENSITY', 'SIMULATIONS', 'REGRESSION', 'ACCURACY']",PETER HALL'S CONTRIBUTIONS TO THE BOOTSTRAP,2016
143,"We consider kernel estimation of marginal densities and regression functions of stationary processes. It is shown that for a wide class of time series, with proper centering and scaling, the maximum deviations of kernel density and regression estimates are asymptotically Gumbel. Our results substantially generalize earlier ones which were obtained under independence or beta mixing assumptions. The asymptotic results can be applied to assess patterns of marginal densities or regression functions via the construction of simultaneous confidence bands for which one can perform goodness-of-fit tests. As an application, we construct simultaneous confidence bands for drift and volatility functions in a dynamic short-term rate model for the U.S. Treasury yield curve rates data.",WOS:000280359400015,ANNALS OF STATISTICS,"['CONFIDENCE BANDS', 'TERM STRUCTURE', 'DENSITY-ESTIMATION', 'WEAK DEPENDENCE', 'PARTIAL SUMS', 'REGRESSION', 'APPROXIMATION', 'MODELS', 'PROBABILITIES', 'CONVOLUTIONS']",SIMULTANEOUS NONPARAMETRIC INFERENCE OF TIME SERIES,2010
144,"Contingency table analysis routinely relies on log-linear models, with latent structure analysis providing a common alternative. Latent structure models lead to a reduced rank tensor factorization of the probability mass function for multivariate categorical data, while log-linear models achieve dimensionality reduction through sparsity. Little is known about the relationship between these notions of dimensionality reduction in the two paradigms. We derive several results relating the support of a log-linear model to nonnegative ranks of the associated probability tensor. Motivated by these findings, we propose a new collapsed Tucker class of tensor decompositions, which bridge existing PARAFAC and Tucker decompositions, providing a more flexible framework for parsimoniously characterizing multivariate categorical data. Taking a Bayesian approach to inference, we illustrate empirical advantages of the new decompositions.",WOS:000396804900001,ANNALS OF STATISTICS,"['GRAPHICAL MODELS', 'CONTINGENCY-TABLES', 'BAYESIAN NETWORKS', 'CATEGORICAL-DATA', 'FACTORIZATIONS', 'APPROXIMATIONS', 'EQUATIONS', 'SELECTION', 'GEOMETRY']",TENSOR DECOMPOSITIONS AND SPARSE LOG-LINEAR MODELS,2017
145,"A common feature of many magnetic resonance image (MRI) data processing methods is the voxel-by-voxel (a voxel is a volume element) manner in which the processing is performed. In general, however, MRI data are expected to exhibit some level of spatial correlation, rendering an independent-voxels treatment inefficient in its use of the data. Bayesian random effect models are expected to be more efficient owing to their information-borrowing behaviour.
To illustrate the Bayesian random effects approach, this paper outlines a Markov chain Monte Carlo (MCMC) analysis of a perfusion MRI dataset, implemented in R using the BRugs package. BRugs provides an interface to WinBUGS and its GeoBUGS add-on. WinBUGS is a widely used programme for performing MCMC analyses, with a focus on Bayesian random effect models. A simultaneous modeling of both voxels (restricted to a region of interest) and multiple subjects is demonstrated. Despite the low signal-to-noise ratio in the magnetic resonance signal intensity data, useful model signal intensity profiles are obtained. The merits of random effects modeling are discussed in comparison with the alternative approaches based on region-of-interest averaging and repeated independent voxels analysis.
This paper focus on perfusion MRI for the purpose of illustration, the main proposition being that random effects modeling is expected to be beneficial in many other MRI applications in which the signal-to-noise ratio is a limiting factor.",WOS:000296228300001,JOURNAL OF STATISTICAL SOFTWARE,"['PHARMACOKINETIC MODELS', 'PRIOR DISTRIBUTIONS', 'ADC', 'MRI']",Markov Chain Monte Carlo Random Effects Modeling in Magnetic Resonance Image Processing Using the BRugs Interface to WinBUGS,2011
146,"The mapmisc package provides functions for visualising geospatial data, including fetching background map layers, producing colour scales and legends, and adding scale bars and orientation arrows to plots. Background maps are returned in the coordinate reference system of the dataset supplied, and inset maps and direction arrows reflect the map projection being plotted. This is a ""light weight"" package having an emphasis on simplicity and ease of use.",WOS:000385276100006,R JOURNAL,,"Maps, Coordinate Reference Systems and Visualising Geographic Data with mapmisc",2016
147,"Moult is the process by which birds replace their feathers. It is a costly process in terms of energy and reduced flight ability but necessary for the maintenance of the plumage and its functions. Because birds generally avoid to moult while engaged with other energy demanding activities such as breeding and migration, the analysis of moult data gives insight into how birds fit this life stage into the annual cycle, on time constraints in the annual cycle, and on the effects of environmental variables on the timing of moult. The analysis of moult data requires non-standard statistical techniques. More than 20 years ago Underhill and Zucchini developed a likelihood approach for estimating duration, mean start date and variation in start date of a population of moulting birds. However, use of these models has been limited, mainly due to the lack of user-friendly software. The moult package for R implements the Underhill-Zucchini models, allowing the user to specify moult models in a regression type formula. In addition the functions allow the moult parameters (duration, and mean and variation in start date) to depend on explanatory variables. We here describe the package, give a brief summary of the theory and illustrate the models on two data sets included in the package.",WOS:000315019500001,JOURNAL OF STATISTICAL SOFTWARE,"['AVIAN PRIMARY MOLT', 'REDSHANK TRINGA-TOTANUS', 'DURATION', 'MODEL']",moult: An R Package to Analyze Moult in Birds,2013
148,"This paper describes the use of GLDEX in R to fit distributions to empirical data using the discretized and maximum likelihood methods. The GLDEX package also provides diagnostic tests to examine the quality of fit through the resample Kolmogorov-Smirnoff test, quantile plots and comparison of the mean, variance, skewness and kurtosis between the empirical data and the fitted distribution.",WOS:000252429000001,JOURNAL OF STATISTICAL SOFTWARE,,Fitting single and mixture of generalized lambda distributions to data via discretized and miximum likelihood methods: GLDEX in R,2007
149,"It is known that all resolution IV regular 2(n-m) designs of run size N = 2(n-m) where 5N/16 < n < N/2 must be projections of the maximal even design with N/2 factors and, therefore, are even designs. This paper derives a general and explicit relationship between the wordlength pattern of any even 2(n-m) design and that of its complement in the maximal even design. Using these identities, we identify some (weak) minimum aberration 2(n-m) designs of resolution IV and the structures of their complementary designs. Based on these results, several families of minimum aberration 2(n-m) designs of resolution IV are constructed.",WOS:000271673500018,ANNALS OF STATISTICS,"['FRACTIONAL FACTORIAL-DESIGNS', 'BINARY LINEAR CODES', 'CAPS']",SOME RESULTS ON 2(n-m) DESIGNS OF RESOLUTION IV WITH (WEAK) MINIMUM ABERRATION,2009
150,"Most results in nonparametric regression theory are developed only for the case of additive noise. In such a setting many smoothing techniques including wavelet thresholding methods have been developed and shown to be highly adaptive. In this paper we consider nonparametric regression in exponential families with the main focus on the natural exponential families with a quadratic variance function, which include, for example, Poisson regression, binomial regression and gamma regression. We propose a unified approach of using a mean-matching variance stabilizing transformation to turn the relatively complicated problem of nonparametric regression in exponential families into a standard homoscedastic Gaussian regression problem. Then in principle any good nonparametric Gaussian regression procedure can be applied to the transformed data. To illustrate our general methodology, in this paper we use wavelet block thresholding to construct the final estimators of the regression function. The procedures are easily implementable. Both theoretical and numerical properties of the estimators are investigated. The estimators are shown to enjoy a high degree of adaptivity and spatial adaptivity with near-optimal asymptotic performance over a wide range of Besov spaces. The estimators also perform well numerically.",WOS:000280359400004,ANNALS OF STATISTICS,"['QUADRATIC VARIANCE FUNCTIONS', 'WAVELET SHRINKAGE', 'TRANSFORMATIONS', 'INEQUALITY', 'MODELS']",NONPARAMETRIC REGRESSION IN EXPONENTIAL FAMILIES,2010
151,"We present a new version (>= 2.0) of the hglm package for fitting hierarchical generalized linear models (HGLMs) with spatially correlated random effects. CAR() and SAR() families for conditional and simultaneous autoregressive random effects were implemented. Eigen decomposition of the matrix describing the spatial structure (e.g., the neighborhood matrix) was used to transform the CAR/SAR random effects into an independent, but heteroscedastic, Gaussian random effect. A linear predictor is fitted for the random effect variance to estimate the parameters in the CAR and SAR models. This gives a computationally efficient algorithm for moderately sized problems.",WOS:000368551800002,R JOURNAL,"['GENERALIZED LINEAR-MODELS', 'PACKAGE', 'INLA']",Fitting Conditional and Simultaneous Autoregressive Spatial Models in hglm,2015
152,"An important task in astroparticle physics is the detection of periodicities in irregularly sampled time series, called light curves. The classic Fourier periodogram cannot deal with irregular sampling and with the measurement accuracies that are typically given for each observation of a light curve. Hence, methods to fit periodic functions using weighted regression were developed in the past to calculate periodograms.
We present the R package RobPer which allows to combine different periodic functions and regression techniques to calculate periodograms. Possible regression techniques are least squares, least absolute deviations, least trimmed squares, M-, S- and tau-regression. Measurement accuracies can be taken into account including weights. Our periodogram function covers most of the approaches that have been tried earlier and provides new model-regression-combinations that have not been used before.
To detect valid periods, RobPer applies an outlier search on the periodogram instead of using fixed critical values that are theoretically only justified in case of least squares regression, independent periodogram bars and a null hypothesis allowing only normal white noise. Finally, the package also includes a generator to generate artificial light curves.",WOS:000373918300001,JOURNAL OF STATISTICAL SOFTWARE,"['LOMB-SCARGLE PERIODOGRAM', 'UNEQUALLY-SPACED DATA', 'SPECTRAL-ANALYSIS', 'PERIODICITY DETECTION', 'TIME-SERIES', 'RAY SOURCES', 'SEARCH', 'MINIMIZATION', 'BURST']",RobPer: An R Package to Calculate Periodograms for Light Curves Based on Robust Regression,2016
153,"We consider the problem of reliably finding filaments in point clouds. Realistic data sets often have numerous filaments of various sizes and shapes. Statistical techniques exist for finding one (or a few) filaments but these methods do not handle noisy data sets with many filaments. Other methods can be found in the astronomy literature but they do not have rigorous statistical guarantees. We propose the following method. Starting at each data point we construct the steepest ascent path along a kernel density estimator. We locate filaments by finding regions where these paths are highly concentrated. Formally, we define the density of these paths and we construct a consistent estimator of this path density.",WOS:000271673500006,ANNALS OF STATISTICS,"['PRINCIPAL CURVES', 'COSMIC FILAMENTS', 'POINT PROCESS', 'EXTRACTION', 'SKELETON', 'PROBE']",ON THE PATH DENSITY OF A GRADIENT FIELD,2009
154,"Recent advances in aquatic ecosystem modelling have particularly focused on trophic network analysis through trophodynamic models. We present here a R package devoted to a recently developed model, EcoTroph. This model enables the analysis of aquatic ecological networks and the related impacts of fisheries. It was available through a plug-in in the well-known Ecopath with Ecosim software or through implementations in Excel sheets. The R package we developed simplifies the access to the EcoTroph model and offers a new interfacing between two widely used software, Ecopath and R.",WOS:000321944400011,R JOURNAL,,An Introduction to the EcoTroph R Package: Analyzing Aquatic Ecosystem Trophic Networks,2013
155,"We consider the convolution model where i.i.d. random variables Xi having unknown density f are observed with additive i.i.d. noise, independent of the X's. We assume that the density f belongs to either a Sobolev class or a class of supersmooth functions. The noise distribution is known and its characteristic function decays either polynornially or exponentially asymptotically.
We consider the problem of goodness-of-fit testing in the convolution model. We prove upper bounds for the risk of a test statistic derived from a kernel estimator of the quadratic functional f f 2 based on indirect observations. When the unknown density is smoother enough than the noise density, we prove that this estimator is n(-1/2) consistent, asymptotically normal and efficient (for the variance we compute). Otherwise, we give nonparametric upper bounds for the risk of the same estimator.
We give an approach unifying the proof of nonparametric minimax lower bounds for both problems. We establish them for Sobolev densities and for supersmooth densities less smooth than exponential noise. In the two setups we obtain exact testing constants associated with the asymptotic minimax rates.",WOS:000251096100003,ANNALS OF STATISTICS,"['SQUARED DENSITY DERIVATIVES', 'INTEGRAL FUNCTIONALS', 'EFFICIENT ESTIMATION', 'BAYESIAN-ANALYSIS', 'UNKNOWN NUMBER', 'DECONVOLUTION', 'CONVERGENCE', 'COMPONENTS', 'MIXTURES', 'WAVELETS']",Goodness-of-fit testing and quadratic functional estimation from indirect observations,2007
156,"When modeling economic relationships it is increasingly common to encounter data sampled at different frequencies. We introduce the R package midasr which enables estimating regression models with variables sampled at different frequencies within a MIDAS regression framework put forward in work by Ghysels, Santa-Clara, and Valkanov (2002). In this article we define a general autoregressive MIDAS regression model with multiple variables of different frequencies and show how it can be specified using the familiar R formula interface and estimated using various optimization methods chosen by the researcher. We discuss how to check the validity of the estimated model both in terms of numerical convergence and statistical adequacy of a chosen regression specification, how to perform model selection based on a information criterion, how to assess forecasting accuracy of the MIDAS regression model and how to obtain a forecast aggregation of different MIDAS regression models. We illustrate the capabilities of the package with a simulated MIDAS regression model and give two empirical examples of application of MIDAS regression.",WOS:000389072100001,JOURNAL OF STATISTICAL SOFTWARE,"['VOLATILITY', 'VARIABLES']",Mixed Frequency Data Sampling Regression Models: The R Package midasr,2016
157,"We address the problem of density estimation with L(s)(-)loss by selection of kernel estimators. We develop a selection procedure and derive corresponding L(s)-risk oracle inequalities. It is shown that the proposed selection rule leads to the estimator being minimax adaptive over a scale of the anisotropic Nikol'skii classes. The main technical tools used in our derivations are uniform bounds on the L(s)-norms of empirical processes developed recently by Goldenshluger and Lepski [Ann. Probab. (2011), to appear].",WOS:000293716500010,ANNALS OF STATISTICS,,BANDWIDTH SELECTION IN KERNEL DENSITY ESTIMATION: ORACLE INEQUALITIES AND ADAPTIVE MINIMAX OPTIMALITY,2011
158,"In a regression setup with deterministic design, we study the pure aggregation problem and introduce a natural extension from the Gaussian distribution to distributions in the exponential family. While this extension bears strong connections with generalized linear models, it does not require identifiability of the parameter or even that the model on the systematic component is true. It is shown that this problem can be solved by constrained and/or penalized likelihood maximization and we derive sharp oracle inequalities that hold both in expectation and with high probability. Finally all the bounds are proved to be optimal in a minimax sense.",WOS:000307608000001,ANNALS OF STATISTICS,"['ORACLE INEQUALITIES', 'STATISTICAL VIEW', 'OPTIMAL RATES', 'REGRESSION', 'PERSISTENCE', 'SELECTION', 'CLASSIFICATION']",KULLBACK-LEIBLER AGGREGATION AND MISSPECIFIED GENERALIZED LINEAR MODELS,2012
159,"We consider a joint asymptotic framework for studying semi-nonparametric regression models where (finite-dimensional) Euclidean parameters and (infinite-dimensional) functional parameters are both of interest. The class of models in consideration share a partially linear structure and are estimated in two general contexts: (i) quasi-likelihood and (ii) true likelihood. We first show that the Euclidean estimator and (pointwise) functional estimator, which are re-scaled at different rates, jointly converge to a zero-mean Gaussian vector. This weak convergence result reveals a surprising joint asymptotics phenomenon: these two estimators are asymptotically independent. A major goal of this paper is to gain first-hand insights into the above phenomenon. Moreover, a likelihood ratio testing is proposed for a set of joint local hypotheses, where a new version of the Wilks phenomenon [Ann. Math. Stat. 9 (1938) 60-62; Ann. Statist. 1 (2001) 153-193] is unveiled. A novel technical tool, called a joint Bahadur representation, is developed for studying these joint asymptotics results.",WOS:000355768700014,ANNALS OF STATISTICS,"['QUASI-LIKELIHOOD ESTIMATION', 'ESTIMATORS', 'INFERENCE', 'RATIO']",JOINT ASYMPTOTICS FOR SEMI-NONPARAMETRIC REGRESSION MODELS WITH PARTIALLY LINEAR STRUCTURE,2015
160,"Hierarchical autocorrelation in the error term of linear models arises when sampling units are related to each other according to a tree. The residual covariance is parametrized using the tree-distance between sampling units. When observations are modeled using an Ornstein-Uhlenbeck (OU) process along the tree, the autocorrelation between two tips decreases exponentially with their tree distance. These models are most often applied in evolutionary biology, when tips represent biological species and the OU process parameters represent the strength and direction of natural selection. For these models, we show that the mean is not microergodic: no estimator can ever be consistent for this parameter and provide a lower bound for the variance of its MLE. For covariance parameters, we give a general sufficient condition ensuring microergodicity. This condition suggests that some parameters may not be estimated at the same rate as others. We show that, indeed, maximum likelihood estimators of the autocorrelation parameter converge at a slower rate than that of generally microergodic parameters. We showed this theoretically in a symmetric tree asymptotic framework and through simulations on a large real tree comprising 4507 mammal species.",WOS:000320488200020,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD ESTIMATOR', 'SPATIAL PREDICTION', 'EVOLUTION', 'MAMMALS', 'COVARIANCE', 'STATISTICS', 'PATTERNS']",ASYMPTOTIC THEORY WITH HIERARCHICAL AUTO CORRELATION: ORNSTEIN-UHLENBECK TREE MODELS,2013
161,"The problem of estimating the mean of random functions based on discretely sampled data arises naturally in functional data analysis. In this paper, we study optimal estimation of the mean function under both common and independent designs. Minimax rates of convergence are established and easily implementable rate-optimal estimators are introduced. The analysis reveals interesting and different phase transition phenomena in the two cases. Under the common design, the sampling frequency solely determines the optimal rate of convergence when it is relatively small and the sampling frequency has no effect on the optimal rate when it is large. On the other hand, under the independent design, the optimal rate of convergence is determined jointly by the sampling frequency and the number of curves when the sampling frequency is relatively small. When it is large, the sampling frequency has no effect on the optimal rate. Another interesting contrast between the two settings is that smoothing is necessary under the independent design, while, somewhat surprisingly, it is not essential under the common design.",WOS:000299186500006,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'LONGITUDINAL DATA', 'CURVES']",OPTIMAL ESTIMATION OF THE MEAN FUNCTION BASED ON DISCRETELY SAMPLED FUNCTIONAL DATA: PHASE TRANSITION,2011
162,"This article presents the CRAN Task View on Web Technologies. We describe the most important aspects of Web Technologies and Web Scraping and list some of the packages that are currently available on CRAN. Finally, we plot the network of Web Technology related package dependencies.",WOS:000343788100019,R JOURNAL,,Web Technologies Task View,2014
163,"We consider a Bayesian approach to variable selection in the presence of high dimensional covariates based on a hierarchical model that places prior distributions on the regression coefficients as well as on the model space. We adopt the well-known spike and slab Gaussian priors with a distinct feature, that is, the prior variances depend on the sample size through which appropriate shrinkage can be achieved. We show the strong selection consistency of the proposed method in the sense that the posterior probability of the true model converges to one even when the number of covariates grows nearly exponentially with the sample size. This is arguably the strongest selection consistency result that has been available in the Bayesian variable selection literature; yet the proposed method can be carried out through posterior sampling with a simple Gibbs sampler. Furthermore, we argue that the proposed method is asymptotically similar to model selection with the Lo penalty. We also demonstrate through empirical work the fine performance of the proposed approach relative to some state of the art alternatives.",WOS:000336888400018,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'GENERALIZED LINEAR-MODELS', 'DIMENSIONAL FEATURE SPACE', 'REGRESSION SHRINKAGE', 'ORACLE PROPERTIES', 'DANTZIG SELECTOR', 'SLAB REGRESSION', 'LASSO', 'SPIKE', 'CONSISTENCY']",BAYESIAN VARIABLE SELECTION WITH SHRINKING AND DIFFUSING PRIORS,2014
164,"In this article the MCPMod package for the R programming environment will be introduced. It implements a recently developed methodology for the design and analysis of dose-response studies that combines aspects of multiple comparison procedures and modeling approaches (Bretz et al. 2005). The MCPMod package provides tools for the analysis of dose finding trials, as well as a variety of tools necessary to plan an experiment to be analyzed using the MCP-Mod methodology.",WOS:000263825100001,JOURNAL OF STATISTICAL SOFTWARE,['COMBINING MULTIPLE COMPARISONS'],MCPMod: An R Package for the Design and Analysis of Dose-Finding Studies,2009
165,"Probability models on graphs are becoming increasingly important in many applications, but statistical tools for fitting such models are not yet well developed. Here we propose a general method of moments approach that can be used to fit a large class of probability models through empirical counts of certain patterns in a graph. We establish some general asymptotic properties of empirical graph moments and prove consistency of the estimates as the graph size grows for all ranges of the average degree including Omega (1). Additional results are obtained for the important special case of degree distributions.",WOS:000299186500004,ANNALS OF STATISTICS,"['STOCHASTIC BLOCKMODELS', 'SOCIAL NETWORKS', 'GRAPHS', 'PREDICTION']",THE METHOD OF MOMENTS AND DEGREE DISTRIBUTIONS FOR NETWORK MODELS,2011
166,"We show that empirical risk minimization procedures and regularized empirical risk minimization procedures satisfy nonexact oracle inequalities in an unbounded framework, under the assumption that the class has a subexponential envelope function. The main novelty, in addition to the boundedness assumption free setup, is that those inequalities can yield fast rates even in situations in which exact oracle inequalities only hold with slower rates.
We apply these results to show that procedures based on l(1) and nuclear norms regularization functions satisfy oracle inequalities with a residual term that decreases like 1/n forevery L-q-loss functions (q >= 2), while only assuming that the tail behavior of the input and output variables are well behaved. In particular, no RIP type of assumption or ""incoherence condition"" are needed to obtain fast residual terms in those setups. We also apply these results to the problems of convex aggregation and model selection.",WOS:000307608000008,ANNALS OF STATISTICS,"['EMPIRICAL RISK MINIMIZATION', 'DANTZIG SELECTOR', 'LOWER BOUNDS', 'LASSO', 'SPARSITY', 'PENALIZATION', 'ALGORITHM', 'OPERATORS', 'RECOVERY']",GENERAL NONEXACT ORACLE INEQUALITIES FOR CLASSES WITH A SUBEXPONENTIAL ENVELOPE,2012
167,"There is an inherent relationship between two-sided hypothesis tests and confidence intervals. A series of two-sided hypothesis tests may be inverted to obtain the matching 100(1-alpha)% confidence interval defined as the smallest interval that contains all point null parameter values that would not be rejected at the alpha level. Unfortunately, for discrete data there are several different ways of defining two-sided exact tests and the most commonly used two-sided exact tests are defined one way, while the most commonly used exact confidence intervals are inversions of tests defined another way. This can lead to inconsistencies where the exact test rejects but the exact confidence interval contains the null parameter value. The packages exactci and exact2x2 provide several exact tests with the matching confidence intervals avoiding these inconsistencies as much as possible. Examples are given for binomial and Poisson parameters and both paired and unpaired 2 x 2 tables.",WOS:000208589900009,R JOURNAL,,Two-sided Exact Tests and Matching Confidence Intervals for Discrete Data,2010
168,"For planar landmark based shapes, taking into account the non-Euclidean geometry of the shape space, a statistical test for a common mean first geodesic principal component (GPC) is devised which rests on one of two asymptotic scenarios. For both scenarios, strong consistency and central limit theorems are established, along with an algorithm for the computation of a Ziezold mean geodesic. In application, this allows to verify the geodesic hypothesis for leaf growth of Canadian black poplars and to discriminate genetically different trees by observations of leaf shape growth over brief time intervals. With a test based on Procrustes tangent space coordinates, not involving the shape space's curvature, neither can be achieved.",WOS:000291183300015,ANNALS OF STATISTICS,"['EXTRINSIC SAMPLE MEANS', 'LIE GROUP-ACTIONS', 'MANIFOLDS', 'SPACES', 'UNIQUENESS', 'CONVEXITY', 'EXISTENCE', 'LOCATION', 'PCA']",INTRINSIC INFERENCE ON THE MEAN GEODESIC OF PLANAR SHAPES AND TREE DISCRIMINATION BY LEAF GROWTH,2011
169,"In the past decades, weak convergence theory for stochastic processes has become a standard tool for analyzing the asymptotic properties of various statistics. Routinely, weak convergence is considered in the space of bounded functions equipped with the supremum metric. However, there are cases when weak convergence in those spaces fails to hold. Examples include empirical copula and tail dependence processes and residual empirical processes in linear regression models in case the underlying distributions lack a certain degree of smoothness. To resolve the issue, a new metric for locally bounded functions is introduced and the corresponding weak convergence theory is developed. Convergence with respect to the new metric is related to epi- and hypo-convergence and is weaker than uniform convergence. Still, for continuous limits, it is equivalent to locally uniform convergence, whereas under mild side conditions, it implies L-P convergence. For the examples mentioned above, weak convergence with respect to the new metric is established in situations where it does not occur with respect to the supremum distance. The results are applied to obtain asymptotic properties of resampling procedures and goodness-of-fit tests.",WOS:000342481700013,ANNALS OF STATISTICS,"['COPULA PROCESSES', 'TAIL DEPENDENCE', 'STOCHASTIC PROCESSES', 'LINEAR-REGRESSION', 'ASYMPTOTICS', 'BOOTSTRAP', 'ESTIMATOR', 'MODELS']",WHEN UNIFORM WEAK CONVERGENCE FAILS: EMPIRICAL PROCESSES FOR DEPENDENCE FUNCTIONS AND RESIDUALS VIA EPI- AND HYPOGRAPHS,2014
170,"Urn models have been widely studied and applied in both scientific and social science disciplines. In clinical studies, the adoption of urn models in treatment allocation schemes has proved to be beneficial to researchers, by providing more efficient clinical trials, and to patients, by increasing the likelihood of receiving the better treatment. In this paper, we propose a new and general class of immigrated urn (IMU) models that incorporates the immigration mechanism into the urn process. Theoretical properties are developed and the advantages of the IMU models are discussed. In general, the IMU models have smaller variabilities than the classical urn models, yielding more powerful statistical inferences in applications. Illustrative examples are presented to demonstrate the wide applicability of the IMU models. The proposed IMU framework, including many popular classical urn models not only offers a unify perspective for us to comprehend the urn process, but also enables us to generate several novel urn models with desirable properties.",WOS:000288183800021,ANNALS OF STATISTICS,"['RESPONSE-ADAPTIVE RANDOMIZATION', 'IMPLEMENTING OPTIMAL ALLOCATION', 'CENTRAL LIMIT-THEOREMS', 'CLINICAL-TRIALS', 'ASYMPTOTIC PROPERTIES', 'BRANCHING-PROCESSES', 'DELAYED-RESPONSE', 'MEDICAL TRIALS', 'DESIGNS', 'VARIABILITY']",IMMIGRATED URN MODELS-THEORETICAL PROPERTIES AND APPLICATIONS,2011
171,,WOS:000312899000006,ANNALS OF STATISTICS,,DISCUSSION: LATENT VARIABLE GRAPHICAL MODEL SELECTION VIA CONVEX OPTIMIZATION,2012
172,"Nonresponse is common in Surveys. When the response probability of a survey variable Y depends on Y through ail observed auxiliary categorical variable Z (i.e., the response probability of Y is conditionally independent of Y given Z), a simple method often used in practice is to use Z categories as imputation cells and construct estimators by imputing nonrespondents or reweighting respondents within each imputation cell. This simple method, however, is inefficient when some Z categories have small sizes Laid ad hoc methods are often applied to collapse small imputation cells. Assuming a parametric model on the conditional probability of Z given Y and a nonparametric model oil the distribution of Y, we develop a pseudo empirical likelihood method to provide more efficient survey estimators. Our method avoids any ad hoc collapsing small Z categories, since reweighting or imputation is done across Z categories. Asymptotic distributions for estimators of population means based on the pseudo empirical likelihood method are derived. For variance estimation, we consider a bootstrap procedure and its consistency is established. Some simulation results are provided to assess the finite sample performance of the proposed estimators.",WOS:000263129000014,ANNALS OF STATISTICS,"['RATIO CONFIDENCE-INTERVALS', 'AUXILIARY INFORMATION', 'MISSING RESPONSE', 'COMPLEX SURVEYS', 'INFERENCE', 'IMPUTATION']",A PSEUDO EMPIRICAL LIKELIHOOD APPROACH FOR STRATIFIED SAMPLES WITH NONRESPONSE,2009
173,"Knowledge of the environmental features affecting habitat selection by animals is important for designing wildlife management and conservation policies. The package adehabitat for the R software is designed to provide a computing environment for the analysis and modelling of such relationships. This paper focuses on the preliminary steps of data exploration and analysis, performed prior to a more formal modelling of habitat selection. In this context, I illustrate the use of a factorial analysis, the K-select analysis. This method is a factorial decomposition of marginality, one measure of habitat selection. This method was chosen to present the package because it illustrates clearly many of its features ( home range estimation, spatial analyses, graphical possibilities, etc.). I strongly stress the powerful capabilities of factorial methods for data analysis, using as an example the analysis of habitat selection by the wild boar ( Sus scrofa L.) in a Mediterranean environment.",WOS:000252430000001,JOURNAL OF STATISTICAL SOFTWARE,"['RADIO-TRACKING', 'HOME-RANGE', 'AVAILABILITY', 'DISTRIBUTIONS', 'ESTIMATORS', 'ANIMALS']",Exploring habitat selection by wildlife with adehabitat,2007
174,"A test is said to control for type I error if it is unlikely to reject the data-generating process. However, if it is possible to produce stochastic processes at random such that, for all possible future realizations of the data, the selected process is unlikely to be rejected, then the test is said to be manipulable. So, a manipulable test has essentially no capacity to reject a strategic expert.
Many tests proposed in the existing literature, including calibration tests, control for type I error but are manipulable. We construct a test that controls for type I error and is nonmanipulable.",WOS:000265500500017,ANNALS OF STATISTICS,"['CALIBRATION', 'PROBABILITY', 'FORECASTS']",A NONMANIPULABLE TEST,2009
175,"We present a general framework of exploratory data analysis defining a set of concepts and prototypes developed within the Lisp-Stat programming environment for a M-to-M links multidimensional approach. We overview the main domains and fundamentals on which we lay the developed interactive GIS. In a second stage, we detail the different prototypes we implemented in a software called ARPEGE' and how they collaborate in providing an interactive spatial data exploration. Then we show four examples of concrete geographical applications and their underlaying data models. We end on a discussion about the contribution and the limitations of our conceptual framework and its associated software, and open to future research.",WOS:000232831700001,JOURNAL OF STATISTICAL SOFTWARE,,Interactive geographical information system using Lisp-Stat: Prototypes and applications,2005
176,"For a response variable Y, and a d dimensional vector of covariates X, the first projective direction, V, is defined as the direction that accounts for the most variability in Y. The asymptotic distribution of an estimator of a trimmed version of V has been characterized only under the assumption of the single index model (SIM). This paper proposes the use of a flexible trimming function in the objective function, which results in the consistent estimation of V. It also derives the asymptotic normality of the proposed estimator, and characterizes the components of the asymptotic variance which vanish when the SIM holds.",WOS:000384397200016,ANNALS OF STATISTICS,"['SINGLE-INDEX MODELS', 'DIMENSION REDUCTION', 'PURSUIT REGRESSION', 'ESTIMATORS', 'COEFFICIENTS']",ASYMPTOTIC THEORY FOR THE FIRST PROJECTIVE DIRECTION,2016
177,"This paper describes an algorithm for fitting finite mixtures of unrestricted Multivariate Skew t (FM-uMST) distributions. The package EMMIXuskew implements a closed-form expectation-maximization (EM) algorithm for computing the maximum likelihood (ML) estimates of the parameters for the (unrestricted) FM-MST model in R. EMMIXuskew also supports visualization of fitted contours in two and three dimensions, and random sample generation from a specified FM-uMST distribution.
Finite mixtures of skew t distributions have proven to be useful in modelling heterogeneous data with asymmetric and heavy tail behaviour, for example, datasets from flow cytometry. In recent years, various versions of mixtures with multivariate skew t (MST) distributions have been proposed. However, these models adopted some restricted characterizations of the component MST distributions so that the E-step of the EM algorithm can be evaluated in closed form. This paper focuses on mixtures with unrestricted MST components, and describes an iterative algorithm for the computation of the ML estimates of its model parameters. Its implementation in R is presented with the package EMMIXuskew.
The usefulness of the proposed algorithm is demonstrated in three applications to real datasets. The first example illustrates the use of the main function fmmst in the package by fitting a MST distribution to a bivariate unimodal flow cytometric sample. The second example fits a mixture of MST distributions to the Australian Institute of Sport (AIS) data, and demonstrates that EMMIXuskew can provide better clustering results than mixtures with restricted MST components. In the third example, EMMIXuskew is applied to classify cells in a trivariate flow cytometric dataset. Comparisons with some other available methods suggest that EMMIXuskew achieves a lower misclassification rate with respect to the labels given by benchmark gating analysis.",WOS:000328131100001,JOURNAL OF STATISTICAL SOFTWARE,"['FLOW-CYTOMETRY DATA', 'INFERENCE', 'MODELS']",EMMIXuskew: An R Package for Fitting Mixtures of Multivariate Skew t Distributions via the EM Algorithm,2013
178,,WOS:000357441000005,ANNALS OF STATISTICS,"['CONFIDENCE BANDS', 'INTERVALS']","DISCUSSION OF ""FREQUENTIST COVERAGE OF ADAPTIVE NONPARAMETRIC BAYESIAN CREDIBLE SETS""",2015
179,"This paper introduces the R package twostageTE for estimation of an inverse regression function at a given point when one can sample an explanatory covariate at different values and measure the corresponding responses. The package implements a number of nonparametric methods for budget constrained threshold value estimation. Specifically, it contains methods for classical one-stage designs and also adaptive two-stage designs, which have been shown to yield more efficient and accurate results. A major advantage of the methods in package twostageTE is that threshold value estimation is performed without penalization or kernel smoothing, and hence, avoids the well-known problems of choosing the corresponding tuning parameter (regularization, bandwidth). The user can easily perform a two-stage analysis with twostageTE by (i) identifying the second stage sampling region from an initial sample, and (ii) computing various types of confidence intervals to ensure a robust analysis. The package twostageTE is illustrated through simulated examples.",WOS:000365981900001,JOURNAL OF STATISTICAL SOFTWARE,['REGRESSION'],Threshold Value Estimation Using Adaptive Two-Stage Plans in R,2015
180,"Consider a continuous random pair (X, Y) whose dependence is characterized by an extreme-value copula with Pickands dependence function A. When the marginal distributions of X and Y are known, several consistent estimators of A are available. Most of them are variants of the estimators due to Pickands [Bull. Inst. Internat. Statist. 49 (1981) 859-878.] and Caperaa, Fougeres and Genest [Biometrika 84 (1997) 567-577]. In this paper, rank-based versions of these estimators are proposed for the more common case where the margins of X and Y are unknown. Results on the limit behavior of a class of weighted bivariate empirical processes are used to show the consistency and asymptotic normality of these rank-based estimators. Their finite- and large-sample performance is then compared to that of their known-margin analogues, as well as with endpoint-corrected versions thereof. Explicit formulas and consistent estimates for their asymptotic variances are also given.",WOS:000268605000014,ANNALS OF STATISTICS,"['VALUE DISTRIBUTIONS', 'NONPARAMETRIC-ESTIMATION', 'DEPENDENCE-FUNCTION', 'ESTIMATORS', 'BEHAVIOR', 'MODELS']",RANK-BASED INFERENCE FOR BIVARIATE EXTREME-VALUE COPULAS,2009
181,"In the nineteen seventies, Jureckova and Jaeckel proposed rank estimation for linear models. Since that time, several authors have developed inference and diagnostic methods for these estimators. These rank-based estimators and their associated inference are highly efficient and are robust to outliers in response space. The methods include estimation of standard errors, tests of general linear hypotheses, confidence intervals, diagnostic procedures including studentized residuals, and measures of influential cases. We have developed an R package, Rfit, for computing of these robust procedures. In this paper we highlight the main features of the package. The package uses standard linear model syntax and includes many of the main inference and diagnostic functions.",WOS:000313198000008,R JOURNAL,"['REGRESSION COEFFICIENTS', 'ROBUST ANALYSIS']",Rfit: Rank-based Estimation for Linear Models,2012
182,"The R package simPH provides tools for effectively communicating results from Cox proportional hazard (PH) models, including models with interactive and nonlinear effects. The Cox (PH) model is a popular tool for examining event data. However, previously available computational tools have not made it easy to explore and communicate quantities of interest and associated uncertainty estimated from them. This is especially true when the effects are interactions or nonlinear transformations of continuous variables. These transformations are especially useful with Cox PH models because they can be employed to correctly specifying models that would otherwise violate the nonproportional hazards assumption. Package simPH makes it easy to simulate and then plot quantities of interest for a variety of effects estimated from Cox PH models including interactive effects, nonlinear effects, as well as standard linear effects. Package simPH employs visual weighting in order to effectively communicate estimation uncertainty. There are options to show either the standard central interval of the simulation's distribution or the shortest probability interval - which can be useful for asymmetrically distributed estimates. This paper uses hypothetical and empirical examples to illustrate package simPH's syntax and capabilities.",WOS:000365973800001,JOURNAL OF STATISTICAL SOFTWARE,"['EVENT HISTORY ANALYSIS', 'NONPROPORTIONAL HAZARDS', 'RESIDUALS']",simPH: An R Package for Illustrating Estimates from Cox Proportional Hazard Models Including for Interactive and Nonlinear Effects,2015
183,"In regression settings, a sufficient dimension reduction (SDR) method seeks the core information in a p-vector predictor that completely captures its relationship with a response. The reduced predictor may reside in a lower dimension d < p, improving ability to visualize data and predict future observations, and mitigating dimensionality issues when carrying out further analysis. We introduce ldr, a new R software package that implements three recently proposed likelihood-based methods for SDR: covariance reduction, likelihood acquired directions, and principal fitted components. All three methods reduce the dimensionality of the data by projection into lower dimensional subspaces. The package also implements a variable screening method built upon principal fitted components which makes use of flexible basis functions to capture the dependencies between the predictors and the response. Examples are given to demonstrate likelihood-based SDR analyses using ldr, including estimation of the dimension of reduction subspaces and selection of basis functions. The ldr package provides a framework that we hope to grow into a comprehensive library of likelihood-based SDR methodologies.",WOS:000349840600001,JOURNAL OF STATISTICAL SOFTWARE,"['SLICED INVERSE REGRESSION', 'COVARIANCE MATRICES', 'MODELS']",ldr: An R Software Package for Likelihood-Based Sufficient Dimension Reduction,2014
184,"Kernel random matrices have attracted a lot of interest in recent years, from both practical and theoretical standpoints. Most of the theoretical work so far has focused on the case were the data is sampled from a low-dimensional structure. Very recently, the first results concerning kernel random matrices with high-dimensional input data were obtained, in a setting where the data was sampled from a genuinely high-dimensional structure-similar to standard assumptions in random matrix theory.
In this paper, we consider the case where the data is of the type ""information + noise."" In other words, each observation is the sum of two independent elements: one sampled from a ""low-dimensional"" structure, the signal part of the data, the other being high-dimensional noise, normalized to not overwhelm but still affect the signal. We consider two types of noise, spherical and elliptical.
In the spherical setting, we show that the spectral properties of kernel random matrices can be understood from a new kernel matrix, computed only from the signal part of the data, but using (in general) a slightly different kernel. The Gaussian kernel has some special properties in this setting.
The elliptical setting, which is important from a robustness standpoint, is less prone to easy interpretation.",WOS:000282402800019,ANNALS OF STATISTICS,['COMPONENT ANALYSIS'],ON INFORMATION PLUS NOISE KERNEL RANDOM MATRICES,2010
185,"The package fitdistrplus provides functions for fitting univariate distributions to different types of data (continuous censored or non-censored data and discrete data) and allowing different estimation methods (maximum likelihood, moment matching, quantile matching and maximum goodness-of-fit estimation). Outputs of fitdist and fitdistcens functions are S3 objects, for which kind generic methods are provided, including summary, plot and quantile. This package also provides various functions to compare the fit of several distributions to a same data set and can handle bootstrap of parameter estimates. Detailed examples are given in food risk assessment, ecotoxicology and insurance contexts.",WOS:000352911200001,JOURNAL OF STATISTICAL SOFTWARE,"['RISK-ASSESSMENT', 'SEED-DISPERSAL', 'SENSITIVITY', 'MODELS', 'PROBABILITY', 'POPULATIONS', 'SALMONELLA', 'PREDICTION', 'STABILITY', 'EXPOSURE']",fitdistrplus: An R Package for Fitting Distributions,2015
186,"The Rankcluster package is the first R package proposing both modeling and clustering tools for ranking data, potentially multivariate and partial. Ranking data are modeled by the Insertion Sorting Rank (ISR) model, which is a meaningful model parametrized by a central ranking and a dispersion parameter. A conditional independence assumption allows multivariate rankings to be taken into account, and clustering is performed by means of mixtures of multivariate ISR models. The parameters of the cluster (central rankings and dispersion parameters) help the practitioners to interpret the clustering. Moreover, the Rankcluster package provides an estimate of the missing ranking positions when rankings are partial. After an overview of the mixture of multivariate ISR models, the Rankcluster package is described and its use is illustrated through the analysis of two real datasets.",WOS:000343788100011,R JOURNAL,"['MODEL', 'LIKELIHOOD', 'ALGORITHM', 'MIXTURE']",Rankcluster: An R Package for Clustering Multivariate Partial Rankings,2014
187,This paper shows how to estimate models by the generalized method of moments and the generalized empirical likelihood using the R package gmm. A brief discussion is offered on the theoretical aspects of both methods and the functionality of the package is presented through several examples in economics and finance.,WOS:000281584400001,JOURNAL OF STATISTICAL SOFTWARE,"['CONSISTENT COVARIANCE-MATRIX', 'FINITE-SAMPLE PROPERTIES', 'HETEROSKEDASTICITY', 'ESTIMATORS', 'GMM']",Computing Generalized Method of Moments and Generalized Empirical Likelihood with R,2010
188,"Sufficient dimension reduction (SDR) in regression, which reduces the dimension by replacing original predictors with a minimal set of their linear combinations without loss of information, is very helpful when the number of predictors is large. The standard SDR methods suffer because the estimated linear combinations usually consist of all original predictors, making it difficult to interpret. In this paper, we propose a unified method-coordinate-independent sparse estimation (CISE)-that can simultaneously achieve sparse sufficient dimension reduction and screen out irrelevant and redundant variables efficiently. CISE is subspace oriented in the sense that it incorporates a coordinate-independent penalty term with a broad series of model-based and model-free SDR approaches. This results in a Grassmann manifold optimization problem and a fast algorithm is suggested. Under mild conditions, based on manifold theories and techniques, it can be shown that CISE would perform asymptotically as well as if the true irrelevant predictors were known, which is referred to as the oracle property. Simulation studies and a real-data example demonstrate the effectiveness and efficiency of the proposed approach.",WOS:000290231500011,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'PRINCIPAL HESSIAN DIRECTIONS', 'CANONICAL CORRELATION', 'COMPONENT ANALYSIS', 'ORACLE PROPERTIES', 'MODEL SELECTION', 'LIKELIHOOD', 'ASYMPTOTICS', 'CONSTRAINTS', 'ALGORITHMS']",COORDINATE-INDEPENDENT SPARSE SUFFICIENT DIMENSION REDUCTION AND VARIABLE SELECTION,2010
189,"Witten and Tibshirani (2010) proposed an algorithim to simultaneously find clusters and select clustering variables, called sparse K-means (SK-means). SK-means is particularly useful when the dataset has a large fraction of noise variables (that is, variables without useful information to separate the clusters). SK-means works very well on clean and complete data but cannot handle outliers nor missing data. To remedy these problems we introduce a new robust and sparse K-means clustering algorithm implemented in the R package RSKC. We demonstrate the use of our package on four datasets. We also conduct a Monte Carlo study to compare the performances of RSK-means and SK-means regarding the selection of important variables and identification of clusters. Our simulation study shows that RSK-means performs well on clean data and better than SK-means and other competitors on outlier-contaminated data.",WOS:000389072200001,JOURNAL OF STATISTICAL SOFTWARE,"['NUMBER', 'DATASET']",RSKC: An R Package for a Robust and Sparse K-Means Clustering Algorithm,2016
190,"In this article, we introduce the R package CryptRndTest that performs eight statistical randomness tests on cryptographic random number sequences. The purpose of the package is to provide software implementing recently proposed cryptographic randomness tests utilizing goodnessof- fit tests superior to the usual chi-square test in terms of statistical performance. Most of the tests included in package CryptRndTest are not available in other software packages such as the R package RDieHarder or the C library TestU01. Chi-square, Anderson-Darling, Kolmogorov-Smirnov, and Jarque-Bera goodness-of-fit procedures are provided along with cryptographic randomness tests. CryptRndTest utilizes multiple precision floating numbers for sequences longer than 64-bit based on the package Rmpfr. By this way, included tests are applied precisely for higher bit-lengths. In addition CryptRndTest provides a user friendly interface to these cryptographic randomness tests. As an illustrative application, CryptRndTest is used to test available random number generators in R.",WOS:000385276100017,R JOURNAL,"['STIRLING NUMBERS', 'LIBRARY']",CryptRndTest: An R Package for Testing the Cryptographic Randomness,2016
191,"We consider the problem of learning causal information between random variables in directed acyclic graphs (DAGs) when allowing arbitrarily many latent and selection variables. The FCI (Fast Causal Inference) algorithm has been explicitly designed to infer conditional independence and causal information in such settings. However, FCI is computationally infeasible for large graphs. We therefore propose the new RFCI algorithm, which is much faster than FCI. In some situations the output of RFCI is slightly less informative, in particular, with respect to conditional independence information. However, we prove that any causal information in the output of RFCI is correct in the asymptotic limit. We also define a class of graphs on which the outputs of FCI and RFCI are identical. We prove consistency of FCI and RFCI in sparse high-dimensional settings, and demonstrate in simulations that the estimation performances of the algorithms are very similar. All software is implemented in the R-package pcalg.",WOS:000304684900012,ANNALS OF STATISTICS,"['EQUIVALENCE CLASSES', 'MARKOV EQUIVALENCE', 'OBSERVATIONAL DATA', 'ANCESTRAL GRAPHS', 'CAUSAL', 'MODELS', 'LASSO']",LEARNING HIGH-DIMENSIONAL DIRECTED ACYCLIC GRAPHS WITH LATENT AND SELECTION VARIABLES,2012
192,"A preliminary attempt at collecting tools and utilities for genetic data as an R package called gap is described. Genomewide association is then described as a specific example, linking the work of Risch and Merikangas (1996), Long and Langley (1997) for family-based and population-based studies, and the counterpart for case-cohort design established by Cai and Zeng (2004). Analysis of staged design as outlined by Skol et al. (2006) and associate methods are discussed. The package is flexible, customizable, and should prove useful to researchers especially in its application to genomewide association studies.",WOS:000252431800001,JOURNAL OF STATISTICAL SOFTWARE,"['GENOME-WIDE ASSOCIATION', 'POLYMORPHIC DNA MARKER', 'UNRELATED INDIVIDUALS', 'MULTIPLE IMPUTATION', 'COMPLEX DISEASES', 'R-PACKAGE', 'LINKAGE', 'TRAITS', 'MODELS', 'HAPLOTYPES']",gap: Genetic analysis package,2007
193,"Simulations are a practical and reliable approach to power calculations, especially for multi-level mixed effects models where the analytic solutions can be very complex. In addition, power calculations are model-specific and multi-level mixed effects models are defined by a plethora of parameters. In other words, model variations in this context are numerous and so are the tailored algebraic calculations. This article describes ipdpower in Stata, a new simulations-based command that calculates power for mixed effects two-level data structures. Although the command was developed having individual patient data meta-analyses and primary care databases analyses in mind, where patients are nested within studies and general practices respectively, the methods apply to any two-level structure.",WOS:000392515200001,JOURNAL OF STATISTICAL SOFTWARE,"['INDIVIDUAL PATIENT DATA', 'RANDOM EFFECTS METAANALYSIS', 'STATISTICAL-METHODS', 'AGGREGATE-DATA', 'HETEROGENEITY']",Simulation-Based Power Calculations for Mixed Effects Modeling: ipdpower in Stata,2016
194,"Kernel estimation is an important technique in exploratory data analysis. Its utility relies on its ease of interpretation, especially based on graphical means. The Ake package is introduced for univariate density or probability mass function estimation and also for continuous and discrete regression functions using associated kernel estimators. These associated kernels have been proposed due to their specific features of variables of interest. The package focuses on associated kernel methods appropriate for continuous (bounded, positive) or discrete (count, categorical) data often found in applied settings. Furthermore, optimal bandwidths are selected by cross-validation for any associated kernel and by Bayesian methods for the binomial kernel. Other Bayesian methods for selecting bandwidths with other associated kernels will complete this package in its future versions; particularly, a Bayesian adaptive method for gamma kernel estimation of density functions is developed. Some practical and theoretical aspects of the normalizing constant in both density and probability mass functions estimations are given.",WOS:000395669800017,R JOURNAL,"['PROBABILITY DENSITY-FUNCTION', 'NONPARAMETRIC-ESTIMATION', 'TRIANGULAR DISTRIBUTIONS', 'BANDWIDTH', 'BOUNDARY', 'PERFORMANCE', 'EXTENSIONS', 'BIAS']",Ake: An R Package for Discrete and Continuous Associated Kernel Estimations,2016
195,,WOS:000208589700012,R JOURNAL,,PMML: An Open Standard for Sharing Models,2009
196,,WOS:000277471000022,ANNALS OF STATISTICS,,"ON SOME PROBLEMS IN THE ARTICLE EFFICIENT LIKELIHOOD ESTIMATION IN STATE SPACE MODELS (vol 38, pg 1279, 2010)",2010
197,"In the paper we present an R package MNM dedicated to multivariate data analysis based on the L-1 norm. The analysis proceeds very much as does a traditional multivariate analysis. The regular L-2 norm is just replaced by different L-1 norms, observation vectors are replaced by their (standardized and centered) spatial signs, spatial ranks, and spatial signed-ranks, and so on. The procedures are fairly efficient and robust, and no moment assumptions are needed for asymptotic approximations. The background theory is briefly explained in the multivariate linear regression model case, and the use of the package is illustrated with several examples using the R package MNM.",WOS:000293390600001,JOURNAL OF STATISTICAL SOFTWARE,"['NONPARAMETRIC-TESTS', 'RANK-TESTS', 'AFFINE-INVARIANT', 'SIGN TEST', 'REGRESSION', 'SCATTER']",Multivariate L-1 Methods: The Package MNM,2011
198,"We study location-scale mixture priors for nonparametric statistical problems, including multivariate regression, density estimation and classification. We show that a rate-adaptive procedure can be obtained if the prior is properly constructed. In particular, we show that adaptation is achieved if a kernel mixture prior on a regression function is constructed using a Gaussian kernel, an inverse gamma bandwidth, and Gaussian mixing weights.",WOS:000283792900003,ANNALS OF STATISTICS,"['POSTERIOR CONVERGENCE-RATES', 'DIRICHLET MIXTURES', 'DENSITY-ESTIMATION', 'GAUSSIAN MEASURES', 'METRIC ENTROPY', 'DISTRIBUTIONS', 'APPROXIMATION', 'CONSISTENCY', 'MODELS']",ADAPTIVE NONPARAMETRIC BAYESIAN INFERENCE USING LOCATION-SCALE MIXTURE PRIORS,2010
199,"We have developed the R package c060 with the aim of improving R software functionality for high-dimensional risk prediction modeling, e.g., for prognostic modeling of survival data using high-throughput genomic data. Penalized regression models provide a statistically appealing way of building risk prediction models from high-dimensional data. The popular CRAN package glmnet implements an efficient algorithm for fitting penalized Cox and generalized linear models. However, in practical applications the data analysis will typically not stop at the point where the model has been fitted. One is for example often interested in the stability of selected features and in assessing the prediction performance of a model and we provide functions to deal with both of these tasks. Our R functions are computationally efficient and off er the possibility of speeding up computing time through parallel computing. Another feature which can drastically reduce computing time is an e fficient interval-search algorithm, which we have implemented for selecting the optimal parameter combination for elastic net penalties. These functions have been useful in our daily work at the Biostatistics department (C060) of the German Cancer Research Center where prognostic modeling of patient survival data is of particular interest. Although we focus on a survival data application of penalized Cox models in this article, the functions in our R package are in general applicable to all types of regression models implemented in the glmnet package, with the exception of prediction error curves, which are specific to time-to-event data.",WOS:000349843400001,JOURNAL OF STATISTICAL SOFTWARE,"['PROPORTIONAL HAZARDS MODEL', 'VARIABLE SELECTION', 'STABILITY SELECTION', 'COORDINATE DESCENT', 'R-PACKAGE', 'SURVIVAL', 'PREDICTION', 'REGRESSION', 'CLASSIFICATION', 'PATHS']",c060: Extended Inference with Lasso and Elastic-Net Regularized Cox and Generalized Linear Models,2014
200,"Time-series-cross-section (TSCS) data are characterized by having repeated observations over time on some set of units, such as states or nations. TSCS data typically display both contemporaneous correlation across units and unit level heteroskedasity making inference from standard errors produced by ordinary least squares incorrect. Panel-corrected standard errors (PCSE) account for these these deviations from spherical errors and allow for better inference from linear models estimated from TSCS data. In this paper, we discuss an implementation of them in the R system for statistical computing. The key computational issue is how to handle unbalanced data.",WOS:000208805800001,JOURNAL OF STATISTICAL SOFTWARE,,Implementing Panel-Corrected Standard Errors in R: The pcse Package,2011
201,"A parameter estimation problem is considered, in which dispersed sensors transmit to the statistician partial information regarding their observations. The sensors observe the paths of continuous semimartingales, whose drifts are linear with respect to a common parameter. A novel estimating scheme is suggested, according to which each sensor transmits only one-bit messages at stopping times of its local filtration. The proposed estimator is shown to be consistent and, for a large class of processes, asymptotically optimal, in the sense that its asymptotic distribution is the same as the exact distribution of the optimal estimator that has full access to the sensor observations. These properties are established under an asymptotically low rate of communication between the sensors and the statistician. Thus, despite being asymptotically efficient, the proposed estimator requires minimal transmission activity, which is a desirable property in many applications. Finally, the case of discrete sampling at the sensors is studied when their underlying processes are independent Brownian motions.",WOS:000321842400004,ANNALS OF STATISTICS,"['MULTITERMINAL DATA-COMPRESSION', 'DECENTRALIZED ESTIMATION', 'DISTRIBUTED DETECTION', 'STOCHASTIC-PROCESSES', 'MULTIPLE SENSORS', 'TIME', 'INFERENCE']",ASYMPTOTICALLY OPTIMAL PARAMETER ESTIMATION UNDER COMMUNICATION CONSTRAINTS,2012
202,"Maximum likelihood estimation of a log-concave density has attracted considerable attention over the last few years. Several algorithms have been proposed to estimate such a density. Two of those algorithms, an iterative convex minorant and an active set algorithm, are implemented in the R package logcondens. While these algorithms are discussed elsewhere, we describe in this paper the use of the logcondens package and discuss functions and datasets related to log-concave density estimation contained in the package. In particular, we provide functions to (1) compute the maximum likelihood estimate (MLE) as well as a smoothed log-concave density estimator derived from the MLE, (2) evaluate the estimated density, distribution and quantile functions at arbitrary points, (3) compute the characterizing functions of the MLE, (4) sample from the estimated distribution, and finally (5) perform a two-sample permutation test using a modi fied Kolmogorov-Smirnov test statistic. In addition, logcondens makes two datasets available that have been used to illustrate log-concave density estimation.",WOS:000288204200001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'DISTRIBUTIONS', 'ALGORITHM']",Logcondens: Computations Related to Univariate Log-Concave Density Estimation,2011
203,"We study the asymptotics for jump-penalized least squares regression aiming at approximating a regression function by piecewise constant functions. Besides conventional consistency and convergence rates of the estimates in L(2)([0, 1)) our results cover other metrics like Skorokhod metric on the space of cadlag functions and uniform metrics on C([0, 1]). We will show that these estimators are in an adaptive sense rate optimal over certain classes of ""approximation spaces."" Special cases are the class of functions of bounded variation (piecewise) Holder continuous functions of order 0 < alpha <= 1 and the class of step functions with a finite but arbitrary number of jumps. In the latter setting, we will also deduce the rates known from change-point analysis for detecting the jumps. Finally, the issue of fully automatic selection of the smoothing parameter is addressed.",WOS:000263129000006,ANNALS OF STATISTICS,"['LARGE UNDERDETERMINED SYSTEMS', 'NONPARAMETRIC REGRESSION', 'BAYESIAN RESTORATION', 'SMOOTH REGRESSION', 'WAVELET SHRINKAGE', 'CHANGE-POINTS', 'SELECTION', 'EQUATIONS', 'SEQUENCE']",CONSISTENCIES AND RATES OF CONVERGENCE OF JUMP-PENALIZED LEAST SQUARES ESTIMATORS,2009
204,"We introduce a multiscale test statistic based on local order statistics and spacings that provides simultaneous confidence statements for the existence and location of local increases and decreases of a density or a failure rate. The procedure provides guaranteed finite-sample significance levels, is easy to implement and possesses certain asymptotic optimality and adaptivity properties.",WOS:000258243000013,ANNALS OF STATISTICS,"['EXCESS MASS', 'NORMALIZED SPACINGS', 'TESTS', 'MULTIMODALITY', 'MODALITY', 'CONTOUR']",Multiscale inference about a density,2008
205,,WOS:000357431900001,R JOURNAL,,Untitled,2015
206,"We Study generalized bootstrap confidence regions for the mean of a random vector whose coordinates have an unknown dependency structure. The random vector is supposed to be either Gaussian or to have a symmetric and bounded distribution. The dimensionality of the vector can possibly be much larger than the number of observations and we focus on a nonasymptotic control of the confidence level, following ideas inspired by recent results in learning theory. We consider two approaches, the first based on a concentration principle (valid for a large class of resampling weights) and the second oil a resampled quantile, specifically using Rademacher weights. Several intermediate results established in the approach based on concentration principles are of interest in their own right. We also discuss the question of accuracy when using Monte Carlo approximations of the resampled quantities.",WOS:000273800100002,ANNALS OF STATISTICS,"['BOOTSTRAP', 'REGRESSION', 'LOCALIZATION', 'ALGORITHMS', 'BALLS', 'SETS']","SOME NONASYMPTOTIC RESULTS ON RESAMPLING IN HIGH DIMENSION, I: CONFIDENCE REGIONS",2010
207,"In the design of efficient simulation algorithms, one is often beset with a poor choice of proposal distributions. Although the performance of a given simulation kernel can clarify a posteriori how adequate this kernel is for the problem at hand, a permanent on-line modification of kernels causes concerns about the validity of the resulting algorithm. While the issue is most often intractable for MCMC algorithms, the equivalent version for importance sampling algorithms can be validated quite precisely. We derive sufficient convergence conditions for adaptive mixtures of population Monte Carlo algorithms and show that Rao-Blackwellized versions asymptotically achieve an optimum in terms of a Kullback divergence criterion, while more rudimentary versions do not benefit from repeated updating.",WOS:000247498100016,ANNALS OF STATISTICS,"['CHAIN MONTE-CARLO', 'METROPOLIS ALGORITHM', 'DISTRIBUTIONS']",Convergence of adaptive mixtures of importance sampling schemes,2007
208,"We study the computational complexity of Markov chain Monte Carlo (MCMC) methods for high-dimensional Bayesian linear regression under sparsity constraints. We first show that a Bayesian approach can achieve variable-selection consistency under relatively mild conditions on the design matrix. We then demonstrate that the statistical criterion of posterior concentration need not imply the computational desideratum of rapid mixing of the MCMC algorithm. By introducing a truncated sparsity prior for variable selection, we provide a set of conditions that guarantee both variable-selection consistency and rapid mixing of a particular Metropolis Hastings algorithm. The mixing time is linear in the number of covariates up to a logarithmic factor. Our proof controls the spectral gap of the Markov chain by constructing a canonical path ensemble that is inspired by the steps taken by greedy algorithms for variable selection.",WOS:000389620800010,ANNALS OF STATISTICS,"['MODEL SELECTION', 'SPARSITY RECOVERY', 'PRIORS', 'REGRESSION', 'LASSO', 'LIKELIHOOD', 'SHRINKAGE']",ON THE COMPUTATIONAL COMPLEXITY OF HIGH-DIMENSIONAL BAYESIAN VARIABLE SELECTION,2016
209,"Boosting and bagging are two widely used ensemble methods for classification. Their common goal is to improve the accuracy of a classifier combining single classifiers which are slightly better than random guessing. Among the family of boosting algorithms, AdaBoost (adaptive boosting) is the best known, although it is suitable only for dichotomous tasks. AdaBoost.M1 and SAMME (stagewise additive modeling using a multi-class exponential loss function) are two easy and natural extensions to the general case of two or more classes. In this paper, the adabag R package is introduced. This version implements AdaBoost.M1, SAMME and bagging algorithms with classification trees as base classifiers. Once the ensembles have been trained, they can be used to predict the class of new samples. The accuracy of these classifiers can be estimated in a separated data set or through cross validation. Moreover, the evolution of the error as the ensemble grows can be analysed and the ensemble can be pruned. In addition, the margin in the class prediction and the probability of each class for the observations can be calculated. Finally, several classic examples in classification literature are shown to illustrate the use of this package.",WOS:000323909400001,JOURNAL OF STATISTICAL SOFTWARE,"['ENSEMBLE', 'CLASSIFIERS', 'PREDICTION', 'ALGORITHMS', 'ADABOOST', 'MODELS']",adabag: An R Package for Classification with Boosting and Bagging,2013
210,"An estimate of the number of species, S, usually called species richness by ecologists, in an area is one of the basic statistics used to ascertain biological diversity. Traditionally ecologists have used the number of species observed in a sample, S-0, to estimate S, realizing that S-0 is a lower bound for S. One alternative to S-0 is to use a nonparametric procedure such as jackknife resampling. For species richness, a closed form of the jackknife estimator is available. Typically statistical software contains only the traditional iterative form of the jackknife estimator. The purpose of this article is to propose an S-PLUS function for calculating the noniterative first order jackknife estimator of species richness and some associated plots and statistics.",WOS:000235180800001,JOURNAL OF STATISTICAL SOFTWARE,['NUMBER'],Jackknife estimator of species richness with S-PLUS,2006
211,"This paper is aimed at deriving the universality of the largest eigenvalue of a class of high-dimensional real or complex sample covariance matrices of the form W-N = Sigma(XX)-X-1/2*E-1/2. Here, X = (xij)(M,N) is an M x N random matrix with independent entries x(ij), 1 <= i <= M, 1 <= j <= N such that Ex(ij) = 0, E vertical bar x(ij)vertical bar(2) = 1/N. On dimensionality, we assume that M = M(N) and N/M -> d is an element of(0, infinity) as N -> infinity. For a class of general deterministic positive-definite M x M matrices Sigma, under some additional assumptions on the distribution of x(ij)'s, we show that the limiting behavior of the largest eigenvalue of W-N is universal, via pursuing a Green function comparison strategy raised in [Probab. Theory Related Fields 154 (2012) 341-407, Adv. Math. 229 (2012) 1435-1515] by Erdos, Yau and Yin for Wigner matrices and extended by Pillai and Yin [Ann. Appl Probab. 24 (2014) 935-1001] to sample covariance matrices in the null case (Sigma = I). Consequently, in the standard complex case (Ex(ij)(2) = 0), combing this universality property and the results known for Gaussian matrices obtained by El Karoui in [Ann. Probab. 35 (2007) 663-714] (nonsingular case) and Onatski in [Ann. Appl. Probab. 18 (2008) 470-490] (singular case), we show that after an appropriate normalization the largest eigenvalue of WN converges weakly to the type 2 Tracy-Widom distribution TW2. Moreover, in the real case, we show that when Sigma is spiked with a fixed number of subcritical spikes, the type 1 Tracy-Widom limit TW1 holds for the normalized largest eigenvalue of W-N, which extends a result of Feral and Peche in [J. Math. Phys. 50 (2009) 073302] to the scenario of nondiagonal Sigma and more generally distributed X. In summary, we establish the Tracy Widom type universality for the largest eigenvalue of generally distributed sample covariance matrices under quite light assumptions on Sigma. Applications of these limiting results to statistical signal detection and structure recognition of separable covariance matrices are also discussed.",WOS:000349738500014,ANNALS OF STATISTICS,"['WIGNER RANDOM MATRICES', 'LIMITING SPECTRAL DISTRIBUTION', 'DIMENSIONAL RANDOM MATRICES', 'TRACY-WIDOM LIMIT', 'EDGE UNIVERSALITY', 'LOCAL STATISTICS', 'SEMICIRCLE LAW', 'MODEL', 'DELOCALIZATION', 'DISTRIBUTIONS']",UNIVERSALITY FOR THE LARGEST EIGENVALUE OF SAMPLE COVARIANCE MATRICES WITH GENERAL POPULATION,2015
212,,WOS:000357441000002,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'VON MISES PHENOMENON', 'CONFIDENCE BANDS']","DISCUSSION OF ""FREQUENTIST COVERAGE OF ADAPTIVE NONPARAMETRIC BAYESIAN CREDIBLE SETS""",2015
213,,WOS:000275510800004,ANNALS OF STATISTICS,['CONDITIONAL GROWTH CHARTS'],Multivariate quantiles and multiple-output regression quantiles: From L-1 optimization to halfspace depth DISCUSSION,2010
214,"We propose an algorithm to compute the cumulative distribution function of the two-sided Kolmogorov-Smirnov test statistic D(n) and its complementary distribution in a fast and reliable way. Different approximations are used in different regions of (n,x). Java and C programs are available.",WOS:000288206100001,JOURNAL OF STATISTICAL SOFTWARE,['CUMULATIVE DISTRIBUTION'],Computing the Two-Sided Kolmogorov-Smirov Distribution,2011
215,"We present QCA, a package for performing Qualitative Comparative Analysis (QCA). QCA is becoming increasingly popular with social scientists, but none of the existing software alternatives covers the full range of core procedures. This gap is now filled by QCA. After a mapping of the method's diffusion, we introduce some of the package's main capabilities, including the calibration of crisp and fuzzy sets, the analysis of necessity relations, the construction of truth tables and the derivation of complex, parsimonious and intermediate solutions.",WOS:000321944400010,R JOURNAL,"['FUZZY-SET ANALYSIS', 'MULTIVALUE QCA', 'PITFALLS', 'POTENTIALS', 'EUROPE', 'POLICY', 'CRISP']",QCA: A Package for Qualitative Comparative Analysis,2013
216,"quickpsy is a package to parametrically fit psychometric functions. In comparison with previous R packages, quickpsy was built to easily fit and plot data for multiple groups. Here, we describe the standard parametric model used to fit psychometric functions and the standard estimation of its parameters using maximum likelihood. We also provide examples of usage of quickpsy, including how allowing the lapse rate to vary can sometimes eliminate the bias in parameter estimation, but not in general. Finally, we describe some implementation details, such as how to avoid the problems associated to round-off errors in the maximisation of the likelihood or the use of closures and non-standard evaluation functions.",WOS:000385276100009,R JOURNAL,"['DECISION-MAKING', 'MODEL']",quickpsy: An R Package to Fit Psychometric Functions for Multiple Groups,2016
217,"Simulation studies are widely used by statisticians to gain insight into the quality of developed methods. Usually some guidelines regarding, e. g., simulation designs, contamination, missing data models or evaluation criteria are necessary in order to draw meaningful conclusions. The R package simFrame is an object-oriented framework for statistical simulation, which allows researchers to make use of a wide range of simulation designs with a minimal effort of programming. Its object-oriented implementation provides clear interfaces for extensions by the user. Since statistical simulation is an embarrassingly parallel process, the framework supports parallel computing to increase computational performance. Furthermore, an appropriate plot method is selected automatically depending on the structure of the simulation results. In this paper, the implementation of si m Frame is discussed in great detail and the functionality of the framework is demonstrated in examples for different simulation designs.",WOS:000284597400001,JOURNAL OF STATISTICAL SOFTWARE,['ROBUST METHODS'],An Object-Oriented Framework for Statistical Simulation: The R Package simFrame,2010
218,"[it this paper we consider two closely related problems: estimation of eigenvalues and eigenfunctions of the covariance kernel of functional data based on (possibly) irregular measurements, and the problem of estimating the eigenvalues and eigenvectors of the covariance matrix for high-dimensional Gaussian vectors. In [A geometric approach to maximum likelihood estimation of covariance kernel from sparse irregular longitudinal data (2007)], a restricted maximum likelihood (REML) approach has been developed to deal with the first problem. In this paper, we establish consistency and derive rate of convergence of the REML estimator for the functional data case, under appropriate smoothness conditions. Moreover, we prove that when the number of measurements per sample curve is bounded, under squared-error loss, the rate of convergence of the REML estimators of eigenfunctions is near-optimal. In the case of Gaussian vectors, asymptotic consistency and;in efficient score representation of the estimators are obtained under the assumption that the effective dimension grows at a rate slower than the sample size. These results are derived through an explicit utilization of the intrinsic geometry of the parameter space, which is non-Euclidean. Moreover, the results derived in this paper Suggest all asymptotic equivalence between the inference on functional data with dense measurements and that of the high-dimensional Gaussian vectors.",WOS:000265619700006,ANNALS OF STATISTICS,"['LONGITUDINAL DATA', 'FUNCTIONAL DATA', 'MODELS']",CONSISTENCY OF RESTRICTED MAXIMUM LIKELIHOOD ESTIMATORS OF PRINCIPAL COMPONENTS,2009
219,"RinRuby is a Ruby library that integrates the R interpreter in Ruby, making R's statistical routines and graphics available within Ruby. The library consists of a single Ruby script that is simple to install and does not require any special compilation or installation of R. Since the library is 100% pure Ruby, it works on a variety of operating systems, Ruby implementations, and versions of R. RinRuby's methods are simple, making for readable code. This paper describes RinRuby usage, provides comprehensive documentation, gives several examples, and discusses RinRuby's implementation. The latest version of RinRuby can be found at the project website:http://rinruby.ddahl.org/.",WOS:000263105300001,JOURNAL OF STATISTICAL SOFTWARE,,RinRuby: Accessing the R Interpreter from Pure Ruby,2009
220,"The focus of this paper is on trend estimation for a general state-space model Y-t = mu(t) + epsilon(t), where the dth difference of the trend {mu(t)} is assumed to be i.i.d., and the error sequence {epsilon(t)} is assumed to be a mean zero stationary process. A fairly precise asymptotic expression of the mean square error is derived for the estimator obtained by penalizing the dth order differences. Optimal rate of convergence is obtained, and it is shown to be ""asymptotically equivalent"" to a nonparametric estimator of a fixed trend model of smoothness of order d - 0.5. The results of this paper show that the optimal rate of convergence for the stochastic and nonstochastic cases are different. A criterion for selecting the penalty parameter and degree of difference d is given, along with an application to the global temperature data, which shows that a longer term history has nonlinearities that are important to take into consideration.",WOS:000271673700001,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'LONG-MEMORY', 'TIME-SERIES']",ESTIMATION OF TREND IN STATE-SPACE MODELS: ASYMPTOTIC MEAN SQUARE ERROR AND RATE OF CONVERGENCE,2009
221,"This paper presents DFIT, an R package that implements the differential functioning of items and tests framework as well as the Monte Carlo item parameter replication approach for producing cut-off points for differential item functioning indices. Furthermore, it illustrates how to use the package to calculate power for the NCDIF index, both post hoc, as has regularly been the case in differential item functioning empirical and simulation studies, as well as a priori given certain item parameters. The version reviewed here implements all DFIT indices and Raju's area measures for tests comprised of items modeled with the same parametric item response unidimensional model (1-, 2-, and 3-parameters, generalized partial credit model or graded response model), the Mantel-Haenszel statistic with an underlying dichotomous item response model, and the item parameter replication method for any of the estimated indices with dichotomous item response models.",WOS:000392706500001,JOURNAL OF STATISTICAL SOFTWARE,"['GRADED RESPONSE MODEL', 'RASCH MIXTURE-MODELS', 'INTERNAL MEASURES', 'PARAMETER', 'ERRORS']",DFIT: An R Package for Raju's Differential Functioning of Items and Tests Framework,2017
222,"poLCA is a software package for the estimation of latent class and latent class regression models for polytomous outcome variables, implemented in the R statistical computing environment. Both models can be called using a single simple command line. The basic latent class model is a finite mixture model in which the component distributions are assumed to be multi-way cross-classification tables with all variables mutually independent. The latent class regression model further enables the researcher to estimate the effects of covariates on predicting latent class membership. poLCA uses expectation-maximization and Newton-Raphson algorithms to find maximum likelihood estimates of the model parameters.",WOS:000292097900001,JOURNAL OF STATISTICAL SOFTWARE,"['MODEL SELECTION', 'EM ALGORITHM', 'SUPPORT']",poLCA: An R Package for Polytomous Variable Latent Class Analysis,2011
223,"In this paper, we study the estimation for a partial-linear single-index model. A two-stage estimation procedure is proposed to estimate the link function for the single index and the parameters in the single index, as well as the parameters in the linear component of the model. Asymptotic normality is established for both parametric components. For the index, a constrained estimating equation leads to an asymptotically more efficient estimator than existing estimators in the sense that it is of a smaller limiting variance. The estimator of the nonparametric link function achieves optimal convergence rates, and the structural error variance is obtained. In addition, the results facilitate the construction of confidence regions and hypothesis testing for the unknown parameters. A simulation study is performed and an application to a real dataset is illustrated. The extension to multiple indices is briefly sketched.",WOS:000273800100008,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'PROJECTION PURSUIT REGRESSION', 'DIMENSION REDUCTION', 'CONVERGENCE-RATES', 'ASYMPTOTICS', 'LINK']",ESTIMATION FOR A PARTIAL-LINEAR SINGLE-INDEX MODEL,2010
224,"Fan and Li propose a family of variable selection methods via penalized likelihood using concave penalty functions. The nonconcave penalized likelihood estimators enjoy the oracle properties, but maximizing the penalized likelihood function is computationally challenging, because the objective function is nondifferentiable and nonconcave. In this article, we propose a new unified algorithm based on the local linear approximation (LLA) for maximizing the penalized likelihood for a broad class of concave penalty functions. Convergence and other theoretical properties of the LLA algorithm are established. A distinguished feature of the LLA algorithm is that at each LLA step, the LLA estimator can naturally adopt a sparse representation. Thus, we suggest using the one-step LLA estimator from the LLA algorithm as the final estimates. Statistically, we show that if the regularization parameter is appropriately chosen, the one-step LLA estimates enjoy the oracle properties with good initial estimators. Computationally, the one-step LLA estimation methods dramatically reduce the computational cost in maximizing the nonconcave penalized likelihood. We conduct some Monte Carlo simulation to assess the finite sample performance of the one-step sparse estimation methods. The results are very encouraging.",WOS:000258243000001,ANNALS OF STATISTICS,"['SURROGATE OBJECTIVE FUNCTIONS', 'FAILURE TIME DATA', 'VARIABLE SELECTION', 'REGRESSION', 'LASSO', 'REGULARIZATION', 'ASYMPTOTICS']",One-step sparse estimates in nonconcave penalized likelihood models,2008
225,"The R package spate implements methodology for modeling of large space-time data sets. A spatio-temporal Gaussian process is defined through a stochastic partial differential equation (SPDE) which is solved using spectral methods. In contrast to the traditional geostatistical way of relying on the covariance function, the spectral SPDE approach is computationally tractable and provides a realistic space-time parametrization.
This package aims at providing tools for simulating and modeling of spatio-temporal processes using an SPDE based approach. The package contains functions for obtaining parametrizations, such as propagator or innovation covariance matrices, of the spatio-temporal model. This allows for building customized hierarchical Bayesian models using the SPDE based model at the process stage. The functions of the package then provide computationally efficient algorithms needed for doing inference with the hierarchical model. Furthermore, an adaptive Markov chain Monte Carlo (MCMC) algorithm implemented in the package can be used as an algorithm for doing inference without any additional modeling. This function is flexible and allows for application specific customizing. The MCMC algorithm supports data that follow a Gaussian or a censored distribution with point mass at zero. Spatio-temporal covariates can be included in the model through a regression term.",WOS:000349846800001,JOURNAL OF STATISTICAL SOFTWARE,"['SPACE-TIME MODELS', 'RANDOM-FIELDS', 'PREDICTION']",spate: An R Package for Spatio-Temporal Modeling with a Stochastic Advection-Diffusion Process,2015
226,"Emerging technologies in the experimental sciences have opened the way for large-scale experiments. Such experiments generate ever growing amounts of data from which researchers need to extract relevant pieces for subsequent analysis. R offers a great environment for statistical analysis. However, due to the diversity of possible data sources and formats, data preprocessing and import can be time consuming especially with data that require user interaction such as editing, filtering or formatting. Writing a code for these tasks can be time-consuming, error prone and rather complex. We present speedR, an R- package for interactive data import, filtering and code generation in order to address these needs. Using speedR, researchers can import new data, make basic corrections, examine current R session objects, open them in the speedR environment for filtering (subsetting), put the filtered data back into R, and even create new R functions with applied import and filtering constraints to speed up their productivity.",WOS:000310774000001,JOURNAL OF STATISTICAL SOFTWARE,,"speedR: An R Package for Interactive Data Import, Filtering and Ready-to-Use Code Generation",2012
227,"Nonparametric data envelopment analysis (DEA) estimators have been widely applied in analysis of productive efficiency. Typically they are defined in terms of convex-hulls of the observed combinations of inputs x outputs in a sample of enterprises. The shape of the convex-hull relies on a hypothesis on the shape of the technology, defined as the boundary of the set of technically attainable points in the inputs x outputs space. So far, only the statistical properties of the smallest convex polyhedron enveloping the data points has been considered which corresponds to a situation where the technology presents variable returns-to-scale (VRS). This paper analyzes the case where the most common constant returns-to-scale (CRS) hypothesis is assumed. Here the DEA is defined as the smallest conical-hull with vertex at the origin enveloping the cloud of observed points. In this paper we determine the asymptotic properties of this estimator, showing that the rate of convergence is better than for the VRS estimator. We derive also its asymptotic sampling distribution with a practical way to simulate it. This allows to define a bias-corrected estimator and to build confidence intervals for the frontier. We compare in a simulated example the bias-corrected estimator with the original conical-hull estimator and show its superiority in terms of median squared error.",WOS:000277471000002,ANNALS OF STATISTICS,"['DEA ESTIMATORS', 'EFFICIENCY', 'BOUNDARIES']",ASYMPTOTIC DISTRIBUTION OF CONICAL-HULL ESTIMATORS OF DIRECTIONAL EDGES,2010
228,,WOS:000253077800002,ANNALS OF STATISTICS,,Discussion: The Dantzig selector: Statistical estimation when p is much larger than n,2007
229,"We analyze the statistical properties of nonparametric regression estimators using covariates which are not directly observable, but have be estimated from data in a preliminary step. These so-called generated covariates appear in numerous applications, including two-stage nonparametric regression, estimation of simultaneous equation models or censored regression models. Yet so far there seems to be no general theory for their impact on the final estimator's statistical properties. Our paper provides such results. We derive a stochastic expansion that characterizes the influence of the generation step on the final estimator, and use it to derive rates of consistency and asymptotic distributions accounting for the presence of generated covariates.",WOS:000307608000019,ANNALS OF STATISTICS,"['SIMULTANEOUS-EQUATIONS MODELS', 'SEMIPARAMETRIC MODELS', 'UNIFORM CONSISTENCY', 'SERIES ESTIMATORS', 'KERNEL ESTIMATION', 'CONVERGENCE', 'VARIANCE', 'RATES']",NONPARAMETRIC REGRESSION WITH NONPARAMETRICALLY GENERATED COVARIATES,2012
230,,WOS:000208590200001,R JOURNAL,,"A peer-reviewed, open-access publication of the R Foundation for Statistical Computing",2011
231,"We consider the problem of simultaneous variable selection and estimation in partially linear models with a divergent number of covariates in the linear part, under the assumption that the vector of regression coefficients is sparse. We apply the SCAD penalty to achieve sparsity in the linear part and use polynomial splines to estimate the nonparametric component. Under reasonable conditions, it is shown that consistency in terms of variable selection and estimation can be achieved simultaneously for the linear and nonparametric components. Furthermore, the SCAD-penalized estimators of the nonzero coefficients are shown to have the asymptotic oracle property, in the sense that it is asymptotically normal with the same means and covariances that they would have if the zero coefficients were known in advance. The finite sample behavior of the SCAD-penalized estimators is evaluated with simulation and illustrated with a data set.",WOS:000265500500005,ANNALS OF STATISTICS,"['VARIABLE SELECTION', 'LASSO', 'CONVERGENCE', 'ESTIMATORS', 'RATES', 'LIKELIHOOD']",SCAD-PENALIZED REGRESSION IN HIGH-DIMENSIONAL PARTIALLY LINEAR MODELS,2009
232,"This paper briefly describes geostatistical models for Gaussian and non-Gaussian data and demonstrates the geostatsp and diease mapping packages for performing inference using these models. Making use of R's spatial data types, and raster objects in particular, makes spatial analyses using geostatistical models simple and convenient. Examples using real data are shown for Gaussian spatial data, binomially distributed spatial data, a log-Gaussian Cox process, and an area-level model for case counts.",WOS:000349846500001,JOURNAL OF STATISTICAL SOFTWARE,"['GAUSSIAN COX PROCESSES', 'BAYESIAN-INFERENCE', 'RANDOM-FIELDS', 'PACKAGE']",Model-Based Geostatistics the Easy Way,2015
233,"We consider the problem of forecasting the next (observable) state of an unknown ergodic dynamical system from a noisy observation of the present state. Our main result shows, for example, that Support vector machines (SVMs) using Gaussian RBF kernels can learn the best forecaster from a sequence of noisy observations if (a) the unknown observational noise process is bounded and has a summable alpha-mixing rate and (b) the unknown ergodic dynamical system is defined by a Lipschitz continuous function on some compact subset of R-d and has a summable decay of correlations for Lipschitz continuous functions. In order to prove this result we first establish a general consistency result for SVMs and all stochastic processes that satisfy a mixing notion that is substantially weaker than alpha-mixing.",WOS:000265500500011,ANNALS OF STATISTICS,"['CHAOTIC TIME-SERIES', 'EXPANDING MAPS', 'REDUCTION', 'CLASSIFICATION', 'PREDICTION', 'REGRESSION']",CONSISTENCY OF SUPPORT VECTOR MACHINES FOR FORECASTING THE EVOLUTION OF AN UNKNOWN ERGODIC DYNAMICAL SYSTEM FROM OBSERVATIONS WITH UNKNOWN NOISE,2009
234,"We investigate the behavior of Fourier transforms for a wide class of nonstationary nonlinear processes. Asymptotic central and noncentral limit theorems are established for a class of nondegenerate and degenerate weighted V-statistics through the angle of Fourier analysis. The established theory for V-statistics provides a unified treatment for many important time and spectral domain problems in the analysis of nonstationary time series, ranging from nonparametric estimation to the inference of periodograms and spectral densities.",WOS:000334256100004,ANNALS OF STATISTICS,"['CENTRAL-LIMIT-THEOREM', 'U-STATISTICS', 'QUADRATIC-FORMS', 'ASYMPTOTIC-DISTRIBUTION', 'STATIONARY-PROCESSES', 'APPROXIMATIONS', 'DISTRIBUTIONS', 'CONSISTENCY', 'DEPENDENCE', 'BOOTSTRAP']",INFERENCE OF WEIGHTED V-STATISTICS FOR NONSTATIONARY TIME SERIES AND ITS APPLICATIONS,2014
235,"In many applications one may be interested in drawing inferences regarding the order of a collection of points on a unit circle. Due to the underlying geometry of the circle standard constrained inference procedures developed for Euclidean space data are not applicable. Recently,statistical inference for parameters under such order constraints on a unit circle was discussed in Rueda ,Fernandez, and Peddada (2009) and Fernandez, Rueda,and Peddada(2012).In this paper we introduce the R package isocir which provides as et of functions that can be used for analyzing angular data subject to order constraints on a unit circle. Since this work is motivated by applications in cellbiology, we illustrate the proposed package using a relevant cell cycle data.",WOS:000323909800001,JOURNAL OF STATISTICAL SOFTWARE,"['CYCLE-REGULATED GENES', 'FISSION YEAST', 'EXPRESSION', 'IDENTIFICATION', 'STATISTICS', 'ORDER', 'WIND', 'SET']","isocir: An R Package for Constrained Inference Using Isotonic Regression for Circular Data, with an Application to Cell Biology",2013
236,"Log-Gaussian Cox processes are an important class of models for spatial and spatiotemporal point-pattern data. Delivering robust Bayesian inference for this class of models presents a substantial challenge, since Markov chain Monte Carlo (MCMC) algorithms require careful tuning in order to work well. To address this issue, we describe recent advances in MCMC methods for these models and their implementation in the R package lgcp. Our suite of R functions provides an extensible framework for inferring covariate effects as well as the parameters of the latent field.
We also present methods for Bayesian inference in two further classes of model based on the log-Gaussian Cox process. The first of these concerns the case where we wish to fit a point process model to data consisting of event-counts aggregated to a set of spatial regions: we demonstrate how this can be achieved using data-augmentation. The second concerns Bayesian inference for a class of marked-point processes specified via a multivariate log-Gaussian Cox process model. For both of these extensions, we give details of their implementation in R.",WOS:000349845600001,JOURNAL OF STATISTICAL SOFTWARE,"['MODEL-BASED GEOSTATISTICS', 'POINT PROCESS', 'RANDOM-FIELDS', 'PREDICTION', 'EPIDEMIOLOGY', 'PATTERNS', 'PACKAGE', 'RISK', 'MCMC', 'INLA']","Bayesian Inference and Data Augmentation Schemes for Spatial, Spatiotemporal and Multivariate Log-Gaussian Cox Processes in R",2015
237,"The statistical analysis and modeling of natural images is an important branch of statistics with applications in image signaling, image compression, computer vision, and human perception. Because the space of all possible images is too large to be sampled exhaustively, natural image models must inevitably make assumptions in order to stay tractable. Subsequent model comparison can then filter out those models that best capture the statistical regularities in natural images. Proper model comparison, however, often requires that the models and the preprocessing of the data match down to the implementation details. Here we present the Natter, a statistical software toolbox for natural images models, that can provide such consistency. The Natter includes powerful but tractable baseline model as well as standardized data preprocessing steps. It has an extensive test suite to ensure correctness of its algorithms, it interfaces to the modular toolkit for data processing toolbox MDP, and provides simple ways to log the results of numerical experiments. Most importantly, its modular structure can be extended by new models with minimal coding effort, thereby providing a platform for the development and comparison of probabilistic models for natural image data.",WOS:000349840800001,JOURNAL OF STATISTICAL SOFTWARE,"['INDEPENDENT COMPONENT ANALYSIS', 'DEAD LEAVES MODEL', 'TELEVISION SIGNALS', 'DISTRIBUTIONS', 'INFORMATION', 'SCENES', 'EMERGENCE', 'FRAMEWORK', 'ENTROPY', 'FILTERS']",Natter: A Python Natural Image Statistics Toolbox,2014
238,"Hierarchical cluster analysis is a valuable tool for exploring data by describing their structure using a dendrogram. However, proper visualization and interactive inspection of the dendrogram are needed to unlock the information in the data. We describe a new R package, idendro, that enables the user to inspect dendrograms interactively: to select and color clusters, to zoom and pan the dendrogram, and to visualize the clustered data not only in a built-in heat map, but also in any interactive plot implemented in the cranvas package. A lightweight version idendr0 with reduced dependencies is also available from the Comprehensive R Archive Network.",WOS:000398466800001,JOURNAL OF STATISTICAL SOFTWARE,['FLOW-CYTOMETRY'],Interactive Dendrograms: The R Packages idendro and idendr0,2017
239,"Over the last two decades, it has been observed that using the gradient vector as a search direction in large-scale optimization may lead to efficient algorithms. The effectiveness relies on choosing the step lengths according to novel ideas that are related to the spectrum of the underlying local Hessian rather than related to the standard decrease in the objective function. A review of these so-called spectral projected gradient methods for convex constrained optimization is presented. To illustrate the performance of these low-cost schemes, an optimization problem on the set of positive definite matrices is described.",WOS:000345288900001,JOURNAL OF STATISTICAL SOFTWARE,"['BARZILAI-BORWEIN METHOD', 'ELECTRONIC-STRUCTURE CALCULATIONS', 'BOUND-CONSTRAINED MINIMIZATION', 'NONLINEAR OPTIMAL PERTURBATION', 'AUGMENTED LAGRANGIAN-METHODS', 'NEAREST CORRELATION MATRIX', 'SUPPORT VECTOR MACHINES', 'LINE SEARCH TECHNIQUE', 'UNCONSTRAINED OPTIMIZATION', 'THIN-FILMS']",Spectral Projected Gradient Methods: Review and Perspectives,2014
240,"Setting the free parameters of classifiers to different values can have a profound impact on their performance. For some methods, specialized tuning algorithms have been developed. These approaches mostly tune parameters according to a single criterion, such as the cross-validation error. However, it is sometimes desirable to obtain parameter values that optimize several concurrent - often conflicting - criteria. The TunePareto package provides a general and highly customizable framework to select optimal parameters for classifiers according to multiple objectives. Several strategies for sampling and optimizing parameters are supplied. The algorithm determines a set of Pareto-optimal parameter configurations and leaves the ultimate decision on the weighting of objectives to the researcher. Decision support is provided by novel visualization techniques.",WOS:000301070700001,JOURNAL OF STATISTICAL SOFTWARE,"['SUPPORT VECTOR MACHINES', 'INTEGRATION', 'BIAS']",Multi-Objective Parameter Selection for Classifiers,2012
241,"In this paper, we present an R package that combines feature-based (X) data and graph-based (G) data for prediction of the response Y. In this particular case, Y is observed for a subset of the observations (labeled) and missing for the remainder (unlabeled). We examine an approach for fitting (Y) over cap = X (beta) over cap + (f) over cap (G) where (beta) over cap is a coefficient vector and (f) over cap f is a function over the vertices of the graph. The procedure is semi-supervised in nature (trained on the labeled and unlabeled sets), requiring iterative algorithms for fitting this estimate. The package provides several key functions for fitting and evaluating an estimator of this type. The package is illustrated on a text analysis data set, where the observations are text documents (papers), the response is the category of paper (either applied or theoretical statistics), the X information is the name of the journal in which the paper resides, and the graph is a co-citation network, with each vertex an observation and each edge the number of times that the two papers cite a common paper. An application involving classification of protein location using a protein interaction graph and an application involving classification on a manifold with part of the feature data converted to a graph are also presented.",WOS:000289932800001,JOURNAL OF STATISTICAL SOFTWARE,,spa: Semi-Supervised Semi-Parametric Graph-Based Estimation in R,2011
242,"We investigate differences between a simple Dominance Principle applied to sums of fair prices for variables and dominance applied to sums of forecasts for variables scored by proper scoring rules. In particular, we consider differences when fair prices and forecasts correspond to finitely additive expectations and dominance is applied with infinitely many prices and/or forecasts.",WOS:000336888400016,ANNALS OF STATISTICS,"['CONDITIONAL PROBABILITIES', 'POINT FORECASTS', 'CONGLOMERABILITY', 'DISINTEGRATIONS', 'COHERENCE']",DOMINATING COUNTABLY MANY FORECASTS,2014
243,"R is a mature open-source programming language for statistical computing and graphics. Many areas of statistical research are experiencing rapid growth in the size of data sets. Methodological advances drive increased use of simulations. A common approach is to use parallel computing.
This paper presents an overview of techniques for parallel computing with R on computer clusters, on multi-core systems, and in grid computing. It reviews sixteen different packages, comparing them on their state of development, the parallel technology used, as well as on usability, acceptance, and performance.
Two packages (snow, Rmpi) stand out as particularly suited to general use on computer clusters. Packages for grid computing are still in development, with only one package currently available to the end user. For multi-core systems five different packages exist, but a number of issues pose challenges to early adopters. The paper concludes with ideas for further developments in high performance computing with R. Example code is available in the appendix.",WOS:000268700300001,JOURNAL OF STATISTICAL SOFTWARE,['OPERATIONS'],State of the Art in Parallel Computing with R,2009
244,"We describe an implementation of simple, multiple and joint correspondence analysis in R. The resulting package comprises two parts, one for simple correspondence analysis and one for multiple and joint correspondence analysis. Within each part, functions for computation, summaries and visualization in two and three dimensions are provided, including options to display supplementary points and perform subset analyses. Special emphasis has been put on the visualization functions that offer features such as different scaling options for biplots and three-dimensional maps using the rgl package. Graphical options include shading and sizing plot symbols for the points according to their contributions to the map and masses respectively.",WOS:000247010800001,JOURNAL OF STATISTICAL SOFTWARE,,"Correspondence analysis in R, with two- and three-dimensional graphics: The ca package",2007
245,"This paper introduces the elliptic package of R routines, for numerical calculation of elliptic and related functions. Elliptic functions furnish interesting and instructive examples of many ideas of complex analysis, and the package illustrates these numerically and visually. A statistical application in fluid mechanics is presented.",WOS:000236151400001,JOURNAL OF STATISTICAL SOFTWARE,,"Introducing elliptic, an R package for elliptic and modular functions",2006
246,"Fan charts, first developed by the Bank of England in 1996, have become a standard method for visualising forecasts with uncertainty. Using shading fan charts focus the attention towards the whole distribution away from a single central measure. This article describes the basics of plotting fan charts using an R add-on package alongside some additional methods for displaying sequential distributions. Examples are based on distributions of both estimated parameters from a time series model and future values with uncertainty.",WOS:000357431900003,R JOURNAL,"['UNCERTAINTY', 'MODEL']",fanplot: An R Package for Visualising Sequential Distributions,2015
247,,WOS:000208589700006,R JOURNAL,,AdMit: Adaptive Mixtures of Student-t Distributions,2009
248,"This paper gives a general method for deriving limiting distributions of complete case statistics for missing data models from corresponding results for the model where all data are observed. This provides a convenient tool for obtaining the asymptotic behavior of complete case versions of established full data methods without lengthy proofs.
The methodology is illustrated by analyzing three inference procedures for partially linear regression models with responses missing at random. We first show that complete case versions of asymptotically efficient estimators of the slope parameter for the full model are efficient, thereby solving the problem of constructing efficient estimators of the slope parameter for this model. Second, we derive an asymptotically distribution free test for fitting a normal distribution to the errors. Finally, we obtain an asymptotically distribution free test for linearity, that is, for testing that the nonparametric component of these models is a constant. This test is new both when data are fully observed and when data are missing at random.",WOS:000321845400010,ANNALS OF STATISTICS,"['ADDITIVE REGRESSION-MODELS', 'OF-FIT TESTS', 'NONPARAMETRIC REGRESSION', 'EFFICIENT ESTIMATION', 'ERROR DISTRIBUTION', 'LINEAR-MODEL', 'RESPONSES', 'CHECKING']",THE TRANSFER PRINCIPLE: A TOOL FOR COMPLETE CASE ANALYSIS,2012
249,"The lmdme package decomposes analysis of variance (ANOVA) through linear models on designed multivariate experiments, allowing ANOVA-principal component analysis (APCA) and ANOVA-simultaneous component analysis (ASCA) in R. It also extends both methods with the application of partial least squares (PLS) through the specification of a desired output matrix. The package is freely available from Bioconductor and licensed under the GNU General Public License.
ANOVA decomposition methods for designed multivariate experiments are becoming popular in ""omics"" experiments (transcriptomics, metabolomics, etc.), where measurements are performed according to a predefined experimental design, with several experimental factors or including subject-specific clinical covariates, such as those present in current clinical genomic studies. ANOVA-PCA and ASCA are well-suited methods for studying interaction patterns on multidimensional datasets. However, currently an R implementation of APCA is only available for Spectra data in the ChemoSpec package, whereas ASCA is based on average calculations on the indices of up to three design matrices. Thus, no statistical inference on estimated effects is provided. Moreover, ASCA is not available in an R package.
Here, we present an R implementation for ANOVA decomposition with PCA/PLS analysis that allows the user to specify (through a flexible formula interface), almost any linear model with the associated inference on the estimated effects, as well as to display functions to explore results both of PCA and PLS. We describe the model, its implementation and two high-throughput microarray examples: one applied to interaction pattern analysis and the other to quality assessment.",WOS:000332109600001,JOURNAL OF STATISTICAL SOFTWARE,"['COMPONENT ANALYSIS', 'GENE-EXPRESSION', 'TOOL']",lmdme: Linear Models on Designed Multivariate Experiments in R,2014
250,"We propose nonparametric estimators of the occupation measure and the occupation density of the diffusion coefficient (stochastic volatility) of a discretely observed Ito semimartingale on a fixed interval when the mesh of the observation grid shrinks to zero asymptotically. In a first step we estimate the volatility locally over blocks of shrinking length, and then in a second step we use these estimates to construct a sample analogue of the volatility occupation time and a kernel-based estimator of its density. We prove the consistency of our estimators and further derive bounds for their rates of convergence. We use these results to estimate nonparametrically the quantiles associated with the volatility occupation measure.",WOS:000326991200006,ANNALS OF STATISTICS,"['DIFFUSION-COEFFICIENT', 'MODELS']",VOLATILITY OCCUPATION TIMES,2013
251,"Application of nonparametric and semiparametric regression techniques to high-dimensional time series data has been hampered due to the lack of effective tools to address the ""curse of dimensionality."" Under rather weak conditions, we propose spline-backfitted kernel estimators of the component functions for the nonlinear additive time series data that are both computationally expedient so they are usable for analyzing very high-dimensional time series, and theoretically reliable so inference can be made on the component functions with confidence. Simulation experiments have provided strong evidence that corroborates the asymptotic theory.",WOS:000253077800012,ANNALS OF STATISTICS,"['REGRESSION-MODELS', 'NONPARAMETRIC-ESTIMATION', 'IDENTIFICATION', 'INTEGRATION']",Spline-backfitted kernel smoothing of nonlinear additive autoregression model,2007
252,"In this paper, we describe %PROC_R, a SAS macro that enables native R language to be embedded in and executed along with a SAS program in the base SAS environment under Windows OS. This macro executes a user-defined R code in batch mode by calling the unnamed pipe method within base SAS. The R textual and graphical output can be routed to the SAS output window and result viewer, respectively. Also, this macro automatically converts data between SAS datasets and R data frames such that the data and results from each statistical environment can be utilized by the other environment. The objective of this work is to leverage the strength of the R programming language within the SAS environment in a systematic manner. Moreover, this macro helps statistical programmers to learn a new statistical language while staying in a familiar environment.",WOS:000326870600001,JOURNAL OF STATISTICAL SOFTWARE,,%PROC_R: ASAS Macro that Enables Native R Programming in the Base SAS Environment,2012
253,"This paper first establishes a strong law of large numbers and a strong invariance principle for forward and backward sums of near-epoch dependent sequences. Using these limiting theorems, we develop a general asymptotic theory on the Wald test for change points in a general class of time series models under the no change-point hypothesis. As an application, we verify our assumptions for the long-memory fractional ARIMA model.",WOS:000248692700012,ANNALS OF STATISTICS,"['INVARIANCE-PRINCIPLES', 'RANDOM-VARIABLES', 'PARAMETER INSTABILITY', 'LINEAR REGRESSIONS', 'INEQUALITY', 'SUMS', 'LAWS']",Testing for change points in time series models and limiting theorems for ned sequences,2007
254,"We provide user friendly software for Bayesian analysis of functional data models using WinBUGS 1.4. The excellent properties of Bayesian analysis in this context are due to: (1) dimensionality reduction, which leads to low dimensional projection bases; (2) mixed model representation of functional models, which provides a modular approach to model extension; and (3) orthogonality of the principal component bases, which contributes to excellent chain convergence and mixing properties. Our paper provides one more, essential, reason for using Bayesian analysis for functional models: the existence of software.",WOS:000273372100001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR MIXED MODELS', 'PRINCIPAL COMPONENT ANALYSIS', 'LIKELIHOOD RATIO TESTS', 'COLON CARCINOGENESIS', 'PENALIZED SPLINES', 'REGRESSION-MODELS', 'LONGITUDINAL DATA', 'SLEEP']",Bayesian Functional Data Analysis Using WinBUGS,2010
255,,WOS:000208589700011,R JOURNAL,,Easier Parallel Computing in R with snowfall and sfCluster,2009
256,We show that the short period of the uniform random number generator in the published implementation of Marsaglia and Tsang's Ziggurat method for generating random deviates can lead to poor distributions. Changing the uniform random number generator used in its implementation fixes this issue.,WOS:000232806900001,JOURNAL OF STATISTICAL SOFTWARE,,A comment on the implementation of the Ziggurat method,2005
257,"The R package HGLMMM has been developed to fit generalized linear models with random effects using the h-likelihood approach. The response variable is allowed to follow a binomial, Poisson, Gaussian or gamma distribution. The distribution of random effects can be specified as Gaussian, gamma, inverse-gamma or beta. Complex structures as multi-membership design or multilevel designs can be handled. Further, dispersion parameters of random components and the residual dispersion (overdispersion) can be modeled as a function of covariates. Overdispersion parameter can be fixed or estimated. Fixed effects in the mean structure can be estimated using extended likelihood or a first order Laplace approximation to the marginal likelihood. Dispersion parameters are estimated using first order adjusted profile likelihood.",WOS:000288206300001,JOURNAL OF STATISTICAL SOFTWARE,"['LIKELIHOOD', 'INFERENCE']",Hierarchical Generalized Linear Models: The R Package HGLMMM,2011
258,"In this paper we review the state space approach to time series analysis and establish the notation that is adopted in this special volume of the Journal of Statistical Software. We first provide some background on the history of state space methods for the analysis of time series. This is followed by a concise overview of linear Gaussian state space analysis including the modelling framework and appropriate estimation methods. We discuss the important class of unobserved component models which incorporate a trend, a seasonal, a cycle, and fixed explanatory and intervention variables for the univariate and multivariate analysis of time series. We continue the discussion by presenting methods for the computation of different estimates for the unobserved state vector: filtering, prediction, and smoothing. Estimation approaches for the other parameters in the model are also considered. Next, we discuss how the estimation procedures can be used for constructing confidence intervals, detecting outlier observations and structural breaks, and testing model assumptions of residual independence, homoscedasticity, and normality. We then show how ARIMA and ARIMA components models fit in the state space framework to time series analysis. We also provide a basic introduction for non-Gaussian state space models. Finally, we present an overview of the software tools currently available for the analysis of time series with state space methods as they are discussed in the other contributions to this special volume.",WOS:000290526200001,JOURNAL OF STATISTICAL SOFTWARE,"['TIME-SERIES MODELS', 'COMPONENTS']",Statistical Software for State Space Methods,2011
259,"Standard approaches to constructing nonparametric confidence bands for functions are frustrated by the impact of bias, which generally is not estimated consistently when using the bootstrap and conventionally smoothed function estimators. To overcome this problem it is common practice to either undersmooth, so as to reduce the impact of bias, or oversmooth, and thereby introduce an explicit or implicit bias estimator. However, these approaches, and others based on nonstandard smoothing methods, complicate the process of inference, for example, by requiring the choice of new, unconventional smoothing parameters and, in the case of undersmoothing, producing relatively wide bands. In this paper we suggest a new approach, which exploits to our advantage one of the difficulties that, in the past, has prevented an attractive solution to the problem-the fact that the standard bootstrap bias estimator suffers from relatively high-frequency stochastic error. The high frequency, together with a technique based on quantiles, can be exploited to dampen down the stochastic error term, leading to relatively narrow, simple-to-construct confidence bands.",WOS:000326991200007,ANNALS OF STATISTICS,"['ESTIMATING RESIDUAL VARIANCE', 'LEAST-SQUARES REGRESSION', 'DENSITY-ESTIMATION', 'BANDWIDTH CHOICE', 'CURVE ESTIMATION', 'INTERVALS', 'ERROR', 'HETEROSCEDASTICITY', 'APPROXIMATION', 'ESTIMATORS']",A SIMPLE BOOTSTRAP METHOD FOR CONSTRUCTING NONPARAMETRIC CONFIDENCE BANDS FOR FUNCTIONS,2013
260,"Estimation of low-rank matrices is of significant interest in a range of contemporary applications. In this paper, we introduce a rank-one projection model for low-rank matrix recovery and propose a constrained nuclear norm minimization method for stable recovery of low-rank matrices in the noisy case. The procedure is adaptive to the rank and robust against small perturbations. Both upper and lower bounds for the estimation accuracy under the Frobenius norm loss are obtained. The proposed estimator is shown to be rate-optimal under certain conditions. The estimator is easy to implement via convex programming and performs well numerically.
The techniques and main results developed in the paper also have implications to other related statistical problems. An application to estimation of spiked covariance matrices from one-dimensional random projections is considered. The results demonstrate that it is still possible to accurately estimate the covariance matrix of a high-dimensional distribution based only on one-dimensional projections.",WOS:000349738500005,ANNALS OF STATISTICS,"['INFORMATION-THEORETIC CRITERIA', 'PRINCIPAL-COMPONENTS-ANALYSIS', 'ADAPTIVE ESTIMATION', 'SPARSE SIGNALS', 'OPTIMAL RATES', 'COMPLETION', 'MINIMIZATION', 'MODEL', 'PCA']",ROP: MATRIX RECOVERY VIA RANK-ONE PROJECTIONS,2015
261,"Although the leave-subject-out cross-validation (CV) has been widely used in practice for tuning parameter selection for various nonparametric and semiparametric models of longitudinal data, its theoretical property is unknown and solving the associated optimization problem is computationally expensive, especially when there are multiple tuning parameters. In this paper, by focusing on the penalized spline method, we show that the leave-subject-out CV is optimal in the sense that it is asymptotically equivalent to the empirical squared error loss function minimization. An efficient Newton-type algorithm is developed to compute the penalty parameters that optimize the CV criterion. Simulated and real data are used to demonstrate the effectiveness of the leave-subject-out CV in selecting both the penalty parameters and the working correlation matrix.",WOS:000321845400009,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'LONGITUDINAL DATA', 'NONPARAMETRIC REGRESSION', 'LINEAR-MODELS', 'SPLINE']",ASYMPTOTIC OPTIMALITY AND EFFICIENT COMPUTATION OF THE LEAVE-SUBJECT-OUT CROSS-VALIDATION,2012
262,,WOS:000321944400001,R JOURNAL,,Untitled,2013
263,"In this paper, we study parametric nonlinear regression under the Harris recurrent Markov chain framework. We first consider the nonlinear least squares estimators of the parameters in the homoskedastic case, and establish asymptotic theory for the proposed estimators. Our results show that the convergence rates for the estimators rely not only on the properties of the nonlinear regression function, but also on the number of regenerations for the Harris recurrent Markov chain. Furthermore, we discuss the estimation of the parameter vector in a conditional volatility function, and apply our results to the nonlinear regression with I (1) processes and derive an asymptotic distribution theory which is comparable to that obtained by Park and Phillips [Econometrica 69 (2001) 117-161]. Some numerical studies including simulation and empirical application are provided to examine the finite sample performance of the proposed approaches and results.",WOS:000384397200009,ANNALS OF STATISTICS,"['INTEGRATED TIME-SERIES', 'ASYMPTOTIC THEORY', 'NONPARAMETRIC-ESTIMATION', 'MODELS', 'CONSISTENCY', 'LIKELIHOOD', 'INFERENCE']",ESTIMATION IN NONLINEAR REGRESSION WITH HARRIS RECURRENT MARKOV CHAINS,2016
264,"This article describes an approach to importing vector-based graphical images into statistical software as implemented in a package called grImport for the R statistical computing environment. This approach assumes that an original image can be transformed into a PostScript format (i.e., the opriginal image is in a standard vector graphics format such as PostScript, PDF, or SVG). The grImport pack-age consists of three components: a function for converting PostScript files to an R-specific XML format; a function for reading the XML format into special Picture objects in R; and functions for manipulating and drawing Picture objects. Several examples and applications are presented, including annotating a statistical plot with an imported logo and using imported images as plotting symbols.",WOS:000266310600001,JOURNAL OF STATISTICAL SOFTWARE,['DISPLAY'],Importing Vector Graphics: The grImport Package for R,2009
265,"Internal pilot designs involve conducting interim power analysis (without interim data analysis) to modify the final sample size. Recently developed techniques have been described to avoid the type I error rate inflation inherent to unadjusted hypothesis tests, while still providing the advantages of an internal pilot design. We present GLUMIP 2.0, the latest version of our free SAS/IML software for planning internal pilot studies in the general linear univariate model (GLUM) framework. The new analytic forms incorporated into the updated software solve many problems inherent to current internal pilot techniques for linear models with Gaussian errors. Hence, the GLUMIP 2.0 software makes it easy to perform exact power analysis for internal pilots under the GLUM framework with independent Gaussian errors and fixed predictors.",WOS:000261526100001,JOURNAL OF STATISTICAL SOFTWARE,"['SAMPLE-SIZE REESTIMATION', 'I ERROR RATE', 'ADAPTIVE DESIGNS', 'CLINICAL-TRIALS', 'LINEAR-MODEL', 'VARIANCE', 'ADJUSTMENT', 'PARAMETER']",GLUMIP 2.0: SAS/IML Software for Planning Internal Pilots,2008
266,"Multiple response categorical variables (MRCVs), also known as ""pick any"" or ""choose all that apply"" variables, summarize survey questions for which respondents are allowed to select more than one category response option. Traditional methods for analyzing the association between categorical variables are not appropriate with MRCVs due to the within-subject dependence among responses. We have developed the MRCV package as the first R package available to correctly analyze MRCV data. Statistical methods offered by our package include counterparts to traditional Pearson chi-square tests for independence and loglinear models, where bootstrap methods and Rao-Scott adjustments are relied on to obtain valid inferences. We demonstrate the primary functions within the package by analyzing data from a survey assessing the swine waste management practices of Kansas farmers.",WOS:000343788100015,R JOURNAL,"['MARGINAL INDEPENDENCE', 'CHOICES', 'ASSOCIATION', 'STRATEGIES']",MRCV: A Package for Analyzing Categorical Variables with Multiple Response Options,2014
267,"We consider estimating the predictive density under Kullback-Leibler loss in an l(0) sparse Gaussian sequence model. Explicit expressions of the first order minimax risk along with its exact constant, asymptotically least favorable priors and optimal predictive density estimates are derived. Compared to the sparse recovery results involving point estimation of the normal mean, new decision theoretic phenomena are seen. Suboptimal performance of the class of plug-in density estimates reflects the predictive nature of the problem and optimal strategies need diversification of the future risk. We find that minimax optimal strategies lie outside the Gaussian family but can be constructed with threshold predictive density estimates. Novel minimax techniques involving simultaneous calibration of the sparsity adjustment and the risk diversification mechanisms are used to design optimal predictive density estimates.",WOS:000355768700001,ANNALS OF STATISTICS,"['DISTRIBUTIONS', 'OBSERVABLES', 'ENTROPY', 'RISK']",EXACT MINIMAX ESTIMATION OF THE PREDICTIVE DENSITY IN SPARSE GAUSSIAN MODELS,2015
268,"Generalized estimating equation solvers in R only allow for a few pre-determined options for the link and variance functions. We provide a package, geeM, which is implemented entirely in R and allows for user specified link and variance functions. The sparse matrix representations provided in the Matrix package enable a fast implementation. To gain speed, we make use of analytic inverses of the working correlation when possible and a trick to find quick numeric inverses when an analytic inverse is not available. Through three examples, we demonstrate the speed of geeM, which is not much worse than C implementations like geepack and gee on small data sets and faster on large data sets.",WOS:000321944400018,R JOURNAL,,Fast Pure R Implementation of GEE: Application of the Matrix Package,2013
269,"This paper discusses asymptotic distributions of various estimators of the underlying parameters in some regression models with long memory (LM) Gaussian design and nonparametric heteroscedastic LM moving average errors. In the simple linear regression model, the first-order asymptotic distribution of the least square estimator of the slope parameter is observed to be degenerate. However, in the second order, this estimator is n(1/2)-consistent and asymptotically normal for h + H < 3/2; nonnormal otherwise, where h and H are LM parameters of design and error processes, respectively. The finite-dimensional asymptotic distributions of a class of kernel type estimators of the conditional variance function sigma(2)(x) in a more general heteroscedastic regression model are found to be normal whenever H < (1 + h)/2, and nonnormal otherwise. In addition, in this general model, log (n)-consistency of the local Whittle estimator of H based on pseudo residuals and consistency of a cross validation type estimator of sigma(2)(x) are established. All of these findings are then used to propose a lack-of-fit test of a parametric regression model, with an application to some currency exchange rate data which exhibit LM.",WOS:000253390000019,ANNALS OF STATISTICS,"['RANGE DEPENDENT ERRORS', 'TIME-SERIES', 'NONPARAMETRIC REGRESSION', 'STATIONARY ERRORS', 'CHECKS', 'ESTIMATORS']",Asymptotic inference in some heteroscedastic regression models with long memory design and errors,2008
270,"Closed testing and partitioning are recognized as fundamental principles of familywise error control. In this paper, we argue that sequential rejection can be considered equally fundamental as a general principle of multiple testing. We present a general sequentially rejective multiple testing procedure and show that many well-known familywise error controlling methods can be constructed as special cases of this procedure, among which are the procedures of Holm, Shaffer and Hochberg, parallel and serial gatekeeping procedures, modern procedures for multiple testing in graphs, resampling-based multiple testing procedures and even the closed testing and partitioning procedures themselves. We also give a general proof that sequentially rejective multiple testing procedures strongly control the familywise error if they fulfill simple criteria of monotonicity of the critical values and a limited form of weak familywise error control in each single step. The sequential rejection principle gives a novel theoretical perspective on many well-known multiple testing procedures, emphasizing the sequential aspect. Its main practical usefulness is for the development of multiple testing procedures for null hypotheses, possibly logically related, that are structured in a graph. We illustrate this by presenting a uniform improvement of a recently published procedure.",WOS:000290231500014,ANNALS OF STATISTICS,"['MULTIPLE TESTING PROCEDURES', 'CLINICAL-TRIAL APPLICATIONS', 'GATEKEEPING STRATEGIES', 'BONFERRONI PROCEDURE', 'HYPOTHESES', 'RATES']",THE SEQUENTIAL REJECTION PRINCIPLE OF FAMILYWISE ERROR CONTROL,2010
271,"stpp is an R package for analyzing, simulating and displaying space-time point patterns. It covers many of the models encountered in applications of point process methods to the study of spatio-temporal phenomena. The package also includes estimators of the space-time inhomogeneous K-function and pair correlation function. stpp is the first dedicated unified computational environment in the area of spatio-temporal point processes. In this paper we describe space-time point processes and introduce the package stpp to new users.",WOS:000318236600001,JOURNAL OF STATISTICAL SOFTWARE,"['GAUSSIAN COX PROCESSES', '2ND-ORDER ANALYSIS', 'FIELDS']","stpp: An R Package for Plotting, Simulating and Analyzing Spatio-Temporal Point Patterns",2013
272,"This paper presents a study of the large-sample behavior of the posterior distribution of a structural parameter which is partially identified by moment inequalities. The posterior density is derived based on the limited information likelihood. The posterior distribution converges to zero exponentially fast on any delta-contraction Outside the identified region. Inside, if is bounded below by a positive constant if the identified region is assumed to have a nonempty interior. Our simulation evidence indicates that the Bayesian approach has advantages over frequentist methods, in the sense that, with a proper choice of the prior, the posterior provides more information about the true parameter inside the identified region. We also address the problem of moment and model selection. Our optimality criterion is the maximum posterior procedure and we show that, asymptotically, it selects the true moment/model combination with the most moment inequalities and the simplest model.",WOS:000273800100009,ANNALS OF STATISTICS,"['PARTIALLY IDENTIFIED PARAMETERS', 'ECONOMETRIC-MODELS', 'PRIOR INFORMATION', 'IDENTIFIABILITY', 'INFERENCE', 'LIKELIHOOD', 'SETS']",BAYESIAN ANALYSIS IN MOMENT INEQUALITY MODELS,2010
273,"This paper describes the ""strucplot"" framework for the visualization of multi-way contingency tables. Strucplot displays include hierarchical conditional plots such as mosaic, association, and sieve plots, and can be combined into more complex, specialized plots for visualizing conditional independence, GLMs, and the results of independence tests. The framework's modular design allows flexible customization of the plots' graphical appearance, including shading, labeling, spacing, and legend, by means of ""graphical appearance control"" functions. The framework is provided by the R package vcd.",WOS:000241808100001,JOURNAL OF STATISTICAL SOFTWARE,"['GRAPHICAL DISPLAY', 'CATEGORICAL-DATA', 'MOSAIC DISPLAYS']",The strucplot framework: visualizing multi-way contingency tables with VCD,2006
274,"The incorporation of additional information into discriminant rules is receiving increasing attention as the rules including this information perform better than the usual rules. In this paper we introduce an R package called dawai, which provides the functions that allow to define the rules that take into account this additional information expressed in terms of restrictions on the means, to classify the samples and to evaluate the accuracy of the results. Moreover, in this paper we extend the results and definitions given in previous papers (Fernandez, Rueda, and Salvador 2006, Conde, Fernandez, Rueda, and Salvador 2012, Conde, Salvador, Rueda, and Fernandez 2013) to the case of unequal co-variances among the populations, and consequently define the corresponding restricted quadratic discriminant rules. We also define estimators of the accuracy of the rules for the general more than two populations case. The wide range of applications of these procedures is illustrated with two data sets from two different fields, i.e., biology and pattern recognition.",WOS:000365978900001,JOURNAL OF STATISTICAL SOFTWARE,"['CELL-CYCLE GENES', 'ERROR RATE', 'CROSS-VALIDATION', 'CLASSIFICATION RULES', 'ORDER RESTRICTIONS', 'PREDICTION ERROR', 'CANCER TRIAL', 'BOOTSTRAP', 'DISTRIBUTIONS', 'INFERENCE']",dawai: An R Package for Discriminant Analysis with Additional Information,2015
275,"Weighted generalized ridge regression offers unique advantages in correlated high-dimensional problems. Such estimators can be efficiently computed using Bayesian spike and slab models and are effective for prediction. For sparse variable selection, a generalization of the elastic net can be used in tandem with these Bayesian estimates. In this article, we describe the R-software package spikeslab for implementing this new spike and slab prediction and variable selection methodology.",WOS:000208590000011,R JOURNAL,,spikeslab: Prediction and Variable Selection Using Spike and Slab Regression,2010
276,"We propose two estimators of a monotone spectral density, that are based on the periodogram. These are the isotonic regression of the periodogram and the isotonic regression of the log-periodogram. We derive pointwise limit distribution results for the proposed estimators for short memory linear processes and long memory Gaussian processes and also that the estimators are rate optimal.",WOS:000288183800014,ANNALS OF STATISTICS,"['LOG-PERIODOGRAM REGRESSION', 'LONG-RANGE DEPENDENCE', 'TIME-SERIES']",MONOTONE SPECTRAL DENSITY ESTIMATION,2011
277,"When the spatial sample size is extremely large, which occurs in many environmental and ecological studies, operations on the large covariance matrix are a numerical challenge. Covariance tapering is a technique to alleviate the numerical challenges. Under the assumption that data are collected along a line in a bounded region, we investigate how the tapering affects the asymptotic efficiency of the maximum likelihood estimator (MLE) for the microergodic parameter in the Matern covariance function by establishing the fixed-domain asymptotic distribution of the exact MLE and that of the tapered MLE. Our results imply that, under some conditions on the taper, the tapered MLE is asymptotically as efficient as the true MLE for the microergodic parameter in the Matern model.",WOS:000271673500009,ANNALS OF STATISTICS,"['COVARIANCE FUNCTION', 'LINEAR PREDICTIONS', 'STOCHASTIC-PROCESS', 'RANDOM-FIELDS', 'INTERPOLATION', 'PARAMETERS', 'ERROR', 'MODEL']",FIXED-DOMAIN ASYMPTOTIC PROPERTIES OF TAPERED MAXIMUM LIKELIHOOD ESTIMATORS,2009
278,"We demonstrate that the processes underlying on-line auction price bids and many other longitudinal data can be represented by an empirical first order stochastic ordinary differential equation with time-varying coefficients and a smooth drift process. This equation may be empirically obtained from longitudinal observations for a sample of subjects and does not presuppose specific knowledge of the underlying processes. For the nonparametric estimation of the components of the differential equation, it suffices to have available sparsely observed longitudinal measurements which may be noisy and are generated by underlying smooth random trajectories for each subject or experimental unit in the sample. The drift process that drives the equation determines how closely individual process trajectories follow a deterministic approximation of the differential equation. We provide estimates for trajectories and especially the variance function of the drift process. At each fixed time point, the proposed empirical dynamic model implies a decomposition of the derivative of the process underlying the longitudinal data into a component explained by a linear component determined by a varying coefficient function dynamic equation and an orthogonal complement that corresponds to the drift process. An enhanced perturbation result enables us to obtain improved asymptotic convergence rates for eigenfunction derivative estimation and consistency for the varying coefficient function and the components of the drift process. We illustrate the differential equation with an application to the dynamics of on-line auction data.",WOS:000290231500005,ANNALS OF STATISTICS,"['FUNCTIONAL DATA-ANALYSIS', 'NONPARAMETRIC REGRESSION-ANALYSIS', 'PRINCIPAL COMPONENT ANALYSIS', 'DIFFERENTIAL-EQUATIONS', 'AUCTION DYNAMICS', 'ONLINE AUCTIONS', 'CURVES', 'MODELS', 'DERIVATIVES', 'GROWTH']",EMPIRICAL DYNAMICS FOR LONGITUDINAL DATA,2010
279,"The paper considers functional linear regression, where scalar responses Y1,..., Yn are modeled in dependence of i.i.d. random functions X1,..., Xn. We study a generalization of the classical functional linear regression model. It is assumed that there exists an unknown number of ""points of impact,"" that is, discrete observation times where the corresponding functional values possess significant influences on the response variable. In addition to estimating a functional slope parameter, the problem then is to determine the number and locations of points of impact as well as corresponding regression coefficients. Identifiability of the generalized model is considered in detail. It is shown that points of impact are identifiable if the underlying process generating X1,..., Xn possesses "" specific local variation."" Examples are well-known processes like the Brownian motion, fractional Brownian motion or the Ornstein-Uhlenbeck process. The paper then proposes an easily implementable method for estimating the number and locations of points of impact. It is shown that this number can be estimated consistently. Furthermore, rates of convergence for location estimates, regression coefficients and the slope parameter are derived. Finally, some simulation results as well as a real data application are presented.",WOS:000368022000001,ANNALS OF STATISTICS,"['MODELS', 'METHODOLOGY', 'ESTIMATORS']",FUNCTIONAL LINEAR REGRESSION WITH POINTS OF IMPACT,2016
280,"Based on recent work by Fox and Anderson (2006), this paper describes substantial extensions to the effects package for R to construct effect displays for multinomial and proportional-odds logit models. The package previously was limited to linear and generalized linear models. Effect displays are tabular and graphical representations of terms - typically high-order terms - in a statistical model. For polytomous logit models, effect displays depict fitted category probabilities under the model, and can include point-wise confidence envelopes for the effects. The construction of effect displays by functions in the effects package is essentially automatic. The package provides several kinds of displays for polytomous logit models.",WOS:000270821500001,JOURNAL OF STATISTICAL SOFTWARE,,Effect Displays in R for Multinomial and Proportional-Odds Logit Models: Extensions to the effects Package,2009
281,"The rivr package provides a computational toolset for simulating steady and unsteady one-dimensional flows in open channels. It is designed primarily for use by instructors of undergraduate- and graduate-level open-channel hydrodynamics courses in such diverse fields as river engineering, physical geography and geophysics. The governing equations used to describe open-channel flows are briefly presented, followed by example applications. These include the computation of gradually-varied flows and two examples of unsteady flows in channels-namely, the tracking of the evolution of a flood wave in a channel and the prediction of extreme variation in the water-surface profile that results when a sluice gate is abruptly closed. Model results for the unsteady flow examples are validated against standard benchmarks. The article concludes with a discussion of potential modifications and extensions to the package.",WOS:000368551800019,R JOURNAL,"['COMPOUND CHANNELS', 'DEPTH']",Open-Channel Computation with R,2015
282,"This paper examines asymptotic equivalence in the sense of Le Cam between density estimation experiments and the accompanying Poisson experiments. The significance of asymptotic equivalence is that all asymptotically optimal statistical procedures can be carried over from one experiment to the other. The equivalence given here is established under a weak assumption on the parameter space T. In particular, a sharp Besov smoothness condition is given on T which is sufficient for Poissonization, namely, if F is in a Besov ball B-p,q(alpha) (M) with alpha p > 1/2. Examples show Poissonization is not possible whenever up < 1/2. In addition, asymptotic equivalence of the density estimation model and the accompanying Poisson experiment is established for all compact subsets of C([0, 1](m)), a condition which includes all Holder balls with smoothness alpha > 0.",WOS:000248692700009,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'ASYMPTOTIC EQUIVALENCE', 'DENSITY-ESTIMATION']",A complement to Le Cam's theorem,2007
283,"LibBi is a software package for state space modelling and Bayesian inference on modern computer hardware, including multi-core central processing units, many-core graphics processing units, and distributed-memory clusters of such devices. The software parses a domain-specific language for model specification, then optimizes, generates, compiles and runs code for the given model, inference method and hardware platform. In presenting the software, this work serves as an introduction to state space models and the specialized methods developed for Bayesian inference with them. The focus is on sequential Monte Carlo (SMC) methods such as the particle filter for state estimation, and the particle Markov chain Monte Carlo and SMC2 methods for parameter estimation. All are well-suited to current computer hardware. Two examples are given and developed throughout, one a linear three-element windkessel model of the human arterial system, the other a nonlinear Lorenz '96 model. These are specified in the prescribed modelling language, and LibBi demonstrated by performing inference with them. Empirical results are presented, including a performance comparison of the software with different hardware configurations.",WOS:000365984000001,JOURNAL OF STATISTICAL SOFTWARE,"['CHAIN MONTE-CARLO', 'PARTICLE FILTERS', 'CONTINUOUS-TIME', 'INFERENCE', 'ERROR', 'DISTRIBUTIONS', 'SIMULATION', 'ALGORITHM', 'FRAMEWORK', 'MCMC']",Bayesian State-Space Modelling on High-Performance Hardware Using LibBi,2015
284,"This paper develops a point impact linear regression model in which the trajectory of a continuous stochastic process, when evaluated at a sensitive time point, is associated with a scalar response. The proposed model complements and is more interpretable than the functional linear regression approach that has become popular in recent years. The trajectories are assumed to have fractal (self-similar) properties in common with a fractional Brownian motion with an unknown Hurst exponent. Bootstrap confidence intervals based on the least-squares estimator of the sensitive time point are developed. Misspecification of the point impact model by a functional linear model is also investigated. Non-Gaussian limit distributions and rates of convergence determined by the Hurst exponent play an important role.",WOS:000280359400020,ANNALS OF STATISTICS,"['FRACTIONAL BROWNIAN MOTIONS', 'CUBE ROOT ASYMPTOTICS', 'HUMAN BREAST-CANCER', 'GENE-EXPRESSION', 'BOOTSTRAP', 'MODELS', 'ESTIMATORS', 'MICROARRAY', 'MAXIMUM']",FRACTALS WITH POINT IMPACT IN FUNCTIONAL LINEAR REGRESSION,2010
285,"We present a new methodology for sufficient dimension reduction (SDR). Our methodology derives directly from the formulation of SDR in terms of the conditional independence of the covariate X from the response Y, given the projection of X on the central subspace [cf. J. Amer Statist. Assoc. 86 (1991) 316-342 and Regression Graphics (1998) Wiley]. We show that this conditional independence assertion can be characterized in terms of conditional covariance operators on reproducing kernel Hilbert spaces and we show how this characterization leads to an M-estimator for the central subspace. The resulting estimator is shown to be consistent under weak conditions; in particular, we do not have to impose linearity or ellipticity conditions of the kinds that are generally invoked for SDR methods. We also present empirical results showing that the new methodology is competitive in practice.",WOS:000268113500009,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'RESPONSE REGRESSION', 'VISUALIZATION']",KERNEL DIMENSION REDUCTION IN REGRESSION,2009
286,"Supersaturated design (SSD) has received much recent interest because of its potential in factor screening experiments. In this paper, we provide equivalent conditions for two columns to be fully aliased and consequently propose methods for constructing E(f(NoD))- and chi(2)-optimal mixed-level SSDs without fully aliased columns, via equidistant designs and difference matrices. The methods can be easily performed and many new optimal mixed-level SSDs have been obtained. Furthermore, it is proved that the nonorthogonality between columns of the resulting design is well controlled by the source designs. A rather complete list of newly generated optimal mixed-level SSDs are tabulated for practical use.",WOS:000291183300022,ANNALS OF STATISTICS,"['ABERRATION', 'NUMBER']",ON CONSTRUCTION OF OPTIMAL MIXED-LEVEL SUPERSATURATED DESIGNS,2011
287,"SamplerCompare is an R package for comparing the performance of Markov chain Monte Carlo (MCMC) samplers. It samples from a collection of distributions with a collection of MCMC methods over a range of tuning parameters. Then, using log density evaluations per uncorrelated observation as a figure of merit, it generates a grid of plots showing the results of the simulation. It comes with a collection of predefined distributions and samplers and provides R and C interfaces for defining additional ones. It also provides the means to import simulation data generated by external systems. This document provides background on the package and demonstrates the basics of running simulations, visualizing results, and defining distributions and samplers in R.",WOS:000294232200001,JOURNAL OF STATISTICAL SOFTWARE,,Introduction to SamplerCompare,2011
288,"We study a class of nonlinear nonparametric inverse problems. Specifically, we propose a nonparametric estimator of the dynamics of a monotonically increasing trajectory defined on a finite time interval. Under suitable regularity conditions, we show that in terms of L-2-loss, the optimal rate of convergence for the proposed estimator is the same as that for the estimation of the derivative of a function. We conduct simulation studies to examine the finite sample behavior of the proposed estimator and apply it to the Berkeley growth data.",WOS:000389620800007,ANNALS OF STATISTICS,"['TIME-VARYING COEFFICIENTS', 'DIFFERENTIAL-EQUATIONS', 'PARAMETER-ESTIMATION', 'HIV-1 DYNAMICS', 'PLANT-GROWTH', 'MODELS', 'DERIVATIVES', 'REGRESSION', 'NETWORKS']",NONPARAMETRIC ESTIMATION OF DYNAMICS OF MONOTONE TRAJECTORIES,2016
289,"We consider tests of hypotheses when the parameters are not identifiable under the null in semiparametric models, where regularity conditions for profile likelihood theory fail. Exponential average tests based on integrated profile likelihood are Constructed and shown to be asymptotically optimal under a weighted average power criterion with respect to a prior oil the nonidentifiable aspect of the model. These results extend existing results for parametric models, which involve more restrictive assumptions on the form of the alternative than do our results. Moreover, the proposed tests accommodate models with infinite dimensional nuisance parameters which either may not be identifiable or may not be estimable at the usual parametric rate. Examples include tests of the presence of a change-point in the Cox model With Current status data and tests of regression parameters in odds-rate models with right censored data. Optimal tests have not previously been Studied for these scenarios. We study the asymptotic distribution of the proposed tests Under the null, fixed Contiguous alternatives and random contiguous alternatives. We also propose a weighted bootstrap procedure for computing the critical values of the test statistics. The optimal tests perform well ill simulation Studies, where they may exhibit improved power over alternative tests.",WOS:000268604900012,ANNALS OF STATISTICS,"['PROPORTIONAL HAZARDS MODEL', 'LIKELIHOOD RATIO TESTS', 'NUISANCE PARAMETER', 'REGRESSION-MODELS', 'EFFICIENT ESTIMATION', 'CHANGE-POINT', 'HYPOTHESIS', 'INFERENCE', 'MIXTURE', 'COVARIATE']",ON ASYMPTOTICALLY OPTIMAL TESTS UNDER LOSS OF IDENTIFIABILITY IN SEMIPARAMETRIC MODELS,2009
290,"The RNetLogo package delivers an interface to embed the agent-based modeling platform NetLogo into the R environment with headless (no graphical user interface) or interactive GUI mode. It provides functions to load models, execute commands, push values, and to get values from NetLogo reporters. Such a seamless integration of a widely used agent-based modeling platform with a well-known statistical computing and graphics environment opens up various possibilities. For example, it enables the modeler to design simulation experiments, store simulation results, and analyze simulation output in a more systematic way. It can therefore help close the gaps in agent-based modeling regarding standards of description and analysis. After a short overview of the agent-based modeling approach and the software used here, the paper delivers a step-by-step introduction to the usage of the RNetLogo package by examples.",WOS:000341583700001,JOURNAL OF STATISTICAL SOFTWARE,"['MODELS', 'SIMULATION', 'ECONOMICS', 'SYSTEMS']",R Marries NetLogo: Introduction to the RNetLogo Package,2014
291,"Consider the problem of estimating the gamma-level set G(gamma)* = {x : f(x) > gamma} of an unknown d-dimensional density function f based on n independent observations X(1), ..., X(n) from the density. This problem has been addressed under global error criteria related to the symmetric set difference. However, in certain applications a spatially uniform mode of convergence is desirable to ensure that the estimated set is close to the target set everywhere. The Hausdorff error criterion provides this degree of uniformity and, hence, is more appropriate in such situations. It is known that the minimax optimal rate of error convergence for the Hausdorff metric is (n/logn)(-1/(d+2 alpha)) for level sets with boundaries that have a Lipschitz functional form, where the parameter alpha characterizes the regularity of the density around the level of interest. However, the estimators proposed in previous work are nonadaptive to the density regularity and require knowledge of the parameter alpha. Furthermore, previously developed estimators achieve the minimax optimal rate for rather restricted classes of sets (e.g., the boundary fragment and star-shaped sets) that effectively reduce the set estimation problem to a function estimation problem. This characterization precludes level sets with multiple connected components, which are fundamental to many applications. This paper presents a fully data-driven procedure that is adaptive to unknown regularity conditions and achieves near minimax optimal Hausdorff error control for a class of density level sets with very general shapes and multiple connected Components.",WOS:000268605000007,ANNALS OF STATISTICS,"['NONPARAMETRIC-ESTIMATION', 'CLASSIFICATION']",ADAPTIVE HAUSDORFF ESTIMATION OF DENSITY LEVEL SETS,2009
292,"Here I introduce package cmvnorm, a complex generalization of the mvtnorm package. A complex generalization of the Gaussian process is suggested and numerical results presented using the package. An application in the context of approximating theWeierstrass sigma-function using a complex Gaussian process is given.",WOS:000357431900007,R JOURNAL,,The Complex Multivariate Gaussian Distribution,2015
293,"Generalized linear mixed models (GLMMs) comprise a class of widely used statistical tools for data analysis with fixed and random effects when the response variable has a conditional distribution in the exponential family. GLMM analysis also has a close relationship with actuarial credibility theory. While readily available programs such as the GLIMMIX procedure in SAS and the Ime4 package in R are powerful tools for using this class of models, these programs are not able to handle models with thousands of levels of fixed and random effects. By using sparse-matrix and other high performance techniques, procedures such as HYMIXed in SAS can easily fit models with thousands of factor levels, but only for normally distributed response variables. In this paper, we present the %HPGLIMMIX SAS macro that fits GLMMS with large number of sparsely populated design matrices using the doubly-iterative linearization (pseudo-likelihood) method, in which the sparse-matrix-based HPMIXED is used for the linear iterations with the pesudo-variable constructed for the inverse-link function and the chosen model. Although the macro does not have the full functionality of the GLIMMIX procedure, time and memory savings can be large with the new macro. In applications in which design matrices contain many zeros and there are hundreds or thousands of factors levels, models cab be fitted without exhausting computer memory, and 90% or better reduction in running time can be observed. Examples with a Poisson, binomial, and gamma conditional distribution are presented to demonstrate the usage and efficiency of this macro.",WOS:000341584800001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR MIXED MODELS', 'APPROXIMATION', 'LIKELIHOOD', 'VARIANCE']",%HPGLIMMIX: A High-Performance SAS Macro for GLMM Estimation,2014
294,"Our data are random fields of multivariate Gaussian observations, and we fit a multivariate linear model with common design matrix at each point. We are interested in detecting those points where some of the coefficients are nonzero using classical multivariate statistics evaluated at each point. The problem is to find the P-value of the maximum of such a random field of test statistics. We approximate this by the expected Euler characteristic of the excursion set. Our main result is a very simple method for calculating this, which not only gives us the previous result of Cao and Worsley [Ann. Statist. 27 (1999) 925-942] for Hotelling's T-2, but also random fields of Roy's maximum root, maximum canonical correlations [Ann. Appl. Probab. 9 (1999) 1021-1057], multilinear forms [Ann. Statist. 29 (2001) 328-371], chi(-2) [Statist. Probab. Lett 32 (1997) 367-376, Ann. Statist. 25 (1997) 2368-2387] and chi(2) scale space [Adv. in Appl. Probab. 33 (2001) 773-793]. The trick involves approaching the problem from the point of view of Roy's union-intersection principle. The results are applied to a problem in shape analysis where we look for brain damage due to nonmissile trauma.",WOS:000253390000001,ANNALS OF STATISTICS,"['EXCURSION SETS', 'TAIL PROBABILITIES', 'UNKNOWN LOCATION', 'CONFIDENCE BANDS', 'GAUSSIAN FIELDS', 'MAXIMA', 'REGRESSION', 'GEOMETRY', 'FORMULA', 'SIGNALS']","Random fields of multivariate test statistics, with applications to shape analysis",2008
295,"The statistical analysis of circular, multivariate circular, and spherical data is very important in different areas, such as paleomagnetism, astronomy and biology. The use of nonnegative trigonometric sums allows for the construction of flexible probability models for these types of data to model datasets with skewness and multiple modes. The R package CircNNTSR includes functions to plot, fit by maximum likelihood, and simulate models based on nonnegative trigonometric sums for circular, multivariate circular, and spherical data. For maximum likelihood estimation of the models for the three different types of data an efficient Newton-like algorithm on a hypersphere is used. Examples of applications of the functions provided in the CircNNTSR package to actual and simulated datasets are presented and it is shown how the package can be used to test for uniformity, homogeneity, and independence using likelihood ratio tests.",WOS:000384911400001,JOURNAL OF STATISTICAL SOFTWARE,"['ACTIVITY PATTERNS', 'DISTRIBUTIONS']","CircNNTSR: An R Package for the Statistical Analysis of Circular, Multivariate Circular, and Spherical Data Using Nonnegative Trigonometric Sums",2016
296,"entropart is a package for R designed to estimate diversity based on HCDT entropy or similarity-based entropy. It allows calculating species-neutral, phylogenetic and functional entropy and diversity, partitioning them and correcting them for estimation bias.",WOS:000365983300001,JOURNAL OF STATISTICAL SOFTWARE,"['PHYLOGENETIC DIVERSITY', 'FUNCTIONAL DIVERSITY', 'MATHEMATICAL-THEORY', 'SPECIES-DIVERSITY', 'BETA DIVERSITY', 'HILL NUMBERS', 'SIMILARITY', 'INDEX', 'COMMUNICATION', 'STATISTICS']",entropart: An R Package to Measure and Partition Diversity,2015
297,"We formulate nonparametric and semiparametric hypothesis testing of multivariate stationary linear time series in a unified fashion and propose new test statistics based on estimators of the spectral density matrix. The limiting distributions of these test statistics under null hypotheses are always normal distributions, and they can be implemented easily for practical use. If null hypotheses are false, as the sample size goes to infinity, they diverge to infinity and consequently are consistent tests for any alternative. The approach can be applied to various null hypotheses such as the independence between the component series, the equality of the autocovariance functions or the autocorrelation functions of the component series, the separability of the covariance matrix function and the time reversibility. Furthermore, a null hypothesis with a nonlinear constraint like the conditional independence between the two series can be tested in the same way.",WOS:000271673500015,ANNALS OF STATISTICS,"['OF-FIT TESTS', 'MODELS', 'COINTEGRATION', 'INDEPENDENCE', 'REGRESSION', 'SELECTION']",ON NONPARAMETRIC AND SEMIPARAMETRIC TESTING FOR MULTIVARIATE LINEAR TIME SERIES,2009
298,"We present the R package bild for the parametric and graphical analysis of binary longitudinal data. The package performs logistic regression for binary longitudinal data, allowing for serial dependence among observations from a given individual and a random intercept term. Estimation is via maximization of the exact likelihood of a suitably defined model. Missing values and unbalanced data are allowed, with some restrictions. The code of bild is written partly in R language, partly in Fortran 77, interfaced through R. The package is built following the S4 formulation of R methods.",WOS:000301072600001,JOURNAL OF STATISTICAL SOFTWARE,"['RESPONSES', 'MODELS', 'REGRESSION']",The R Package bild for the Analysis of Binary Longitudinal Data,2012
299,"Monotonic convergence is established for a general class of multiplicative algorithms introduced by Silvey, Titterington and Torsney [Comm. Statist. Theory Methods 14 (1978) 1379-1389] or computing optimal designs. A conjecture of Titterington [Appl. Stat. 27(1978) 227-234] is confirmed as a consequence. Optimal designs for logistic regression are used as an illustration.",WOS:000277471000011,ANNALS OF STATISTICS,"['MULTIPLICATIVE ALGORITHMS', 'SUPPORT-POINTS', 'EM ALGORITHM']",MONOTONIC CONVERGENCE OF A GENERAL ALGORITHM FOR COMPUTING OPTIMAL DESIGNS,2010
300,"Information criteria, such as Akaike's information criterion and Bayesian information criterion are often applied in model selection. However, their asymptotic behaviors for selecting geostatistical regression models have not been well studied, particularly under the fixed domain asymptotic framework with more and more data observed in a bounded fixed region. In this article, we study the generalized information criterion (GIC) for selecting geostatistical regression models under a more general mixed domain asymptotic framework. Via uniform convergence developments of some statistics, we establish the selection consistency and the asymptotic loss efficiency of GIC under some regularity conditions, regardless of whether the covariance model is correctly or wrongly specified. We further provide specific examples with different types of explanatory variables that satisfy the conditions. For example, in some situations, GIC is selection consistent, even when some spatial covariance parameters cannot be estimated consistently. On the other hand, GIC fails to select the true polynomial order consistently under the fixed domain asymptotic framework. Moreover, the growth rate of the domain and the degree of smoothness of candidate regressors in space are shown to play key roles for model selection.",WOS:000345884900010,ANNALS OF STATISTICS,"['MIXED-EFFECTS MODELS', 'SEMIPARAMETRIC REGRESSION', 'TIME-SERIES', 'PREDICTIONS', 'COVARIANCE', 'ERRORS']",ASYMPTOTIC THEORY OF GENERALIZED INFORMATION CRITERION FOR GEOSTATISTICAL REGRESSION MODEL SELECTION,2014
301,"The presence of a sparse ""truth"" has been a constant assumption in the theoretical analysis of sparse PCA and is often implicit in its methodological development. This naturally raises questions about the properties of sparse PCA methods and how they depend on the assumption of sparsity. Under what conditions can the relevant variables be selected consistently if the truth is assumed to be sparse? What can be said about the results of sparse PCA without assuming a sparse and unique truth? We answer these questions by investigating the properties of the recently proposed Fantope projection and selection (FPS) method in the high-dimensional setting. Our results provide general sufficient conditions for sparsistency of the FPS estimator. These conditions are weak and can hold in situations where other estimators are known to fail. On the other hand, without assuming sparsity or identifiability, we show that FPS provides a sparse, linear dimension-reducing transformation that is close to the best possible in terms of maximizing the predictive covariance.",WOS:000349738500011,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'HIGH-DIMENSIONAL ANALYSIS', 'VARIABLE SELECTION', 'POWER METHOD', 'MATRIX', 'LASSO', 'CONSISTENCY', 'LIKELIHOOD', 'MODELS', 'RATES']",SPARSISTENCY AND AGNOSTIC INFERENCE IN SPARSE PCA,2015
302,"We propose a block-resampling penalization method for marginal density estimation with nonnecessary independent observations. When the data are beta or tau-mixing, the selected estimator satisfies oracle inequalities with leading constant asymptotically equal to 1.
We also prove in this setting the slope heuristic, which is a data-driven method to optimize the leading constant in the penalty.",WOS:000296995500002,ANNALS OF STATISTICS,"['EMPIRICAL PROCESSES', 'RISK BOUNDS', 'PENALIZATION', 'REGRESSION', 'PENALTIES']",OPTIMAL MODEL SELECTION FOR DENSITY ESTIMATION OF STATIONARY DATA UNDER VARIOUS MIXING CONDITIONS,2011
303,"The singular value decomposition is widely used to approximate data matrices with lower rank matrices. Feng and He [Ann. Appl. Stat. 3 (2009) 1634-1654] developed tests on dimensionality of the mean structure of a data matrix based on the singular value decomposition. However, the first singular values and vectors can be driven by a small number of outlying measurements. In this paper, we consider a robust alternative that moderates the effect of outliers in low-rank approximations. Under the assumption of random row effects, we provide the asymptotic representations of the robust low-rank approximation. These representations may be used in testing the adequacy of a low-rank approximation. We use oligonucleotide gene microarray data to demonstrate how robust singular value decomposition compares with the its traditional counterparts. Examples show that the robust methods often lead to a more meaningful assessment of the dimensionality of gene intensity data matrices.",WOS:000334256100008,ANNALS OF STATISTICS,"['LEAST-SQUARES', 'REGRESSION', 'REPRODUCIBILITY', 'ESTIMATORS', 'MODEL']",STATISTICAL INFERENCE BASED ON ROBUST LOW-RANK DATA MATRIX APPROXIMATION,2014
304,,WOS:000389620800004,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'SPARSE PCA']","DISCUSSION OF ""INFLUENTIAL FEATURE PCA FOR HIGH DIMENSIONAL CLUSTERING""",2016
305,"We consider situations in Bayesian analysis where we have a family of priors v(h) on the parameter theta, where h varies continuously over a space H, and we deal with two related problems. The first involves sensitivity analysis and is stated as follows. Suppose we fix a function f of theta. How do we efficiently estimate the posterior expectation of f(theta) simultaneously for all h in H? The second problem is how do we identify subsets of H which give rise to reasonable choices of v(h)? We assume that we are able to generate Markov chain samples from the posterior for a finite number of the priors, and we develop a methodology, based on a combination of importance sampling and the use of control variates, for dealing with these two problems. The methodology applies very generally, and we show how it applies in particular to a commonly used model for variable selection in Bayesian linear regression, and give an illustration on the US crime data of Vandaele.",WOS:000299186500018,ANNALS OF STATISTICS,"['EXPLORING POSTERIOR DISTRIBUTIONS', 'MONTE-CARLO INTEGRATION', 'VARIABLE SELECTION', 'MARKOV-CHAINS', 'MODELS', 'MIXTURES', 'PRIORS']",COMPUTATIONAL APPROACHES FOR EMPIRICAL BAYES METHODS AND BAYESIAN SENSITIVITY ANALYSIS,2011
306,"This paper outlines a computerized adaptive testing (CAT) framework and presents an R package for the simulation of response patterns under CAT procedures. This package, called catR, requires a bank of items, previously calibrated according to the four-parameter logistic (4PL) model or any simpler logistic model. The package proposes several methods to select the early test items, several methods for next item selection, different estimators of ability (maximum likelihood, Bayes modal, expected a posteriori, weighted likelihood), and three stopping rules (based on the test length, the precision of ability estimates or the classification of the examinee). After a short description of the different steps of a CAT process, the commands and options of the catR package are presented and practically illustrated.",WOS:000305117900001,JOURNAL OF STATISTICAL SOFTWARE,"['BAYESIAN-ESTIMATION', 'LOGISTIC MODEL', 'ABILITY', 'PARAMETERS', 'ESTIMATORS', 'PROGRAM']",Random Generation of Response Patterns under Computerized Adaptive Testing with the R Package catR,2012
307,"It has been well documented that ignoring measurement error may result in substantially biased estimates in many contexts including linear and nonlinear regressions. For survival data with measurement error in covariates, there has been extensive discussion in the literature with the focus typically centered on proportional hazards models. The impact of measurement error on inference under accelerated failure time models has received relatively little attention, although these models are very useful in survival data analysis. He et al. (2007) discussed accelerated failure time models with error-prone covariates and studied the bias induced by the naive approach of ignoring measurement error in covariates. To adjust for the effects of covariate measurement error, they described a simulation and extrapolation method. This method has theoretical advantages such as robustness to distributional assumptions for error prone covariates. Moreover, this method enjoys simplicity and flexibility for practical use. It is quite appealing to analysts who would like to accommodate covariate measurement error in their analysis. To implement this method, in this paper, we develop an R package for general users. Two data sets arising from clinical trials are employed to illustrate the use of the package.",WOS:000326870400001,JOURNAL OF STATISTICAL SOFTWARE,"['PROPORTIONAL HAZARDS MODEL', 'COVARIATES SUBJECT', 'REGRESSION-MODEL', 'SURVIVAL-DATA']",SIMEX R Package for Accelerated Failure time Model with cobariate Measurement Error,2012
308,"Developing efficient gamma variate generators is important for Monte Carlo methods. With a brief review of existing methods for generating gamma random numbers, this article proposes two simple gamma variate generators that are obtained from the ratio-of-uniforms method and based on two logarithmic transformations of the gamma random variable. One transformation allows for the generators to work for all shape parameter values. The other is introduced to have improved efficiency for shape parameters smaller than or equal to one.",WOS:000325947700001,JOURNAL OF STATISTICAL SOFTWARE,,Logarithmic Transformation-Based Gamma Random Number Generators,2013
309,Regular factorial designs with randomization restrictions are widely used in practice. This paper provides a unified approach to the construction of such designs using randomization defining contrast subspaces for the representation of randomization restrictions. We use finite projective geometry to determine the existence of designs with the required structure and develop a systematic approach for their construction. An attractive feature is that commonly used factorial designs with randomization restrictions are special cases of this general representation. Issues related to the use of these designs for particular factorial experiments are also addressed.,WOS:000271673500017,ANNALS OF STATISTICS,"['GENERALIZED CYCLIC DESIGNS', 'SPLIT-PLOT DESIGNS', 'MINIMUM-ABERRATION', 'FRACTIONAL FACTORIALS', '2-LEVEL EXPERIMENTS', 'OPTIMAL BLOCKING', 'RESOLUTION']",EXISTENCE AND CONSTRUCTION OF RANDOMIZATION DEFINING CONTRAST SUBSPACES FOR REGULAR FACTORIAL DESIGNS,2009
310,"The paper aims at reconsidering the famous Le Cam LAN theory. The main features of the approach which make it different from the classical one are as follows: (1) the study is nonasymptotic, that is, the sample size is fixed and does not tend to infinity; (2) the parametric assumption is possibly misspecified and the underlying data distribution can lie beyond the given parametric family. These two features enable to bridge the gap between parametric and nonparametric theory and to build a unified framework for statistical estimation. The main results include large deviation bounds for the (quasi) maximum likelihood and the local quadratic bracketing of the log-likelihood process. The latter yields a number of important corollaries for statistical inference: concentration, confidence and risk bounds, expansion of the maximum likelihood estimate, etc. All these corollaries are stated in a nonclassical way admitting a model misspecification and finite samples. However, the classical asymptotic results including the efficiency bounds can be easily derived as corollaries of the obtained nonasymptotic statements. At the same time, the new bracketing device works well in the situations with large or growing parameter dimension in which the classical parametric theory fails. The general results are illustrated for the i.i.d. setup as well as for generalized linear and median estimation. The results apply for any dimension of the parameter space and provide a quantitative lower bound on the sample size yielding the root-n accuracy.",WOS:000321845400005,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD ESTIMATORS', 'MINIMUM CONTRAST ESTIMATORS', 'MAJORIZING MEASURES', 'CONVERGENCE', 'RATES']",PARAMETRIC ESTIMATION. FINITE SAMPLE THEORY,2012
311,"We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data.
We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.",WOS:000256504400007,ANNALS OF STATISTICS,"['SUPPORT VECTOR MACHINES', 'INDEPENDENT COMPONENT ANALYSIS', 'LOG-LINEAR MODELS', 'PROJECTION PURSUIT', 'PATTERN-RECOGNITION', 'CANONICAL-ANALYSIS', 'METRIC-SPACES', 'REGRESSION', 'REGULARIZATION', 'APPROXIMATION']",Kernel methods in machine learning,2008
312,,WOS:000253077800004,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'REGRESSION', 'SHRINKAGE', 'LASSO']",Discussion: The Dantzig selector: Statistical estimation when p is much larger than n,2007
313,"It has long been known that for the comparison of pairwise nested models, a decision based on the Bayes factor produces I consistent model selector (in the frequentist sense). Here we go beyond the usual consistency for nested pairwise models, and show that for a wide class of prior distributions, including intrinsic priors, the corresponding Bayesian procedure for variable selection in normal regression is consistent in the entire class of normal linear models. We find that the asymptotics of the Bayes factors for intrinsic priors are equivalent to those of the Schwarz (BIC) criterion. Also, recall that the Jeffreys-Lindley paradox refers to the well-known fact that a point null hypothesis on the normal mean parameter is always accepted when the variance of the conjugate prior goes to infinity. This implies that some limiting forms of proper prior distributions are not necessarily Suitable for testing problems. Intrinsic priors are limits of proper prior distributions, and for finite sample sizes they have been proved to behave extremely well for variable selection in regression;,I consequence of our results is that for intrinsic priors Lindley's paradox does not arise.",WOS:000265619700005,ANNALS OF STATISTICS,"['MODEL SELECTION', 'LINEAR-MODELS', 'HYPOTHESES', 'PARADOX']",CONSISTENCY OF BAYESIAN PROCEDURES FOR VARIABLE SELECTION,2009
314,"In this paper we propose two new methods to estimate the dimension-reduction directions of the central subspace (CS) by constructing a regression model such that the directions are all captured in the regression mean. Compared with the inverse regression estimation methods [e.g., J. Amer Statist. Assoc. 86 (1991) 328-332, J Amer Statist. Assoc. 86 (1991) 316-342, J Amer Statist. Assoc. 87 (1992) 1025-1039], the new methods require no strong assumptions on the design of covariates or the functional relation between regressors and the response variable, and have better perforrnance than the inverse regression estimation methods for finite samples. Compared with the direct regression estimation methods [e.g., J. Amer. Statist. Assoc. 84 (1989) 986-995, Ann. Statist. 29 (2001) 1537-1566, J R. Stat. Soc. Ser B Stat. Methodol. 64 (2002) 363-410], which can only estimate the directions of CS in the regression mean, the new methods can detect the directions of CS exhaustively. Consistency of the estimators and the convergence of corresponding algorithms are proved.",WOS:000253077800019,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'PRINCIPAL HESSIAN DIRECTIONS', 'SINGLE-INDEX MODELS']",A constructive approach to the estimation of dimension reduction directions,2007
315,"Since R was first launched, it has managed to gain the support of an ever-increasing percentage of academic and professional statisticians. However, the spread of its use among novice and occasional users of statistics have not progressed at the same pace, which can be atributed partially to the lack of a graphical user interface (GUI). Nevertheless, this situation has changed in the last years and there is currently several projects that have added GUIs to R. This article discusses briefly the history of GUIs for data analysis and then introduces the papers submitted to an special issue of the Journal of Statistical Software on GUIs for R",WOS:000305987700001,JOURNAL OF STATISTICAL SOFTWARE,,Graphical User Interfaces for R,2012
316,"Multiple imputation is becoming increasingly established as the leading practical approach to modelling partially observed data, under the assumption that the data are missing at random. However, many medical and social datasets are multilevel, and this structures should be reflected not only in the model of interest, but also in the imputation model. In particular, the imputation model should reflect the differences between level 1 variables and level 2 variables (which are constant across level 1 units). This led us to develop the REALCOM-IMPUTE software, which we describe in this article. This software performs multilevel multiple imputation, and handles ordinal and unordered categorical data appropriately . Is is freely available on-line, and many be used either as a standalone package, or in conjuction with the multilevel software MLwiN or Stata.",WOS:000298032700001,JOURNAL OF STATISTICAL SOFTWARE,,REALCOM-IMPUTE Software for Multilevel Multiple Imputation with Mixed Response Types,2011
317,"Biological pathways or modules represent sets of interactions or functional relationships occurring at the molecular level in living cells. A large body of knowledge on pathways is organized in public databases such as the KEGG, Reactome, or in more specialized repositories, the Atlas of Cancer Signaling Network (ACSN) being an example. All these open biological databases facilitate analyses, improving our understanding of cellular systems. We hereby describe ACSNMineR for calculation of enrichment or depletion of lists of genes of interest in biological pathways. ACSNMineR integrates ACSN molecular pathways gene sets, but can use any gene set encoded as a GMT file, for instance sets of genes available in the Molecular Signatures Database (MSigDB). We also present RNaviCell, that can be used in conjunction with ACSNMineR to visualize different data types on web-based, interactive ACSN maps. We illustrate the functionalities of the two packages with biological data taken from large-scale cancer datasets.",WOS:000395669800019,R JOURNAL,"['PATHWAYS', 'CANCER', 'GENES']",Calculating Biological Module Enrichment or Depletion and Visualizing Data on Large-scale Molecular Maps with ACSNMineR and RNaviCell Packages,2016
318,"The MLDS package in the R programming language can be used to estimate perceptual scales based on the results of psychophysical experiments using the method of difference scaling. In a difference scaling experiment, observers compare two 'supra-threshold differences (a, b) and (c, d) on each trial. The approach is based on a stochastic model of how the observer decides which perceptual difference (or interval) (a;b) or (c;d) is greater, and the parameters of the model are estimated using a maximum likelihood criterion. We also propose a method to test the model by evaluating the self-consistency of the estimated scale. The package includes an example in which an observer judges the differences in correlation between scatter plots. The example may be readily adapted to estimate perceptual scales for arbitrary physical continua.",WOS:000254619500001,JOURNAL OF STATISTICAL SOFTWARE,"['GRAPHICAL PERCEPTION', 'INTERVALS', 'JUDGMENT']",MLDS: Maximum likelihood difference scaling in R,2008
319,,WOS:000208589900011,R JOURNAL,,Introducing NppToR: R Interaction for Notepad plus,2010
320,"We introduce a method for the theoretical analysis of exponential random graph models. The method is based on a large-deviations approximation to the normalizing constant shown to be consistent using theory developed by Chatterjee and Varadhan [European J. Combin. 32 (2011) 1000-1017]. The theory explains a host of difficulties encountered by applied workers: many distinct models have essentially the same MLE, rendering the problems ""practically"" ill-posed. We give the first rigorous proofs of ""degeneracy"" observed in these models. Here, almost all graphs have essentially no edges or are essentially complete. We supplement recent work of Bhamidi, Bresler and Sly [2008 IEEE 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS) (2008) 803-812 IEEE] showing that for many models, the extra sufficient statistics are useless: most realizations look like the results of a simple Erdos-Renyi model. We also find classes of models where the limiting graphs differ from Erdos-Renyi graphs. A limitation of our approach, inherited from the limitation of graph limit theory, is that it works only for dense graphs.",WOS:000327746100006,ANNALS OF STATISTICS,"['CONVERGENT SEQUENCES', 'NETWORK DATA', 'INEQUALITIES', 'ALGEBRAS', 'MATRICES', 'RANK']",ESTIMATING AND UNDERSTANDING EXPONENTIAL RANDOM GRAPH MODELS,2013
321,"It is shown that the fiducial distribution in a group model, or more generally a quasigroup model, determines the optimal equivariant frequentist inference procedures. The proof does not rely on existence of invariant measures, and generalizes results corresponding to the choice of the right Haar measure as a Bayesian prior. Classical and more recent examples show that fiducial arguments can be used to give good candidates for exact or approximate confidence distributions. It is here suggested that the fiducial algorithm can be considered as an alternative to the Bayesian algorithm for the construction of good frequentist inference procedures more generally.",WOS:000317451200013,ANNALS OF STATISTICS,"['STATISTICAL-INFERENCE', 'DISTRIBUTIONS', 'PROBABILITY', 'INTERVALS', 'PRIORS', 'MODEL']",FIDUCIAL THEORY AND OPTIMAL INFERENCE,2013
322,"We propose, for multivariate Gaussian copula models with unknown margins and structured correlation matrices, a rank-based, semiparametrically efficient estimator for the Euclidean copula parameter. This estimator is defined as a one-step update of a rank-based pilot estimator in the direction of the efficient influence function, which is calculated explicitly. Moreover, finite-dimensional algebraic conditions are given that completely characterize efficiency of the pseudo-likelihood estimator and adaptivity of the model with respect to the unknown marginal distributions. For correlation matrices structured according to a factor model, the pseudo-likelihood estimator turns out to be semiparametrically efficient. On the other hand, for Toeplitz correlation matrices, the asymptotic relative efficiency of the pseudo-likelihood estimator can be as low as 20%. These findings are confirmed by Monte Carlo simulations. We indicate how our results can be extended to joint regression models.",WOS:000344632400008,ANNALS OF STATISTICS,"['BIVARIATE SURVIVAL-DATA', 'GRAPHICAL MODELS', 'PARAMETERS', 'REGRESSION', 'INFERENCE']",SEMIPARAMETRIC GAUSSIAN COPULA MODELS: GEOMETRY AND EFFICIENT RANK-BASED ESTIMATION,2014
323,"This paper presents the metric-frequency calculator (MF Calculator), an online application to analyze similarity. The MF Calculator implements a metric-frequency similarity algorithm for the quantitative assessment of similarity in ill-structured data sets. It is widely applicable as it can be used with nominal, ordinal, or interval data when there is little prior control over the variables to be observed regarding number or content. The MF Calculator generates a proximity matrix in CSV, XML or DOC format that can be used as input to traditional statistical techniques such as hierarchical clustering, additive trees, or multidimensional scaling. The MF Calculator also displays a graphical representation of outputs using additive similarity trees. A simulated example illustrates the implementation of the MF calculator. An additional example with real data is presented, in order to illustrate the potential of combining the MF Calculator with cluster analysis. The MF Calculator is a user-friendly tool available free of charge. It can be accessed from http : //mfcalculator.celiasales.org/Calculator.aspx, and it can be used by non-experts from a wide range of social sciences.",WOS:000365975600001,JOURNAL OF STATISTICAL SOFTWARE,"['EFFICIENT METHOD', 'THERAPY']",MF Calculator: A Web-Based Application for Analyzing Similarity,2015
324,"The sufficient-component cause framework assumes the existence of sets of sufficient causes that bring about an event. For a binary outcome and an arbitrary number of binary causes any set of potential outcomes can be replicated by positing a set of sufficient causes; typically this representation is not unique. A sufficient cause interaction is said to be present if within all representations there exists a sufficient cause in which two or more particular causes are all present. A singular interaction is said to be present if for some subset of individuals there is a unique minimal sufficient cause. Empirical and counterfactual conditions are given for sufficient cause interactions and singular interactions between an arbitrary number of causes. Conditions are given for cases in which none, some or all of a given set of causes affect the outcome monotonically. The relations between these results, interactions in linear statistical models and Pearl's probability of causation are discussed.",WOS:000321842400001,ANNALS OF STATISTICS,"['DIRECTED ACYCLIC GRAPHS', 'INFERENCE', 'EPISTASIS', 'TESTS']",GENERAL THEORY FOR INTERACTIONS IN SUFFICIENT CAUSE MODELS WITH DICHOTOMOUS EXPOSURES,2012
325,"The package support.CEs provides seven basic functions that support the implementation of choice experiments (CEs) in R : two functions for creating a CE design, which is based on orthogonal main-effect arrays; a function for converting a CE design into questionnaire format; a function for converting a CE design into a design matrix; a function for making the data set suitable for the implementation of a conditional logit model; a function for calculating the goodness-of-fit measures of an estimated model; and a function for calculating the marginal willingness to pay for the attributes and/or levels of the estimated model.",WOS:000326872100001,JOURNAL OF STATISTICAL SOFTWARE,,Basic Functions for Supporting an Implementation of Choice Experiments in R,2012
326,"The R package pdfCluster performs cluster analysis based on a nonparametric estimate of the density of the observed variables. Functions are provided to encompass the whole process of clustering, from kernel density estimation, to clustering itself and subsequent graphical diagnostics. After summarizing the main aspects of the methodology, we describe the features and the usage of the package, and finally illustrate its application with the aid of two data sets.",WOS:000341020600001,JOURNAL OF STATISTICAL SOFTWARE,['TREE'],Clustering via Nonparametric Density Estimation: The R Package pdfCluster,2014
327,"The R package ClickClust is a new piece of software devoted to finite mixture modeling and model-based clustering of categorical sequences. As a special kind of time series, categorical sequences, also known as categorical time series, exhibit a time-dependent nature and are traditionally modeled by means of Markov chains. Clustering categorical sequences is an important problem with multiple applications, but grouping sequences of sites or web-pages, also known as clickstreams, is one of the most well-known problems that helps discover common navigation patterns and routes taken by users. This popular application is recognized in the package title ClickClust. The paper discusses methodological and algorithmic foundations of the package based on finite mixtures of Markov models. The number of Markov chain states can often be large leading to high-dimensional transition probability matrices. The high number of model parameters can affect clustering performance severely. As a remedy to this problem, backward and forward selection algorithms are proposed for grouping states. This extends the original clustering problem to a biclustering framework. Among other capabilities of ClickClust, there are the estimation of the variance-covariance matrix corresponding to model parameter estimates, prediction of future states visited, and the construction of a display named click-plot that helps illustrate the obtained clustering solutions. All available functions and the utility of the package are thoroughly discussed and illustrated on multiple examples.",WOS:000392514500001,JOURNAL OF STATISTICAL SOFTWARE,"['EM ALGORITHM', 'LIKELIHOOD']",ClickClust: An R Package for Model-Based Clustering of Categorical Sequences,2016
328,"Checking the validity of test scores is important in both educational and psychological measurement. Person-fit analysis provides several statistics that help practitioners assessing whether individual item score vectors conform to a prespecified item response theory model or, alternatively, to a group of test takers. Software enabling easy access to most person-fit statistics was lacking up to now. The PerFit R package was written in order to fill in this void. A theoretical overview of relatively simple person-fit statistics is provided. A practical guide showing how the main functions of PerFit can be used is also given. Both numerical and graphical tools are described and illustrated using examples. The goal is to show how person-fit statistics can be easily applied to testing of questionnaire data.",WOS:000392513500001,JOURNAL OF STATISTICAL SOFTWARE,"['UNUSUAL RESPONSE PATTERNS', 'TEST-SCORES', 'APPROPRIATENESS MEASURES', 'LATENT TRAIT', 'MISSING DATA', 'SUM SCORE', 'STATISTICS', 'INDEXES', 'MODEL', 'PROGRAM']",PerFit: An R Package for Person-Fit Analysis in IRT,2016
329,"Chronic illness treatment strategies must adapt to the evolving health status of the patient receiving treatment. Data-driven dynamic treatment regimes can offer guidance for clinicians and intervention scientists on how to treat patients over time in order to bring about the most favorable clinical outcome on average. Methods for estimating optimal dynamic treatment regimes, such as Q-learning, typically require modeling nonsmooth, nonmonotone transformations of data. Thus, building well-fitting models can be challenging and in some cases may result in a poor estimate of the optimal treatment regime. Interactive Q-learning (IQ-learning) is an alternative to Q-learning that only requires modeling smooth, monotone transformations of the data. The R package iqLearn provides functions for implementing both the IQ-learning and Q-learning algorithms. We demonstrate how to estimate a two-stage optimal treatment policy with iqLearn using a generated data set bmiData which mimics a two-stage randomized body mass index reduction trial with binary treatments at each stage.",WOS:000352910400001,JOURNAL OF STATISTICAL SOFTWARE,,iqLearn: Interactive Q-Learning in R,2015
330,"Multiple imputation provides a useful strategy for dealing with data sets that have missing values. Instead of filling in a single value for each missing value, a multiple imputation procedure replaces each value to impute. These multiply imputed data sets are then analyzed by using standard procedures for complete data and combining the results from these analyses. No matter which complete-data analysis is used, the process of combining results of parameter estimates and their associated standard errors from different imputed data sets is essentially the same. This process results in valid statistical inferences that properly reflect the uncertainty due to missing values.
This paper reviews methods for analyzing missing data and applications of multiple imputation techniques. This paper presents the SAS/STAT MI and MIANALYZE procedures, which perform inference by multiple imputation under numerous settings. PROC MI implements popular methods for creating imputations under monotone nonmonotone (arbitary) patterns of missing data, and PROC MIANALYZE analyzes results from multiply imputed data sets.",WOS:000298032800001,JOURNAL OF STATISTICAL SOFTWARE,,Multiple Imputation Using SAS Software,2011
331,"Classical multivariate principal component analysis has been extended to functional data and termed functional principal component analysis (FPCA). Most existing FPCA approaches do not accommodate covariate information, and it is the goal of this paper to develop two methods that do. In the first approach, both the mean and covariance functions depend on the covariate Z and time scale t while in the second approach only the mean function depends on the covariate Z. Both new approaches accommodate additional measurement errors and functional data sampled at regular time grids as well as sparse longitudinal data sampled at irregular time grids. The first approach to fully adjust both the mean and covariance functions adapts more to the data but is computationally more intensive than the approach to adjust the covariate effects on the mean function only. We develop general asymptotic theory for both approaches and compare their performance numerically through simulation Studies and a data set.",WOS:000275510800021,ANNALS OF STATISTICS,"['MIXED EFFECTS MODELS', 'CURVES', 'ESTIMATORS', 'INFERENCE']",COVARIATE ADJUSTED FUNCTIONAL PRINCIPAL COMPONENTS ANALYSIS FOR LONGITUDINAL DATA,2010
332,"Automatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovations state space models that underly exponential smoothing methods. The second is a step-wise algorithm for forecasting with ARIMA models. The algorithms are applicable to both seasonal and non-seasonal data, and are compared and illustrated using four real time series. We also briefly describe some of the other functionality available in the forecast package.",WOS:000258205200001,JOURNAL OF STATISTICAL SOFTWARE,"['PREDICTION INTERVALS', 'PARAMETER SPACE', 'HOLT-WINTERS', 'MODELS', 'STATE', 'ACCURACY']",Automatic time series forecasting: The forecast package for R,2008
333,"Fisher's exact test, named for Sir Ronald Aylmer Fisher, tests contingency tables for homogeneity of proportion. This paper discusses a generalization of Fisher's exact test for the case where some of the table entries are constrained to be zero. The resulting test is useful for assessing cases where the null hypothesis of conditional multinomial distribution is suspected to be false. The test is implemented in the form of a new R package, aylmer.",WOS:000261527200001,JOURNAL OF STATISTICAL SOFTWARE,"['PAIRED COMPARISONS', 'PAPER', 'DISTRIBUTIONS', 'RANKING', 'GAME', '2X2']",Exact Tests for Two-Way Contingency Tables with Structural Zeros,2008
334,"Among the major difficulties that one may encounter when estimating parameters in a nonlinear regression model are the nonuniqueness of the estimator, its instability with respect to small perturbations of the observations and the presence of local optimizers of the estimation criterion.
We show that these estimability issues can be taken into account at the design stage, through the definition of suitable design criteria. Extensions of E-, c- and G-optimality criteria are considered, which when evaluated at a given theta(0) (local optimal design), account for the behavior of the model response eta(theta) for theta far from theta(0). In particular, they ensure some protection against close-to-overlapping situations where parallel to eta(theta) - eta(theta(0))parallel to is small for some theta far from theta(0). These extended criteria are concave and necessary and sufficient conditions for optimality (equivalence theorems) can be formulated. They are not differentiable, but when the design space is finite and the set Theta of admissible theta is discretized, optimal design forms a linear programming problem which can be solved directly or via relaxation when Theta is just compact. Several examples are presented.",WOS:000342481700007,ANNALS OF STATISTICS,['REGRESSION'],OPTIMUM DESIGN ACCOUNTING FOR THE GLOBAL NONLINEAR BEHAVIOR OF THE MODEL,2014
335,"Mokken scale analysis (MSA) is a scaling procedure for both dichotomous and polytomous items. It consists of an item selection algorithm to partition a set of items into Mokken scales and several methods to check the assumptions of two nonparametric item response theory models: the monotone homogeneity model and the double monotonicity model. First, we present an R package mokken for MSA and explain the procedures. Second, we show how to perform MSA in R using test data obtained with the Adjective Checklist.",WOS:000247012200001,JOURNAL OF STATISTICAL SOFTWARE,"['POLYTOMOUS IRT MODELS', 'MONOTONE LIKELIHOOD RATIO', 'LATENT TRAIT', 'SUM SCORE']",Mokken scale analysis in R,2007
336,,WOS:000275510800003,ANNALS OF STATISTICS,"['TUKEY DEPTH', 'ASYMPTOTICS', 'NOTION']",Multivariate quantiles and multiple-output regression quantiles: From L-1 optimization to halfspace depth DISCUSSION,2010
337,The analysis of the joint cumulative distribution function (CDF) with bivariate event time data is a challenging problem both theoretically and numerically. This paper develops a tensor spline-based sieve maximum likelihood estimation method to estimate the joint CDF with bivariate current status data. The I-splines are used to approximate the joint CDF in order to simplify the numerical computation of a constrained maximum likelihood estimation problem. The generalized gradient projection algorithm is used to compute the constrained optimization problem. Based on the properties of B-spline basis functions it is shown that the proposed tensor spline-based nonparametric sieve maximum likelihood estimator is consistent with a rate of convergence potentially better than n(1/3) under some mild regularity conditions. The simulation studies with moderate sample sizes are carried out to demonstrate that the finite sample performance of the proposed estimator is generally satisfactory.,WOS:000310650900012,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'INTERVAL-CENSORED-DATA', 'FAILURE TIME DATA', 'EFFICIENT ESTIMATION', 'STATISTICAL-ANALYSIS', 'REGRESSION-ANALYSIS', 'POLYNOMIAL SPLINES', 'DENSITY-ESTIMATION', 'MODEL', 'ASSOCIATION']",PARTIALLY MONOTONE TENSOR SPLINE ESTIMATION OF THE JOINT DISTRIBUTION FUNCTION WITH BIVARIATE CURRENT STATUS DATA,2012
338,"The R package equateIRT implements item response theory (IRT) methods for equating different forms composed of dichotomous items. In particular, the IRT models included are the three-parameter logistic model, the two-parameter logistic model, the one-parameter logistic model and the Rasch model. Forms can be equated when they present common items (direct equating) or when they can be linked through a chain of forms that present common items in pairs (indirect or chain equating). When two forms can be equated through different paths, a single conversion can be obtained by averaging the equating coefficients. The package calculates direct and chain equating coefficients. The averaging of direct and chain coefficients that link the same two forms is performed through the bisector method. Furthermore, the package provides analytic standard errors of direct, chain and average equating coefficients.",WOS:000384909800001,JOURNAL OF STATISTICAL SOFTWARE,['ITEM RESPONSE THEORY'],equateIRT: An R Package for IRT Test Equating,2015
339,"Data analysis sometimes requires the relaxation of parametric assumptions in order to gain modeling flexibility and robustness against mis-specification of the probability model. In the Bayesian context, this is accomplished by placing a prior distribution on a function space, such as the space of all probability distributions or the space of all regression functions. Unfortunately, posterior distributions ranging over function spaces are highly complex and hence sampling methods play a key role. This paper provides an introduction to a simple, yet comprehensive, set of programs for the implementation of some Bayesian nonparametric and semiparametric models in R, DPpackage. Currently, DPpackage includes models for marginal and conditional density estimation, receiver operating characteristic curve analysis, interval-censored data, binary regression data, item response data, longitudinal and clustered data using generalized linear mixed models, and regression data using generalized additive models. The package also contains functions to compute pseudo-Bayes factors for model comparison and for eliciting the precision parameter of the Dirichlet process prior, and a general purpose Metropolis sampling algorithm. To maximize computational efficiency, the actual sampling for each model is carried out using compiled C, C++ or Fortran code.",WOS:000289228600001,JOURNAL OF STATISTICAL SOFTWARE,"['POLYA TREE DISTRIBUTIONS', 'BERNSTEIN POLYNOMIALS', 'DIRICHLET PROCESSES', 'DENSITY-ESTIMATION', 'SURVIVAL ANALYSIS', 'INFERENCE', 'MIXTURES', 'REGRESSION', 'PACKAGE', 'FRAILTY']",DPpackage: Bayesian Semi- and Nonparametric Modeling in R,2011
340,"In this paper nonparametric methods to assess the multivariate Levy measure are introduced. Starting from high-frequency observations of a Levy process X, we construct estimators for its tail integrals and the Pareto-Levy copula and prove weak convergence of these estimators in certain function spaces. Given n observations of increments over intervals of length Delta(n), the rate of convergence is k(n)(-1/2) for k(n) = n Delta(n) which is natural concerning inference on the Levy measure. Besides extensions to nonequidistant sampling schemes analytic properties of the Pareto-Levy copula which, to the best of our knowledge, have not been mentioned before in the literature are provided as well. We conclude with a short simulation study on the performance of our estimators and apply them to real data.",WOS:000323271500005,ANNALS OF STATISTICS,"['HIGH-FREQUENCY DATA', 'OBSERVED DIFFUSION-PROCESSES', 'POWER VARIATIONS', 'LIMIT-THEOREMS', 'JUMP TAILS', 'SEMIMARTINGALES', 'ASYMPTOTICS', 'DEPENDENCE', 'NOISE']",NONPARAMETRIC INFERENCE ON LEVY MEASURES AND COPULAS,2013
341,"Nested space-filling designs are nested designs with attractive low-dimensional stratification. Such designs are gaining popularity in statistics, applied mathematics and engineering. Their applications include multifidelity computer models, stochastic optimization problems, multi-level fitting of nonparametric functions, and linking parameters. We propose methods for constructing several new classes of nested space-filling designs. These methods are based on a new group projection and other algebraic techniques. The constructed designs can accommodate a nested structure with an arbitrary number of layers and are more flexible in run size than the existing families of nested space-filling designs. As a byproduct, the proposed methods can also be used to obtain sliced space-filling designs that are appealing for conducting computer experiments with both qualitative and quantitative factors.",WOS:000342481700006,ANNALS OF STATISTICS,"['COMPUTER EXPERIMENTS', 'QUANTITATIVE FACTORS', 'ORTHOGONAL ARRAYS', 'INPUT VARIABLES', 'MODELS']",ON THE CONSTRUCTION OF NESTED SPACE-FILLING DESIGNS,2014
342,"Random fields play a central role in the analysis of spatially correlated data and, as a result, have a significant impact on a broad array of scientific applications. This paper studies the cepstral random field model, providing recursive formulas that connect the spatial cepstral coefficients to an equivalent moving-average random field, which facilitates easy computation of the autocovariance matrix. We also provide a comprehensive treatment of the asymptotic theory for two-dimensional random field models: we establish asymptotic results for Bayesian, maximum likelihood and quasi-maximum likelihood estimation of random field parameters and regression parameters. The theoretical results are presented generally and are of independent interest, pertaining to a wide class of random field models. The results for the cepstral model facilitate model-building: because the cepstral coefficients are unconstrained in practice, numerical optimization is greatly simplified, and we are always guaranteed a positive definite covariance matrix. We show that inference for individual coefficients is possible, and one can refine models in a disciplined manner. Our results are illustrated through simulation and the analysis of straw yield data in an agricultural field experiment.",WOS:000334256100003,ANNALS OF STATISTICS,"['PARAMETER-ESTIMATION', 'TIME-SERIES', 'MODELS', 'REGRESSION', 'LATTICE']",ASYMPTOTIC THEORY OF CEPSTRAL RANDOM FIELDS,2014
343,"Equating is a family of statistical models and methods that are used to adjust scores on two or more versions of a test, so that the scores from different tests may be used interchangeably. In this paper we present the R package SNSequate which implements both standard and nonstandard statistical models and methods for test equating. The package construction was motivated by the need of having a modular, simple, yet comprehensive, and general software that carries out traditional and new equating methods. SNSequate currently implements the traditional mean, linear and equipercentile equating methods, as well as the mean-mean, mean-sigma, Haebara and Stocking-Lord item response theory linking methods. It also supports the newest methods such as local equating, kernel equating, and item response theory parameter linking methods based on asymmetric item characteristic functions. Practical examples are given to illustrate the capabilities of the software. A list of other programs for equating is presented, highlighting the main differences between them. Future directions for the package are also discussed.",WOS:000341806400001,JOURNAL OF STATISTICAL SOFTWARE,"['TEST SCORE DISTRIBUTIONS', 'LOGLINEAR MODELS', 'RESPONSE THEORY', 'R PACKAGE', 'ISSUES']",SNSeqyate: Standard and Nonstandard Statistical Models and Methods for Test Equating,2014
344,"Over the last twenty years there have been numerous developments in diagnostic procedures for hierarchical linear models; however, these procedures are not widely implemented in statistical software packages, and those packages that do contain a complete framework for model assessment are not open source. The lack of availability of diagnostic procedures for hierarchical linear models has limited their adoption in statistical practice. The R package HLMdiag provides diagnostic tools targeting all aspects and levels of continuous response hierarchical linear models with strictly nested dependence structures fit using the lmer() function in the lme4 package. In this paper we discuss the tools implemented in HLMdiag for both residual and influence analysis.",WOS:000332108900001,JOURNAL OF STATISTICAL SOFTWARE,"['MIXED-EFFECTS MODELS', 'CASE-DELETION DIAGNOSTICS', 'MULTILEVEL MODELS', 'LONGITUDINAL DATA', 'T-DISTRIBUTION', 'INFERENCE', 'SOFTWARE']",HLMdiag: A Suite of Diagnostics for Hierarchical Linear Models in R,2014
345,"The concept of k-FWER has received much attention lately as an appropriate error rate for multiple testing when one seeks to control at least k false rejections, for some fixed k >= 1. A less conservative notion, the k-FDR, has been introduced very recently by Sarkar [Ann. Statist. 34 (2006) 394-415], generalizing the false discovery rate of Benjamini and Hochberg [J. Roy. Statist. Soc. Ser. B 57 (1995) 289-300]. In this article, we bring newer insight to the k-FDR considering a mixture model involving independent p-values before motivating the developments of some new procedures that control it. We prove the k-FDR control of the proposed methods under a slightly weaker condition than in the mixture model. We provide numerical evidence of the proposed methods' superior power performance over some k-FWER and k-FDR methods. Finally, we apply our methods to a real data set.",WOS:000265619700016,ANNALS OF STATISTICS,"['MULTIPLE TESTING PROCEDURES', 'FAMILYWISE ERROR RATE', 'STEPUP PROCEDURES', 'PROPORTION', 'NUMBER', 'NULL', 'FDR']",ON A GENERALIZED FALSE DISCOVERY RATE,2009
346,"A prominent issue in statistics education is the sometimes large disparity between the theoretical and the computational coursework. discreteRV is an R package for manipulation of discrete random variables which uses clean and familiar syntax similar to the mathematical notation in introductory probability courses. The package offers functions that are simple enough for users with little experience with statistical programming, but has more advanced features which are suitable for a large number of more complex applications. In this paper, we introduce and motivate discreteRV, describe its functionality, and provide reproducible examples illustrating its use.",WOS:000357431900016,R JOURNAL,,Manipulation of Discrete Random Variables with discreteRV,2015
347,"Modeling the interaction between persons and items at the item level for binary response data, item response theory (IRT) models have been found useful in a wide variety of applications in various fields. This paper provides the requisite information and description of software that implements the Gibbs sampling procedures for the one-, two- and three-parameter normal ogive models. The software developed is written in the MATLAB package IRTuno. The package is flexible enough to allow a use the choice to simulate binary response data, set the number of total or burn-in iterations, specify starting values or prior distributions for model parameters, check convergence of the Markov chain, and obtain Bayesian fit statistics. Illustrative examples are provided to demonstrate and validate the use of the software package. The m-file v25i08. m is also provided as a guide for the use of the MCMC algorithms with the three dichotomous IRT models.",WOS:000255794500001,JOURNAL OF STATISTICAL SOFTWARE,"['ITEM RESPONSE THEORY', 'BAYES FACTORS', 'FIT ANALYSIS', 'DISTRIBUTIONS', 'ABILITY', 'ALGORITHM', 'CURVES', 'MCMC']",Markov chain Monte Carlo estimation of normal ogive IRT models in MATLAB,2008
348,"Prediction error curves are increasingly used to assess and compare predictions in survival analysis. This article surveys the R package pec which provides a set of functions for efficient computation of prediction error curves. The software implements inverse probability of censoring weights to deal with right censored data and several variants of cross-validation to deal with the apparent error problem. In principle, all kinds of prediction models can be assessed, and the package readily supports most traditional regression modeling strategies, like Cox regression or additive hazard regression, as well as state of the art machine learning methods such as random forests, a nonparametric method which provides promising alternatives to traditional strategies in low and high-dimensional settings. We show how the functionality of pec can be extended to yet unsupported prediction models. As an example, we implement support for random forest prediction models based on the R packages randomSurvivalForest and party. Using data of the Copenhagen Stroke Study we use pec to compare random forests to a Cox regression model derived from stepwise variable selection.",WOS:000308910200001,JOURNAL OF STATISTICAL SOFTWARE,"['CROSS-VALIDATION', 'CLASSIFICATION', 'MODELS', 'INFARCTION', 'SELECTION', 'SAMPLES']",Evaluating Random Forests for Survival Analysis Using Prediction Error Curves,2012
349,"Directional data is ubiquitious in science. Due to its circular nature such data cannot be analyzed with commonly used statistical techniques. Despite the rapid development of specialized methods for directional statistics over the last fifty years, there is only little software available that makes such methods easy to use for practioners. Most importantly, one of the most commonly used programming languages in biosciences, MATLAB, is currently not supporting directional statistics. To remedy this situation, we have implemented the CircStat toolbox for MATLAB which provides methods for the descriptive and inferential statistical analysis of directional data. We cover the statistical background of the available methods and describe how to apply them to data. Finally, we analyze a dataset from neurophysiology to demonstrate the capabilities of the CircStat toolbox.",WOS:000270513700001,JOURNAL OF STATISTICAL SOFTWARE,"['MAGNETIC COMPASS', 'NEURONS', 'TESTS', 'WIND']",CircStat: A MATLAB Toolbox for Circular Statistics,2009
350,"Because many illnesses show heterogeneous response to treatment, there is increasing interest in individualizing treatment to patients [Arch. Gen. Psychiatry 66 (2009) 128-133]. An individualized treatment rule is a decision rule that recommends treatment according to patient characteristics. We consider the use of clinical trial data in the construction of an individualized treatment rule leading to highest mean response. This is a difficult computational problem because the objective function is the expectation of a weighted indicator function that is nonconcave in the parameters. Furthermore, there are frequently many pretreatment variables that may or may not be useful in constructing an optimal individualized treatment rule, yet cost and interpretability considerations imply that only a few variables should be used by the individualized treatment rule. To address these challenges, we consider estimation based on l(1)-penalized least squares. This approach is justified via a finite sample upper bound on the difference between the mean response due to the estimated individualized treatment rule and the mean response due to the optimal individualized treatment rule.",WOS:000291183300017,ANNALS OF STATISTICS,"['2-STAGE RANDOMIZATION DESIGNS', 'CLINICAL-TRIALS', 'SURVIVAL DISTRIBUTIONS', 'ORACLE INEQUALITIES', 'LASSO', 'SELECTION', 'SPARSITY', 'MODEL', 'CHEMOTHERAPY', 'COMBINATION']",PERFORMANCE GUARANTEES FOR INDIVIDUALIZED TREATMENT RULES,2011
351,"This paper compares the higher criticism statistic (Donoho and Jin [Ann. Statist. 32 (2004) 962-994]), a modification of the higher criticism statistic also suggested by Donoho and Jin, and two statistics of the Berk Jones [Z Wahrsch. Verw. Gebiete 47 (1979) 47-59] type. New approximations to the significance levels of the statistics are derived, and their accuracy is studied by simulations. By numerical examples it is shown that over a broad range of sample sizes the Berk Jones statistics have a better power function than the higher criticism statistics to detect sparse mixtures. The applications suggested by Meinshausen and Rice [Ann. Statist. 34 (2006) 373-393], to find lower confidence bounds for the number of false hypotheses, and by Jeng, Cai and Li [Biometrika 100 (2013) 157-172], to detect copy number variants, are also studied.",WOS:000355768700013,ANNALS OF STATISTICS,"['ASYMPTOTIC-DISTRIBUTION', 'STATISTICS', 'DISCOVERY', 'SEQUENCES', 'INTERVALS', 'TESTS']",HIGHER CRITICISM: p-VALUES AND CRITICISM,2015
352,"Recent results in quantization theory show that the mean-squared expected distortion can reach a rate of convergence of O(1/n), where n is the sample size [see, e.g., IEEE Trans. Inform. Theory 60 (2014) 7279-7292 or Electron. J. Stat. 7 (2013) 1716-1746]. This rate is attained for the empirical risk minimizer strategy, if the source distribution satisfies some regularity conditions. However, the dependency of the average distortion on other parameters is not known, and these results are only valid for distributions over finite-dimensional Euclidean spaces.
This paper deals with the general case of distributions over separable, possibly infinite dimensional, Hilbert spaces. A condition is proposed, which may be thought of as a margin condition [see, e.g., Ann. Statist. 27 (1999) 1808-1829], under which a nonasymptotic upper bound on the expected distortion rate of the empirically optimal quantizer is derived. The dependency of the distortion on other parameters of distributions is then discussed, in particular through a minimax lower bound.",WOS:000352757100005,ANNALS OF STATISTICS,"['DESIGN', 'RATES', 'CONVERGENCE', 'PERFORMANCE', 'QUANTIZERS', 'DISTORTION', 'THEOREM']",NONASYMPTOTIC BOUNDS FOR VECTOR QUANTIZATION IN HILBERT SPACES,2015
353,"Bayesian methods are becoming increasingly popular in the field of food-safety risk assessment. Risk assessment models often require the integration of a dose-response function over the distribution of all possible doses of a pathogen ingested with a specific food. This requires the evaluation of an integral for every sample for a Markov chain Monte Carlo analysis of a model. While many statistical software packages have functions that allow for the evaluation of the integral, this functionality is lacking in WinBUGS. A probabilistic model, that incorporates a novel numerical integration technique, is presented to facilitate the use of WinBUGS for food-safety risk assessments. The numerical integration technique is described in the context of a typical food-saftey risk assesment, some theoretical results are given, and a snippet of WinBUGS code is provided.",WOS:000208806000001,JOURNAL OF STATISTICAL SOFTWARE,,Bayesian Analysis for Food-Safety Risk Assessment: Evaluation of Dose-Response Functions within WinBUGS,2011
354,"Label switching is a well-known and fundamental problem in Bayesian estimation of mixture or hidden Markov models. In case that the prior distribution of the model parameters is the same for all states, then both the likelihood and posterior distribution are invariant to permutations of the parameters. This property makes Markov chain Monte Carlo (MCMC) samples simulated from the posterior distribution non-identifiable. In this paper, the label. switching package is introduced. It contains one probabilistic and seven deterministic relabeling algorithms in order to post-process a given MCMC sample, provided by the user. Each method returns a set of permutations that can be used to reorder the MCMC output. Then, any parametric function of interest can be inferred using the reordered MCMC sample. A set of user-defined permutations is also accepted, allowing the researcher to benchmark new relabeling methods against the available ones.",WOS:000373913900001,JOURNAL OF STATISTICAL SOFTWARE,"['MONTE-CARLO METHODS', 'MIXTURE-MODELS', 'BAYESIAN-ANALYSIS', 'UNKNOWN NUMBER', 'DISTRIBUTIONS', 'LIKELIHOOD', 'COMPONENTS', 'DENSITIES', 'ALGORITHM']",label.switching: An R Package for Dealing with the Label Switching Problem in MCMC Outputs,2016
355,"This paper presents the R package H A C, which provides user friendly methods for dealing with hierarchical Archimedean copulae (HAC). Computationally efficient estimation procedures allow to recover the structure and the parameters of HAC from data. In addition, arbitrary HAC can be constructed to sample random vectors and to compute the values of the corresponding cumulative distribution plus density functions. Accurate graphics of the HAC structure can be produced by the plot method implemented for these objects.",WOS:000341583900001,JOURNAL OF STATISTICAL SOFTWARE,,Hierarchical Archimedean Copulae: The HAC Package,2014
356,"We consider challenges that arise in the estimation of the mean outcome under an optimal individualized treatment strategy defined as the treatment rule that maximizes the population mean outcome, where the candidate treatment rules are restricted to depend on baseline covariates. We prove a necessary and sufficient condition for the pathwise differentiability of the optimal value, a key condition needed to develop a regular and asymptotically linear (RAL) estimator of the optimal value. The stated condition is slightly more general than the previous condition implied in the literature. We then describe an approach to obtain root-n rate confidence intervals for the optimal value even when the parameter is not pathwise differentiable. We provide conditions under which our estimator is RAL and asymptotically efficient when the mean outcome is pathwise differentiable. We also outline an extension of our approach to a multiple time point problem. All of our results are supported by simulations.",WOS:000372594300010,ANNALS OF STATISTICS,"['REGIMES TECHNICAL CHALLENGES', 'INDIVIDUALIZED TREATMENT RULES', 'PERFORMANCE', 'CLASSIFIERS']",STATISTICAL INFERENCE FOR THE MEAN OUTCOME UNDER A POSSIBLY NON-UNIQUE OPTIMAL TREATMENT STRATEGY,2016
357,"We introduce a three-parameter random walk with reinforcement, called the (theta, alpha, beta) scheme, which generalizes the linearly edge reinforced random walk to uncountable spaces. The parameter beta smoothly tunes the (theta, alpha, beta) scheme between this edge reinforced random walk and the classical exchangeable two-parameter Hoppe urn scheme, while the parameters a and theta modulate how many states are typically visited. Resorting to de Finetti's theorem for Markov chains, we use the (theta, alpha, beta) scheme to define a nonparametric prior for Bayesian analysis of reversible Markov chains. The prior is applied in Bayesian nonparametric inference for species sampling problems with data generated from a reversible Markov chain with an unknown transition kernel. As a real example, we analyze data from molecular dynamics simulations of protein folding.",WOS:000320488200017,ANNALS OF STATISTICS,['REINFORCED RANDOM-WALK'],BAYESIAN NONPARAMETRIC ANALYSIS OF REVERSIBLE MARKOV CHAINS,2013
358,"We consider the asymptotic behavior of posterior distributions and Bayes estimators based on observations which are required to be neither independent nor identically distributed. We give general results on the rate of convergence of the posterior measure relative to distances derived from a testing criterion. We then specialize our results to independent, nonidentically distributed observations, Markov processes, stationary Gaussian time series and the white noise model. We apply our general results to several examples of infinite-dimensional statistical models including nonparametric regression with normal errors, binary regression, Poisson regression, an interval censoring model, Whittle estimation of the spectral density of a time series and a nonlinear autoregressive model.",WOS:000247498100009,ANNALS OF STATISTICS,"['TIME-SERIES', 'NONPARAMETRIC PROBLEMS', 'BERNSTEIN POLYNOMIALS', 'MAXIMUM-LIKELIHOOD', 'DENSITY-ESTIMATION', 'CONSISTENCY', 'MODELS']",Convergence rates of posterior distributions for noniid observations,2007
359,"Although asymptotic analyses of undirected network models based on degree sequences have started to appear in recent literature, it remains an open problem to study statistical properties of directed network models. In this paper, we provide for the first time a rigorous analysis of directed exponential random graph models using the in-degrees and out-degrees as sufficient statistics with binary as well as continuous weighted edges. We establish the uniform consistency and the asymptotic normality for the maximum likelihood estimate, when the number of parameters grows and only one realized observation of the graph is available. One key technique in the proofs is to approximate the inverse of the Fisher information matrix using a simple matrix with high accuracy. Numerical studies confirm our theoretical findings.",WOS:000368022000002,ANNALS OF STATISTICS,"['P-ASTERISK MODELS', 'PROBABILITY-DISTRIBUTIONS', 'KANTOROVICH THEOREM', 'EXPOTENTIAL FAMILY', 'SOCIAL NETWORKS', 'BETA-MODEL', 'CONSISTENCY', 'NUMBER', 'PARAMETERS', 'NORMALITY']",ASYMPTOTICS IN DIRECTED EXPONENTIAL RANDOM GRAPH MODELS WITH AN INCREASING BI-DEGREE SEQUENCE,2016
360,"Kawaguchi, Koch, and Wang (2011) provide methodology and applications for a stratified Mann-Whitney estimator that addresses the same comparison between two randomized groups for a strictly ordinal response variable as the van Elteren test statistic for randomized clinical trials with strata. The sanon package provides the implementation of the method within the R programming environment. The usage of sanon is illustrated with five examples. The first example is a randomized clinical trial with eight strata and a univariate ordinal response variable. The second example is a randomized clinical trial with four strata, two covariables, and four ordinal response variables. The third example is a crossover design randomized clinical trial with two strata, one covariable, and two ordinal response variables. The fourth example is a randomized clinical trial with seven strata (which are managed as a categorical covariable), three ordinal covariables with missing values, and three ordinal response variables with missing values. The fifth example is a randomized clinical trial with six strata, a categorical covariable with three levels, and three ordinal response variables with missing values.",WOS:000365983900001,JOURNAL OF STATISTICAL SOFTWARE,"['MISSING DATA', 'TRIALS']",sanon: An R Package for Stratified Analysis with Nonparametric Covariable Adjustment,2015
361,"We present the package RcmdrPlugin.temis, a graphical user interface for user-friendly text mining in R. Built as a plug-in to the R Commander provided by the Rcmdr package, it brings together several existing packages and provides new features streamlining the process of importing, managing and analyzing a corpus, in addition to saving results and plots to a report file. Beyond common file formats, automated import of corpora from the Dow Jones Factiva content provider and Twitter is supported. Featured analyses include vocabulary and dissimilarity tables, terms frequencies, terms specific of levels of a variable, term co-occurrences, time series, correspondence analysis and hierarchical clustering.",WOS:000321944400019,R JOURNAL,,"RcmdrPlugin.temis, a Graphical Integrated Text Mining Solution in R",2013
362,"A new single-index model that reflects the time-dynamic effects of the single index is proposed for longitudinal and functional response data, possibly measured with errors, for both longitudinal and time-invariant covariates. With appropriate initial estimates of the parametric index, the proposed estimator is shown to be root n-consistent and asymptotically normally distributed. We also address the nonparametric estimation of regression functions and provide estimates with optimal convergence rates. One advantage of the new approach is that the same bandwidth is used to estimate both the nonparametric mean function and the parameter in the index. The finite-sample performance for the proposed procedure is studied numerically.",WOS:000288183800012,ANNALS OF STATISTICS,"['PROJECTION PURSUIT REGRESSION', 'DIMENSION REDUCTION', 'ASYMPTOTIC DISTRIBUTIONS', 'ESTIMATORS']",FUNCTIONAL SINGLE INDEX MODELS FOR LONGITUDINAL DATA,2011
363,"Most papers on high-dimensional statistics are based on the assumption that none of the regressors are correlated with the regression error, namely, they are exogenous. Yet, endogeneity can arise incidentally from a large pool of regressors in a high-dimensional regression. This causes the inconsistency of the penalized least-squares method and possible false scientific discoveries. A necessary condition for model selection consistency of a general class of penalized regression methods is given, which allows us to prove formally the inconsistency claim. To cope with the incidental endogeneity, we construct a novel penalized focused generalized method of moments (FGMM) criterion function. The FGMM effectively achieves the dimension reduction and applies the instrumental variable methods. We show that it possesses the oracle property even in the presence of endogenous predictors, and that the solution is also near global minimum under the over-identification assumption. Finally, we also show how the semi-parametric efficiency of estimation can be achieved via a two-step approach.",WOS:000338477800003,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'CONDITIONAL MOMENT RESTRICTIONS', 'ADAPTIVE ELASTIC-NET', 'VARIABLE SELECTION', 'REGRESSION-MODELS', 'NONPARAMETRIC MODELS', 'EFFICIENT ESTIMATION', 'GENERALIZED-METHOD', 'ORACLE PROPERTIES', 'DANTZIG SELECTOR']",ENDOGENEITY IN HIGH DIMENSIONS,2014
364,"The metaplus package is described with examples of its use for fitting meta-analysis and meta-regression. For either meta-analysis or meta-regression it is possible to fit one of three models: standard normal random effect, t-distribution random effect or mixture of normal random effects. The latter two models allow for robustness by allowing for a random effect distribution with heavier tails than the normal distribution, and for both robust models the presence of outliers may be tested using the parametric bootstrap. For the mixture of normal random effects model the outlier studies may be identified through their posterior probability of membership in the outlier component of the mixture. Plots allow the results of the different models to be compared. The package is demonstrated on three examples: a meta-analysis with no outliers, a meta-analysis with an outlier and a meta-regression with an outlier.",WOS:000385276100002,R JOURNAL,"['MAGNESIUM TRIALS', 'MIXTURE', 'MODELS', 'OUTLIER']",metaplus: An R Package for the Analysis of Robust Meta-Analysis and Meta-Regression,2016
365,"Sandwich covariance matrix estimators are a popular tool in applied regression modeling for performing inference that is robust to certain types of model misspecification. Suitable implementations are available in the R system for statistical computing for certain model fitting functions only ( in particular lm()), but not for other standard regression functions, such as glm(), nls(), or survreg().
Therefore, conceptual tools and their translation to computational tools in the package sandwich are discussed, enabling the computation of sandwich estimators in general parametric models. Object orientation can be achieved by providing a few extractor functions - most importantly for the empirical estimating functions - from which various types of sandwich estimators can be computed.",WOS:000240206500001,JOURNAL OF STATISTICAL SOFTWARE,"['COVARIANCE-MATRIX ESTIMATION', 'UNKNOWN FORM', 'HETEROSKEDASTICITY', 'REGRESSION']",Object-oriented computation of sandwich estimators,2006
366,"We consider the fundamental problem of estimating the mean of a vector y = X beta + 7, where X is an n x p design matrix in which one can have far more variables than observations, and z is a stochastic error term-the so-called ""p > n"" setup. When beta is sparse, or, more generally, when there is a sparse subset of covariates providing a close approximation to the unknown mean vector, we ask whether or not it is possible to accurately, estimate X beta using a computationally tractable algorithm.
We show that, in a Surprisingly wide range of situations. the lasso happens to nearly select the best Subset of variables. Quantitatively speaking, we prove that solving a simple quadratic program achieves a squared error within a logarithmic factor of the ideal mean squared error that one Would achieve with an oracle Supplying perfect information about which variables should and should not be included in the model. Interestingly, our results describe the average performance of the lasso; that is, the performance one can expect in an vast majority of cases where X beta is a sparse or nearly sparse superposition of variables, but not in all cases.
Our results are nonasymptotic and widely applicable. since they simply require that pairs of predictor variables are not too collinear.",WOS:000268604900003,ANNALS OF STATISTICS,"['DANTZIG SELECTOR', 'LASSO', 'REGRESSION', 'REPRESENTATIONS', 'PERSISTENCE']",NEAR-IDEAL MODEL SELECTION BY l(1) MINIMIZATION,2009
367,,WOS:000253077800008,ANNALS OF STATISTICS,"['ROBUST UNCERTAINTY PRINCIPLES', 'LARGE UNDERDETERMINED SYSTEMS', 'SIGNAL RECOVERY', 'REGRESSION', 'EQUATIONS']",Rejoinder: The Dantzig selector: Statistical estimation when p is much larger than n,2007
368,"The asymptotic theory of various estimators based on Gaussian likelihood has been developed for the unit root and near unit root cases of a first-order moving average model. Previous studies of the MA(1) unit root problem rely on the special autocovariance structure of the MA(1) process, in which case, the eigenvalues and eigenvectors of the covariance matrix of the data vector have known analytical forms. In this paper, we take a different approach to first consider the joint likelihood by including an augmented initial value as a parameter and then recover the exact likelihood by integrating out the initial value. This approach by-passes the difficulty of computing an explicit decomposition of the covariance matrix and can be used to study unit root behavior in moving averages beyond first order. The asymptotics of the generalized likelihood ratio (GLR) statistic for testing unit roots are also studied. The GLR test has operating characteristics that are competitive with the locally best invariant unbiased (LBIU) test of Tanaka for some local alternatives and dominates for all other alternatives.",WOS:000300383200010,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'ABSOLUTE DEVIATION ESTIMATION', 'TIME-SERIES MODELS', 'REGRESSION-MODELS', 'AUTOREGRESSIVE PROCESSES', 'ERRORS', 'CIRCLE']",UNIT ROOTS IN MOVING AVERAGES BEYOND FIRST ORDER,2011
369,"This article extends the scope of empirical likelihood methodology ill three directions: to allow for plug-in estimates Of nuisance parameters in estimating equations, slower than root n-rates of convergence, and settings in which there are a relatively large number of estimating equations compared to the sample size. Calibrating empirical likelihood confidence regions with plug-in is sometimes intractable due to the complexity of the asymptotics, so we introduce a bootstrap approximation that call be used in such situations. We provide a range of examples from survival analysis and nonparametric statistics to illustrate the main results.",WOS:000265619700001,ANNALS OF STATISTICS,"['STRONG UNIFORM CONSISTENCY', 'KAPLAN-MEIER ESTIMATOR', 'CENTRAL-LIMIT-THEOREM', 'CONFIDENCE-INTERVALS', 'SEMIPARAMETRIC MODELS', 'REGRESSION-MODELS', 'CENSORED-DATA', 'DENSITY', 'CONVERGENCE', 'FUNCTIONALS']",EXTENDING THE SCOPE OF EMPIRICAL LIKELIHOOD,2009
370,"Let f : [0, 1)(d) -> R be an integrable function. An objective of many computer experiments is to estimate integral(d)([0, 1)) f (x) dx by evaluating f at a finite number of points in [0, 1)(d). There is a design issue in the choice of these points and a popular choice is via the use of randomized orthogonal arrays. This article proves a multivariate central limit theorem for a class of randomized orthogonal array sampling designs [Owen Statist. Sinica 2 (1992a) 439-452] as well as for a class of OA-based Latin hypercubes.",WOS:000258243000020,ANNALS OF STATISTICS,"['MONTE-CARLO VARIANCE', 'CONVERGENCE']",A multivariate central limit theorem for randomized orthogonal array sampling designs in computer experiments,2008
371,"An estimation method is proposed for a wide variety of discrete time stochastic processes that have an intractable likelihood function but are otherwise conveniently specified by an integral transform such as the characteristic function, the Laplace transform or the probability generating function. This method involves the construction of classes of transform-based martingale estimating functions that fit into the general framework of quasi-likelihood. In the parametric setting of a discrete time stochastic process, we obtain transform quasi-score functions by projecting the unavailable score function onto the special linear spaces formed by these classes. The specification of the process by any of the main integral transforms makes possible an arbitrarily close approximation of the score function in an infinite-dimensional Hilbert space by optimally combining transform martingale quasi-score functions. It also allows an extension of the domain of application of quasi-likelihood methodology to processes with infinite conditional second moment.",WOS:000251096100006,ANNALS OF STATISTICS,"['EMPIRICAL CHARACTERISTIC FUNCTION', 'QUASI-LIKELIHOOD ESTIMATION', 'MODELS', 'DISTRIBUTIONS', 'SEMIMARTINGALES', 'PARAMETERS', 'EFFICIENCY', 'INFERENCE']","Transform, martingale estimating functions",2007
372,,WOS:000275510800025,ANNALS OF STATISTICS,,"REPLY TO ""ON SOME PROBLEMS IN THE ARTICLE EFFICIENT LIKELIHOOD ESTIMATION IN STATE SPACE MODELS""(vol 34, pg 2026, 2006)",2010
373,"We explore the limits of the autoregressive (AR) sieve bootstrap, and show that its applicability extends well beyond the realm of linear time series as has been previously thought. In particular, for appropriate statistics, the AR-sieve bootstrap is valid for stationary processes possessing a general Wold-type autoregressive representation with respect to a white noise; in essence, this includes all stationary, purely nondeterministic processes, whose spectral density is everywhere positive. Our main theorem provides a simple and effective tool in assessing whether the AR-sieve bootstrap is asymptotically valid in any given situation. In effect, the large-sample distribution of the statistic in question must only depend on the first and second order moments of the process; prominent examples include the sample mean and the spectral density. As a counterexample, we show how the AR-sieve bootstrap is not always valid for the sample autocovariance even when the underlying process is linear.",WOS:000296995500010,ANNALS OF STATISTICS,"['TIME-SERIES ANALYSIS', 'FREQUENCY-DOMAIN', 'MODELS', 'PHASE', 'ORDER']",ON THE RANGE OF VALIDITY OF THE AUTOREGRESSIVE SIEVE BOOTSTRAP,2011
374,"The generalized varying coefficient partially linear model with a growing number of predictors arises in many contemporary scientific endeavor. In this paper we set foot on both theoretical and practical sides of profile likelihood estimation and inference. When the number of parameters grows with sample size, the existence and asymptotic normality of the profile likelihood estimator are established under some regularity conditions. Profile likelihood ratio inference for the growing number of parameters is proposed and Wilk's phenomenon is demonstrated. A new algorithm, called the accelerated profile-kernel algorithm, for computing profile-kernel estimator is proposed and investigated. Simulation studies show that the resulting estimates are as efficient as the fully iterative profile-kernel estimates. For moderate sample sizes, our proposed procedure saves much computational time over the fully iterative profile-kernel one and gives stabler estimates. A set of real data is analyzed using our proposed algorithm.",WOS:000260554100009,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'PARTIALLY LINEAR-MODELS', 'EFFICIENT ESTIMATION', 'REGRESSION', 'STATISTICS']",PROFILE-KERNEL LIKELIHOOD INFERENCE WITH DIVERGING NUMBER OF PARAMETERS,2008
375,"We introduce ctsem, an R package for continuous time structural equation modeling of panel (N > 1) and time series (N = 1) data, using full information maximum likelihood. Most dynamic models (e.g., cross-lagged panel models) in the social and behavioural sciences are discrete time models. An assumption of discrete time models is that time intervals between measurements are equal, and that all subjects were assessed at the same intervals. Violations of this assumption are often ignored due to the difficulty of accounting for varying time intervals, therefore parameter estimates can be biased and the time course of effects becomes ambiguous. By using stochastic differential equations to estimate an underlying continuous process, continuous time models allow for any pattern of measurement occasions. By interfacing to OpenMx, ctsem combines the flexible specification of structural equation models with the enhanced data gathering opportunities and improved estimation of continuous time models. ctsem can estimate relationships over time for multiple latent processes, measured by multiple noisy indicators with varying time intervals between observations. Within and between effects are estimated simultaneously by modeling both observed covariates and unobserved heterogeneity. Exogenous shocks with different shapes, group differences, higher order diffusion effects and oscillating processes can all be simply modeled. We first introduce and define continuous time models, then show how to specify and estimate a range of continuous time models using ctsem.",WOS:000399024700001,JOURNAL OF STATISTICAL SOFTWARE,"['SERIES ANALYSIS', 'CROSS-SECTION', 'PANEL MODELS', 'SEM']",Continuous Time Structural Equation Modeling with R Package ctsem,2017
376,"The problem of existence of adaptive confidence bands for an unknown density f that belongs to a nested scale of Holder classes over R or [0, 1] is considered. Whereas honest adaptive inference in this problem is impossible already for a pair of Holder balls Sigma (r), Sigma(s), r not equal s, of fixed radius, a non-parametric distinguishability condition is introduced under which adaptive confidence bands can be shown to exist. It is further shown that this condition is necessary and sufficient for the existence of honest asymptotic confidence bands, and that it is strictly weaker than similar analytic conditions recently employed in Gine and Nickl [Ann. Statist. 38 (2010) 1122-1170]. The exceptional sets for which honest inference is not possible have vanishingly small probability under natural priors on Holder balls Sigma (s). If no upper bound for the radius of the Holder balls is known, a price for adaptation has to be paid, and near-optimal adaptation is possible for standard procedures. The implications of these findings for a general theory of adaptive inference are discussed.",WOS:000299186500008,ANNALS OF STATISTICS,"['SUP-NORM LOSS', 'DENSITY-ESTIMATION', 'POSTERIOR DISTRIBUTIONS', 'WAVELET', 'ESTIMATORS', 'REGRESSION', 'RATES', 'INTERVALS', 'BALLS']",ON ADAPTIVE INFERENCE AND CONFIDENCE BANDS,2011
377,"We present the R package mnlogit for estimating multinomial logistic regression models, particularly those involving a large number of categories and variables. Compared to existing software, mnlogit offers speedups of 10-50 times for modestly sized problems and more than 100 times for larger problems. Running in parallel mode on a multicore machine gives up to 4 times additional speedup on 8 processor cores. mnlogit achieves its computational efficiency by drastically speeding up computation of the log-likelihood function's Hessian matrix through exploiting structure in matrices that arise in intermediate calculations. This efficient exploitation of intermediate data structures allows mnlogit to utilize system memory much more efficiently, such that for most applications mnlogit requires less memory than comparable software by a factor that is proportional to the number of model categories.",WOS:000392704200001,JOURNAL OF STATISTICAL SOFTWARE,"['LARGE-SCALE OPTIMIZATION', 'MEMORY BFGS METHOD', 'LOGISTIC-REGRESSION', 'NEWTON METHOD']",Fast Estimation of Multinomial Logit Models: R Package mnlogit,2016
378,"We study nonparametric estimation of the sub-distribution functions for current status data with competing risks. Our main interest is in the nonparametric maximum likelihood estimator (MLE), and for comparison we also consider a simpler ""naive estimator."" Both types of estimators were studied by Jewell, van der Laan and Henneman [Biometrika (2003) 90 183-197], but little was known about their large sample properties. We have started to fill this gap, by proving that the estimators are consistent and converge globally and locally at rate n(1/3). We also show that this local rate of convergence is optimal in a minimax sense. The proof of the local rate of convergence of the MLE uses new methods, and relies on a rate result for the sum of the MLEs of the sub-distribution functions which holds uniformly on a fixed neighborhood of a point. Our results are used in Groeneboom, Maathuis and Wellner [Ann. Statist. (2008) 36 1064-1089] to obtain the local limiting distributions of the estimators.",WOS:000256504400001,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD ESTIMATORS', 'INTERVAL-CENSORED DATA']",Current status data with competing risks: Consistency and rates of convergence of the MLE,2008
379,,WOS:000307608000024,ANNALS OF STATISTICS,,"MOMENTS OF MINORS OF WISHART MATRICES (vol 36, pg 2261, 2008)",2012
380,"Motivated by a neuroscience question about synchrony detection in spike train analysis, we deal with the independence testing problem for point processes. We introduce nonparametric test statistics, which are resealed general U-statistics, whose corresponding critical values are constructed from bootstrap and randomization/permutation approaches, making as few assumptions as possible on the underlying distribution of the point processes. We derive general consistency results for the bootstrap and for the permutation w.r.t. Wasserstein's metric, which induces weak convergence as well as convergence of second-order moments. The obtained bootstrap or permutation independence tests are thus proved to be asymptotically of the prescribed size, and to be consistent against any reasonable alternative. A simulation study is performed to illustrate the derived theoretical results, and to compare the performance of our new tests with existing ones in the neuroscientific literature.",WOS:000363437900008,ANNALS OF STATISTICS,"['CENTRAL LIMIT-THEOREM', 'U-STATISTICS', 'RANK CORRELATION', 'MULTIPLE TESTS', 'UNITARY EVENTS', 'HIGH DIMENSION', 'HYPOTHESES', 'SAMPLES']",BOOTSTRAP AND PERMUTATION TESTS OF INDEPENDENCE FOR POINT PROCESSES,2015
381,"mefa is an R package for multivariate data handling in ecology and biogeography. It provides object classes to represent the data coded by samples, taxa and segments (i.e., subpopulations, repeated measures). It supports easy processing of the data along with relational data tables for samples and taxa. An object of class 'mefa' is a project specific compendium of the dataset and can be easily used in further analyses. Methods are provided for extraction, aggregation, conversion, plotting, summary and reporting of 'mefa' objects. Reports can be generated in plain text or LATEX format. This paper presents worked examples on a variety of ecological analyses.",WOS:000263825200001,JOURNAL OF STATISTICAL SOFTWARE,['SPECIES-DIVERSITY'],Processing Ecological Data in R with the mefa Package,2009
382,"We consider the statistical experiment of functional linear regression (FLR). Furthermore, we introduce a white noise model where one observes an Ito process, which contains the covariance operator of the corresponding FLR model in its construction. We prove asymptotic equivalence of FLR and this white noise model in LeCam's sense under known design distribution. Moreover, we show equivalence of FLR and an empirical version of the white noise model for finite sample sizes. As an application, we derive sharp minimax constants in the FLR model which are still valid in the case of unknown design distribution.",WOS:000293716500005,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'ESTIMATORS', 'MODELS', 'PREDICTION']",ASYMPTOTIC EQUIVALENCE OF FUNCTIONAL LINEAR REGRESSION AND A WHITE NOISE INVERSE PROBLEM,2011
383,"Recently, network analysis has gained more and more attention in statistics, as well as in computer science, probability and applied mathematics. Community detection for the stochastic block model (SBM) is probably the most studied topic in network analysis. Many methodologies have been proposed. Some beautiful and significant phase transition results are obtained in various settings. In this paper, we provide a general minimax theory for community detection. It gives minimax rates of the mis-match ratio for a wide rage of settings including homogeneous and inhomogeneous SBMs, dense and sparse networks, finite and growing number of communities. The minimax rates are exponential, different from polynomial rates we often see in statistical literature. An immediate consequence of the result is to establish threshold phenomenon for strong consistency (exact recovery) as well as weak consistency (partial recovery). We obtain the upper bound by a range of penalized likelihood-type approaches. The lower bound is achieved by a novel reduction from a global mis-match ratio to a local clustering problem for one node through an exchangeability property.",WOS:000384397200019,ANNALS OF STATISTICS,"['NETWORKS', 'CONSISTENCY', 'BLOCKMODELS', 'LIKELIHOOD']",MINIMAX RATES OF COMMUNITY DETECTION IN STOCHASTIC BLOCK MODELS,2016
384,"The delta method is a popular and elementary tool for deriving limiting distributions of transformed statistics, while applications of asymptotic distributions do not allow one to obtain desirable accuracy of approximation for tail probabilities. The large and moderate deviation theory can achieve this goal. Motivated by the delta method in weak convergence, a general delta method in large deviations is proposed. The new method can be widely applied to driving the moderate deviations of estimators and is illustrated by examples including the Wilcoxon statistic, the Kaplan-Meier estimator, the empirical quantile processes and the empirical copula function. We also improve the existing moderate deviations results for M-estimators and L-statistics by the new method. Some applications of moderate deviations to statistical hypothesis testing are provided.",WOS:000291183300018,ANNALS OF STATISTICS,"['KERNEL DENSITY ESTIMATOR', 'KAPLAN-MEIER ESTIMATOR', 'EMPIRICAL PROCESSES', 'LIMIT-THEOREMS', 'STATISTICS', 'INEQUALITY', 'EFFICIENCY', 'PARAMETER']",DELTA METHOD IN LARGE DEVIATIONS AND MODERATE DEVIATIONS FOR ESTIMATORS,2011
385,,WOS:000389620800002,ANNALS OF STATISTICS,"['HIGHER CRITICISM', 'FEATURE-SELECTION', 'MIXTURE-MODELS']","DISCUSSION OF ""INFLUENTIAL FEATURES PCA FOR HIGH DIMENSIONAL CLUSTERING""",2016
386,"The spatial distribution has been widely used to develop various non-parametric procedures for finite dimensional multivariate data. In this paper, we investigate the concept of spatial distribution for data in infinite dimensional Banach spaces. Many technical difficulties are encountered in such spaces that are primarily due to the noncompactness of the closed unit ball. In this work, we prove some Glivenko-Cantelli and Donsker-type results for the empirical spatial distribution process in infinite dimensional spaces. The spatial quantiles in such spaces can be obtained by inverting the spatial distribution function. A Bahadur-type asymptotic linear representation and the associated weak convergence results for the sample spatial quantiles in infinite dimensional spaces are derived. A study of the asymptotic efficiency of the sample spatial median relative to the sample mean is carried out for some standard probability distributions in function spaces. The spatial distribution can be used to define the spatial depth in infinite dimensional Banach spaces, and we study the asymptotic properties of the empirical spatial depth in such spaces. We also demonstrate the spatial quantiles and the spatial depth using some real and simulated functional data.",WOS:000338477800014,ANNALS OF STATISTICS,"['FUNCTIONAL DATA', 'NOTION', 'TESTS']",THE SPATIAL DISTRIBUTION IN INFINITE DIMENSIONAL SPACES AND RELATED QUANTILES AND DEPTHS,2014
387,"Consider a random sample from a bivariate distribution function F in the max-domain of attraction of all extreme-value distribution function G. This G is characterized by two extreme-value indices and a spectral measure, the latter determining the tail dependence structure of F. A major issue in multivariate extreme-value theory is the estimation of the spectral measure (1)p with respect to the L(p) norm. For every p is an element of [1, infinity], a nonparametric maximum empirical likelihood estimator is proposed for Phi(p). The main novelty is that these estimators are guaranteed to satisfy the moment constraints by which spectral measures are characterized. Asymptotic normality of the estimators is proved under conditions that allow for tail independence. Moreover, the conditions are easily verifiable as we demonstrate through a number of theoretical examples. A simulation study shows a substantially improved performance of the new estimators. Two case Studies illustrate how to implement the methods in practice.",WOS:000268605000013,ANNALS OF STATISTICS,['TAIL DEPENDENCE'],MAXIMUM EMPIRICAL LIKELIHOOD ESTIMATION OF THE SPECTRAL MEASURE OF AN EXTREME-VALUE DISTRIBUTION,2009
388,,WOS:000312899000007,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'COVARIANCE ESTIMATION', 'LASSO']",DISCUSSION: LATENT VARIABLE GRAPHICAL MODEL SELECTION VIA CONVEX OPTIMIZATION,2012
389,"Animated graphs that demonstrate statistical ideas and methods can both attract interest and assist understanding. In this paper we first discuss how animations can be related to some statistical topics such as iterative algorithms, random simulations, (re)sampling methods and dynamic trends, then we describe the approaches that may be used to create animations, and give an overview to the R package animation, including its design, usage and the statistical topics in the package. With the animation package, we can export the animations produced by R into a variety of formats, such as a web page, a GIF animation, a Flash movie, a PDF document, or an MP4/AVI video, so that users can publish the animations fairly easily. The design of this package is flexible enough to be readily incorporated into web applications, e.g., we can generate animations online with Rweb, which means we do not even need R to be installed locally to create animations. We will show examples of the use of animations in teaching statistics and in the presentation of statistical reports using Sweave or knitr. In fact, this paper itself was written with the knitr and animation package, and the animations are embedded in the PDF document, so that readers can watch the animations in real time when they read the paper (the Adobe Reader is required).
Animations can add insight and interest to traditional static approaches to teaching statistics and reporting, making statistics a more interesting and appealing subject.",WOS:000318236500001,JOURNAL OF STATISTICAL SOFTWARE,['TEACHING STATISTICS'],animation: An R Package for Creating Animations and Demonstrating Statistical Methods,2013
390,"The hazard function is a key component in the inferential process in survival analysis and relevant for describing the pattern of failures. However, it is rarely shown in research papers due to the difficulties in nonparametric estimation. We developed the bshazard package to facilitate the computation of a nonparametric estimate of the hazard function, with data-driven smoothing. The method accounts for left truncation, right censoring and possible covariates. B-splines are used to estimate the shape of the hazard within the generalized linear mixed models framework. Smoothness is controlled by imposing an autoregressive structure on the baseline hazard coefficients. This perspective allows an 'automatic' smoothing by avoiding the need to choose the smoothing parameter, which is estimated from the data as a dispersion parameter. A simulation study demonstrates the capability of our software and an application to estimate the hazard of Non-Hodgkin's lymphoma in Swedish population data shows its potential.",WOS:000348651700010,R JOURNAL,"['PROPORTIONAL-HAZARDS', 'MODEL', 'ESTIMATORS']",bshazard: A Flexible Tool for Nonparametric Smoothing of the Hazard Function,2014
391,"Unlike S-PLUS, R does not incorporate a statistical graphical user interface (GUI), but it does include tools for building GUIs. Based on the tcltk package (which furnishes an interface to the Tcl/Tk GUI toolkit), the Rcmdr package provides a basic-statistics graphical user interface to R called the ""R Commander."" The design objectives of the R Commander were as follows: to support, through an easy-to-use, extensible, cross-platform GUI, the statistical functionality required for a basic-statistics course (though its current functionality has grown to include support for linear and generalized-linear models, and other more advanced features); to make it relatively difficult to do unreasonable things; and to render visible the relationship between choices made in the GUI and the R commands that they generate. The R Commander uses a simple and familiar menu/dialog-box interface. Top-level menus include File, Edit, Data, Statistics, Graphs, Models, Distributions, Tools, and Help, with the complete menu tree given in the paper. Each dialog box includes a Help button, which leads to a relevant help page. Menu and dialog-box selections generate R commands, which are recorded in a script window and are echoed, along with output, to an output window. The script window also provides the ability to edit, enter, and re-execute commands. Error messages, warnings, and some other information appear in a separate messages window. Data sets in the R Commander are simply R data frames, and can be read from attached packages or imported from files. Although several data frames may reside in memory, only one is ""active"" at any given time. There may also be an active statistical model (e.g., an R 1m or g1m object). The purpose of this paper is to introduce and describe the use of the R Commander GUI; to describe the design and development of the R Commander; and to explain how the R Commander GUI can be extended. The second part of the paper (following a brief introduction) can serve as an introductory guide for students who will use the R Commander.",WOS:000232891600001,JOURNAL OF STATISTICAL SOFTWARE,,The R commander: A basic-statistics graphical user interface to R,2005
392,"The beta-model of random graphs is an exponential family model with the degree sequence as a sufficient statistic. In this paper, we contribute three key results. First, we characterize conditions that lead to a quadratic time algorithm to check for the existence of MLE of the beta-model, and show that the MLE never exists for the degree partition beta-model. Second, motivated by privacy problems with network data, we derive a differentially private estimator of the parameters of beta-model, and show it is consistent and asymptotically normally distributed-it achieves the same rate of convergence as the non-private estimator. We present an efficient algorithm for the private estimator that can be used to release synthetic graphs. Our techniques can also be used to release degree distributions and degree partitions accurately and privately, and to perform inference from noisy degrees arising from contexts other than privacy. We evaluate the proposed estimator on real graphs and compare it with a current algorithm for releasing degree distributions and find that it does significantly better. Finally, our paper addresses shortcomings of current approaches to a fundamental problem of how to perform valid statistical inference from data released by privacy mechanisms, and lays a foundational groundwork on how to achieve optimal and private statistical inference in a principled manner by modeling the privacy mechanism; these principles should be applicable to a class of models beyond the beta-model.",WOS:000368022000004,ANNALS OF STATISTICS,"['SOCIAL NETWORKS', 'DEGREE SEQUENCE', 'VERTICES', 'NUMBER']",INFERENCE USING NOISY DEGREES: DIFFERENTIALLY PRIVATE beta-MODEL AND SYNTHETIC GRAPHS,2016
393,"Asymmetric least squares regression is an important method that has wide applications in statistics, econometrics and finance. The existing work on asymmetric least squares only considers the traditional low dimension and large sample setting. In this paper, we systematically study the Sparse Asymmetric LEast Squares (SALES) regression under high dimensions where the penalty functions include the Lasso and nonconvex penalties. We develop a unified efficient algorithm for fitting SALES and establish its theoretical properties. As an important application, SALES is used to detect heteroscedasticity in high-dimensional data. Another method for detecting heteroscedasticity is the sparse quantile regression. However, both SALES and the sparse quantile regression may fail to tell which variables are important for the conditional mean and which variables are important for the conditional scale/variance, especially when there are variables that are important for both the mean and the scale. To that end, we further propose a COupled Sparse Asymmetric LEast Squares (COSALES) regression which can be efficiently solved by an algorithm similar to that for solving SALES. We establish theoretical properties of COSALES. In particular, COSALES using the SCAD penalty or MCP is shown to consistently identify the two important subsets for the mean and scale simultaneously, even when the two subsets overlap. We demonstrate the empirical performance of SALES and COSALES by simulated and real data.",WOS:000389620800015,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'ULTRA-HIGH DIMENSION', 'DANTZIG SELECTOR', 'COORDINATE DESCENT', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'LASSO', 'MODELS', 'RISK', 'MINIMIZATION']",HIGH-DIMENSIONAL GENERALIZATIONS OF ASYMMETRIC LEAST SQUARES REGRESSION AND THEIR APPLICATIONS,2016
394,"Some effort has been undertaken over the last decade to provide conditions for the control of the false discovery rate by the linear step-up procedure (LSU) for testing n hypotheses when test statistics are dependent. In this paper we investigate the expected error rate (EER) and the false discovery rate (FDR) in some extreme parameter configurations when n tends to infinity for test statistics being exchangeable under null hypotheses. All results are derived in terms of p-values. In a general setup we present a series of results concerning the interrelation of Simes' rejection curve and the (limiting) empirical distribution function of the p-values. Main objects under investigation are largest (limiting) crossing points between these functions, which play a key role in deriving explicit formulas for EER and FDR. As specific examples we investigate equi-correlated normal and t-variables in more detail and compute the limiting EER and FDR theoretically and numerically. A surprising limit behavior occurs if these models tend to independence.",WOS:000249568000004,ANNALS OF STATISTICS,"['I ERRORS', 'INEQUALITIES']",Dependency and false discovery rate: Asymptotics,2007
395,"We analyze the performance of spectral clustering for community extraction in stochastic block models. We show that, under mild conditions, spectral clustering applied to the adjacency matrix of the network can consistently recover hidden communities even when the order of the maximum expected degree is as small as log n, with n the number of nodes. This result applies to some popular polynomial time spectral clustering algorithms and is further extended to degree corrected stochastic block models using a spherical k-median spectral clustering method. A key component of our analysis is a combinatorial bound on the spectrum of binary random matrices, which is sharper than the conventional matrix Bernstein inequality and may be of independent interest.",WOS:000349738500008,ANNALS OF STATISTICS,"['RANDOM GRAPHS', 'BLOCKMODELS', 'DIMENSIONS', 'ALGORITHM', 'NETWORKS']",CONSISTENCY OF SPECTRAL CLUSTERING IN STOCHASTIC BLOCK MODELS,2015
396,"Estimation of genewise variance arises from two important applications in microarray data analysis: selecting significantly differentially expressed genes and validation tests for normalization of microarray data. We approach the problem by introducing a two-way nonparametric model, which is an extension of the famous Neyman-Scott model and is applicable beyond microarray data. The problem itself poses interesting challenges because the number of nuisance parameters is proportional to the sample size and it is not obvious how the variance function can be estimated when measurements are correlated. In such a high-dimensional nonparametric problem, we proposed two novel nonparametric estimators for genewise variance function and semiparametric estimators for measurement correlation, via solving a system of nonlinear equations. Their asymptotic normality is established. The finite sample property is demonstrated by simulation studies. The estimators also improve the power of the tests for detecting statistically differentially expressed genes. The methodology is illustrated by the data from microarray quality control (MAQC) project.",WOS:000282402800005,ANNALS OF STATISTICS,"['WITHIN-ARRAY REPLICATIONS', 'NORMALIZATION', 'EXPRESSION', 'MODEL']",NONPARAMETRIC ESTIMATION OF GENEWISE VARIANCE FOR MICROARRAY DATA,2010
397,"Given independent samples from P and Q, two-sample permutation tests allow one to construct exact level tests when the null hypothesis is P = Q. On the other hand, when comparing or testing particular parameters theta of P and Q, such as their means or medians, permutation tests need not be level a, or even approximately level alpha in large samples. Under very weak assumptions for comparing estimators, we provide a general test procedure whereby the asymptotic validity of the permutation test holds while retaining the exact rejection probability alpha in finite samples when the underlying distributions are identical. The ideas are broadly applicable and special attention is given to the k-sample problem of comparing general parameters, whereby a permutation test is constructed which is exact level alpha under the hypothesis of identical distributions, but has asymptotic rejection probability alpha under the more general null hypothesis of equality of parameters. A Monte Carlo simulation study is performed as well. A quite general theory is possible based on a coupling construction, as well as a key contiguity argument for the multinomial and multivariate hypergeometric distributions.",WOS:000320488200004,ANNALS OF STATISTICS,"['BEHRENS-FISHER PROBLEM', 'RANDOMIZATION TESTS', 'UNEQUAL VARIANCES', 'RANK-TESTS', 'BOOTSTRAP', 'HYPOTHESES']",EXACT AND ASYMPTOTICALLY ROBUST PERMUTATION TESTS,2013
398,"In extreme value theory, there are two fundamental approaches, both widely used: the block maxima (BM) method and the peaks-over-threshold (POT) method. Whereas much theoretical research has gone into the POT method, the BM method has not been studied thoroughly. The present paper aims at providing conditions under which the BM method can be justified. We also provide a theoretical comparative study of the methods, which is in general consistent with the vast literature on comparing the methods all based on simulated data and fully parametric models. The results indicate that the BM method is a rather efficient method under usual practical conditions.
In this paper, we restrict attention to the i.i.d. case and focus on the probability weighted moment (PWM) estimators of Hosking, Wallis and Wood [Technometrics (1985) 27 251-261].",WOS:000349738500010,ANNALS OF STATISTICS,"['GENERALIZED PARETO DISTRIBUTION', 'PROBABILITY-WEIGHTED MOMENTS', 'DURATION SERIES METHODS', 'HYDROLOGIC EVENTS', 'ORDER-STATISTICS', 'PARAMETERS']",ON THE BLOCK MAXIMA METHOD IN EXTREME VALUE THEORY: PWM ESTIMATORS,2015
399,"This paper is concerned with estimating the intersection point of two densities, given a sample of both of the densities. This problem arises in classification theory. The main results provide lower bounds for the probability of the estimation errors to be large on a scale determined by the inverse cube root of the sample size. As corollaries, we obtain probabilistic bounds for the prediction error in a classification problem. The key to the proof is an entropy estimate. The lower bounds are based on bounds for general estimators, which are applicable in other contexts as well. Furthermore, we introduce a class of optimal estimators whose errors asymptotically meet the border permitted by the lower bounds.",WOS:000251096100014,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'CONVERGENCE', 'RATES']",Optimal third root asymptotic bounds in the statistical estimation of thresholds,2007
400,"In this article we introduce the rotations package which provides users with the ability to simulate, analyze and visualize three-dimensional rotation data. More specifically it includes four commonly used distributions from which to simulate data, four estimators of the central orientation, six confidence region estimation procedures and two approaches to visualizing rotation data. All of these features are available for two different parameterizations of rotations: three-by-three matrices and quaternions. In addition, two datasets are included that illustrate the use of rotation data in practice.",WOS:000343788100008,R JOURNAL,"['ELECTRON BACKSCATTER DIFFRACTION', 'MEASURED CRYSTAL ORIENTATIONS', 'VON MISES-FISHER', 'STATISTICS', 'DISTRIBUTIONS', 'INFERENCE']",rotations: An R Package for SO(3) Data,2014
401,"We extend the available asymptotic theory for autoregressive sieve estimators to cover the case of stationary and invertible linear processes driven by independent identically distributed (i.i.d.) infinite variance (IV) innovations. We show that the ordinary least squares sieve estimates, together with estimates of the impulse responses derived from these, obtained from an autoregression whose order is an increasing function of the sample size, are consistent and exhibit asymptotic properties analogous to those which obtain for a finite-order autoregressive process driven by i.i.d. IV errors. As these limit distributions cannot be directly employed for inference because they either may not exist or, where they do, depend on unknown parameters, a second contribution of the paper is to investigate the usefulness of bootstrap methods in this setting. Focusing on three sieve bootstraps: the wild and permutation bootstraps, and a hybrid of the two, we show that, in contrast to the case of finite variance innovations, the wild bootstrap requires an infeasible correction to be consistent, whereas the other two bootstrap schemes are shown to be consistent (the hybrid for symmetrically distributed innovations) under general conditions.",WOS:000379972900004,ANNALS OF STATISTICS,"['MOVING AVERAGES', 'LIMIT THEORY', 'AUTOREGRESSIVE PROCESSES', 'BOOTSTRAP', 'STATISTICS', 'SIZE']",SIEVE-BASED INFERENCE FOR INFINITE-VARIANCE LINEAR PROCESSES,2016
402,"We propose an effective and fast method to simulate multidimensional conditional fractional Gaussian fields with the package FieldSim. Our method is valid not only for conditional simulations associated to fractional Brownian fields, but to any Gaussian field and on any (non regular) grid of points.",WOS:000385276100004,R JOURNAL,"['BROWNIAN SURFACES', 'SIMULATION']",Conditional Fractional Gaussian Fields with the Package FieldSim,2016
403,A popular framework for false discovery control is the random effects model in which the null hypotheses are assumed to be independent. This paper generalizes the random effects model to a conditional dependence model which allows dependence between null hypotheses. The dependence can be useful to characterize the spatial structure of the null hypotheses. Asymptotic properties of false discovery proportions and numbers of rejected hypotheses are explored and a large-sample distributional theory is obtained.,WOS:000253390000014,ANNALS OF STATISTICS,"['MULTIPLE', 'RATES']",On false discovery control under dependence,2008
404,"This paper introduces a new approach to the study of rates of convergence for posterior distributions. It is a natural extension of a recent approach to the study of Bayesian consistency. In particular, we improve on current rates of convergence for models including the mixture of Dirichlet process model and the random Bernstein polynomial model.",WOS:000248987600011,ANNALS OF STATISTICS,"['BAYESIAN DENSITY-ESTIMATION', 'BERNSTEIN POLYNOMIALS', 'CONSISTENCY', 'MIXTURES']",On rates of convergence for posterior distributions in infinite-dimensional models,2007
405,"We propose generalized additive partial linear models for complex data which allow one to capture nonlinear patterns of some covariates, in the presence of linear components. The proposed method improves estimation efficiency and increases statistical power for correlated data through incorporating the correlation information. A unique feature of the proposed method is its capability of handling model selection in cases where it is difficult to specify the likelihood function. We derive the quadratic inference function-based estimators for the linear coefficients and the nonparametric functions when the dimension of covariates diverges, and establish asymptotic normality for the linear coefficient estimators and the rates of convergence for the nonparametric functions estimators for both finite and high-dimensional cases. The proposed method and theoretical development are quite challenging since the numbers of linear covariates and nonlinear components both increase as the sample size increases. We also propose a doubly penalized procedure for variable selection which can simultaneously identify nonzero linear and nonparametric components, and which has an asymptotic oracle property. Extensive Monte Carlo studies have been conducted and show that the proposed procedure works effectively even with moderate sample sizes. A pharmacokinetics study on renal cancer data is illustrated using the proposed method.",WOS:000336888400011,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'VARYING-COEFFICIENT MODELS', 'VARIABLE SELECTION', 'LONGITUDINAL DATA', 'REGRESSION', 'INFERENCE', 'SPLINE', 'ASYMPTOTICS']",ESTIMATION AND MODEL SELECTION IN GENERALIZED ADDITIVE PARTIAL LINEAR MODELS FOR CORRELATED DATA WITH DIVERGING NUMBER OF COVARIATES,2014
406,"Testing for the significance of a subset of regression coefficients in a linear model, a staple of statistical analysis, goes back at least to the work of Fisher who introduced the analysis of variance (ANOVA). We study this problem under the assumption that the coefficient vector is sparse, a common situation in modern high-dimensional settings. Suppose we have p covariates and that under the alternative, the response only depends upon the order of p(1-alpha) of those, 0 <= alpha <= 1. Under moderate sparsity levels, that is, 0 <= alpha <= 1/2, we show that ANOVA is essentially optimal under some conditions on the design. This is no longer the case under strong sparsity constraints, that is, alpha > 1/2. In such settings, a multiple comparison procedure is often preferred and we establish its optimality when alpha >= 3/4. However, these two very popular methods are suboptimal, and sometimes powerless, under moderately strong sparsity where 1/2 < alpha < 3/4. We suggest a method based on the higher criticism that is powerful in the whole range alpha > 1/2. This optimality property is true for a variety of designs, including the classical (balanced) multi-way designs and more modern ""p > n"" designs arising in genetics and signal processing. In addition to the standard fixed effects model, we establish similar results for a random effects model where the nonzero coefficients of the regression vector are normally distributed.",WOS:000299186500013,ANNALS OF STATISTICS,"['UNCERTAINTY PRINCIPLES', 'DECOMPOSITION', 'VARIANCE', 'SIGNALS']","GLOBAL TESTING UNDER SPARSE ALTERNATIVES: ANOVA, MULTIPLE COMPARISONS AND THE HIGHER CRITICISM",2011
407,"Nonlinear regression is usually implemented in SAS either by using PROC NLIN or PROC NLMIXED. Apart from the model structure, initial values need to be specified for each parameter. And after some convergence criteria are fulfilled, the second order conditions need to be analyzed. But numerical problems are expected to appear in case the likelihood is nearly discontinuous, has plateaus, multiple maxima, or the initial values are distant from the true parameter estimates. The usual solution consists of using a grid, and then choosing the set of parameters reporting the highest log-likelihood. However, if the amount of parameters or grid points is large, the computational burden will be excessive. Furthermore, there is no guarantee that, as the number of grid points increases, an equal or better set of points will be found. Genetic algorithms can overcome these problems by replicating how nature optimizes its processes. The MLGA macro is presented; it solves a maximum likelihood estimation problem under normality through PROC GA, and the resulting values can later be used as the starting values in SAS nonlinear procedures. As will be demonstrated, this macro can avoid the usual trial and error approach that is needed when convergence problems arise. Finally, it will be shown how this macro can deal with complicated restrictions involving multiple parameters.",WOS:000365980200001,JOURNAL OF STATISTICAL SOFTWARE,,MLGA: A SAS Macro to Compute Maximum Likelihood Estimators via Genetic Algorithms,2015
408,"In crossed, two-factor studies with one observation per factor-level combination, interaction effects between factors can be hard to detect and can make the choice of a suitable statistical model difficult. This article describes hiddenf, an R package that enables users to quantify and characterize a certain form of interaction in two-factor layouts. When effects of one factor (a) fall into two groups depending on the level of another factor, and (b) are constant within these groups, the interaction pattern is deemed ""hidden additivity"" because within groups, the effects of the two factors are additive, while between groups the factors are allowed to interact. The hiddenf software can be used to estimate, test, and report an appropriate factorial effects model corresponding to hidden additivity, which is intermediate between the unavailable full factorial model and the overly-simplistic additive model. Further, the software also conducts five statistical tests for interaction proposed between 1949 and 2014. A collection of 17 datasets is used for illustration.",WOS:000385276100012,R JOURNAL,"['2-WAY ANOVA TABLES', 'NO REPLICATION', 'ADDITIVITY', 'VARIANCE', 'CLASSIFICATIONS', 'NONADDITIVITY', 'MODEL']",Exploring Interaction Effects in Two-Factor Studies using the hiddenf Package in R,2016
409,"Antimicrobial peptides (AMP) are a promising source of antibiotics with a broad spectrum activity against bacteria and low incidence of developing resistance. The mechanism by which an AMP executes its function depends on a set of computable physicochemical properties from the amino acid sequence. The Peptides package was designed to allow the quick and easy computation of ten structural characteristics own of the antimicrobial peptides, with the aim of generating data to increase the accuracy in classification and design of new amino acid sequences. Moreover, the options to read and plot XVG output files from GROMACS molecular dynamics package are included.",WOS:000357431900002,R JOURNAL,"['AMINO-ACID', 'PROTEINS', 'INDEX', 'HELIX']",Peptides: A Package for Data Mining of Antimicrobial Peptides,2015
410,"We propose several statistics to test the Markov hypothesis for beta-mixing stationary processes sampled at discrete time intervals. Our tests are based on the Chapman Kolmogorov equation. We establish the asymptotic null distributions of the proposed test statistics, showing that Wilks's phenomenon holds. We compute the power of the test and provide simulations to investigate the finite sample performance of the test statistics when the null model is a diffusion process, with alternatives consisting of models with a stochastic mean reversion level, stochastic volatility and jumps.",WOS:000282402800017,ANNALS OF STATISTICS,"['TERM STRUCTURE', 'INTEREST-RATES', 'CONDITIONAL DENSITIES', 'SPECIFICATION', 'REGRESSION', 'BOND']",NONPARAMETRIC TESTS OF THE MARKOV HYPOTHESIS IN CONTINUOUS-TIME MODELS,2010
411,"Although Bayes's theorem demands a prior that is a probability distribution on the parameter space, the calculus associated with Bayes's theorem sometimes generates sensible procedures from improper priors, Pitman's estimator being a good example. However, improper priors may also lead to Bayes procedures that are paradoxical or otherwise unsatisfactory, prompting some authors to insist that all priors be proper. This paper begins with the observation that an improper measure on 8 satisfying Kingman's countability condition is in fact a probability distribution on the power set. We show how to extend a model in such a way that the extended parameter space is the power set. Under an additional finiteness condition, which is needed for the existence of a sampling region, the conditions for Bayes's theorem are satisfied by the extension. Lack of interference ensures that the posterior distribution in the extended space is compatible with the original parameter space. Provided that the key finiteness condition is satisfied, this probabilistic analysis of the extended model may be interpreted as a vindication of improper Bayes procedures derived from the original model.",WOS:000296995500006,ANNALS OF STATISTICS,['INFERENCE'],ON BAYES' THEOREM FOR IMPROPER MIXTURES,2011
412,"This article introduces the package Vdgraph that is used for making variance dispersion graphs of response surface designs. The package includes functions that make the variance dispersion graph of one design or compare variance dispersion graphs of two designs, which are stored in data frames or matrices. The package also contains several minimum run response surface designs (stored as matrices) that are not available in other R packages.",WOS:000313197700006,R JOURNAL,['RESPONSE-SURFACE DESIGNS'],Vdgraph: A Package for Creating Variance Dispersion Graphs,2012
413,Graphical processing units are rapidly gaining maturity as powerful general parallel computing devices. The package cudaBayesreg uses GPU-oriented procedures to improve the performance of Bayesian computations. The paper motivates the need for devising high-performance computing strategies in the context of fMRI data analysis. Some features of the package for Bayesian analysis of brain fMRI data are illustrated. Comparative computing performance figures between sequential and parallel implementations are presented as well.,WOS:000208590000008,R JOURNAL,,cudaBayesreg: Bayesian Computation in CUDA,2010
414,"Most of the work on the structural nested model and g-estimation for causal inference in longitudinal data assumes a discrete-time underlying data generating process. However, in some observational studies, it is more reasonable to assume that the data are generated from a continuous-time process and are only observable at discrete time points. When these circumstances arise, the sequential randomization assumption in the observed discrete-time data, which is essential in justifying discrete-time g-estimation, may not be reasonable. Under a deterministic model, we discuss other useful assumptions that guarantee the consistency of discrete-time g-estimation. In more general cases, when those assumptions are violated, we propose a controlling-the-future method that performs at least as well as g-estimation in most scenarios and which provides consistent estimation in some cases where g-estimation is severely inconsistent. We apply the methods discussed in this paper to simulated data, as well as to a data set collected following a massive flood in Bangladesh, estimating the effect of diarrhea on children's height. Results from different methods are compared in both simulation and the real application.",WOS:000288183800004,ANNALS OF STATISTICS,"['MARGINAL STRUCTURAL MODEL', 'DIARRHEAL DISEASE', 'MORTALITY']",CAUSAL INFERENCE FOR CONTINUOUS-TIME PROCESSES WHEN COVARIATES ARE OBSERVED ONLY AT DISCRETE TIMES,2011
415,"This article details a Bayesian analysis of the Nile river flow data, using a similar state space model as other articles in this volume. For this data set, Metropolis-Hastings and Gibbs sampling algorithms are implemented in the programming language Ox. These Markov chain Monte Carlo methods only provide output conditioned upon the full data set. For filtered output, conditioning only on past observations, the particle filter is introduced. The sampling methods are flexible, and this advantage is used to extend the model to incorporate a stochastic volatility process. The volatility changes both in the Nile data and also in daily S&P 500 return data are investigated. The posterior density of parameters and states is found to provide information on which elements of the model are easily identifiable, and which elements are estimated with less precision.",WOS:000290528200001,JOURNAL OF STATISTICAL SOFTWARE,"['STOCHASTIC VOLATILITY', 'SIMULATION', 'INFERENCE']",A Bayesian Analysis of Unobserved Component Models Using Ox,2011
416,"A computer program for estimating Gompertz curve using Gauss-Newton method of least squares is described in detail. It is based on the estimation technique proposed in Reddy (1985). The program is developed using Scilab (version 3.1.1), a freely available scientific software package that can be downloaded from http://www.scilab.org/. Data is to be fed into the program from an external disk file which should be in Microsoft Excel format. The output will contain sample size, tolerance limit, a list of initial as well as the final estimate of the parameters, standard errors, value of Gauss-Normal equations namely GN(1) GN(2) and GN(3), No. of iterations, variance(sigma(2)), Durbin-Watson statistic, goodness of fit measures such as R-2, D value, covariance matrix and residuals. It also displays a graphical output of the estimated curve vis a vis the observed curve. It is an improved version of the program proposed in Dastidar (2005).",WOS:000237294000001,JOURNAL OF STATISTICAL SOFTWARE,,Gompertz: A Scilab program for estimating Gompertz curve using Gauss-Newton method of least squares,2006
417,"PReMiuM is a recently developed R package for Bayesian clustering using a Dirichlet process mixture model. This model is an alternative to regression models, non-parametrically linking a response vector to covariate data through cluster membership (Molitor, Papathomas, Jerrett, and Richardson 2010). The package allows binary, categorical, count and continuous response, as well as continuous and discrete covariates. Additionally, predictions may be made for the response, and missing values for the covariates are handled. Several samplers and label switching moves are implemented along with diagnostic tools to assess convergence. A number of R functions for post-processing of the output are also provided. In addition to fitting mixtures, it may additionally be of interest to determine which covariates actively drive the mixture components. This is implemented in the package as variable selection.",WOS:000352914600001,JOURNAL OF STATISTICAL SOFTWARE,"['VARIABLE SELECTION', 'SAMPLING METHODS', 'ASSOCIATION', 'INFERENCE', 'PRIORS']",PReMiuM: An R Package for Profile Regression Mixture Models Using Dirichlet Processes,2015
418,"In this article a procedure is proposed to simulate fractional fields, which are non Gaussian counterpart of the fractional Brownian motion. These fields, called real harmonizable ( multi) fractional Levy motions, allow fixing the Holder exponent at each point. FracSim is an R package developed in R and C language. Parallel computers have been also used.",WOS:000234226900001,JOURNAL OF STATISTICAL SOFTWARE,['REPRESENTATION'],FracSim: An R package to simulate multifractional Levy motions,2005
419,"Diffusion weighted imaging (DWI) is a magnetic resonance (MR) based method to investigate water diffusion in tissue like the human brain. Inference focuses on integral properties of the tissue microstructure. The acquired data are usually modeled using the diffusion tensor model, a three-dimensional Gaussian model for the diffusion process. Since the homogeneity assumption behind this model is not valid in large portion of the brain voxel more sophisticated approaches have been developed.
This paper describes the R package dti. The package offers capabilities for the analysis of diffusion weighted MR experiments. Here, we focus on recent extensions of the package, for example models for high angular resolution diffusion weighted imaging (HARDI) data, including Q-ball imaging and tensor mixture models, and fiber tracking. We provide a detailed description of the package structure and functionality. Examples are used to guide the reader through a typical analysis using the package. Data sets and R scripts used are available as electronic supplements.",WOS:000296719400001,JOURNAL OF STATISTICAL SOFTWARE,"['FIBER ORIENTATION DISTRIBUTIONS', 'JUVENILE MYOCLONIC EPILEPSY', 'AXONAL PROJECTIONS', 'IN-VIVO', 'TENSOR', 'RECONSTRUCTION', 'MRI', 'TRACTOGRAPHY', 'TRACKING', 'RESOLUTION']",dti: Beyond the Gaussian Model in Diffusion-Weighted Imaging,2011
420,"We introduce an R package SPECIES for species richness or diversity estimation. This package provides simple R functions to compute point and confidence interval estimates of species number from a few nonparametric and semi-parametric methods. For the methods based on nonparametric maximum likelihood estimation, the R functions are wrappers for Fortran codes for better efficiency. All functions in this package are illustrated using real data sets.",WOS:000289832600001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'CAPTURE PROBABILITIES VARY', 'POPULATION-SIZE', 'NUMBER', 'HETEROGENEITY', 'ANIMALS', 'SAMPLE', 'MODEL']",SPECIES: An R Package for Species Richness Estimation,2011
421,"We provide a nonparametric method for the computation of instantaneous multivariate volatility for continuous serni-martingales, which is based on Fourier analysis. The co-volatility is reconstructed as a stochastic function of time by establishing a connection between the Fourier transform of the prices process and the Fourier transform of the co-volatility process. A nonparametric estimator is derived given a discrete unevenly spaced and asynchronously sampled observations of the asset price processes. The asymptotic properties of the random estimator are studied: namely, consistency in probability uniformly in time and convergence in law to a mixture of Gaussian distributions.",WOS:000268113500012,ANNALS OF STATISTICS,"['HIGH-FREQUENCY DATA', 'MARKET MICROSTRUCTURE NOISE', 'INTEGRATED VOLATILITY', 'DIFFUSION-COEFFICIENT', 'COVARIANCE ESTIMATION', 'MODELS', 'SAMPLE', 'VARIANCE', 'FEEDBACK', 'PRICES']",A FOURIER TRANSFORM METHOD FOR NONPARAMETRIC ESTIMATION OF MULTIVARIATE VOLATILITY,2009
422,"Calculating a Monte Carlo standard error (MCSE) is an important step in the statistical analysis of the simulation output obtained from a Markov chain Monte Carlo experiment. An MCSE is usually based on an estimate of the variance of the asymptotic normal distribution. We consider spectral and batch means methods for estimating this variance. In particular, we establish conditions which guarantee that these estimators are strongly consistent as the simulation effort increases. In addition, for the batch means and overlapping batch means methods we establish conditions ensuring consistency in the mean-square sense which in turn allows us to calculate the optimal batch size up to a constant of proportionality. Finally, we examine the empirical finite-sample properties of spectral variance and batch means estimators and provide recommendations for practitioners.",WOS:000275510800016,ANNALS OF STATISTICS,"['SIMULATION OUTPUT ANALYSIS', 'EXPLORING POSTERIOR DISTRIBUTIONS', 'RANDOM EFFECTS MODEL', 'PARTIAL SUMS', 'GEOMETRIC ERGODICITY', 'METROPOLIS ALGORITHMS', 'STRONG CONSISTENCY', 'CONVERGENCE-RATES', 'DATA AUGMENTATION', 'INDEPENDENT RVS']",BATCH MEANS AND SPECTRAL VARIANCE ESTIMATORS IN MARKOV CHAIN MONTE CARLO,2010
423,"This paper introduces a new class of robust estimates for ARMA models. They are M-estimates, but the residuals are computed so the effect of one outlier is limited to the period where it occurs. These estimates are closely related to those based on a robust filter, but they have two important advantages: they are consistent and the asymptotic theory is tractable. We perform a Monte Carlo where we show that these estimates compare favorably with respect to standard M-estimates and to estimates based on a diagnostic procedure.",WOS:000265500500010,ANNALS OF STATISTICS,"['TIME-SERIES', 'INFINITE VARIANCE', 'OUTLIERS', 'PARAMETERS']",ROBUST ESTIMATION FOR ARMA MODELS,2009
424,"There is an ever-increasing number of applications, which use quantitative PCR (qPCR) or digital PCR (dPCR) to elicit fundamentals of biological processes. Moreover, quantitative isothermal amplification (qIA) methods have become more prominent in life sciences and point-of-care-diagnostics. Additionally, the analysis of melting data is essential during many experiments. Several software packages have been developed for the analysis of such datasets. In most cases, the software is either distributed as closed source software or as monolithic block with little freedom to perform highly customized analysis procedures. We argue, among others, that R is an excellent foundation for reproducible and transparent data analysis in a highly customizable cross-platform environment. However, for novices it is often challenging to master R or learn capabilities of the vast number of packages available. In the paper, we describe exemplary workflows for the analysis of qPCR, qIA or dPCR experiments including the analysis of melting curve data. Our analysis relies entirely on R packages available from public repositories. Additionally, we provide information related to standardized and reproducible research.",WOS:000357431900012,R JOURNAL,"['DIGITAL PCR', 'PACKAGE', 'QPCR', 'GUIDELINES', 'GRAPHICS', 'ACCURACY', 'DATASETS', 'ASSAYS']",R as an Environment for Reproducible Analysis of DNA Amplification Experiments,2015
425,This paper presents a new approach to the estimation of the deformation of an isotropic Gaussian random field on R-2 based on dense observations of a single realization of the deformed random field. Under this framework we investigate the identification and estimation of deformations. We then present a complete methodological package-from model assumptions to algorithmic recovery of the deformation-for the class of nonstationary processes obtained by deforming isotropic Gaussian random fields.,WOS:000254502700009,ANNALS OF STATISTICS,['NONSTATIONARY SPATIAL STRUCTURE'],Estimating deformations of isotropic Gaussian random fields on the plane,2008
426,"We show that there do not exist adaptive confidence bands for curve estimation except under very restrictive assumptions. We propose instead to construct adaptive bands that cover a surrogate function f* which is close to, but simpler than, f. The surrogate captures the significant features in f. We establish lower bounds on the width for any confidence band for f* and construct a procedure that comes within a small constant factor of attaining the lower bound for finite-samples.",WOS:000254502700014,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'DENSITY-ESTIMATION', 'INTERVALS', 'RATES', 'SETS', 'INFERENCE', 'SELECTION', 'BALLS']",Adaptive confidence bands,2008
427,"Polya trees fix partitions and use random probabilities in order to construct random probability measures. With quantile pyramids we instead fix probabilities and use random partitions. For nonparametric Bayesian inference we use a prior which supports piecewise linear quantile functions, based on the need to work with a finite set of partitions, yet we show that the limiting version of the prior exists. We also discuss and investigate an alternative model based on the so-called substitute likelihood, Both approaches factorize in a convenient way leading to relatively straightforward analysis via MCMC, since analytic summaries of posterior distributions are too complicated. We give conditions securing the existence of an absolute continuous quantile process, and discuss consistency and approximate normality for the sequence of posterior distributions. Illustrations are included.",WOS:000263129000004,ANNALS OF STATISTICS,"['POLYA TREE DISTRIBUTIONS', 'VON-MISES THEOREM', 'POSTERIOR DISTRIBUTIONS', 'INFERENCE', 'PROBABILITY', 'CONSISTENCY', 'PARAMETERS', 'MODELS']",QUANTILE PYRAMIDS FOR BAYESIAN NONPARAMETRICS,2009
428,"Partially observed Markov process (POMP) models, also known as hidden Markov models or state space models, are ubiquitous tools for time series analysis. The R package pomp provides a very flexible framework for Monte Carlo statistical investigations using nonlinear, non-Gaussian POMP models. A range of modern statistical methods for POMP models have been implemented in this framework including sequential Monte Carlo, iterated filtering, particle Markov chain Monte Carlo, approximate Bayesian computation, maximum synthetic likelihood estimation, nonlinear forecasting, and trajectory matching. In this paper, we demonstrate the application of these methodologies using some simple toy problems. We also illustrate the specification of more complex POMP models, using a nonlinear epidemiological model with a discrete population, seasonality, and extra-demographic stochasticity. We discuss the specification of user-defined models and the development of additional methods within the programming environment provided by pomp.",WOS:000373919100001,JOURNAL OF STATISTICAL SOFTWARE,"['APPROXIMATE BAYESIAN COMPUTATION', 'SEQUENTIAL MONTE-CARLO', 'DYNAMICAL-SYSTEMS', 'MODELS', 'PARAMETER', 'NOISE', 'STOCHASTICITY', 'POPULATIONS', 'SIMULATION', 'EPIDEMICS']",Statistical Inference for Partially Observed Markov Processes via the R Package pomp,2016
429,"In the popular approach of ""Bayesian variable selection"" (BVS), one uses prior and posterior distributions to select a subset of candidate variables to enter the model. A completely new direction will be considered here to study BVS with I Gibbs posterior originating in statistical mechanics. The Gibbs posterior is constructed from a risk function of practical interest (Such as the classification error) and aims at minimizing a risk function without modeling the data probabilistically. This can improve the performance over the Usual Bayesian approach, which depends on a probability model which may be misspecified. Conditions will be provided to achieve good risk performance, even in the presence of high dimensionality, when the number of candidate variables ""K"" can be much larger than the sample size ""n."" In addition, we develop a convenient Markov chain Monte Carlo algorithm to implement BVS with the Gibbs posterior.",WOS:000260554100008,ANNALS OF STATISTICS,"['LOGISTIC-REGRESSION', 'GENE SELECTION', 'DISTRIBUTIONS', 'MODELS', 'CHOICE']",GIBBS POSTERIOR FOR VARIABLE SELECTION IN HIGH-DIMENSIONAL CLASSIFICATION AND DATA MINING,2008
430,"We introduce a class of dimension reduction estimators based on an ensemble of the minimum average variance estimates of functions that characterize the central subspace, such as the characteristic functions, the Box-Cox transformations and wavelet basis. The ensemble estimators exhaustively estimate the central subspace without imposing restrictive conditions on the predictors, and have the same convergence rate as the minimum average variance estimates. They are flexible and easy to implement, and allow repeated use of the available sample, which enhances accuracy. They are applicable to both univariate and multivariate responses in a unified form. We establish the consistency and convergence rate of these estimators, and the consistency of a cross validation criterion for order determination. We compare the ensemble estimators with other estimators in a wide variety of models, and establish their competent performance.",WOS:000311639700010,ANNALS OF STATISTICS,"['PRINCIPAL HESSIAN DIRECTIONS', 'SLICED INVERSE REGRESSION', 'CENTRAL SUBSPACE', 'TRANSFORMATIONS']",SUFFICIENT DIMENSION REDUCTION BASED ON AN ENSEMBLE OF MINIMUM AVERAGE VARIANCE ESTIMATORS,2011
431,"The problem of signal detection using sparse, faint information is closely related to a variety of contemporary statistical problems, including the control of false-discovery rate, and classification using very high-dimensional data. Each problem can be solved by conducting a large number of simultaneous hypothesis tests, the properties of which are readily accessed under the assumption of independence. In this paper we address the case of dependent data, in the context of higher criticism methods for signal detection. Short-range dependence has no first-order impact on performance, but the situation changes dramatically under strong dependence. There, although higher criticism can continue to perform well, it can be bettered using methods based on differences of signal values or on the maximum of the data. The relatively inferior performance of higher criticism in such cases can be explained in terms of the fact that, under strong dependence, the higher criticism statistic behaves as though the data were partitioned into very large blocks, with all but a single representative of each block being eliminated from the dataset.",WOS:000253390000015,ANNALS OF STATISTICS,['FALSE DISCOVERY RATE'],Properties of higher criticism under strong dependence,2008
432,"We give a finite-sample analysis of predictive inference procedures after model selection in regression with random design. The analysis is focused on a statistically challenging scenario where the number of potentially important explanatory variables can be infinite, where no regularity conditions are imposed on unknown parameters, where the number of explanatory variables in a ""good"" model can be of the same order as sample size and where the number of candidate models can be of larger order than sample size. The performance of inference procedures is evaluated conditional on the training sample. Under weak conditions on only the number of candidate models and on their complexity, and uniformly over all data-generating processes under consideration, we show that a certain prediction interval is approximately valid and short with high probability in finite samples, in the sense that its actual coverage probability is close to the nominal one and in the sense that its length is close to the length of an infeasible interval that is constructed by actually knowing the ""best"" candidate model. Similar results are shown to hold for predictive inference procedures other than prediction intervals like, for example, tests of whether a future response will lie above of below a given threshold.",WOS:000268605000010,ANNALS OF STATISTICS,"['BAYESIAN CONFIDENCE-INTERVALS', 'BREAST-CANCER', 'REGRESSION', 'ESTIMATORS', 'SETS', 'VARIABLES', 'APPROXIMATIONS', 'BALLS']",CONDITIONAL PREDICTIVE INFERENCE POST MODEL SELECTION,2009
433,"There is increasing interest in the problem of nonparametric regression with high-dimensional predictors. When the number of predictors D is large, one encounters a daunting problem in attempting to estimate a D-dimensional surface based on limited data. Fortunately, in many applications, the support of the data is concentrated on a d-dimensional subspace with d << D. Manifold learning attempts to estimate this subspace. Our focus is on developing computationally tractable and theoretically supported Bayesian nonparametric regression methods in this context. When the subspace corresponds to a locally-Euclidean compact Riemannian manifold, we show that a Gaussian process regression approach can be applied that leads to the minimax optimal adaptive rate in estimating the regression function under some conditions. The proposed model bypasses the need to estimate the manifold, and can be implemented using standard algorithms for posterior computation in Gaussian processes. Finite sample performance is illustrated in a data analysis example.",WOS:000372594300015,ANNALS OF STATISTICS,"['NONLINEAR DIMENSIONALITY REDUCTION', 'GAUSSIAN PROCESS', 'POSTERIOR DISTRIBUTIONS', 'UNIVERSAL ALGORITHMS', 'INTRINSIC DIMENSION', 'VARIABLE SELECTION', 'CONVERGENCE-RATES', 'LEARNING-THEORY', 'PROJECTION', 'BANDWIDTH']",BAYESIAN MANIFOLD REGRESSION,2016
434,"Scientists and investigators in such diverse fields as geological and environmental sciences, ecology, forestry, disease mapping, and economics often encounter spatially referenced data collected over a fixed set of locations with coordinates ( latitude - longitude, Easting - Northing etc.) in a region of study. Such point-referenced or geostatistical data are often best analyzed with Bayesian hierarchical models. Unfortunately, fitting such models involves computationally intensive Markov chain Monte Carlo (MCMC) methods whose efficiency depends upon the specific problem at hand. This requires extensive coding on the part of the user and the situation is not helped by the lack of available software for such algorithms. Here, we introduce a statistical software package, spBayes, built upon the R statistical computing platform that implements a generalized template encompassing a wide variety of Gaussian spatial process models for univariate as well as multivariate point-referenced data. We discuss the algorithms behind our package and illustrate its use with a synthetic and real data example.",WOS:000246391500001,JOURNAL OF STATISTICAL SOFTWARE,,spBayes: An R package for univariate and multivariate hierarchical point-referenced spatial models,2007
435,"We consider the estimation of the slope function in functional linear regression, where scalar responses are modeled in dependence of random functions. Cardot and Johannes [J. Multivariate Anal. 101 (2010) 395-408] have shown that a thresholded projection estimator can attain up to a constant minimax-rates of convergence in a general framework which allows us to cover the prediction problem with respect to the mean squared prediction error as well as the estimation of the slope function and its derivatives. This estimation procedure, however, requires an optimal choice of a tuning parameter with regard to certain characteristics of the slope function and the covariance operator associated with the functional regressor. As this information is usually inaccessible in practice, we investigate a fully data-driven choice of the tuning parameter which combines model selection and Lepski's method. It is inspired by the recent work of Goldenshluger and Lepski [Ann. Statist. 39 (2011) 1608-1632]. The tuning parameter is selected as minimizer of a stochastic penalized contrast function imitating Lepski's method among a random collection of admissible values. This choice of the tuning parameter depends only on the data and we show that within the general framework the resulting data-driven thresholded projection estimator can attain minimax-rates up to a constant over a variety of classes of slope functions and covariance operators. The results are illustrated considering different configurations which cover in particular the prediction problem as well as the estimation of the slope and its derivatives. A simulation study shows the reasonable performance of the fully data-driven estimation procedure.",WOS:000321845400001,ANNALS OF STATISTICS,"['INVERSE PROBLEMS', 'MODEL SELECTION', 'ESTIMATORS', 'INEQUALITIES', 'PREDICTION', 'PARAMETERS', 'SPACES', 'BOUNDS', 'ERROR']",ADAPTIVE FUNCTIONAL LINEAR REGRESSION,2012
436,"We develop a new method for constructing ""good"" designs for computer experiments. The method derives its power from its basic structure that builds large designs using small designs. We specialize the method for the construction of orthogonal Latin hypercubes and obtain many results along the way. In terms of run sizes, the existence problem of orthogonal Latin hypercubes is completely solved. We also present an explicit result showing how large orthogonal Latin hypercubes can be constructed using small orthogonal Latin hypercubes. Another appealing feature of our method is that it can easily be adapted to construct other designs; we examine how to make use of the method to construct nearly orthogonal and cascading Latin hypercubes.",WOS:000277471000007,ANNALS OF STATISTICS,"['LATIN HYPERCUBE DESIGNS', 'MODELS', 'EFFICIENT', 'ARRAYS']",A NEW AND FLEXIBLE METHOD FOR CONSTRUCTING DESIGNS FOR COMPUTER EXPERIMENTS,2010
437,"A primary issue in industrial hygiene is the estimation of a worker's exposure to chemical, physical and biological agents. Mathematical modeling is increasingly being used as a method for assessing occupational exposures. However, predicting exposure in real settings is constrained by lack of quantitative knowledge of exposure determinants. Recently, Zhang, Banerjee, Yang, Lungu, and Ramachandran (2009) proposed Bayesian hierarchical models for estimating parameters and exposure concentrations for the two-zone differential equation models and for predicting concentrations in a zone near and far away from the source of contamination.
Bayesian estimation, however, can often require substantial amounts of user-defined code and tuning. In this paper, we introduce a statistical software package, B2Z, built upon the R statistical computing platform that implements a Bayesian model for estimating model parameters and exposure concentrations in two-zone models. We discuss the algorithms behind our package and illustrate its use with simulated and real data examples.",WOS:000293389900001,JOURNAL OF STATISTICAL SOFTWARE,"['EXPOSURE', 'DISTRIBUTIONS']",B2Z: An R Package for Bayesian Two-Zone Models,2011
438,"Consider a network where the nodes split into K different communities. The community labels for the nodes are unknown and it is of major interest to estimate them (i.e., community detection). Degree Corrected Block Model (DCBM) is a popular network model. How to detect communities with the DCBM is an interesting problem, where the main challenge lies in the degree heterogeneity.
We propose a new approach to community detection which we call the Spectral Clustering On Ratios-of-Eigenvectors (SCORE). Compared to classical spectral methods, the main innovation is to use the entry-wise ratios between the first leading eigenvector and each of the other leading eigenvectors for clustering. Let A be the adjacency matrix of the network. We first obtain the K leading eigenvectors of A, say, (eta) over cap (1),..., (eta) over cap (K) and let (R) over cap be the n x (K - 1) matrix such that (R) over cap (i, k) = (eta) over cap (k+1)(i)/(eta) over cap (1)(i), 1 <= i <= n, 1 <= k <= K - 1. We then use (R) over cap for clustering by applying the k-means method.
The central surprise is, the effect of degree heterogeneity is largely ancillary, and can be effectively removed by taking entry-wise ratios between (eta) over cap (k+1) and (eta) over cap (1), 1 <= k <= K - 1.
The method is successfully applied to the web blogs data and the karate club data, with error rates of 58/1222 and 1/34, respectively. These results are more satisfactory than those by the classical spectral methods. Additionally, compared to modularity methods, SCORE is easier to implement, computationally faster, and also has smaller error rates.
We develop a theoretic framework where we show that under mild conditions, the SCORE stably yields consistent community detection. In the core of the analysis is the recent development on Random Matrix Theory (RMT), where the matrix-form Bernstein inequality is especially helpful.",WOS:000349738500003,ANNALS OF STATISTICS,"['INFORMATION', 'LIKELIHOOD', 'NETWORKS']",FAST COMMUNITY DETECTION BY SCORE,2015
439,"Through topological expectations regarding smooth, thresholded n-dimensional Gaussian continua, random field theory (RFT) describes probabilities associated with both the field-wide maximum and threshold-surviving upcrossing geometry. A key application of RFT is a correction for multiple comparisons which affords field-level hypothesis testing for both univariate and multivariate fields. For unbroken isotropic fields just one parameter in addition to the mean and variance is required: the ratio of a field's size to its smoothness. Ironically the simplest manifestation of RFT (1D unbroken fields) has rarely surfaced in the literature, even during its foundational development in the late 1970s. This Python package implements 1D RFT primarily for exploring and validating RFT expectations, but also describes how it can be applied to yield statistical inferences regarding sets of experimental 1D fields.",WOS:000384915400001,JOURNAL OF STATISTICAL SOFTWARE,"['IMAGES', 'TESTS']",rft1d: Smooth One-Dimensional Random Field Upcrossing Probabilities in Python,2016
440,"We consider an empirical likelihood inference for parameters defined by general estimating equations when some components of the random observations are subject to missingness. As the nature of the estimating equations is wide-ranging, we propose a nonparametric imputation of the missing values from a kernel estimator of the conditional distribution of the missing variable given the always observable variable. The empirical likelihood is used to construct a profile likelihood for the parameter of interest. We demonstrate that the proposed nonparametric imputation can remove the selection bias in the missingness and the empirical likelihood leads to more efficient parameter estimation. The proposed method is further evaluated by simulation and an empirical study on a genetic dataset on recombinant inbred mice.",WOS:000263129000018,ANNALS OF STATISTICS,"['GENERALIZED LINEAR-MODELS', 'SEMIPARAMETRIC EFFICIENCY', 'REGRESSION-MODELS', 'IMPUTATION', 'SCORE']",EMPIRICAL LIKELIHOOD FOR ESTIMATING EQUATIONS WITH MISSING VALUES,2009
441,"Ancestral graphs can encode conditional independence relations that arise in directed acyclic graph (DAG) models with latent and selection variables. However, for any ancestral graph, there may be several other graphs to which it is Markov equivalent. We state and prove conditions under which two maximal ancestral graphs are Markov equivalent to each other, thereby extending analogous results for DAGs given by other authors. These conditions lead to an algorithm for determining Markov equivalence that runs in time that is polynomial in the number of vertices in the graph.",WOS:000268605000009,ANNALS OF STATISTICS,"['SEEMINGLY UNRELATED REGRESSIONS', 'CONDITIONAL-INDEPENDENCE', 'MODELS']",MARKOV EQUIVALENCE FOR ANCESTRAL GRAPHS,2009
442,,WOS:000256504400018,ANNALS OF STATISTICS,,"Estimation for almost periodic processes (vol 34, pg 1115, 2006)",2008
443,,WOS:000348651700018,R JOURNAL,,News from the Bioconductor Project,2014
444,"Under mild assumptions on the kernel, we obtain the best known error rates in a regularized learning scenario taking place in the corresponding reproducing kernel Hilbert space (RKHS). The main novelty in the analysis is a proof that one can use a regularization term that grows significantly slower than the standard quadratic growth in the RKHS norm.",WOS:000273800100016,ANNALS OF STATISTICS,"['SUPPORT VECTOR MACHINES', 'LOCAL RADEMACHER COMPLEXITIES', 'EMPIRICAL PROCESSES', 'ORACLE INEQUALITIES', 'ENTROPY NUMBERS', 'FAST RATES', 'PERFORMANCE', 'OPERATORS', 'ERROR', 'MINIMIZATION']",REGULARIZATION IN KERNEL LEARNING,2010
445,"We introduce new quantile estimators with adaptive importance sampling. The adaptive estimators are based on weighted samples that are neither independent nor identically distributed. Using a new law of iterated logarithm for martingales, we prove the convergence of the adaptive quantile estimators for general distributions with nonunique quantiles thereby extending the work of Feldman and Tucker [Ann. Math. Statist. 37 (1996) 451-457]. We illustrate the algorithm with an example from credit portfolio risk analysis.",WOS:000275510800023,ANNALS OF STATISTICS,"['STOCHASTIC-APPROXIMATION', 'CREDIT RISK', 'CONVERGENCE', 'ALGORITHM']",QUANTILE ESTIMATION WITH ADAPTIVE IMPORTANCE SAMPLING,2010
446,"We obtain two theorems extending the use of a saddlepoint approximation to multiparameter problems for likelihood ratio-like statistics which allow their use in permutation and rank tests and could be used in bootstrap approximations. In the first, we show that in some cases when no density exists, the integral of the formal saddlepoint density over the set corresponding to large values of the likelihood ratio-like statistic approximates the true probability with relative error of order 1/n. In the second, we give multivariate generalizations of the Lugannani-Rice and Barndorff- Nielsen or r* formulas for the approximations. These theorems are applied to obtain permutation tests based on the likelihood ratio-like statistics for the k sample and the multivariate two-sample cases. Numerical examples are given to illustrate the high degree of accuracy, and these statistics are compared to the classical statistics in both cases.",WOS:000311639700008,ANNALS OF STATISTICS,['LARGE DEVIATIONS'],SADDLEPOINT APPROXIMATIONS FOR LIKELIHOOD RATIO LIKE STATISTICS WITH APPLICATIONS TO PERMUTATION TESTS,2011
447,"Starting from a parallel between some minimax adaptive tests of a single null hypothesis, based on aggregation approaches, and some tests of multiple hypotheses, we propose a new second kind error-related evaluation criterion, as the core of an emergent minimax theory for multiple tests. Aggregation based tests, proposed for instance by Baraud [Bernoulli 8 (2002) 577-606], Baraud, Huet and Laurent [Ann. Statist. 31 (2003) 225-251] or Fromont and Laurent [Ann. Statist. 34 (2006) 680-720], are justified through their first kind error rate, which is controlled by the prescribed level on the one hand, and through their separation rates over various classes of alternatives, rates which are minimax on the other hand. We show that some of these tests can be viewed as the first steps of classical step-down multiple testing procedures, and accordingly be evaluated from the multiple testing point of view also, through a control of their Family-Wise Error Rate (FWER). Conversely, many multiple testing procedures, from the historical ones of Bonferroni and Holm, to more recent ones like min-p procedures or randomized procedures such as the ones proposed by Romano and Wolf [J. Amer. Statist. Assoc. 100 (2005) 94-108], can be investigated from the minimax adaptive testing point of view. To this end, we extend the notion of separation rate to the multiple testing field, by defining the weak Family-Wise Separation Rate and its stronger counterpart, the Family-Wise Separation Rate (FWSR). As for non parametric tests of a single null hypothesis, we prove that these new concepts allow an accurate analysis of the second kind error of a multiple testing procedure, leading to clear definitions of minimax and minimax adaptive multiple tests. Some illustrations in classical Gaussian frameworks corroborate several expected results under particular conditions on the tested hypotheses, but also lead to new questions and perspectives.",WOS:000389620800011,ANNALS OF STATISTICS,"['GENERALIZED ERROR RATES', 'ADAPTIVE TESTS', 'SIGNAL-DETECTION', 'MINIMAX RATES', 'HYPOTHESES', 'OPTIMALITY', 'STEPDOWN', 'DENSITY', 'MODEL']",FAMILY-WISE SEPARATION RATES FOR MULTIPLE TESTING,2016
448,"Recently, computational methods and software have been receiving more attention in the econometrics literature, emphasizing that they are integral components of modern econometric research. This has also promoted the development of many new econometrics software packages written in R and made available on the Comprehensive R Archive Network. This special volume on ""Econometrics in R"" features a selection of these recent activities that includes packages for econometric analysis of cross-section, time series and panel data. This introduction to the special volume highlights the contents of the contributions and embeds them into a brief overview of other past, present, and future projects for econometrics in R.",WOS:000258204500001,JOURNAL OF STATISTICAL SOFTWARE,"['PACKAGE', 'MODELS']","Econometrics in R: Past, present and future",2008
449,"Proteomics is the study of the abundance, function and dynamics of all proteins present in a living organism, and mass spectrometry ( MS) has become its most important tool due to its unmatched sensitivity, resolution and potential for high-throughput experimentation. A frequently used variant of mass spectrometry is coupled with liquid chromatography ( LC) and is denoted as ""LC/MS"". It produces two-dimensional raw data, where significant distortions along one of the dimensions can occur between different runs on the same instrument, and between instruments. A compensation of these distortions is required to allow for comparisons between and inference based on different experiments. This article introduces the amsrpm software package. It implements a variant of the Robust Point Matching ( RPM) algorithm that is tailored for the alignment of LC and LC/MS experiments. Problem-specific enhancements include a specialized dissimilarity measure, and means to enforce smoothness and monotonicity of the estimated transformation function. The algorithm does not rely on pre-specified landmarks, it is insensitive towards outliers and capable of modeling nonlinear distortions. Its usefulness is demonstrated using both simulated and experimental data. The software is available as an open source package for the statistical programming language R.",WOS:000244067400001,JOURNAL OF STATISTICAL SOFTWARE,"['CHROMATOGRAPHY-MASS SPECTROMETRY', 'ALGORITHM']",amsrpm: Robust point matching for retention time alignment of LC/MS data with R,2007
450,"We consider the model of nonregular nonparametric regression where smoothness constraints are imposed on the regression function f and the regression errors are assumed to decay with some sharpness level at their endpoints. The aim of this paper is to construct an adaptive estimator for the regression function f. In contrast to the standard model where local averaging is fruitful, the nonregular conditions require a substantial different treatment based on local extreme values. We study this model under the realistic setting in which both the smoothness degree beta > 0 and the sharpness degree a is an element of (0, infinity) are unknown in advance. We construct adaptation procedures applying a nested version of Lepski's method and the negative Hill estimator which show no loss in the convergence rates with respect to the general L-q-risk and a logarithmic loss with respect to the pointwise risk. Optimality of these rates is proved for a is an element of (0, infinity). Some numerical simulations and an application to real data are provided.",WOS:000344632400010,ANNALS OF STATISTICS,"['EXTREME-VALUE THEORY', 'MAXIMUM-LIKELIHOOD', 'EFFICIENT ESTIMATION', 'FRONTIER ESTIMATION', 'NONREGULAR ERRORS', 'HULL ESTIMATORS', 'DEA ESTIMATORS', 'VALUE INDEX', 'BOUNDARIES', 'MODELS']",ADAPTIVE FUNCTION ESTIMATION IN NONPARAMETRIC REGRESSION WITH ONE-SIDED ERRORS,2014
451,"The R package stochvol provides a fully Bayesian implementation of heteroskedasticity modeling within the framework of stochastic volatility. It utilizes Markov chain Monte Carlo (MCMC) samplers to conduct inference by obtaining draws from the posterior distribution of parameters and latent variables which can then be used for predicting future volatilities. The package can straightforwardly be employed as a stand-alone tool; moreover, it allows for easy incorporation into other MCMC samplers. The main focus of this paper is to show the functionality of stochvol. In addition, it provides a brief mathematical description of the model, an overview of the sampling schemes used, and several illustrative examples using exchange rate data.",WOS:000373917400001,JOURNAL OF STATISTICAL SOFTWARE,"['INTERWEAVING STRATEGY ASIS', 'LIKELIHOOD INFERENCE', 'BAYESIAN-ANALYSIS', 'MODELS', 'LEVERAGE']",Dealing with Stochastic Volatility in Time Series Using the R Package stochvol,2016
452,"Situations of a functional predictor paired with a scalar response are increasingly encountered in data analysis. Predictors are often appropriately modeled as square integrable smooth random functions. Imposing minimal assumptions on the nature of the functional relationship, we aim to estimate the directional derivatives and gradients of the response with respect to the predictor functions. In statistical applications and data analysis, functional derivatives provide a quantitative measure of the often intricate relationship between changes in predictor trajectories and those in scalar responses. This approach provides a natural extension of classical gradient fields in vector space and provides directions of steepest descent. We suggest a kernel-based method for the nonparametric estimation of functional derivatives that utilizes the decomposition of the random predictor functions into their eigenfunctions. These eigenfunctions define a canonical set of directions into which the gradient field is expanded. The proposed method is shown to lead to asymptotically consistent estimates of functional derivatives and is illustrated in an application to growth curves.",WOS:000271673500008,ANNALS OF STATISTICS,"['LINEAR-REGRESSION', 'LONGITUDINAL DATA', 'CURVES', 'MODEL', 'DISCRIMINATION', 'SAMPLE', 'TOOLS']",ESTIMATION OF FUNCTIONAL DERIVATIVES,2009
453,"We apply the cyclic coordinate descent algorithm of Friedman, Hastie, and Tibshirani (2010) to the fitting of a conditional logistic regression model with lasso (l(1)) and elastic net penalties. The sequential strong rules of Tibshirani, Bien, Hastie, Friedman, Taylor, Simon, and Tibshirani (2012) are also used in the algorithm and it is shown that these offer a considerable speed up over the standard coordinate descent algorithm with warm starts.
Once implemented, the algorithm is used in simulation studies to compare the variable selection and prediction performance of the conditional logistic regression model against that of its unconditional (standard) counterpart. We find that the conditional model performs admirably on datasets drawn from a suitable conditional distribution, outperforming its unconditional counterpart at variable selection. The conditional model is also fit to a small real world dataset, demonstrating how we obtain regularization paths for the parameters of the model and how we apply cross validation for this method where natural unconditional prediction rules are hard to come by.",WOS:000341642900001,JOURNAL OF STATISTICAL SOFTWARE,"['PROPORTIONAL HAZARDS MODEL', 'COORDINATE DESCENT', 'LIKELIHOOD']",Regularization Paths for Conditional Logistic Regression: The clogitL1 Package,2014
454,"Bayesian variable selection has gained much empirical success recently in a variety of applications when the number K of explanatory variables (x(1),..., x(K)) is possibly much larger than the sample size a. For generalized linear models, if most of the x(j)'s have very small effects on the response y, we show that it is possible to use Bayesian variable selection to reduce overtitting caused by the curse of dimensionality K >> n. In this approach a suitable prior can be used to choose a few out of the many xj's to model y, so that the posterior will propose probability densities p that are ""often close"" to the true density p* in some sense. The closeness can be described by a Hellinger distance between p and p* that scales at a power very close to n(-1/2), which is the ""finite-dimensional rate"" corresponding to a low-dimensional situation. These findings extend some recent work of Jiang [Technical Report 05-02 (2005) Dept. Statistics, Northwestern Univ.] on consistency of Bayesian variable selection for binary classification.",WOS:000249568000006,ANNALS OF STATISTICS,"['POSTERIOR DISTRIBUTIONS', 'GENE SELECTION', 'REGRESSION', 'CLASSIFICATION', 'CONSISTENCY', 'PERSISTENCE']",Bayesian variable selection for high dimensional generalized linear models: Convergence rates of the fitted densities,2007
455,,WOS:000253077800003,ANNALS OF STATISTICS,['REGRESSION'],Discussion: The Dantzig selector: Statistical estimation when p is much larger than n,2007
456,"We consider the limit distribution of maxima of periodograms for stationary processes. Our method is based on m-dependent approximation for stationary processes and a moderate deviation result,",WOS:000268605000004,ANNALS OF STATISTICS,"['HARMONIC-ANALYSIS', 'RANDOM-VARIABLES', 'TIME-SERIES']",ON MAXIMA OF PERIODOGRAMS OF STATIONARY PROCESSES,2009
457,"Peter Hall died in Melbourne on January 9, 2016. He was an extremely prolific researcher and contributed to many different areas of statistics. In this paper, I talk about my experience with Peter and I summarise his main contributions to deconvolution, which include measurement error problems and problems in image analysis.",WOS:000384397200004,ANNALS OF STATISTICS,"['ERRORS-IN-VARIABLES', 'NONPARAMETRIC REGRESSION', 'IMAGE-ANALYSIS', 'SIMULATION-EXTRAPOLATION', 'OPTIMAL RATES', 'MODELS', 'DENSITY', 'CONVERGENCE', 'NOISE', 'DISTRIBUTIONS']",PETER HALL'S MAIN CONTRIBUTIONS TO DECONVOLUTION,2016
458,"The Lexis class in the R package Epi provides an object-based framework for managing follow-up time on multiple time scales, which is an important feature of prospective epidemiological studies with long duration. Follow-up time may be split either into fixed time bands, or on individual event times and the split data may be used in Poisson regression models that account for the evolution of disease risk on multiple time scales. The summary and plot methods for Lexis objects allow inspection of the follow-up times.",WOS:000285981600001,JOURNAL OF STATISTICAL SOFTWARE,['THOROTRAST'],Lexis: An R Class for Epidemiological Studies with Long-Term Follow-Up,2011
459,"OpenStreetMap provides freely accessible and editable geographic data. The osmar package smoothly integrates the OpenStreetMap project into the R ecosystem. The osmar package provides infrastructure to access OpenStreetMap data from different sources, to enable working with the OSM data in the familiar R idiom, and to convert the data into objects based on classes provided by existing R packages. This paper explains the package's concept and shows how to use it. As an application we present a simple navigation device.",WOS:000321944400006,R JOURNAL,,osmar: OpenStreetMap and R,2013
460,"Archetypal analysis has the aim to represent observations in a multivariate data set as convex combinations of extremal points. This approach was introduced by Cutler and Breiman (1994); they defined the concrete problem, laid out the theoretical foundations and presented an algorithm written in Fortran. In this paper we present the R package archetypes which is available on the Comprehensive R Archive Network. The package provides an implementation of the archetypal analysis algorithm within R and different exploratory tools to analyze the algorithm during its execution and its final result. The application of the package is demonstrated on two examples.",WOS:000266311100001,JOURNAL OF STATISTICAL SOFTWARE,,From Spider-Man to Hero - Archetypal Analysis in R,2009
461,"From R 3.0.0, there is a new recommended way to develop new grob classes in grid. In a nutshell, two new ""hook"" functions, makeContext() and makeContent() have been added to grid to provide an alternative to the existing hook functions preDrawDetails(), drawDetails(), and postDrawDetails(). There is also a new function called grid. force(). This article discusses why these changes have been made, provides a simple demonstration of the use of the new functions, and discusses some of the implications for packages that build on grid.",WOS:000330193300016,R JOURNAL,,Changes to grid for R 3.0.0,2013
462,"We show that one can perform causal inference in a natural way for continuous-time scenarios using tools from stochastic analysis. This provides new alternatives to the positivity condition for inverse probability weighting. The probability distribution that would govern the frequency of observations in the counterfactual scenario can be characterized in terms of a so-called martingale problem. The counterfactual and factual probability distributions may be related through a likelihood ratio given by a stochastic differential equation. We can perform inference for counterfactual scenarios based on the original observations, re-weighted according to this likelihood ratio. This is possible if the solution of the stochastic differential equation is uniformly integrable, a property that can be determined by comparing the corresponding factual and counterfactual short-term predictions.
Local independence graphs are directed, possibly cyclic, graphs that represent short-term prediction among sufficiently autonomous stochastic processes. We show through an example that these graphs can be used to identify and provide consistent estimators for counterfactual parameters in continuous time. This is analogous to how Judea Pearl uses graphical information to identify causal effects in finite state Bayesian networks.",WOS:000321842400002,ANNALS OF STATISTICS,"['MARGINAL STRUCTURAL MODELS', 'POINT-PROCESSES', 'CONTINUOUS-TIME', 'MARTINGALES']",COUNTERFACTUAL ANALYSES WITH GRAPHICAL MODELS BASED ON LOCAL INDEPENDENCE,2012
463,"We consider a flexible semiparametric quantile regression model for analyzing high dimensional heterogeneous data. This model has several appealing features: (1) By considering different conditional quantiles, we may obtain a more complete picture of the conditional distribution of a response variable given high dimensional covariates. (2) The sparsity level is allowed to be different at different quantile levels. (3) The partially linear additive structure accommodates nonlinearity and circumvents the curse of dimensionality. (4) It is naturally robust to heavy-tailed distributions. In this paper, we approximate the nonlinear components using B-spline basis functions. We first study estimation under this model when the nonzero components are known in advance and the number of covariates in the linear part diverges. We then investigate a nonconvex penalized estimator for simultaneous variable selection and estimation. We derive its oracle property for a general class of nonconvex penalty functions in the presence of ultra-high dimensional covariates under relaxed conditions. To tackle the challenges of nonsmooth loss function, nonconvex penalty function and the presence of nonlinear components, we combine a recently developed convex-differencing method with modern empirical process techniques. Monte Carlo simulations and an application to a microarray study demonstrate the effectiveness of the proposed method. We also discuss how the method for a single quantile of interest can be extended to simultaneous variable selection and estimation at multiple quantiles.",WOS:000368022000010,ANNALS OF STATISTICS,"['VARYING COEFFICIENT MODELS', 'NONCONCAVE PENALIZED LIKELIHOOD', 'VARIABLE SELECTION', 'DIVERGING NUMBER', 'BIRTH-WEIGHT', 'PARAMETERS', 'SHRINKAGE', 'INFERENCE']",PARTIALLY LINEAR ADDITIVE QUANTILE REGRESSION IN ULTRA-HIGH DIMENSION,2016
464,"Biomarker identification is an ever more important topic in the life sciences. With the advent of measurement methodologies based on microarrays and mass spectrometry, thousands of variables are routinely being measured on complex biological samples. Often, the question is what makes two groups of samples different. Classical hypothesis testing suffers from the multiple testing problem; however, correcting for this often leads to a lack of power. In addition, choosing alpha cutoff levels remains somewhat arbitrary. Also in a regression context, a model depending on few but relevant variables will be more accurate and precise, and easier to interpret biologically.
We propose an R package, BioMark, implementing two meta-statistics for variable selection. The first, higher criticism, presents a data-dependent selection threshold for significance, instead of a cookbook value of alpha = 0.05. It is applicable in all cases where two groups are compared. The second, stability selection, is more general, and can also be applied in a regression context. This approach uses repeated subsampling of the data in order to assess the variability of the model coefficients and selects those that remain consistently important. It is shown using experimental spike-in data from the field of metabolomics that both approaches work well with real data. BioMark also contains functionality for simulating data with specific characteristics for algorithm development and testing.",WOS:000312289600001,JOURNAL OF STATISTICAL SOFTWARE,"['HIGHER CRITICISM', 'DATA SET', 'DISCOVERY', 'IDENTIFICATION']",Meta-Statistics for Variable Selection: The R Package BioMark,2012
465,"Sample covariance matrices are widely used in multivariate statistical analysis. The central limit theorems (CLTs) for linear spectral statistics of high-dimensional noncentralized sample covariance matrices have received considerable attention in random matrix theory and have been applied to many high-dimensional statistical problems. However, known population mean vectors are assumed for noncentralized sample covariance matrices, some of which even assume Gaussian-like moment conditions. In fact, there are still another two most frequently used sample covariance matrices: the ME (moment estimator, constructed by subtracting the sample mean vector from each sample vector) and the unbiased sample covariance matrix (by changing the denominator n as N = n - 1 in the ME) without depending on unknown population mean vectors. In this paper, we not only establish the new CLTs for noncentralized sample covariance matrices when the Gaussian-like moment conditions do not hold but also characterize the nonnegligible differences among the CLTs for the three classes of high-dimensional sample covariance matrices by establishing a substitution principle: by substituting the adjusted sample size N = n - 1 for the actual sample size n in the centering term of the new CLTs, we obtain the CLT of the unbiased sample covariance matrices. Moreover, it is found that the difference between the CLTs for the ME and unbiased sample covariance matrix is nonnegligible in the centering term although the only difference between two sample covariance matrices is a normalization by n and n - 1, respectively. The new results are applied to two testing problems for high-dimensional covariance matrices.",WOS:000352757100004,ANNALS OF STATISTICS,"['CENTRAL-LIMIT-THEOREM', 'EIGENVALUES']",SUBSTITUTION PRINCIPLE FOR CLT OF LINEAR SPECTRAL STATISTICS OF HIGH-DIMENSIONAL SAMPLE COVARIANCE MATRICES WITH APPLICATIONS TO HYPOTHESIS TESTING,2015
466,"Consider the problem of estimating the mean of a Gaussian random vector when the mean vector is assumed to be in a given convex set. The most natural solution is to take the Euclidean projection of the data vector on to this convex set; in other words, performing ""least squares under a convex constraint."" Many problems in modern statistics and statistical signal processing theory are special cases of this general situation. Examples include the lasso and other high-dimensional regression techniques, function estimation problems, matrix estimation and completion, shape-restricted regression, constrained denoising, linear inverse problems, etc. This paper presents three general results about this problem, namely, (a) an exact computation of the main term in the estimation error by relating it to expected maxima of Gaussian processes (existing results only give upper bounds), (b) a theorem showing that the least squares estimator is always admissible up to a universal constant in any problem of the above kind and (c) a counterexample showing that least squares estimator may not always be minimax rate-optimal. The result from part (a) is then used to compute the error of the least squares estimator in two examples of contemporary interest.",WOS:000345884900007,ANNALS OF STATISTICS,"['SPARSITY ORACLE INEQUALITIES', 'DANTZIG SELECTOR', 'REGRESSION SHRINKAGE', 'ISOTONIC REGRESSION', 'WAVELET SHRINKAGE', 'LASSO', 'CONSISTENCY', 'ESTIMATORS', 'RECOVERY', 'FREEDOM']",A NEW PERSPECTIVE ON LEAST SQUARES UNDER CONVEX CONSTRAINT,2014
467,"IndElec is a software addressed to compute a wide range of indices from electoral data, which are intended to analyze both party systems and electoral systems in political studies. Further, IndElec can calculate such indices from electoral data at several levels of aggregation, even when the acronyms of some political parties change across districts. As the amount of information provided by IndElec may be considerable, this software also aids the user in the analysis of electoral data through three capabilities. First, IndElec automatically elaborates preliminary descriptive statistical reports of computed indices. Second, IndElec saves the computed information into text files in data matrix format, which can be directly loaded by any statistical software to facilitate more sophisticated statistical studies. Third, IndElec provides results in several file formats (text, CSV, HTML, R) to facilitate their visualization and management by using a wide range of application softwares (word processors, spreadsheets, web browsers, etc.). Finally, a graphical user interface is provided for IndElec to manage calculation processes, but no visualization facility is available in this environment. In fact, both the inputs and outputs for IndElec are arranged in files with the aforementioned formats.",WOS:000292097100001,JOURNAL OF STATISTICAL SOFTWARE,"['CHANGING PATTERNS', 'VOLATILITY', 'DISPROPORTIONALITY', 'INDEXES', 'NUMBER']",IndElec: A Software for Analyzing Party Systems and Electoral Systems,2011
468,"We introduce a new sufficient dimension reduction framework that targets a statistical functional of interest, and propose an efficient estimator for the semiparametric estimation problems of this type. The statistical functional covers a wide range of applications, such as conditional mean, conditional variance and conditional quantile. We derive the general forms of the efficient score and efficient information as well as their specific forms for three important statistical functionals: the linear functional, the composite linear functional and the implicit functional. In conjunction with our theoretical analysis, we also propose a class of one-step Newton-Raphson estimators and show by simulations that they substantially outperform existing methods. Finally, we apply the new method to construct the central mean and central variance subspaces for a data set involving the physical measurements and age of abalones, which exhibits a strong pattern of heteroscedasticity.",WOS:000334256100015,ANNALS OF STATISTICS,"['DISTRIBUTED PREDICTORS', 'QUANTILE REGRESSION', 'CENTRAL SUBSPACE', 'DIRECTIONS', 'VARIANCE', 'MODELS']",ON EFFICIENT DIMENSION REDUCTION WITH RESPECT TO A STATISTICAL FUNCTIONAL OF INTEREST,2014
469,"The generalized word length pattern of an orthogonal array allows a ranking of orthogonal arrays in terms of the generalized minimum aberration criterion (Xu and Wu [Ann. Statist. 29 (2001) 1066-1077]). We provide a statistical interpretation for the number of shortest words of an orthogonal array in terms of sums of R-2 values (based on orthogonal coding) or sums of squared canonical correlations (based on arbitrary coding). Directly related to these results, we derive two versions of generalized resolution for qualitative factors, both of which are generalizations of the generalized resolution by Deng and Tang [Statist. Sinica 9 (1999) 1071-1082] and Tang and Deng [Ann. Statist. 27 (1999) 1914-1926]. We provide a sufficient condition for one of these to attain its upper bound, and we provide explicit upper bounds for two classes of symmetric designs. Factor-wise generalized resolution values provide useful additional detail.",WOS:000338477800004,ANNALS OF STATISTICS,"['FRACTIONAL FACTORIAL-DESIGNS', 'MINIMUM ABERRATION', 'PROJECTION JUSTIFICATION', 'G(2)-ABERRATION']",GENERALIZED RESOLUTION FOR ORTHOGONAL ARRAYS,2014
470,"The Dutch and the French schools of data analysis differ in their approaches to the question: How does one understand and summarize the information contained in a data set? The commonalities and discrepancies between the schools are explored here with a focus on methods dedicated to the analysis of categorical data, which are known either as homogeneity analysis (HOMALS) or multiple correspondence analysis (MCA).",WOS:000389127000001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIPLE-CORRESPONDENCE-ANALYSIS', 'ADDITIVE UTILITY-FUNCTIONS', 'MULTIVARIATE-ANALYSIS']",Jan de Leeuw and the French School of Data Analysis,2016
471,"We propose a new test to determine whether jumps are present in asset returns or other discretely sampled processes. As the sampling interval tends to 0, our test statistic converges to I if there are jumps, and to another deterministic and known value (such as 2) if there are no jumps. The test is valid 4 for all Ito semi martingales, depends neither on the law of the process nor on the coefficients of the equation which it solves, does not require a preliminary estimation of these coefficients, and when there are jumps the test is applicable whether jumps have finite or infinite-activity and for an arbitrary Blumenthal-Getoor index. We finally implement the test on simulations and asset returns data.",WOS:000263129000007,ANNALS OF STATISTICS,['DIFFUSION'],TESTING FOR JUMPS IN A DISCRETELY OBSERVED PROCESS,2009
472,"Fuzzy rule-based systems (FRBSs) are a well-known method family within soft computing. They are based on fuzzy concepts to address complex real-world problems. We present the R package frbs which implements the most widely used FRBS models, namely, Mamdani and Takagi Sugeno Kang (TSK) ones, as well as some common variants. In addition a host of learning methods for FRBSs, where the models are constructed from data, are implemented. In this way, accurate and interpretable systems can be built for data analysis and modeling tasks. In this paper, we also provide some examples on the usage of the package and a comparison with other common classification and regression methods available in R.",WOS:000365974400001,JOURNAL OF STATISTICAL SOFTWARE,"['PATTERN-CLASSIFICATION', 'INFERENCE SYSTEM', 'NEURAL-NETWORKS', 'ALGORITHMS', 'IDENTIFICATION', 'PREDICTION', 'ANFIS', 'SETS']",frbs: Fuzzy Rule-Based Systems for Classification and Regression in R,2015
473,"The R package compareGroups provides functions meant to facilitate the construction of bivariate tables (descriptives of several variables for comparison between groups) and generates reports in several formats (LATEX, HTML or plain text CSV). Moreover, bivariate tables can be viewed directly on the R console in a nice format. A graphical user interface (GUI) has been implemented to build the bivariate tables more easily for those users who are not familiar with the R software. Some new functions and methods have been incorporated in the newest version of the compareGroups package (version 1.x) to deal with time-to-event variables, stratifying tables, merging several tables, and revising the statistical methods used. The GUI interface also has been improved, making it much easier and more intuitive to set the inputs for building the bivariate tables. The first version (version 0.x) and this version were presented at the 2010 useR! conference (Sanz, Subirana, and Vila 2010) and the 2011 useR! conference (Sanz, Subirana, and Vila 2011), respectively.",WOS:000341020800001,JOURNAL OF STATISTICAL SOFTWARE,,Building Bivariate Tables: The compareGroups Package for R,2014
474,"The classical Poisson, geometric and negative binomial regression models for count data belong to the family of generalized linear models and are available at the core of the statistics toolbox in the R system for statistical computing. After reviewing the conceptual and computational features of these methods, a new implementation of hurdle and zero-inflated regression models in the functions hurdle () and zeroinfl () from the package pscl is introduced. It re-uses design and functionality of the basic R functions just as the underlying conceptual tools extend the classical models. Both hurdle and zero-in inflated model, are able to incorporate over-dispersion and excess zeros-two problems that typically occur in count data sets in economics and the social sciences-better than their classical counterparts. Using cross-section data on the demand for medical care, it is illustrated how the classical as well as the zero-augmented models can be fitted, inspected and tested in practice.",WOS:000258207100001,JOURNAL OF STATISTICAL SOFTWARE,,Regression models for count data in R,2008
475,"Reproducible research and data archiving are increasingly important issues in research involving statistical analyses of quantitative data. This article introduces the dvn package, which allows R users to publicly archive datasets, analysis files, codebooks, and associated metadata in Dataverse Network online repositories, an open-source data archiving project sponsored by Harvard University. In this article I review the importance of data archiving in the context of reproducible research, introduce the Dataverse Network, explain the implementation of the dvn package, and provide example code for archiving and releasing data using the package.",WOS:000343788100016,R JOURNAL,['REPLICATION'],Archiving Reproducible Research with R and Dataverse,2014
476,"This paper focuses on obtaining clustering information about a distribution from its i.i.d. samples. We develop theoretical results to understand and use clustering information contained in the eigenvectors of data adjacency matrices based on a radial kernel function with a sufficiently fast tail decay. In particular, we provide population analyses to gain insights into which eigenvectors should be used and when the clustering information for the distribution can be recovered from the sample. We learn that a fixed number of top eigenvectors might at the same time contain redundant clustering information and miss relevant clustering information. We use this insight to design the data spectroscopic clustering (DaSpec) algorithm that utilizes properly selected eigenvectors to determine the number of clusters automatically and to group the data accordingly. Our findings extend the intuitions underlying existing spectral techniques such as spectral clustering and Kernel Principal Components Analysis, and provide new understanding into their usability and modes of failure. Simulation Studies and experiments on real-world data are conducted to show the potential of our algorithm. In particular, DaSpec is found to handle unbalanced groups and recover clusters of different shapes better than the competing methods.",WOS:000271673700009,ANNALS OF STATISTICS,['IMAGE SEGMENTATION'],DATA SPECTROSCOPY: EIGENSPACES OF CONVOLUTION OPERATORS AND CLUSTERING,2009
477,"The change point model framework introduced in Hawkins, Qiu, and Kang (2003) and Hawkins and Zamba (2005a) provides an effective and computationally efficient method for detecting multiple mean or variance change points in sequences of Gaussian random variables, when no prior information is available regarding the parameters of the distribution in the various segments. It has since been extended in various ways by Hawkins and Deng (2010), Ross, Tasoulis, and Adams (2011), Ross and Adams (2012) to allow for fully nonparametric change detection in non-Gaussian sequences, when no knowledge is available regarding even the distributional form of the sequence. Another extension comes from Ross and Adams (2011) and Ross (2014) which allows change detection in streams of Bernoulli and Exponential random variables respectively, again when the values of the parameters are unknown.
This paper describes the R package cpm, which provides a fast implementation of all the above change point models in both batch (Phase I) and sequential (Phase II) settings, where the sequences may contain either a single or multiple change points.",WOS:000365976600001,JOURNAL OF STATISTICAL SOFTWARE,"['STATISTICAL PROCESS-CONTROL', 'CHANGE-POINT MODELS', 'CONTROL CHARTS', 'VARIANCE', 'INFERENCE', 'LOCATION', 'SHIFT']",Parametric and Nonparametric Sequential Change Detection in R: The cpm Package,2015
478,"QPot (pronounced kyoo+ pat) is an R package for analyzing two-dimensional systems of stochastic differential equations. It provides users with a wide range of tools to simulate, analyze, and visualize the dynamics of these systems. One of QPot's key features is the computation of the quasi-potential, an important tool for studying stochastic systems. Quasi-potentials are particularly useful for comparing the relative stabilities of equilibria in systems with alternative stable states. This paper describes QPot's primary functions, and explains how quasi-potentials can yield insights about the dynamics of stochastic systems. Three worked examples guide users through the application of QPot's functions.",WOS:000395669800003,R JOURNAL,"['HAMILTON-JACOBI EQUATIONS', 'ORDERED UPWIND METHODS']",QPot: An R Package for Stochastic Differential Equation Quasi-Potential Analysis,2016
479,"We present an improved Foreign Function Interface (FFI) for R to call arbitary native functions without the need for C wrapper code. Further we discuss a dynamic linkage framework for binding standard C libraries to R across platforms using a universal type information format. The package rdyncall comprises the framework and an initial repository of cross-platform bindings for standard libraries such as (legacy and modern) OpenGL, the family of SDL libraries and Expat. The package enables system-level programming using the R language; sample applications are given in the article. We outline the underlying automation tool-chain that extracts cross-platform bindings from C headers, making the repository extendable and open for library developers.",WOS:000313197700005,R JOURNAL,,Foreign Library Interface,2012
480,"A general method is presented for deriving the limiting behavior of estimators that are defined as the values of parameters optimizing an empirical criterion function. The asymptotic behavior of such estimators is typically deduced from uniform limit theorems for rescaled and reparametrized criterion functions. The new method can handle cases where the standard approach does not yield the complete limiting behavior of the estimator. The asymptotic analysis depends on a decomposition of criterion functions into sums of components with different rescalings. The method is explained by examples from Lasso-type estimation, k-means clustering, Shorth estimation and partial linear models.",WOS:000253390000011,ANNALS OF STATISTICS,"['THEOREM', 'LASSO']",Mixed-rates asymptotics,2008
481,"In many applications of multiple hypothesis testing where more than one false rejection can be tolerated, procedures controlling error rates measuring at least k false rejections, instead of at least one, for some fixed k >= 1 can potentially increase the ability of a procedure to detect false null hypotheses. The k-FWER, a generalized version of the usual familywise error rate (FWER), is such an error rate that has recently been introduced in the literature and procedures controlling it have been proposed. A further generalization of a result on the k-FWER is provided in this article. In addition, an alternative and less conservative notion of error rate, the k-FDR, is introduced in the same spirit as the k-FWER by generalizing the usual false discovery rate (FDR). A k-FWER procedure is constructed given any set of increasing constants by utilizing the kth order joint null distributions of the p-values without assuming any specific form of dependence among all the p-values. Procedures controlling the k-FDR are also developed by using the kth order joint null distributions of the p-values, first assuming that the sets of null and nonnull p-values are mutually independent or they are jointly positively dependent in the sense of being multivariate totally positive of order two (MTP2) and then discarding that assumption about the overall dependence among the p-values.",WOS:000253077800009,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'MULTIPLE TESTING PROCEDURES', 'FAMILYWISE ERROR RATE', 'BONFERRONI PROCEDURE', 'RATES', 'INEQUALITIES', 'NUMBER']",Stepup procedures controlling generalized FWER and generalized FDR,2007
482,"In this paper, we consider regression models with a Hilbert-space-valued predictor and a scalar response, where the response depends on the predictor only through a finite number of projections. The linear subspace spanned by these projections is called the effective dimension reduction (EDR) space. To determine the dimensionality of the EDR space, we focus on the leading principal component scores of the predictor, and propose two sequential chi(2) testing procedures under the assumption that the predictor has an elliptically contoured distribution. We further extend these procedures and introduce a test that simultaneously takes into account a large number of principal component scores. The proposed procedures are supported by theory, validated by simulation studies, and illustrated by a real-data example. Our methods and theory are applicable to functional data and high-dimensional multivariate data.",WOS:000282402800014,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'ASYMPTOTIC-DISTRIBUTION', 'LINEAR-REGRESSION', 'ESTIMATORS', 'MODELS']",DECIDING THE DIMENSION OF EFFECTIVE DIMENSION REDUCTION SPACE FOR FUNCTIONAL AND HIGH-DIMENSIONAL DATA,2010
483,"This paper is concerned with density estimation of directional data on the sphere. We introduce a procedure based on thresholding on a new type of spherical wavelets called needlets. We establish a minimax result and prove its optimality. We are motivated by astrophysical applications, in particular in connection with the analysis of ultra high-energy cosmic rays.",WOS:000271673500010,ANNALS OF STATISTICS,"['STRONG UNIFORM-CONVERGENCE', 'ASYMPTOTIC MINIMAX RISK', 'RIEMANNIAN-MANIFOLDS', 'SPHERICAL DATA', 'SPHERES']",ADAPTIVE DENSITY ESTIMATION FOR DIRECTIONAL DATA USING NEEDLETS,2009
484,"This paper introduces the BACCO bundle of R routines for carrying out Bayesian analysis of computer code output. The bundle comprises packages emulator and calibrator, computerized implementations of the ideas of Oakley and O'Hagan ( 2002) and Kennedy and O'Hagan (2001a) respectively. The bundle is self-contained and fully documented R code, and includes a toy dataset that furnishes a working example of the functions.
Package emulator carries out Bayesian emulation of computer code output; package calibrator allows the incorporation of observational data into model calibration using Bayesian techniques.
The package is then applied to a dataset taken from climate science.",WOS:000232929200001,JOURNAL OF STATISTICAL SOFTWARE,"['MODEL', 'UNCERTAINTY']","Introducing BACCO, an R bundle for Bayesian analysis of computer code output",2005
485,"The goal of binary classification is to estimate a discriminant function gamma from observations of covariate vectors and corresponding binary labels. We consider an elaboration of this problem in which the covariates are not available directly but are transformed by a dimensionality-reducing quantizer Q. We present conditions on loss functions such that empirical risk minimization yields Bayes consistency when both the discriminant function and the quantizer are estimated. These conditions are stated in terms of a general correspondence between loss functions and a class of functionals known as Ali-Silvey or f-divergence functionals. Whereas this correspondence was established by Blackwell [Proc. 2nd Berkeley Symp. Probab. Statist. 1 (1951) 93-102. Univ. California Press, Berkeley] for the 0-1 loss, we extend the correspondence to the broader class of surrogate loss functions that play a key role in the general theory of Bayes consistency for binary classification. Our result makes it possible to pick out the (strict) subset of surrogate loss functions that yield Bayes consistency for joint estimation of the discriminant function and the quantizer.",WOS:000265500500012,ANNALS OF STATISTICS,"['DECENTRALIZED DETECTION', 'DISTANCE MEASURES', 'CONSISTENCY', 'CLASSIFICATION', 'DESIGN']",ON SURROGATE LOSS FUNCTIONS AND f-DIVERGENCES,2009
486,"For a directed acyclic graph, there are two known criteria to decide whether any specific conditional independence statement is implied for all distributions factorized according to the given graph. Both criteria are based on special types of path in graphs. They are called separation criteria because independence holds whenever the conditioning set is a separating set in a graph theoretical sense. We introduce and discuss an alternative approach using binary matrix representations of graphs in which zeros indicate independence statements. A matrix condition is shown to give a new path criterion for separation and to be equivalent to each of the previous two path criteria.",WOS:000265500500015,ANNALS OF STATISTICS,['SYSTEMS'],MATRIX REPRESENTATIONS AND INDEPENDENCIES IN DIRECTED ACYCLIC GRAPHS,2009
487,"This article describes the activity of R in the 2011 Google Summer of Code (GSoC), which Wikipedia describes as follows.
The Google Summer of Code (GSoC) is an annual program, first held from May to August 2005, in which Google awards stipends (of 5000 USD, as of 2010) to hundreds of students who successfully complete a requested free or open source software coding project during the summer.",WOS:000208590200012,R JOURNAL,,R's Participation in the Google Summer of Code 2011,2011
488,"In this paper we describe the main features of the Bergm package for the open-source R software which provides a comprehensive framework for Bayesian analysis of exponential random graph models: tools for parameter estimation, model selection and goodness-of fit diagnostics. We illustrate the capabilities of this package describing the algorithms through a tutorial analysis of three network datasets.",WOS:000349840300001,JOURNAL OF STATISTICAL SOFTWARE,"['SOCIAL NETWORKS', 'MARKOV GRAPHS', 'FAMILY MODELS']",Bergm: Bayesian Exponential Random Graphs in R,2014
489,"Many statistical hypotheses can be formulated in terms of polynomial equalities and inequalities in the unknown parameters and thus correspond to semi-algebraic Subsets of the parameter space. We consider large sample asymptotics for the likelihood ratio test of such hypotheses in models that satisfy standard probabilistic regularity conditions. We show that the assumptions of Chernoff's theorem hold for semi-algebraic sets such that the asymptotics are determined by the tangent cone at the true parameter point. At boundary points or singularities, the tangent cone need not be I linear space and limiting distributions Other than chi-square distributions may arise. While boundary points often lead to mixtures of chi-square distributions, Singularities give rise to nonstandard limits. We demonstrate that minima of chisquare random variables are important for locally identifiable models, and in a study of the factor analysis model with one factor, we reveal connections to eigenvalues of Wishart matrices.",WOS:000265500500016,ANNALS OF STATISTICS,"['ASYMPTOTIC PROPERTIES', 'MAXIMUM', 'MODELS', 'ESTIMATORS', 'SMOOTH', 'CONES']",LIKELIHOOD RATIO TESTS AND SINGULARITIES,2009
490,"This paper studies sparse density estimation via l(1) penalization (SPADES). We focus on estimation in high-dimensional mixture models and nonparametric adaptive density estimation. We show, respectively, that SPADES can recover, with high probability, the unknown components of a mixture of probability densities and that it yields minimax adaptive density estimates. These results are based on a general sparsity oracle inequality that the SPADES estimates satisfy. We offer a data driven method for the choice of the tuning parameter used in the construction of SPADES. The method uses the generalized bisection method first introduced in [10]. The suggested procedure bypasses the need for a grid search and offers substantial computational savings. We complement our theoretical results with a simulation study that employs this method for approximations of one and two-dimensional densities with mixtures. The numerical results strongly support our theoretical findings.",WOS:000280359400019,ANNALS OF STATISTICS,"['KERNEL DENSITY ESTIMATORS', 'SELECTION', 'LASSO', 'REGRESSION', 'SPARSITY', 'AGGREGATION']",SPADES AND MIXTURE MODELS,2010
491,"Continuous diagnostic tests are often used for discriminating between healthy and diseased populations. For the clinical application of such tests, it is useful to select a cutpoint or discrimination value c that defines positive and negative test results. In general, individuals with a diagnostic test value of c or higher are classified as diseased. Several search strategies have been proposed for choosing optimal cutpoints in diagnostic tests, depending on the underlying reason for this choice. This paper introduces an R package, known as OptimalCutpoints, for selecting optimal cutpoints in diagnostic tests. It incorporates criteria that take the costs of the different diagnostic decisions into account, as well as the prevalence of the target disease and several methods based on measures of diagnostic test accuracy. Moreover, it enables optimal levels to be calculated according to levels of given (categorical) covariates. While the numerical output includes the optimal cutpoint values and associated accuracy measures with their confidence intervals, the graphical output includes the receiver operating characteristic (ROC) and predictive ROC curves. An illustration of the use of OptimalCutpoints is provided, using a real biomedical dataset.",WOS:000349841300001,JOURNAL OF STATISTICAL SOFTWARE,"['OPERATING CHARACTERISTIC CURVES', 'PROGNOSTIC-FACTORS', 'BINOMIAL PROPORTIONS', 'INTERVAL ESTIMATION', 'VERIFICATION BIAS', 'DECISION-MAKING', 'PROSTATE-CANCER', 'ROC CURVES', 'CONFIDENCE', 'RATIO']",OptimalCutpoints: An R Package for Selecting Optimal Cutpoints in Diagnostic Tests,2014
492,"This paper describes Mateda-2.0, a MATLAB package for estimation of distribution algorithms (EDAs). This package can be used to solve single and multi-objective discrete and continuous optimization problems using EDAs based on undirected and directed probabilistic graphical models. The implementation contains several methods commonly employed by EDAs. It is also conceived as an open package to allow users to incorporate different combinations of selection, learning, sampling, and local search procedures. Additionally, it includes methods to extract, process and visualize the structures learned by the probabilistic models. This way, it can unveil previously unknown information about the optimization problem domain. Mateda-2.0 also incorporates a module for creating and validating function models based on the probabilistic models learned by EDAs.",WOS:000281587800001,JOURNAL OF STATISTICAL SOFTWARE,"['EVOLUTIONARY ALGORITHMS', 'KIKUCHI APPROXIMATIONS', 'MODEL', 'OPTIMIZATION', 'CLASSIFIER', 'NETWORKS']",Mateda-2.0: Estimation of Distribution Algorithms in MATLAB,2010
493,"This paper deals with the trace regression model where n entries or linear combinations of entries of an unknown m(1) x m(2) matrix A(0) corrupted by noise are observed. We propose a new nuclear-norm penalized estimator of A(0) and establish a general sharp oracle inequality for this estimator for arbitrary values of n, m(1), m(2) under the condition of isometry in expectation. Then this method is applied to the matrix completion problem. In this case, the estimator admits a simple explicit form, and we prove that it satisfies oracle inequalities with faster rates of convergence than in the previous works. They are valid, in particular, in the high-dimensional setting m(1)m(2) >> n. We show that the obtained rates are optimal up to logarithmic factors in a minimax sense and also derive, for any fixed matrix A(0), a nonminimax lower bound on the rate of convergence of our estimator, which coincides with the upper bound up to a constant factor. Finally, we show that our procedure provides an exact recovery of the rank of A(0) with probability close to 1. We also discuss the statistical learning setting where there is no underlying model determined by A(0), and the aim is to find the best trace regression model approximating the data. As a by-product, we show that, under the restricted eigenvalue condition, the usual vector Lasso estimator satisfies a sharp oracle inequality (i.e., an oracle inequality with leading constant 1).",WOS:000299186500005,ANNALS OF STATISTICS,"['DANTZIG SELECTOR', 'MINIMIZATION']",NUCLEAR-NORM PENALIZATION AND OPTIMAL RATES FOR NOISY LOW-RANK MATRIX COMPLETION,2011
494,"Along with increased complexity of the models used for scientific activities and engineering come diverse and greater uncertainties. Today, effectively quantifying the uncertainties contained in a model appears to be more important than ever. Scientific fellows know how serious it is to calibrate their model in a robust way, and decision-makers describe how critical it is to keep the best effort to reduce the uncertainties about the model. Effectively accessing the uncertainties about the model requires mastering all the tasks involved in the numerical experiments, from optimizing the experimental design to managing the very time consuming aspect of model simulation and choosing the adequate indicators and analysis methods.
In this paper, we present an open framework for organizing the complexity associated with numerical model simulation and analyses. Named mtk (Mexico Toolkit), the developed system aims at providing practitioners from different disciplines with a systematic and easy way to compare and to find the best method to effectively uncover and quantify the uncertainties contained in the model and further to evaluate their impact on the performance of the model. Such requirements imply that the system must be generic, universal, homogeneous, and extensible. This paper discusses such an implementation using the R scientific computing platform and demonstrates its functionalities with examples from agricultural modeling.
The package mtk is of general purpose and easy to extend. Numerous methods are already available in the actual release version, including Fast, Sobol, Morris, Basic Monte-Carlo, Regression, LHS (Latin Hypercube Sampling), PLMM (Polynomial Linear metamodel). Most of them are compiled from available R packages with extension tools delivered by package mtk.",WOS:000368551800016,R JOURNAL,['SYSTEMS'],mtk: A General-Purpose and Extensible R Environment for Uncertainty and Sensitivity Analyses of Numerical Experiments,2015
495,"Chen and Cheng [Ann. Statist. 34 (2006) 546-558] discussed the method of doubling for constructing two-level fractional factorial designs. They showed that for 9N/32 <= n <= 5N/16, all minimum aberration designs with N runs and n factors are projections of the maximal design with 5N/16 factors which is constructed by repeatedly doubling the 2(5-1) design defined by I = ABCDE. This paper develops a general complementary design theory for doubling. For any design obtained by repeated doubling, general identities are established to link the wordlength patterns of each pair of complementary projection designs. A rule is developed for choosing minimum aberration projection designs from the maximal design with 5N/16 factors. It is further shown that for 17N/64 <= n <= 5N/16, all minimum aberration designs with N runs and n factors are projections of the maximal design with N runs and 5N/16 factors.",WOS:000253390000018,ANNALS OF STATISTICS,"['FRACTIONAL FACTORIAL-DESIGNS', 'MINIMUM ABERRATION']",A complementary design theory for doubling,2008
496,"A Cramer moderate deviation theorem for Hotelling's T-2-statistic is proved under a finite (3 + delta)th moment. The result is applied to large scale tests on the equality of mean vectors and is shown that the number of tests can be as large as e(0)(n(1/3)) before the chi-squared distribution calibration becomes inaccurate. As an application of the moderate deviation results, a global test on the equality of m mean vectors based on the maximum of Hotelling's T-2-statistics is developed and its asymptotic null distribution is shown to be an extreme value type I distribution. A novel intermediate approximation to the null distribution is proposed to improve the slow convergence rate of the extreme distribution approximation. Numerical studies show that the new test procedure works well even for a small sample size and performs favorably in analyzing a breast cancer dataset.",WOS:000317451200012,ANNALS OF STATISTICS,"['HIGHER CRITICISM', 'SHAPE-ANALYSIS', 'STUDENTS-T', 'APPROXIMATIONS', 'FIELDS']",A CRAMER MODERATE DEVIATION THEOREM FOR HOTELLING'S T-2-STATISTIC WITH APPLICATIONS TO GLOBAL TESTS,2013
497,"This paper formulates a penalized empirical likelihood (PEL) method for inference on the population mean when the dimension of the observations may grow faster than the sample size. Asymptotic distributions of the PEL ratio statistic is derived under different component-wise dependence structures of the observations, namely, (i) non-Ergodic, (ii) long-range dependence and (iii) short-range dependence. It follows that the limit distribution of the proposed PEL ratio statistic can vary widely depending on the correlation structure, and it is typically different from the usual chi-squared limit of the empirical likelihood ratio statistic in the fixed and finite dimensional case. A unified subsampling based calibration is proposed, and its validity is established in all three cases, (i)-(iii). Finite sample properties of the method are investigated through a simulation study.",WOS:000321844300006,ANNALS OF STATISTICS,"['LONG-RANGE DEPENDENCE', 'SEMIPARAMETRIC MODELS', 'CONFIDENCE-INTERVALS', 'RATIO', 'CONVERGENCE']",A PENALIZED EMPIRICAL LIKELIHOOD METHOD IN HIGH DIMENSIONS,2012
498,"We show that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior. Forboth methods, wederive, inparallel, oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the e(p) estimation loss for 1 <= p <= 2 in the linear model when the number of variables can be Much larger than the sample size.",WOS:000268113500004,ANNALS OF STATISTICS,"['STATISTICAL ESTIMATION', 'VARIABLE SELECTION', 'LEAST-SQUARES', 'REGRESSION', 'AGGREGATION', 'SPARSITY', 'LARGER']",SIMULTANEOUS ANALYSIS OF LASSO AND DANTZIG SELECTOR,2009
499,"It is possible to model life contingency insurances with the life contingencies R package, which is capable of performing financial and actuarial mathematics calculations. Its functions permit one to determine both the expected value and the stochastic distribution of insured bene fits. Therefore, life insurance coverage can be priced and portfolios risk-based capital requirements can be assessed. This paper briefly summarizes the theory regarding life contingencies that is based on financial mathematics and demographic concepts. Then, with the aid of applied examples, it shows how the lifecontingencies package can be a useful tool for executing routine, deterministic, or stochastic calculations for life-contingencies actuarial mathematics.",WOS:000328130300001,JOURNAL OF STATISTICAL SOFTWARE,,The lifecontingencies Package: Performing Financial and Actuarial Mathematics Calculations in R,2013
500,"ROC analysis is a standard method for estimating and comparing diagnostic tests' accuracies when the gold standard is binary. However, there are many situations when the gold standard is not binary. In these situations, traditional ROC methods applied have lead to biased and uninformative outcomes. This article introduces nonbinROC, software for R that implements nonparametric estimators proposed by Obuchowski (2005) for estimating and comparing diagnostic tests' accuracies when the gold standard is measured on a continuous, ordinal or nominal scale. The results produced from these estimators are interpreted in the same manner as in ROC analysis but are not associated with any ROC curve.",WOS:000252429100001,JOURNAL OF STATISTICAL SOFTWARE,['AREA'],NonbinROC: Software for evaluating accuracies with non-binary gold diagnostic standards,2007
501,"In this paper, Cramer-type moderate deviations for the maximum of the periodogram and its studentized version are derived. The results are then applied to a simultaneous testing problem in gene expression time series. It is shown that the level of the simultaneous tests is accurate provided that the number of genes C and the sample size n satisfy G = exp(o(n(1/3))).",WOS:000277471000021,ANNALS OF STATISTICS,"['STATIONARY-PROCESSES', 'STUDENTS-T', 'SEQUENCE']",CRAMER-TYPE MODERATE DEVIATION FOR THE MAXIMUM OF THE PERIODOGRAM WITH APPLICATION TO SIMULTANEOUS TESTS IN GENE EXPRESSION TIME SERIES,2010
502,"We discuss the possibilities and limitations of estimating the mean of a real-valued random variable from independent and identically distributed observations from a nonasymptotic point of view. In particular, we define estimators with a sub-Gaussian behavior even for certain heavy-tailed distributions. We also prove various impossibility results for mean estimators.",WOS:000389620800016,ANNALS OF STATISTICS,['ADAPTIVE ESTIMATION'],SUB-GAUSSIAN MEAN ESTIMATORS,2016
503,"Moving from univariate to bivariate jointly dependent long-memory time series introduces a phase parameter (gamma), at the frequency of principal interest. zeros for short-memory series gamma = 0 automatically. The latter case has also been stressed under long memory, along with the ""fractional differencing"" case gamma = (delta(2) - delta(1))pi/2, where delta(1), delta(2) are the memory parameters of the two series. We develop time domain conditions under which these are and are not relevant, and relate the consequent properties of cross-autocovariances to ones of the (possibly bilateral) moving average representation which. with martingale difference innovations of arbitrary dimension, is used in asymptotic theory for local Whittle parameter estimates depending on a single smoothing, number. Incorporating also a regression parameter (beta) which, when nonzero, indicates cointegration, the consistency proof of these implicitly defined estimates is nonstandard due to the beta estimate converging faster than the others. We also establish joint asymptotic normality of the estimates, and indicate how this Outcome can apply in statistical inference on several questions of interest. Issues of implemention are discussed, along with implications of knowing beta and of correct or incorrect specification of gamma, and possible extensions to higher-dimensional systems and nonstationary series.",WOS:000260554100019,ANNALS OF STATISTICS,"['LONG-RANGE DEPENDENCE', 'GAUSSIAN SEMIPARAMETRIC ESTIMATION', 'CENTRAL-LIMIT-THEOREM', 'TIME-SERIES', 'FRACTIONAL COINTEGRATION', 'MEMORY', 'VOLATILITY', 'SAMPLE', 'NONSTATIONARY', 'REGRESSION']",MULTIPLE LOCAL WHITTLE ESTIMATION IN STATIONARY SYSTEMS,2008
504,"Sample selection models deal with the situation in which an outcome of interest is observed for a restricted non-randomly selected sample of the population. The estimation of these models is based on a binary equation, which describes the selection process, and an outcome equation, which is used to examine the substantive question of interest. Classic sample selection models assume a priori that continuous covariates have a linear or pre- specified non-linear relationship to the outcome, and that the distribution linking the two equations is bivariate normal.
We introduce the R package SemiParSampleSel which implements copula regression spline sample selection models. The proposed implementation can deal with non-random sample selection, non-linear covariate-response relationships, and non-normal bivariate distributions between the model equations. We provide details of the model and algorithm and describe the implementation in SemiParSampleSel. The package is illustrated using simulated and real data examples.",WOS:000384915100001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED ADDITIVE-MODEL', 'SEMIPARAMETRIC ESTIMATION', 'WAGE COMPARISONS', 'P-VALUES', 'BIAS', 'TESTS', 'COMPONENTS', 'ESTIMATOR']",Copula Regression Spline Sample Selection Models: The R Package SemiParSampleSel,2016
505,"Meta-analysis is a statistical method for combining information from different studies about the same issue of interest. Meta-analysis is widely diffuse in medical investigation and more recently it received a growing interest also in social disciplines. Typical applications involve a small number of studies, thus making ordinary inferential methods based on first-order asymptotics unreliable. More accurate results can be obtained by exploiting the theory of higher-order asymptotics. This paper describes the metaLik package which provides an R implementation of higher-order likelihood methods in meta-analysis. The extension to meta-regression is included. Two real data examples are used to illustrate the capabilities of the package.",WOS:000307534100001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION', 'ASYMPTOTICS', 'TESTS', 'MODEL']",The R Package metaLik for Likelihood Inference in Meta-Analysis,2012
506,"Study of the bivariate normal distribution raises the full range of issues involving objective Bayesian inference, including the different types of objective priors (e.g., Jeffreys, invariant, reference, matching), the different modes of inference (e.g., Bayesian, frequentist, fiducial) and the criteria involved in deciding on optimal objective priors (e.g., ease of computation, frequentist performance, marginalization paradoxes). Summary recommendations as to optimal objective priors are made for a variety of inferences involving the bivariate normal distribution.
In the course of the investigation, a variety of surprising results were found, including the availability of objective priors that yield exact frequentist inferences for many functions of the bivariate normal parameters, including the correlation coefficient.",WOS:000254502700017,ANNALS OF STATISTICS,"['POSTERIOR DISTRIBUTIONS', 'BAYESIAN-INFERENCE', 'PROBABILITY']",Objective priors for the bivariate normal model,2008
507,"Functional magnetic resonance imaging (fMRI) technology is popularly used in many fields for studying how the brain reacts to mental stimuli. The identification of optimal fMRI experimental designs is crucial for rendering precise statistical inference on brain functions, but research on this topic is very lacking. We develop a general theory to guide the selection of fMRI designs for estimating a hemodynamic response function (HRF) that models the effect over time of the mental stimulus, and for studying the comparison of two HRFs. We provide a useful connection between fMRI designs and circulant biased weighing designs, establish the statistical optimality of some well-known fMRI designs and identify several new classes of fMRI designs. Construction methods of high-quality fMRI designs are also given.",WOS:000363437900009,ANNALS OF STATISTICS,"['EVENT-RELATED FMRI', 'MULTIPLE TRIAL TYPES', 'OPTIMIZATION', 'SEQUENCES', 'MATRICES', 'ENTROPY', 'POWER']",OPTIMAL EXPERIMENTAL DESIGNS FOR FMRI VIA CIRCULANT BIASED WEIGHING DESIGNS,2015
508,"Support in R for state space estimation via Kalman filtering was limited to one package, until fairly recently. In the last five years, the situation has changed with no less than four additional packages offering general implementations of the Kalman filter, including in some cases smoothing, simulation smoothing and other functionality. This paper reviews some of the offerings in R to help the prospective user to make an informed choice.",WOS:000287815100001,JOURNAL OF STATISTICAL SOFTWARE,"['STATE-SPACE MODELS', 'TIME-SERIES MODELS', 'SIMULATION SMOOTHER']",Kalman Filtering in R,2011
509,"Outcomes of continuous proportions arise in many applied areas. Such data are typically measured as percentages, rates or proportions confined in the unitary interval. In this paper, the R package simplexreg which provides dispersion model fitting of the simplex distribution is introduced to model such proportional outcomes. The maximum likelihood method and generalized estimating equations techniques are available for parameter estimation in cross-sectional and longitudinal studies, respectively. This paper presents methods and algorithms implemented in the package, including parameter estimation, model checking as well as density, cumulative distribution, quantile and random number generating functions of the simplex distribution. The package is applied to real data sets for illustration.",WOS:000384916200001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED LINEAR-MODELS', 'VIABLE CD34(+) CELLS', 'MARGINAL MODELS', 'BETA REGRESSION', 'ENGRAFTMENT', 'DISPERSION', 'RESPONSES']",simplexreg: An R Package for Regression Analysis of Proportional Data Using the Simplex Distribution,2016
510,"Many statistical M-estimators are based on convex optimization problems formed by the combination of a data-dependent loss function with a norm-based regularizer. We analyze the convergence rates of projected gradient and composite gradient methods for solving such problems, working within a high-dimensional framework that allows the ambient dimension d to grow with (and possibly exceed) the sample size n. Our theory identifies conditions under which projected gradient descent enjoys globally linear convergence up to the statistical precision of the model, meaning the typical distance between the true unknown parameter theta* and an optimal solution (theta) over cap. By establishing these conditions with high probability for numerous statistical models, our analysis applies to a wide range of M-estimators, including sparse linear regression using Lasso; group Lasso for block sparsity; log-linear models with regularization; low-rank matrix recovery using nuclear norm regularization; and matrix decomposition using a combination of the nuclear and l(1) norms. Overall, our analysis reveals interesting connections between statistical and computational efficiency in high-dimensional estimation.",WOS:000321844300004,ANNALS OF STATISTICS,"['LOW-RANK MATRICES', 'VARIABLE SELECTION', 'LINEAR-REGRESSION', 'LASSO', 'DECOMPOSITION', 'SPARSITY', 'PURSUIT', 'COMPLETION', 'SHRINKAGE', 'NOISY']",FAST GLOBAL CONVERGENCE OF GRADIENT METHODS FOR HIGH-DIMENSIONAL STATISTICAL RECOVERY,2012
511,"This special volume features eleven contributions to psychometric computing, a research area that integrates psychometrics and computational methods in statistics. Topics covered include structural equation modeling, item response theory, probabilistic choice modeling, and other modeling approaches prevalent in or useful for psychometric research. Each contributed paper is accompanied by a software package published on the Comprehensive R Archive Network. This introduction gives a brief overview of the volume.",WOS:000305116900001,JOURNAL OF STATISTICAL SOFTWARE,['MODELS'],Psychoco: Psychometric Computing in R,2012
512,"In a variety of settings it is extremely helpful to be able to apply R functions through buttons, sliders and other types of graphical control. This is particularly true in plotting activities where immediate communication between such controls and a graphical display allows the user to interact with a plot in a very effective manner. The tcltk package provides extensive tools for this and the aim of the rpanel package is to provide simple and well documented functions which make these facilities as accessible as possible. In addition, the operations which form the basis of communication within tcltk are managed in a way which allows users to write functions with a more standard form of parameter passing. This paper describes the basic design of the software and illustrates it on a variety of examples of interactive control of graphics. The tkrplot system is used to allow plots to be integrated with controls into a single panel. An example of the use of a graphical image, and the ability to interact with this, is also discussed.",WOS:000243591100001,JOURNAL OF STATISTICAL SOFTWARE,,rpanel: Simple interactive controls for R functions using the tcltk package,2007
513,"We propose a generalized partially linear functional single index risk score model for repeatedly measured outcomes where the index itself is a function of time. We fuse the nonparametric kernel method and regression spline method, and modify the generalized estimating equation to facilitate estimation and inference. We use local smoothing kernel to estimate the unspecified coefficient functions of time, and use B-splines to estimate the unspecified function of the single index component. The covariance structure is taken into account via a working model, which provides valid estimation and inference procedure whether or not it captures the true covariance. The estimation method is applicable to both continuous and discrete outcomes. We derive large sample properties of the estimation procedure and show a different convergence rate for each component of the model. The asymptotic properties when the kernel and regression spline methods are combined in a nested fashion has not been studied prior to this work, even in the independent data case.",WOS:000362697700003,ANNALS OF STATISTICS,['DISEASE'],FUSED KERNEL-SPLINE SMOOTHING FOR REPEATEDLY MEASURED OUTCOMES IN A GENERALIZED PARTIALLY LINEAR MODEL WITH FUNCTIONAL SINGLE INDEX,2015
514,"bnlearn is an R package (R Development Core Team 2010) which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables. Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the snow package (Tierney et al. 2008) to improve their performance via parallel computing. Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use. Advanced plotting options are provided by the Rgraphviz package (Gentry et al. 2010).",WOS:000281587200001,JOURNAL OF STATISTICAL SOFTWARE,['ALGORITHM'],Learning Bayesian Networks with the bnlearn R Package,2010
515,"Mixture toxicity assessment is indeed necessary for humans and ecosystems that are continually exposed to a variety of chemical mixtures. This paper describes an R package, called mixtox, which offers a general framework of curve fitting, mixture experimental design, and mixture toxicity prediction for practitioners in toxicology. The unique features of mixtox include: (1) constructing a uniform table for mixture experimental design; and (2) predicting toxicity of a mixture with multiple components based on reference models such as concentration addition, independent action, and generalized concentration addition. We describe the various functions of the package and provide examples to illustrate their use and show the collaboration of mixtox with other existing packages (e.g., drc) in predicting toxicity of chemical mixtures.",WOS:000395669800028,R JOURNAL,"['JOINT ALGAL TOXICITY', 'CHEMICAL-MIXTURES', 'UNIFORM DESIGN', 'PREDICTION', 'REGRESSION']",mixtox: An R Package for Mixture Toxicity Assessment,2016
516,"A general lower bound is developed for the minimax risk when estimating an arbitrary functional. The bound is based on testing two composite hypotheses and is shown to be effective in estimating the nonsmooth functional 1/n Sigma vertical bar theta(i)vertical bar from an observation Y similar to N (theta, I-n). This problem exhibits some features that are significantly different from those that occur in estimating conventional smooth functionals. This is a setting where standard techniques fail to yield sharp results.
A sharp minimax lower bound is established by applying the general lower bound technique based on testing two composite hypotheses. A key step is the construction of two special priors and bounding the chi-square distance between two normal mixtures. An estimator is constructed using approximation theory and Hermite polynomials and is shown to be asymptotically sharp minimax when the means are bounded by a given value M. It is shown that the minimax risk equals beta(2)(*) M-2(log log n/log n)(2) asymptotically, where beta(*) is the log n Bernstein constant.
The general techniques and results developed in the present paper can also be used to solve other related problems.",WOS:000291183300012,ANNALS OF STATISTICS,"['CONVERGENCE', 'REGRESSION']","TESTING COMPOSITE HYPOTHESES, HERMITE POLYNOMIALS AND OPTIMAL ESTIMATION OF A NONSMOOTH FUNCTIONAL",2011
517,"Tobler (1965) introduced bidimensional regression to the research field of geography in 1965 to provide a method for estimating mapping relations between two planes on the basis of regression modeling. The bidimensional regression method has been widely used within geographical research. However, the applicability in assessing the degree of similarity of two-dimensional patterns has not much explored in the area of psychological research, particularly in the domains of cognitive maps, face research and comparison of 2D-data patterns. Describing Tobler's method in detail, Friedman and Kohler (2003) made an attempt to bridge the gulf between geographical methodological knowledge and psychological research practice. Still, the method has not been incorporated into psychologists' standard methodical repertoire to date. The present paper aims to make bidimensional regression applicable also for researchers and users unfamiliar with its theoretical basis. The BiDimRegression function provides a manageable computing option for bidimensional regression models with affine and Euclidean transformation, which makes it easy to assess the similarity of any planar configuration of points. Typical applications are, for instance, assessments of the similarity of facial images defined by discrete features or of (cognitive) maps characterized by landmarks. BiDimRegression can be a valuable tool since it provides estimation, statistical inference, and goodness-of-fit measures for bidimensional regression.",WOS:000326872500001,JOURNAL OF STATISTICAL SOFTWARE,['MAPS'],BiDimRegression: Bidimensional Regression Modeling Using R,2013
518,"We consider higher order frequentist inference for the parametric component of a semiparametric model based on sampling from the posterior profile distribution. The first order validity of this procedure established by Lee, Kosorok and Fine in [J. American Statist. Assoc. 100 (2005) 960969] is extended to second-order validity in the setting where the infinite-dimensional nuisance parameter achieves the parametric rate. Specifically, we obtain higher order estimates of the maximum profile likelihood estimator and of the efficient Fisher information. Moreover, we prove that an exact frequentist confidence interval for the parametric component at level alpha can be estimated by the alpha-level credible set from the profile sampler with an error of order O p (n(-1)). Simulation studies are used to assess second-order asymptotic validity of the profile sampler. As far as we are aware, these are the first higher order accuracy results for semiparametric frequentist inference.",WOS:000258243000014,ANNALS OF STATISTICS,"['POSTERIOR DISTRIBUTIONS', 'MAXIMUM-LIKELIHOOD', 'REGRESSION-MODELS', 'INFORMATION', 'EFFICIENCY']",Higher order semiparametric frequentist inference with the profile sampler,2008
519,"A new model-free screening method called the fused Kolmogorov filter is proposed for high-dimensional data analysis. This new method is fully nonparametric and can work with many types of covariates and response variables, including continuous, discrete and categorical variables. We apply the fused Kolmogorov filter to deal with variable screening problems emerging from a wide range of applications, such as multiclass classification, nonparametric regression and Poisson regression, among others. It is shown that the fused Kolmogorov filter enjoys the sure screening property under weak regularity conditions that are much milder than those required for many existing nonparametric screening methods. In particular, the fused Kolmogorov filter can still be powerful when covariates are strongly dependent on each other. We further demonstrate the superior performance of the fused Kolmogorov filter over existing screening methods by simulations and real data examples.",WOS:000357441000008,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'DIMENSION REDUCTION', 'VARIABLE SELECTION', 'CLASSIFICATION', 'ESTIMATORS', 'LIKELIHOOD']",THE FUSED KOLMOGOROV FILTER: A NONPARAMETRIC MODEL-FREE SCREENING METHOD,2015
520,"The RcppEigen package provides access from R (R Core Team 2012a) to the Eigen (Guennebaud, Jacob, and others 2012) C++ template library for numerical linear algebra. Rcpp (Eddelbuettel and Francois 2011, 2012) classes and specializations of the C++ templated functions as and wrap from Rcpp provide the ""glue"" for passing objects from R to C++ and back. Several introductory examples are presented. This is followed by an in-depth discussion of various available approaches for solving least-squares problems, including rank-revealing methods, concluding with an empirical run-time comparison. Last but not least, sparse matrix methods are discussed.",WOS:000315019200001,JOURNAL OF STATISTICAL SOFTWARE,,Fast and Elegant Numerical Linear Algebra Using the RcppEigen Package,2013
521,"This article describes the R package costat. This package enables a user to (i) perform a test for time series stationarity; (ii) compute and plot time-localized autocovariances, and (iii) to determine and explore any costationary relationship between two locally stationary time series. Two locally stationary time series are said to be costationary if there exists two time-varying combination functions such that the linear combination of the two series with the functions produces another time series which is stationary. Costationarity existing between two time series indicates a relationship between the series that might be usefully exploited in a number of ways. Sometimes the relationship itself is of interest, sometimes the derived stationary series is of interest and useful as a substitute for either of the original stationary series in some applications.",WOS:000325947100001,JOURNAL OF STATISTICAL SOFTWARE,"['UNIT-ROOT', 'TESTS']",Costationarity of Locally Stationary Time Series Using costat,2013
522,"We describe the R package CovSel, which reduces the dimension of the covariate vector for the purpose of estimating an average causal effect under the unconfoundedness assumption. Covariate selection algorithms developed in De Luna, Waernbaum, and Richardson (2011) are implemented using model- free backward elimination. We show how to use the package to select minimal sets of covariates. The package can be used with continuous and discrete covariates and the user can choose between marginal co- ordinate hypothesis tests and kernel- based smoothing as model- free dimension reduction techniques.",WOS:000366013800001,JOURNAL OF STATISTICAL SOFTWARE,"['TRAINING-PROGRAMS', 'ECONOMETRICS', 'MULTIVARIATE', 'REGRESSION', 'EFFICIENCY', 'INFERENCE', 'SOFTWARE']",CovSel: An R Package for Covariate Selection When Estimating Average Causal Effects,2015
523,"While R has proven itself to be a powerful and flexible tool for data exploration and analysis, it lacks the ease of use present in other software such as SPSS and Minitab. An easy to use graphical user interface (GUI) can help new users accomplish tasks that would otherwise be out of their reach, and improves the efficiency of expert users by replacing fifty key strokes with five mouse clicks. With this in mind, Deducer presents dialogs that are understandable for the beginner, and yet contain all (or most) of the options that an experienced statistician, performing the same task, would want. An Excel-like spreadsheet is included for easy data viewing and editing. Deducer is based on Java's Swing GUI library and can be used on any common operating system. The GUI is independent of the specific R console and can easily be used by calling a text-based menu system. Graphical menus are provided for the JGR console and the Windows R GUI.",WOS:000305991500001,JOURNAL OF STATISTICAL SOFTWARE,,Deducer: A Data Analysis GUI for R,2012
524,"The glmmBUGS package is a bridging tool between Generalized Linear Mixed Models (GLMMs) in R and the BUGS language. It provides a simple way of performing Bayesian inference using Markov Chain Monte Carlo (MCMC) methods, taking a model formula and data frame in R and writing a BUGS model file, data file, and initial values files. Functions are provided to reformat and summarize the BUGS results. A key aim of the package is to provide files and objects that can be modified prior to calling BUGS, giving users a platform for customizing and extending the models to accommodate a wide variety of analyses.",WOS:000208589900003,R JOURNAL,,MCMC for Generalized Linear Mixed Models with glmmBUGS,2010
525,"While hidden class models of various types arise in many statistical applications, it is often difficult to establish the identifiability of their parameters. Focusing on models in which there is some structure of independence of some of the observed variables conditioned on hidden ones, we demonstrate,I general approach for establishing identifiability utilizing algebraic arguments. A theorem of J. Kruskal for a simple latent-class model with finite state space lies at the core of our results, though we apply it to a diverse set of models. These include mixtures of both finite and nonparametric product distributions, hidden Markov models and random graph mixture models, and lead to a number of new results and improvements to old ones.
In the parametric setting, this approach indicates that for such models, the classical definition of identifiability is typically too strong. Instead generic identifiability holds, which implies that the set of nonidentifiable parameters has measure zero, so that parameter inference is still meaningful. In particular, this sheds light on the properties of finite mixtures of Bernoulli products, which have been used for decades despite being known to have nonidentifiable parameters. In the nonparametric setting, we again obtain identifiability only when certain restrictions are placed on the distributions that are mixed, but we explicitly describe the conditions.",WOS:000271673500001,ANNALS OF STATISTICS,"['MIXTURE-MODELS', 'SECANT VARIETIES', 'SEGRE VARIETIES', 'STOCHASTIC BLOCKSTRUCTURES', 'NONPARAMETRIC-INFERENCE', 'FINITE MIXTURES', 'MARKOV-CHAINS', 'IDENTIFICATION', 'CLASSIFICATION', 'DISTRIBUTIONS']",IDENTIFIABILITY OF PARAMETERS IN LATENT STRUCTURE MODELS WITH MANY OBSERVED VARIABLES,2009
526,"Permutation distribution clustering is a complexity-based approach to clustering time series. The dissimilarity of time series is formalized as the squared Hellinger distance between the permutation distribution of embedded time series. The resulting distance measure has linear time complexity, is invariant to phase and monotonic transformations, and robust to outliers. A probabilistic interpretation allows the determination of the number of significantly different clusters. An entropy-based heuristic relieves the user of the need to choose the parameters of the underlying time-delayed embedding manually and, thus, makes it possible to regard the approach as parameter-free. This approach is illustrated with examples on empirical data.",WOS:000365982600001,JOURNAL OF STATISTICAL SOFTWARE,"['PERMUTATION ENTROPY', 'EEG']",pdc: An R Package for Complexity-Based Clustering of Time Series,2015
527,"Because of the advance in technologies, modem statistical studies often encounter linear models with the number of explanatory variables much larger than the sample size. Estimation and variable selection in these high-dimensional problems with deterministic design points is very different from those in the case of random covariates, due to the identifiability of the high-dimensional regression parameter vector. We show that a reasonable approach is to focus on the projection of the regression parameter vector onto the linear space generated by the design matrix. In this work, we consider the ridge regression estimator of the projection vector and propose to threshold the ridge regression estimator when the projection vector is sparse in the sense that many of its components are small. The proposed estimator has an explicit form and is easy to use in application. Asymptotic properties such as the consistency of variable selection and estimation and the convergence rate of the prediction mean squared error are established under some sparsity conditions on the projection vector. A simulation study is also conducted to examine the performance of the proposed estimator.",WOS:000307608000007,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'FEATURE SPACE', 'LASSO', 'REGRESSION']",ESTIMATION IN HIGH-DIMENSIONAL LINEAR MODELS WITH DETERMINISTIC DESIGN MATRICES,2012
528,"High throughput genetic sequencing arrays with thousands of measurements per sample and a great amount of related censored clinical data have increased demanding need for better measurement specific model selection. In this paper we establish strong oracle properties of nonconcave penalized methods for nonpolynomial (NP) dimensional data with censoring in the framework of Cox's proportional hazards model. A class of folded-concave penalties are employed and both LASSO and SCAD are discussed specifically. We unveil the question under which dimensionality and correlation restrictions can an oracle estimator be constructed and grasped. It is demonstrated that nonconcave penalties lead to significant reduction of the ""irrepresentable condition"" needed for LASSO model selection consistency. The large deviation result for martingales, bearing interests of its own, is developed for characterizing the strong oracle property. Moreover, the nonconcave regularized estimator, is shown to achieve asymptotically the information bound of the oracle estimator. A coordinate-wise algorithm is developed for finding the grid of solution paths for penalized hazard regression problems, and its performance is evaluated on simulated and gene association study examples.",WOS:000300383200011,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'SPARSITY ORACLE INEQUALITIES', 'VARIABLE SELECTION', 'DANTZIG SELECTOR', 'EXPONENTIAL INEQUALITIES', 'COORDINATE DESCENT', 'COUNTING-PROCESSES', 'LASSO', 'REGRESSION', 'MARTINGALES']",REGULARIZATION FOR COX'S PROPORTIONAL HAZARDS MODEL WITH NP-DIMENSIONALITY,2011
529,"The objective of the present paper is to introduce the concept of a spatially inhomogeneous linear inverse problem where the degree of ill-posedness of operator Q depends not only on the scale but also on location. In this case, the rates of convergence are determined by the interaction of four parameters, the smoothness and spatial homogeneity of the unknown function f and degrees of ill-posedness and spatial inhomogeneity of operator Q.
Estimators obtained in the paper are based either on wavelet-vaguelette decomposition (if the norms of all vaguelettes are finite) or on a hybrid of wavelet-vaguelette decomposition and Galerkin method (if vaguelettes in the neighborhood of the singularity point have infinite norms). The hybrid estimator is a combination of a linear part in the vicinity of the singularity point and the nonlinear block thresholding wavelet estimator elsewhere. To attain adaptivity, an optimal resolution level for the linear, singularity affected, portion of the estimator is obtained using Lepski [Theory Probab. Appl. 35 (1990) 454-466 and 36 (1991) 682-697] method and is used subsequently as the lowest resolution level for the nonlinear wavelet estimator. We show that convergence rates of the hybrid estimator lie within a logarithmic factor of the optimal minimax convergence rates.
The theory presented in the paper is supplemented by examples of deconvolution with a spatially inhomogeneous kernel and deconvolution in the presence of locally extreme noise or extremely inhomogeneous design. The first two problems are examined via a limited simulation study which demonstrates advantages of the hybrid estimator when the degree of spatial inhomogeneity is high. In addition, we apply the technique to recovery of a convolution signal transmitted via amplitude modulation.",WOS:000327746100013,ANNALS OF STATISTICS,"['NONLINEAR ESTIMATION', 'ORACLE INEQUALITIES', 'ADAPTIVE ESTIMATION', 'REGULARIZATION', 'DECONVOLUTION', 'DECOMPOSITION']",SPATIALLY INHOMOGENEOUS LINEAR INVERSE PROBLEMS WITH POSSIBLE SINGULARITIES,2013
530,"This paper illustrates the usage of the betategarch package, a package for the simulation, estimation and forecasting of Beta-Skew-t-EGARCH models. The Beta-Skew-t-EGARCH model is a dynamic model of the scale or volatility of financial returns. The model is characterised by its robustness to jumps or outliers, and by its exponential specification of volatility. The latter enables richer dynamics, since parameters need not be restricted to be positive to ensure positivity of volatility. In addition, the model also allows for heavy tails and skewness in the conditional return (i.e. scaled return), and for leverage and a time-varying long-term component in the volatility specification. More generally, the model can be viewed as a model of the scale of the error in a dynamic regression.",WOS:000330193300015,R JOURNAL,['CONDITIONAL HETEROSKEDASTICITY'],"betategarch: Simulation, estimation and forecasting of Beta-Skew-t-EGARCH Models",2013
531,"The linked micromap graphical design uses color to link each geograhic unit's name with its statistical graphic elements and map location across columns in a single row. Perceptual grouping of these rows into smaller chunks of data facilitates local focus and visual queries. Sorting the geographic units (the rows) in different ways can reveal patterns in the statistics. in the maps, and in the association between them. This design supports both exploration and communication in a multivariate geospatial context. This paper describes micromapST, an R package that implements the linked micromap graphical design specificially formatted for US state data, a common geographic unit used to display geographic patterns of health and other factors within the US. This package creates a graphic for the 51 geographic units (50 states plus DC) that fits on a single page, with states comprising the rows and state names, graphs and maps the columns. The graphical element for each state/column combination may represent a single statistical value, e.g., by a dot or horizontal bar, with or without an uncertanity measure. The distibution of values within each state, e.g., for counties, may be displayed by a boxplot. Two values per state may be represented by an arrow indicating the change in values, e.g., between two time points, or a scatter plot of the paired data. Categorical counts may be displayed as horizontal stacked bars, with optional standardization to percents or centering of the bars. Layout options include specification of the sort order for the rows, the graph/map linking colors, a vertical refernce line and others. Output may be directed to the screen but is best displayed on a pre-defined linked micromap layout specifically for the 51 US states with graphical displays of single values, data distributions, change between two values, scatter plots of paired values, time series data catagorical data, facilitates quick exploration and communication of US state data for most common data types.",WOS:000349844900001,JOURNAL OF STATISTICAL SOFTWARE,['PLOTS'],micromapST: Exploring and Communicating Geospatial Patterns in US State Data,2015
532,"There is a surge in medical follow-up studies that include longitudinal covariates in the modeling of survival data. So far, the focus has been largely on right-censored survival data. We consider survival data that are subject to both left truncation and right censoring. Left truncation is well known to produce biased sample. The sampling bias issue has been resolved in the literature for the case which involves baseline or time-varying covariates that are observable. The problem remains open, however, for the important case where longitudinal covariates are present in survival models. A joint likelihood approach has been shown in the literature to provide an effective way to overcome those difficulties for right-censored data, but this approach faces substantial additional challenges in the presence of left truncation. Here we thus propose an alternative likelihood to overcome these difficulties and show that the regression coefficient in the survival component can be estimated un-biasedly and efficiently. Issues about the bias for the longitudinal component are discussed. The new approach is illustrated numerically through simulations and data from a multi-center AIDS cohort study.",WOS:000310650900007,ANNALS OF STATISTICS,"['TO-EVENT DATA', 'DISEASE PROGRESSION', 'LIKELIHOOD APPROACH', 'MAXIMUM-LIKELIHOOD', 'FAILURE TIME', 'DRUG-USERS', 'JOINT', 'SELECTION', 'ERROR']",MODELING LEFT-TRUNCATED AND RIGHT-CENSORED SURVIVAL DATA WITH LONGITUDINAL COVARIATES,2012
533,"This paper studies the identification of the Levy jump measure of a discretely-sampled semimartingale. We define successive Blumenthal-Getoor indices of jump activity, and show that the leading index can always be identified, but that higher order indices are only identifiable if they are sufficiently close to the previous one, even if the path is fully observed. This result establishes a clear boundary on which aspects of the jump measure can be identified on the basis of discrete observations, and which cannot. We then propose an estimation procedure for the identifiable indices and compare the rates of convergence of these estimators with the optimal rates in a special parametric case, which we can compute explicitly.",WOS:000310650900006,ANNALS OF STATISTICS,"['HIGH-FREQUENCY DATA', 'NONPARAMETRIC-ESTIMATION', 'LEVY PROCESSES', 'INDEPENDENT INCREMENTS', 'JUMPS']",IDENTIFYING THE SUCCESSIVE BLUMENTHAL-GETOOR INDICES OF A DISCRETELY OBSERVED PROCESS,2012
534,"Survival estimates are an essential compliment to multivariable regression models for time-to-event data, both for prediction and illustration of covariate effects. They are easily obtained under the Cox Proportional-hazards model. In populations defined by an intial, acute event, like myocardial infarction, or in studies with long-term follow-up, the proportional hazards assumption of constant hazard ratios is frequently violated. One alternative is to fit an interaction between covariates and a prespecified function of time, implemented as a time-dependent covariate. This effectively creates a time-varying coefficient that is easily estimated in software such as SAS and R. However, the usual programming statements for survival estimation are not directly applicable. Unique data manipulation and syntax is required, but is not well documented for either software. This paper offers a tutorial in surviva; estimation for time-varying coefficient model, implemented in SAS and R. We provide a macro coxtve to facilitate estimation in SAS where the current functionality is more limited. The macro is validated in simulated data and illustrated in an application.",WOS:000349842400001,JOURNAL OF STATISTICAL SOFTWARE,,Tutorial Survival Estimation for Cox Regression Models with Time-Varying coefficients,2014
535,"Based on mixture models, we present a Bayesian method ( called BClass) to classify biological entities ( e. g. genes) when variables of quite heterogeneous nature are analyzed. Various statistical distributions are used to model the continuous/ categorical data commonly produced by genetic experiments and large-scale genomic projects. We calculate the posterior probability of each entry to belong to each element ( group) in the mixture. In this way, an original set of heterogeneous variables is transformed into a set of purely homogeneous characteristics represented by the probabilities of each entry to belong to the groups. The number of groups in the analysis is controlled dynamically by rendering the groups as ' alive' and 'dormant' depending upon the number of entities classified within them. Using standard Metropolis-Hastings and Gibbs sampling algorithms, we constructed a sampler to approximate posterior moments and grouping probabilities. Since this method does not require the definition of similarity measures, it is especially suitable for data mining and knowledge discovery in biological databases. We applied BClass to classify genes in RegulonDB, a database specialized in information about the transcriptional regulation of gene expression in the bacterium Escherichia coli. The classification obtained is consistent with current knowledge and allowed prediction of missing values for a number of genes.
BClass is object-oriented and fully programmed in Lisp-Stat. The output grouping probabilities are analyzed and interpreted using graphical ( dynamically linked plots) and query-based approaches. We discuss the advantages of using Lisp-Stat as a programming language as well as the problems we faced when the data volume increased exponentially due to the ever-growing number of genomic projects.",WOS:000232807200001,JOURNAL OF STATISTICAL SOFTWARE,['GENOME'],BClass: A Bayesian approach based on mixture models for clustering and classification of heterogeneous biological data,2005
536,"This paper investigates the asymptotic theory of the quasi-maximum exponential likelihood estimators (QMELE) for ARMA-GARCH models. Under only a fractional moment condition, the strong consistency and the asymptotic normality of the global self-weighted QMELE are obtained. Based on this self-weighted QMELE, the local QMELE is showed to be asymptotically normal for the ARMA model with GARCH (finite variance) and IGARCH errors. A formal comparison of two estimators is given for some cases. A simulation study is carried out to assess the performance of these estimators, and a real example on the world crude oil price is given.",WOS:000296995500011,ANNALS OF STATISTICS,"['ABSOLUTE DEVIATION ESTIMATION', 'TIME-SERIES MODELS', 'GARCH PROCESSES', 'CONDITIONAL HETEROSCEDASTICITY', 'INFINITE VARIANCE', 'ASYMPTOTIC THEORY', 'ARCH', 'ERRORS', 'REGRESSION']",GLOBAL SELF-WEIGHTED AND LOCAL QUASI-MAXIMUM EXPONENTIAL LIKELIHOOD ESTIMATORS FOR ARMA-GARCH/IGARCH MODELS,2011
537,"We consider asymptotic problems in spectral analysis of stationary causal processes. Limiting distributions of periodograms and smoothed periodograrn spectral density estimates are obtained and applications to the spectral domain bootstrap are given. Instead of the commonly used strong mixing conditions, in our asymptotic spectral theory we impose conditions only involving (conditional) moments, which are easily verifiable for a variety of nonlinear time series.",WOS:000249568000017,ANNALS OF STATISTICS,"['ITERATED RANDOM FUNCTIONS', 'AUTOREGRESSIVE PROCESSES', 'EMPIRICAL DISTRIBUTION', 'FOURIER COEFFICIENTS', 'STATIONARY PROCESS', 'GARCH PROCESSES', 'LIMIT-THEOREMS', 'MARKOV-CHAINS', 'PERIODOGRAM', 'MODELS']",Asymptotic spectral theory for nonlinear time series,2007
538,"The generalized order-restricted information criterion (GORIC) is a generalization of the Akaike information criterion such that it can evaluate hypotheses that take on specific, but widely applicable, forms (namely, closed convex cones) for multivariate normal linear models. It can examine the traditional hypotheses H-0: beta(1,1) = ... = beta(t,k) and H-u: beta(1,1),..., beta(t,k) and hypotheses containing simple order restrictions H-m: beta(1,1) >= ... >= beta(t,k), where any "">="" may be replaced by ""="" and m is the model/hypothesis index; with beta(h,j) the parameter for the h-th dependent variable and the j-th predictor in a t-variate regression model with k predictors (which might include the intercept). But, the GORIC can also be applied to restrictions of the form H-m: R-1 beta = r(1); R-2 beta >= r(2), with beta a vector of length tk, R-1 a c(m1) x tk matrix, r(1) a vector of length c(m1), R-2 a c(m2) x t(k) matrix, and r(2) a vector of length c(m2). It should be noted that [R-1(inverted perpendicular), R-2(inverted perpendicular)](inverted perpendicular) should be of full rank when [r(1)(inverted perpendicular), r(2)(inverted perpendicular)](inverted perpendicular) not equal 0. In practice, this implies that one cannot examine range restrictions (e. g., 0 < beta(1,1) < 2 or beta(1,2) < beta(1,1) < 2 beta(1,2)) with the GORIC. A Fortran 90 program is presented, which enables researchers to compute the GORIC for hypotheses in the context of multivariate regression models. Additionally, an R package called goric is made by Daniel Gerhard and the first author.",WOS:000324371600001,JOURNAL OF STATISTICAL SOFTWARE,,A Fortran 90 Program for the Generalized Order-Restricted Information Criterion,2013
539,Item Tree Analysis ( ITA) is an explorative method of data analysis which can be used to establish a hierarchical structure on a set of dichotomous items from a questionnaire or test. There are currently two different algorithms available to perform an ITA. We describe a computer program called ITA 2.0 which implements both of these algorithms. In addition we show with a concrete data set how the program can be used for the analysis of questionnaire data.,WOS:000240504200001,JOURNAL OF STATISTICAL SOFTWARE,"['CORRELATIONAL AGREEMENT COEFFICIENT', 'ORDERING-THEORETIC METHOD', 'HYPOTHESES']",ITA 2.0: A program for classical and inductive item tree analysis,2006
540,"The sos package provides a means to quickly and flexibly search the help pages of contributed packages, finding functions and datasets in seconds or minutes that could not be found in hours or days by any other means we know. Its findFn function accesses Jonathan Baron's R Site Search database and returns the matches in a data frame of class ""findFn"", which can be further manipulated by other sos functions to produce, for example, an Excel file that starts with a summary sheet that makes it relatively easy to prioritize alternative packages for further study. As such, it provides a very powerful way to do a literature search for functions and packages relevant to a particular topic of interest and could become virtually mandatory for authors of new packages or papers in publications such as The R Journal and the Journal of Statistical Software.",WOS:000208589800009,R JOURNAL,,sos: Searching Help Pages of R Packages,2009
541,"This paper considers a general class of nonparametric time series regression models where the regression function can be time-dependent. We establish an asymptotic theory for estimates of the time-varying regression functions. For this general class of models, an important issue in practice is to address the necessity of modeling the regression function as nonlinear and time-varying. To tackle this, we propose an information criterion and prove its selection consistency property. The results are applied to the U.S. Treasury interest rate data.",WOS:000352757100010,ANNALS OF STATISTICS,"['TESTING PARAMETER CONSTANCY', 'COEFFICIENT MODELS', 'KERNEL ESTIMATION', 'ASYMPTOTIC THEORY', 'LONGITUDINAL DATA', 'TERM STRUCTURE', 'LINEAR-MODELS', 'BANDWIDTH SELECTION', 'DENSITY-ESTIMATION', 'CONFIDENCE BANDS']",TIME-VARYING NONLINEAR REGRESSION MODELS: NONPARAMETRIC ESTIMATION AND MODEL SELECTION,2015
542,"Let nu be a vector field in a bounded open set G subset of R-d. Suppose that nu is observed with a random noise at random points X-i, i = 1,..., n, that are independent and uniformly distributed in G. The problem is to estimate the integral curve of the differential equation
dx(t)/dt = nu(x(t)), t >= 0, x(0) = x(0) epsilon G,
starting at a given point x(0) = x0 epsilon G and to develop statistical tests for the hypothesis that the integral curve reaches a specified set Gamma subset of G. We develop an estimation procedure based on a Nadaraya-Watson type kernel regression estimator, show the asymptotic normality of the estimated integral curve and derive differential and integral equations for the mean and covariance function of the limit Gaussian process. This provides a method of tracking not only the integral curve, but also the covariance matrix of its estimate. We also study the asymptotic distribution of the squared minimal distance from the integral curve to a smooth enough surface Gamma subset of G. Building upon this, we develop testing procedures for the hypothesis that the integral curve reaches Gamma.
The problems of this nature are of interest in diffusion tensor imaging, a brain imaging technique based on measuring the diffusion tensor at discrete locations in the cerebral white matter, where the diffusion of water molecules is typically anisotropic. The diffusion tensor data is used to estimate the dominant orientations of the diffusion and to track white matter fibers from the initial location following these orientations. Our approach brings more rigorous statistical tools to the analysis of this problem providing, in particular, hypothesis testing procedures that might be useful in the study of axonal connectivity of the white matter.",WOS:000249568000010,ANNALS OF STATISTICS,"['DT-MRI DATA', 'CONNECTIVITY', 'TRACTOGRAPHY', 'BRAIN', 'SPACE', 'VIVO', 'TRACKING', 'TRACT']",Integral curves of noisy vector fields and statistical problems in diffusion tensor imaging: Nonparametric kernel estimation and hypotheses testing,2007
543,"The partial least squares procedure was originally developed to estimate the slope parameter in multivariate parametric models. More recently it has gained popularity in the functional data literature. There, the partial least squares estimator of slope is either used to construct linear predictive models, or as a tool to project the data onto a one-dimensional quantity that is employed for further statistical analysis. Although the partial least squares approach is often viewed as an attractive alternative to projections onto the principal component basis, its properties are less well known than those of the latter, mainly because of its iterative nature. We develop an explicit formulation of Partial least squares for functional data, which leads to insightful results and Motivates new theory, demonstrating consistency and establishing convergence rates.",WOS:000304684900013,ANNALS OF STATISTICS,"['LINEAR-REGRESSION', 'STOCHASTIC-PROCESS', 'PLS REGRESSION', 'CLASSIFICATION', 'PREDICTION', 'GRADIENTS', 'MODELS']",METHODOLOGY AND THEORY FOR PARTIAL LEAST SQUARES APPLIED TO FUNCTIONAL DATA,2012
544,"Social scientists and statisticians often use aggregate data to predict individual-level behavior because the latter are not always available. Various statistical techniques have been developed to make inferences from one level (e.g., precinct) to another level (e.g., individual voter) that minimize errors associated with ecological inference. While ecological inference has been shown to be highly problematic in a wide array of scientific fields, many political scientists and analysis employ the techniques when studying voting patterns. Indeed, federal voting rights lawsuits now require such an analysis, yet expert reports are not consistent in which type of ecological inference is used. This is especially the case in the analysis of racially polarized voting when there are multiple candidates and multiple racial groups. The eiCompare package was developed to easily assess two of the more common ecological inference methods: EI and EI: R x C. The package facilitates a seamless comparison between these methods so that scholars and legal practitioners can easily assess the two methods and whether they produce similar or disparate findings.",WOS:000395669800007,R JOURNAL,"['REGRESSIONS', 'INDIVIDUALS']",eiCompare: Comparing Ecological Inference Estimates across EI and EI:R x C,2016
545,"Recent statistical literature has paid attention to the presentation of pairwise comparisons either from the point of view of the reference category problem in generalized linear models (GLMs) or in terms of multiple comparisons. Both schools of thought are interested in the parsimonious presentation of sufficient information to enable readers to evaluate the significance of contrasts resulting from the inclusion of qualitative variables in GLMs. These comparisons also arise when trying to interpret multinomial models where one category of the dependent variable is omitted as a reference. While considerable advances have been made, opportunities remain to improve the presentation of this information, especially in graphical form. The factorplot package provides new functions for graphically and numerically presenting results of hypothesis tests related to pairwise comparisons resulting from qualitative covariates in GLMs or coefficients in multinomial logistic regression models.",WOS:000330193300002,R JOURNAL,"['PRESENTING STATISTICAL UNCERTAINTY', 'DOSE-RESPONSE RELATIONS', 'FLOATING ABSOLUTE RISK', 'TRENDS', 'RE']",factorplot: Improving Presentation of Simple Contrasts in Generalized Linear Models,2013
546,"The ""large p, small n"" paradigm arises in microarray studies, image analysis, high throughput molecular screening, astronomy, and in many other high dimensional applications. False discovery rate (FDR) methods are useful for resolving the accompanying multiple testing problems. In cDNA microarray studies, for example, p-values may be computed for each of p genes using data from n arrays, where typically p is in the thousands and n is less than 30. For FDR methods to be valid in identifying differentially expressed genes, the p-values for the nondifferentially expressed genes must simultaneously have uniform distributions marginally. While feasible for permutation p-values, this uniformity is problematic for asymptotic based p-values since the number of p-values involved goes to infinity and intuition suggests that at least some of the p-values should behave erratically. We examine this neglected issue when n is moderately large but p is almost exponentially large relative to n. We show the somewhat surprising result that, under very general dependence structures and for both mean and median tests., the p-values are simultaneously valid. A small simulation study and data analysis are used for illustration.",WOS:000249568000005,ANNALS OF STATISTICS,"['HIGH-DIMENSIONAL MODEL', 'FALSE DISCOVERY RATES', 'GENE-EXPRESSION DATA', 'THEORETICAL-ANALYSIS', 'PARTIAL CONSISTENCY', 'NORMALIZATION', 'ESTIMATOR']","Marginal asymptotics for the ""large P, small N"" paradigm: With applications to microarray data",2007
547,"Networks or graphs can easily represent a diverse set of data sources that are characterized by interacting units or actors. Social networks, representing people who communicate with each other, are one example. Communities or clusters of highly connected actors form an essential feature in the structure of several empirical networks. Spectral clustering is a popular and computationally feasible method to discover these communities.
The stochastic blockmodel [Social Networks 5 (1983) 109-137] is a social network model with well-defined communities; each node is a member of one community. For a network generated from the Stochastic Blockmodel, we bound the number of nodes ""misclustered"" by spectral clustering. The asymptotic results in this paper are the first clustering results that allow the number of clusters in the model to grow with the number of nodes, hence the name high-dimensional.
In order to study spectral clustering under the stochastic blockmodel, we first show that under the more general latent space model, the eigenvectors of the normalized graph Laplacian asymptotically converge to the eigenvectors of a ""population"" normalized graph Laplacian. Aside from the implication for spectral clustering, this provides insight into a graph visualization technique. Our method of studying the eigenvectors of random matrices is original.",WOS:000296995500003,ANNALS OF STATISTICS,"['SOCIAL NETWORK ANALYSIS', 'COMMUNITY STRUCTURE', 'DIRECTED-GRAPHS', 'ALGORITHM', 'EIGENVECTORS', 'CONSISTENCY', 'PREDICTION', 'MODEL']",SPECTRAL CLUSTERING AND THE HIGH-DIMENSIONAL STOCHASTIC BLOCKMODEL,2011
548,"A class of estimators of the Renyi and Tsallis entropies of an unknown distribution f in R-m is presented. These estimators are based on the kth nearest-neighbor distances computed from a sample of N i.i.d. vectors with distribution f. We show that entropies of any order q, including Shannon's entropy, can be estimated consistently with minimal assumptions on f. Moreover, we show that it is straightforward to extend the nearest-neighbor method to estimate the statistical distance between two distributions using one i.i.d. sample from each.",WOS:000260554100006,ANNALS OF STATISTICS,"['NEAREST NEIGHBOR DISTANCES', 'LIMIT-THEOREMS', 'ENTROPY ESTIMATORS', 'SAMPLE ENTROPY', 'GRAPHS', 'DISTRIBUTIONS', 'ASYMPTOTICS', 'CONSISTENCY', 'FUNCTIONALS', 'STATISTICS']",A CLASS OF RENYI INFORMATION ESTIMATORS FOR MULTIDIMENSIONAL DENSITIES,2008
549,"This paper deals with recovering an unknown vector theta from the noisy data Y = A theta + sigma xi, where A is a known (m x n)-matrix and xi is a white Gaussian noise. It is assumed that n is large and A may be severely ill-posed. Therefore, in order to estimate theta, a spectral regularization method is used, and our goal is to choose its regularization parameter with the help of the data Y. For spectral regularization methods related to the so-called ordered smoothers [see Kneip Ann. Statist. 22 (1994) 835-866], we propose new penalties in the principle of empirical risk minimization. The heuristical idea behind these penalties is related to balancing excess risks. Based on this approach, we derive a sharp oracle inequality controlling the mean square risks of data-driven spectral regularization methods.",WOS:000282402800006,ANNALS OF STATISTICS,"['INVERSE PROBLEMS', 'REGULARIZATION', 'PRINCIPLE', 'RISK']",ON UNIVERSAL ORACLE INEQUALITIES RELATED TO HIGH-DIMENSIONAL LINEAR MODELS,2010
550,"This paper discusses a generalization of the Dirichlet distribution, the 'hyperdirichlet', in which various types of incomplete observations may be incorporated. It is conjugate to the multinomial distribution when some observations are censored or grouped. The hyperdirichlet R package is introduced and examples given. A number of statistical tests are performed on the example datasets, which are drawn from diverse disciplines including sports statistics, the sociology of climate change, and psephology.",WOS:000275204200001,JOURNAL OF STATISTICAL SOFTWARE,,A Generalization of the Dirichlet Distribution,2010
551,"This paper develops goodness of fit statistics that can be used to formally assess Markov random field models for spatial data, when the model distributions are discrete or continuous and potentially parametric. Test statistics are formed from generalized spatial residuals which are collected over groups of nonneighboring spatial observations, called concliques. Under a hypothesized Markov model structure, spatial residuals within each conclique are shown to be independent and identically distributed as uniform variables. The information from a series of concliques can be then pooled into goodness of fit statistics. Under some conditions, large sample distributions of these statistics are explicitly derived for testing both simple and composite hypotheses, where the latter involves additional parametric estimation steps. The distributional results are verified through simulation, and a data example illustrates the method for model assessment.",WOS:000304684900005,ANNALS OF STATISTICS,"['CONDITIONAL DISTRIBUTIONS', 'KOLMOGOROV-SMIRNOV', 'REGRESSION', 'TRANSFORMATION', 'PREDICTION', 'PARAMETERS', 'DEPENDENCE', 'RESIDUALS', 'FORECASTS']",GOODNESS OF FIT TESTS FOR A CLASS OF MARKOV RANDOM FIELD MODELS,2012
552,"In this paper, the R package DTDA for analyzing truncated data is described. This package contains tools for performing three different but related algorithms to compute the nonparametric maximum likelihood estimator of the survival function in the presence of random truncation. More precisely, the package implements the algorithms proposed by Efron and Petrosian (1999) and Shen (2008), for analyzing randomly one-sided and two-sided (i.e., doubly) truncated data. These algorithms and some recent extensions are briefly reviewed. Two real data sets are used to show how DTDA package works in practice.",WOS:000284598500001,JOURNAL OF STATISTICAL SOFTWARE,,DTDA: An R Package to Analyze Randomly Truncated Data,2010
553,"LARF is an R package that provides instrumental variable estimation of treatment effects when both the endogenous treatment and its instrument (i.e., the treatment inducement) are binary. The method (Abadie 2003) involves two steps. First, pseudo-weights are constructed from the probability of receiving the treatment inducement. By default LARF estimates the probability by a probit regression. It also provides semiparametric power series estimation of the probability and allows users to employ other external methods to estimate the probability. Second, the pseudo-weights are used to estimate the local average response function conditional on treatment and covariates. LARF provides both least squares and maximum likelihood estimates of the conditional treatment effects.",WOS:000389073600001,JOURNAL OF STATISTICAL SOFTWARE,"['IDENTIFICATION', 'MODELS']",LARF: Instrumental Variable Estimation of Causal Effects through Local Average Response Functions,2016
554,Cramer-type large deviations for means of samples from a finite population are established under weak conditions. The results are comparable to results for the so-called self-normalized large deviation for independent random variables. Cramer-type large deviations for the finite population Student t-statistic are also investigated.,WOS:000248987600008,ANNALS OF STATISTICS,"['BERRY-ESSEEN BOUNDS', 'EDGEWORTH EXPANSION', 'U-STATISTICS', 'RANDOM-VARIABLES', 'INEQUALITIES']",Cramer-type large deviations for samples from a finite population,2007
555,"rworldmap is a relatively new package available on CRAN for the mapping and visualisation of global data. The vision is to make the display of global data easier, to facilitate understanding and communication. The initial focus is on data referenced by country or grid due to the frequency of use of such data in global assessments. Tools to link data referenced by country (either name or code) to a map, and then to display the map are provided as are functions to map global gridded data. Country and gridded functions accept the same arguments to specify the nature of categories and colour and how legends are formatted. This package builds on the functionality of existing packages, particularly sp, maptools and fields. Example code is provided to produce maps, to link with the packages classInt, RColorBrewer and ncdf, and to plot examples of publicly available country and gridded data.",WOS:000208590100007,R JOURNAL,,rworldmap: A New R package for Mapping Global Data,2011
556,"Peter Hall made wide-ranging and far-reaching contributions to nonparametric modeling. He was one of the leading figures in the developments of nonparametric techniques with over 300 published papers in the field alone. This article gives a selective overview on the contributions of Peter Hall to nonparametric function estimation and modeling. The focuses are on density estimation, nonparametric regression, bandwidth selection, boundary corrections, inference under shape constraints, estimation of residual variances, analysis of wavelet estimators, multivariate regression and applications of nonparametric methods.",WOS:000384397200003,ANNALS OF STATISTICS,"['KERNEL DENSITY-ESTIMATION', 'VARYING-COEFFICIENT MODELS', 'INTEGRATED SQUARE ERROR', 'BOOTSTRAP CONFIDENCE-INTERVALS', 'GROUP-TESTING DATA', 'CROSS-VALIDATION', 'CURVE ESTIMATION', 'WAVELET METHODS', 'MONOTONICITY CONSTRAINTS', 'INTERPOLATION METHODS']",PETER HALL'S CONTRIBUTIONS TO NONPARAMETRIC FUNCTION ESTIMATION AND MODELING,2016
557,"Two-sample U-statistics are widely used in a broad range of applications, including those in the fields of biostatistics and econometrics. In this paper, we establish sharp Cramer-type moderate deviation theorems for Studentized two-sample U-statistics in a general framework, including the two-sample t-statistic and Studentized Mann Whitney test statistic as prototypical examples. In particular, a refined moderate deviation theorem with second-order accuracy is established for the two-sample t-statistic. These results extend the applicability of the existing statistical methodologies from the one-sample t-statistic to more general nonlinear statistics. Applications to two-sample large-scale multiple testing problems with false discovery rate control and the regularized bootstrap method are also discussed.",WOS:000384397200008,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'MARGINAL EMPIRICAL LIKELIHOOD', 'HIGH-DIMENSIONAL DATA', 'PERMUTATION TESTS', 'LIMIT-THEOREMS', 'T-TESTS', 'BOOTSTRAP', 'DEPENDENCE', 'RATES']",CRAMER-TYPE MODERATE DEVIATIONS FOR STUDENTIZED TWO-SAMPLE U-STATISTICS WITH APPLICATIONS,2016
558,"Clustering of data sets is a standard problem in many areas of science and engineering. The method of spectral clustering is based on embedding the data set using a kernel function, and using the top eigenvectors of the normalized Laplacian to recover the connected components. We study the performance of spectral clustering in recovering the latent labels of i.i.d. samples from a finite mixture of nonparametric distributions. The difficulty of this label recovery problem depends on the overlap between mixture components and how easily a mixture component is divided into two nonoverlapping components. When the overlap is small compared to the indivisibility of the mixture components, the principal eigenspace of the population-level normalized Laplacian operator is approximately spanned by the square-root kernelized component densities. In the finite sample setting, and under the same assumption, embedded samples from different components are approximately orthogonal with high probability when the sample size is large. As a corollary we control the fraction of samples mislabeled by spectral clustering under finite mixtures with nonparametric components.",WOS:000352757100013,ANNALS OF STATISTICS,"['OPERATORS', 'BOUNDS', 'GRAPHS']",THE GEOMETRY OF KERNELIZED SPECTRAL CLUSTERING,2015
559,"A key question in evaluation of computer models is Does the computer model adequately represent reality? A six-step process for computer model validation is set out in Bayarri et al. [Technometrics 49 (2007) 138-154] (and briefly summarized below), based on comparison of computer model runs with field data of the process being modeled. The methodology is particularly suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models; combining multiple sources of information; and being able to adapt to different, but related scenarios.
Two complications that frequently arise in practice are the need to deal with highly irregular functional data and the need to acknowledge and incorporate uncertainty in the inputs. We develop methodology to deal with both complications. A key part of the approach utilizes a wavelet representation of the functional data, applies a hierarchical version of the scalar validation methodology to the wavelet coefficients, and transforms back, to ultimately compare computer model output with field output. The generality of the methodology is only limited by the capability of a combination of computational tools and the appropriateness of decompositions of the sort (wavelets) employed here.
The methods and analyses we present are illustrated with a test bed dynamic stress analysis for a particular engineering system.",WOS:000251096100002,ANNALS OF STATISTICS,"['PREDICTION', 'SYSTEMS', 'DESIGN']",Computer model validation with functional output,2007
560,"Sightability models are binary logistic-regression models used to estimate and adjust for visibility bias in wildlife-population surveys (Steinhorst and Samuel 1989). Estimation proceeds in 2 stages: (1) Sightability trials are conducted with marked individuals, and logistic regression is used to estimate the probability of detection as a function of available covariates (e.g., visual obstruction, group size). (2) The fitted model is used to adjust counts (from future surveys) for animals that were not observed. A modified Horvitz-Thompson estimator is used to estimate abundance: counts of observed animal groups are divided by their inclusion probabilites (determined by plot-level sampling probabilities and the detection probabilities estimated from stage 1). We provide a brief historical account of the approach, clarifying and documenting suggested modifications to the variance estimators originally proposed by Steinhorst and Samuel (1989). We then introduce a new R package, SightabilityModel, for estimating abundance using this technique. Lastly, we illustrate the software with a series of examples using data collected from moose (Alcesalces) in northeastern Minnesota and mountain goats (Oreamnos americanus) in Washington State.",WOS:000312289500001,JOURNAL OF STATISTICAL SOFTWARE,"['VISIBILITY BIAS', 'AERIAL SURVEYS', 'ELK', 'WILDLIFE', 'SHEEP']",Estimating Population Abundance Using Sightability Models: R Sightability Model Package,2012
561,"This paper investigates the Gaussian quasi-likelihood estimation of an exponentially ergodic multidimensional Markov process, which is expressed as a solution to a Levy driven stochastic differential equation whose coefficients are known except for the finite-dimensional parameters to be estimated, where the diffusion coefficient may be degenerate or even null. We suppose that the process is discretely observed under the rapidly increasing experimental design with step size h(n). By means of the polynomial-type large deviation inequality, convergence of the corresponding statistical random fields is derived in a mighty mode, which especially leads to the asymptotic normality at rate root nh(n) for all the target parameters, and also to the convergence of their moments. As our Gaussian quasi-likelihood solely looks at the local-mean and local-covariance structures, efficiency loss would be large in some instances. Nevertheless, it has the practically important advantages: first, the computation of estimates does not require any fine tuning, and hence it is straightforward; second, the estimation procedure can be adopted without full specification of the Levy measure.",WOS:000323271500009,ANNALS OF STATISTICS,"['RATIO RANDOM FIELDS', 'DISCRETE OBSERVATIONS', 'FISHERS INFORMATION', 'WEAK CONVERGENCE', 'TIME-SERIES', 'ESTIMATORS', 'MODELS', 'DIFFUSIONS', 'INEQUALITIES', 'EQUATIONS']",CONVERGENCE OF GAUSSIAN QUASI-LIKELIHOOD RANDOM FIELDS FOR ERGODIC LEVY DRIVEN SDE OBSERVED AT HIGH FREQUENCY,2013
562,"Model averaging has been shown to be a useful method for incorporating model uncertainty in quantitative risk estimation. In certain circumstances this technique is computationally complex, requiring sophisticated software to carry out the computation. We introduce software that implements model averaging for risk assessment based upon dichotomous dose-response data. This software, which we call Model Averaging for Dichotomous Response Benchmark Dose (MADr-BMD), fits the quantal response models, which are also used in the US Environmental Protection Agency benchmark dose software suite, and generates a model-averaged dose response model to generate benchmark dose and benchmark dose lower bound estimates. The software fulfills a need for risk assessors, allowing them to go beyond one single model in their risk assessments based on quantal data by focusing on a set of models that describes the experimental data.",WOS:000257322700001,JOURNAL OF STATISTICAL SOFTWARE,"['SELECTION', 'CRITERION']",Model averaging software for dichotomous dose response risk estimation,2008
563,"Sparse Bayesian factor models are routinely implemented for parsimonious dependence modeling and dimensionality reduction in high-dimensional applications. We provide theoretical understanding of such Bayesian procedures in terms of posterior convergence rates in inferring high-dimensional covariance matrices where the dimension can be larger than the sample size. Under relevant sparsity assumptions on the true covariance matrix, we show that commonly-used point mass mixture priors on the factor loadings lead to consistent estimation in the operator norm even when p >> n. One of our major contributions is to develop a new class of continuous shrinkage priors and provide insights into their concentration around sparse vectors. Using such priors for the factor loadings, we obtain similar rate of convergence as obtained with point mass mixture priors. To obtain the convergence rates, we construct test functions to separate points in the space of high-dimensional covariance matrices using insights from random matrix theory; the tools developed may be of independent interest. We also derive minimax rates and show that the Bayesian posterior rates of convergence coincide with the minimax rates upto a root log n term.",WOS:000338477800010,ANNALS OF STATISTICS,"['APPROXIMATE FACTOR MODELS', 'CONVERGENCE-RATES', 'ASYMPTOTIC NORMALITY', 'VARIABLE-SELECTION', 'LINEAR-MODELS', 'DISTRIBUTIONS', 'NUMBER', 'REGRESSION', 'INFERENCE', 'INFINITY']",POSTERIOR CONTRACTION IN SPARSE BAYESIAN FACTOR MODELS FOR MASSIVE COVARIANCE MATRICES,2014
564,"Mining frequent itemsets and association rules is a popular and well researched approach for discovering interesting relationships between variables in large databases. The R package arules presented in this paper provides a basic infrastructure for creating and manipulating input data sets and for analyzing the resulting itemsets and rules. The package also includes interfaces to two fast mining algorithms, the popular C implementations of Apriori and Eclat by Christian Borgelt. These algorithms can be used to mine frequent itemsets, maximal frequent itemsets, closed frequent itemsets and association rules.",WOS:000232929000001,JOURNAL OF STATISTICAL SOFTWARE,,arules - A computational environment for mining association rules and frequent item sets,2005
565,"The existence of limiting spectral distribution (LSD) of (Gamma) over cap (u) + (Gamma) over cap (u)*, the symmetric sum of the sample autocovariance matrix (Gamma) over cap (u) of order u, is known when the observations are from an infinite dimensional vector linear process with appropriate (strong) assumptions on the coefficient matrices. Under significantly weaker conditions, we prove, in a unified way, that the LSD of any symmetric polynomial in these matrices such as (Gamma) over cap (u) + (Gamma) over cap (u)*, (Gamma) over cap (u)(Gamma) over cap (u)*, (Gamma) over cap (u)(Gamma) over cap (u)* + (Gamma) over cap (k)(Gamma) over cap (k)* exist. Our approach is through the more intuitive algebraic method of free probability in conjunction with the method of moments. Thus, we are able to provide a general description for the limits in terms of some freely independent variables. All the previous results follow as special cases. We suggest statistical uses of these LSD and related results in order determination and white noise testing.",WOS:000372594300006,ANNALS OF STATISTICS,"['DYNAMIC-FACTOR MODEL', 'TIME-SERIES', 'EMPIRICAL DISTRIBUTION', 'COVARIANCE MATRICES', 'EIGENVALUES']",LARGE SAMPLE BEHAVIOUR OF HIGH DIMENSIONAL AUTOCOVARIANCE MATRICES,2016
566,"Structured additive regression (STAR) models provide a flexible framework for modeling possible nonlinear e ff ects of covariates: They contain the well established frameworks of generalized linear models and generalized additive models as special cases but also allow a wider class of effects, e.g., for geographical or spatio-temporal data, allowing for speci fi cation of complex and realistic models. BayesX is standalone software package providing software for fi tting general class of STAR models. Based on a comprehensive open-source regression toolbox written in C++, BayesX uses Bayesian inference for estimating STAR models based on Markov chain Monte Carlo simulation techniques, a mixed model representation of STAR models, or stepwise regression techniques combining penalized least squares estimation with model selection. BayesX not only covers models for responses from univariate exponential families, but also models from less-standard regression situations such as models for multi-categorical responses with either ordered or unordered categories, continuous time survival data, or continuous time multi-state models. This paper presents a new fully interactive R interface to BayesX : the R package R2BayesX. With the new package, STAR models can be conveniently speci fi ed using R's formula language (with some extended terms), fitted using the BayesX binary, represented in R with objects of suitable classes, and fi nally printed/ summarized/ plotted. This makes BayesX much more accessible to users familiar with R and adds extensive graphics capabilities for visualizing fi tted STAR models. Furthermore, R2BayesX complements the already impressive capabilities for semiparametric regression in R by a comprehensive toolbox comprising in particular more complex response types and alternative inferential procedures such as simulation-based Bayesian inference.",WOS:000349847900001,JOURNAL OF STATISTICAL SOFTWARE,"['SPACE-TIME DATA', 'MIXED MODELS', 'INFERENCE', 'SPLINES']",Structured Additive Regression Models: An R Interface to BayesX,2015
567,"In partially linear single-index models, we obtain the semiparametrically efficient profile least-squares estimators of regression coefficients. We also employ the smoothly clipped absolute deviation penalty (SCAD) approach to simultaneously select variables and estimate regression coefficients. We show that the resulting SCAD estimators are consistent and possess the oracle property. Subsequently, we demonstrate that a proposed tuning parameter selector, BIC, identifies the true model consistently. Finally, we develop a linear hypothesis test for the parametric coefficients and a goodness-of-fit test for the nonparametric component, respectively. Monte Carlo studies are also presented.",WOS:000290231500015,ANNALS OF STATISTICS,"['SEMIPARAMETRIC ESTIMATION', 'PROFILE LIKELIHOOD', 'VARIABLE SELECTION', 'REGRESSION', 'VARIANCE']",ESTIMATION AND TESTING FOR PARTIALLY LINEAR SINGLE-INDEX MODELS,2010
568,"We consider the nonparametric functional estimation of the drift of a Gaussian process via minimax and Bayes estimators. In this context, we construct superefficient estimators of Stein type for such drifts using the Malliavin integration by parts formula and superharmonic functionals on Gaussian space. Our results are illustrated by numerical simulations and extend the construction of James-Stein type estimators for Gaussian processes by Berger and Wolpert [J. Multivariate Anal. 13 (1983) 401-424].",WOS:000260554100020,ANNALS OF STATISTICS,,STEIN ESTIMATION FOR THE DRIFT OF GAUSSIAN PROCESSES USING THE MALLIAVIN CALCULUS,2008
569,"TIGER/Line shapefiles from the United States Census Bureau are commonly used for the mapping and analysis of US demographic trends. The tigris package provides a uniform interface for R users to download and work with these shapefiles. Functions in tigris allow R users to request Census geographic datasets using familiar geographic identifiers and return those datasets as objects of class ""Spatial*DataFrame"". In turn, tigris ensures consistent and high-quality spatial data for R users' cartographic and spatial analysis projects that involve US Census data. This article provides an overview of the functionality of the tigris package, and concludes with an applied example of a geospatial workflow using data retrieved with tigris.",WOS:000395669800015,R JOURNAL,,tigris: An R Package to Access andWork with Geographic Data from the US Census Bureau,2016
570,,WOS:000343788100020,R JOURNAL,,"Statistical Software from a Blind Person's Perspective: R is the best, but we can make it better. (vol 5, pg 73, 2013)",2014
571,"Testing covariance structure is of importance in many areas of statistical analysis, such as microarray analysis and signal processing. Conventional tests for finite-dimensional covariance cannot be applied to high-dimensional data in general, and tests for high-dimensional covariance in the literature usually depend on some special structure of the matrix. In this paper, we propose some empirical likelihood ratio tests for testing whether a covariance matrix equals a given one or has a banded structure. The asymptotic distributions of the new tests are independent of the dimension.",WOS:000326991200013,ANNALS OF STATISTICS,['EMPIRICAL LIKELIHOOD'],TESTS FOR COVARIANCE MATRIX WITH FIXED OR DIVERGENT DIMENSION,2013
572,"Mediation analysis is routinely adopted by researchers from a wide range of applied disciplines as a statistical tool to disentangle the causal pathways by which an exposure or treatment affects an outcome. The counterfactual framework provides a language for clearly defining path-specific effects of interest and has fostered a principled extension of mediation analysis beyond the context of linear models. This paper describes medflex, an R package that implements some recent developments in mediation analysis embedded within the counterfactual framework. The medflex package offers a set of ready-made functions for fitting natural effect models, a novel class of causal models which directly parameterize the path-specific effects of interest, thereby adding flexibility to existing software packages for mediation analysis, in particular with respect to hypothesis testing and parsimony. In this paper, we give a comprehensive overview of the functionalities of the medflex package.",WOS:000398467000001,JOURNAL OF STATISTICAL SOFTWARE,"['SENSITIVITY-ANALYSIS', 'CAUSAL MECHANISMS', 'INFERENCE', 'IDENTIFICATION', 'EXPOSURE', 'DECOMPOSITION', 'CONFOUNDER', 'IMPUTATION', 'VARIABLES', 'FORMULA']",medflex: An R Package for Flexible Mediation Analysis using Natural Effect Models,2017
573,"Sumo is a web application intended as a template for developers. It is distributed as a Java 'war' file that deploys automatically when placed in a Servlet container's 'webapps' directory. If a user supplies proper credentials, Sumo creates a session-specific Secure Shell connection to the host and a user-specific R session over that connection. Developers may write dynamic server pages that make use of the persistent R session and user-specific file space. The supplied example plots a data set conditional on preferences indicated by the user; it also displays some static text. A companion server page allows the user to interact directly with the R session. Sumo's novel feature set complements previous efforts to supply R functionality over the internet.",WOS:000313197700009,R JOURNAL,,Sumo: An Authenticating Web Application with an Embedded R Session,2012
574,"This paper presents an R package for magnetic resonance imaging (MRI) tissue classification. The methods include using normal mixture models, hidden markov normal mixture models, and a higher resolution hidden Markov normal mixture model fitted by various optimization algorithms and by a Bayesian Markov chain Monte Carlo (MCMC) methods. Functions to obtain initial values of parameters of normal mixture models and spatial parameters are provided. Supported input formats are ANALYZE, NIfTI, and a raw byte format. the function slices3d in misc3d is used for visualizing data and results. Various performance evaluation indices are provided to evaluate classification results. To improve performance, table lookup methods are used in several places, and vectorized computation taking advantage of conditional independence properties are used. Some computations are performed by C code, and OpenMP is used to parallelize key loops in the C code.",WOS:000296718600001,JOURNAL OF STATISTICAL SOFTWARE,"['STATISTICAL-ANALYSIS', 'SEGMENTATION', 'ALGORITHM', 'IMAGES']",mritc: A Package for MRI Tissue Classification,2011
575,"Inference for partially observed Markov process models has been a long-standing methodological challenge with many scientific and engineering applications. Iterated filtering algorithms maximize the likelihood function for partially observed Markov process models by solving a recursive sequence of filtering problems. We present new theoretical results pertaining to the convergence of iterated filtering algorithms implemented via sequential Monte Carlo filters. This theory complements the growing body of empirical evidence that iterated filtering algorithms provide an effective inference strategy for scientific models of nonlinear dynamic systems. The first step in our theory involves studying a new recursive approach for maximizing the likelihood function of a latent variable model, when this likelihood is evaluated via importance sampling. This leads to the consideration of an iterated importance sampling algorithm which serves as a simple special case of iterated filtering, and may have applicability in its own right.",WOS:000293716500016,ANNALS OF STATISTICS,"['MONTE-CARLO METHODS', 'LATENT VARIABLE MODELS', 'TIME-SERIES ANALYSIS', 'STATE-SPACE MODELS', 'STOCHASTIC-APPROXIMATION', 'PARTICLE FILTERS', 'DISEASE DYNAMICS', 'INFERENCE', 'MEASLES', 'POPULATIONS']",ITERATED FILTERING,2011
576,"The Lasso is an attractive technique for regularization and variable selection for high-dimensional data, where the number of predictor variables p(n) is potentially much larger than the number of samples n. However, it was recently discovered that the sparsity pattern of the Lasso estimator can only be asymptotically identical to the true sparsity pattern if the design matrix satisfies the so-called irrepresentable condition. The latter condition can easily be violated in the presence of highly correlated variables.
Here we examine the behavior of the Lasso estimators if the irrepresentable condition is relaxed. Even though the Lasso cannot recover the correct sparsity pattern, we show that the estimator is still consistent in the l(2)-norm sense for fixed designs under conditions on (a) the number s(n) Of nonzero components of the vector beta(n) and (b) the minimal singular values of design matrices that are induced by selecting small subsets of variables. Furthermore, a rate of convergence result is obtained on the l(2) error with an appropriate choice of the smoothing parameter. The rate is shown to be optimal under the condition of bounded maximal and minimal sparse eigenvalues. Our results imply that, with high probability, all important variables are selected. The set of selected variables is a meaningful reduction on the original set of variables. Finally, our results are illustrated with the detection of closely adjacent frequencies, a problem encountered in astrophysics.",WOS:000263129000009,ANNALS OF STATISTICS,"['MODEL SELECTION', 'ADAPTIVE LASSO', 'REGRESSION', 'NOISE', 'ASYMPTOTICS', 'DANTZIG']",LASSO-TYPE RECOVERY OF SPARSE REPRESENTATIONS FOR HIGH-DIMENSIONAL DATA,2009
577,"This article presents maximum likelihood estimators (MLEs) and log-likelihood ratio (LLR) tests for the eigenvalues and eigenvectors of Gaussian random symmetric matrices of arbitrary dimension, where the observations are independent repeated samples from one or two populations. These inference problems are relevant in the analysis of diffusion tensor imaging data and polarized cosmic background radiation data, where the observations are, respectively, 3 x 3 and 2 x 2 symmetric positive definite matrices. The parameter sets involved in the inference problems for eigenvalues and eigenvectors are subsets of Euclidean space that are either affine subspaces, embedded submanifolds that are invariant under orthogonal transformations or polyhedral convex cones. We show that for a class of sets that includes the ones considered in this paper, the MLEs of the mean parameter do not depend on the covariance parameters if and only if the covariance structure is orthogonally invariant. Closed-form expressions for the MLEs and the associated LLRs are derived for this covariance structure.",WOS:000262731400011,ANNALS OF STATISTICS,"['LIKELIHOOD RATIO TESTS', 'DIFFUSION-TENSOR MRI', 'STATISTICAL-ANALYSIS', 'EXPONENTIAL FAMILIES', 'GEOMETRY', 'REGRESSION']",INFERENCE FOR EIGENVALUES AND EIGENVECTORS OF GAUSSIAN SYMMETRIC MATRICES,2008
578,"A moment bound for the normalized conditional-sum-of-squares (CSS) estimate of a general autoregressive fractionally integrated moving average (ARFIMA) model with an arbitrary unknown memory parameter is derived in this paper. To achieve this goal, a uniform moment bound for the inverse of the normalized objective function is established. An important application of these results is to establish asymptotic expressions for the one-step and multi-step mean squared prediction errors (MSPE) of the CSS predictor. These asymptotic expressions not only explicitly demonstrate how the multi-step MSPE of the CSS predictor manifests with the model complexity and the dependent structure, but also offer means to compare the performance of the CSS predictor with the least squares (LS) predictor for integrated autoregressive models. It turns out that the CSS predictor can gain substantial advantage over the LS predictor when the integration order is high. Numerical findings are also conducted to illustrate the theoretical results.",WOS:000321847600009,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'STOCHASTIC REGRESSION-MODELS', 'MOVING AVERAGE MODELS', 'AUTOREGRESSIVE PROCESSES', 'PARAMETER']",MOMENT BOUNDS AND MEAN SQUARED PREDICTION ERRORS OF LONG-MEMORY TIME SERIES,2013
579,"Graphical models are popular statistical tools which are used to represent dependent or causal complex systems. Statistically equivalent causal or directed graphical models are said to belong to a Markov equivalent class. It is of great interest to describe and understand the space of such classes. However, with currently known algorithms, sampling over such classes is only feasible for graphs with fewer than approximately 20 vertices. In this paper, we design reversible irreducible Markov chains on the space of Markov equivalent classes by proposing a perfect set of operators that determine the transitions of the Markov chain. The stationary distribution of a proposed Markov chain has a closed form and can be computed easily. Specifically, we construct a concrete perfect set of operators on sparse Markov equivalence classes by introducing appropriate conditions on each possible operator. Algorithms and their accelerated versions are provided to efficiently generate Markov chains and to explore properties of Markov equivalence classes of sparse directed acyclic graphs (DAGs) with thousands of vertices. We find experimentally that in most Markov equivalence classes of sparse DAGs, (1) most edges are directed, (2) most undirected subgraphs are small and (3) the number of these undirected subgraphs grows approximately linearly with the number of vertices.",WOS:000326991200003,ANNALS OF STATISTICS,"['BAYESIAN NETWORKS', 'MODELS', 'DIGRAPHS']",REVERSIBLE MCMC ON MARKOV EQUIVALENCE CLASSES OF SPARSE DIRECTED ACYCLIC GRAPHS,2013
580,"This article introduces Rcapture, an R package for capture-recapture experiments. The data for analysis consists of the frequencies of the observable capture histories over the t capture occasions of the experiment. A capture history is a vector of zeros and ones where one stands for a capture and zero for a miss. Rcapture can fit three types of models. With a closed population model, the goal of the analysis is to estimate the size N of the population which is assumed to be constant throughout the experiment. The estimator depends on the way in which the capture probabilities of the animals vary. Rcapture features several models for these capture probabilities that lead to different estimators for N. In an open population model, immigration and death occur between sampling periods. The estimation of survival rates is of primary interest. Rcapture can fit the basic Cormack-Jolly-Seber and Jolly-Seber model to such data. The third type of models fitted by Rcapture are robust design models. It features two levels of sampling; closed population models apply within primary periods and an open population model applies between periods. Most models in Rcapture have a loglinear form; they are fitted by carrying out a Poisson regression with the R function glm. Estimates of the demographic parameters of interest are derived from the loglinear parameter estimates; their variances are obtained by linearization. The novel feature of this package is the provision of several new options for modeling capture probabilities heterogeneity between animals in both closed population models and the primary periods of a robust design. It also implements many of the techniques developed by R. M. Cormack for open population models.",WOS:000246391600001,JOURNAL OF STATISTICAL SOFTWARE,"['MARK-RECAPTURE', 'UNEQUAL CATCHABILITY', 'SURVIVAL ESTIMATION', 'MULTINOMIAL MODELS', 'POPULATION-SIZE', 'ROBUST DESIGN', 'ESTIMATORS', 'ABUNDANCE', 'INFERENCE', 'POISSON']",Rcapture: Loglinear models for capture-recapture in R,2007
581,"We reconsider the existing kernel estimators for a copula function. as proposed in Gijbels and Mielniczuk [Comm. Statist. Theory, Methods 19 (1990) 445-464] Fermaman, Radulovic and Wegkamp [Bernoulli 10 (2004) 847-860] and Chen and Huang [Canad. J. Statist. 35 (2007) 265-282]. All of these estimators have as a drawback that they can suffer from a corner bias problem. A way to deal with this is to impose rather stringent conditions on the copula, outruling as such many classical families of copulas. In this paper, we propose improved estimators that take care of the typical corner bias problem. For Gijbels and Mielniczuk [Comm. Statist. Theory Methods 19 (1990) 445-464] and Chen and Huang [Canad. J. Statist. 35 (2007) 265-282], the improvement involves shrinking the bandwidth with ail appropriate functional factor; for Fermanian, Radulovic and Wegkamp [Bernoulli 10 (2004) 847-860], this is done by using a transformation. The theoretical contribution of the paper is a weak convergence result for the three improved estimators under conditions that are met for Most copula families. We also discuss the choice of bandwidth parameters, theoretically and practically, and illustrate the finite-sample behaviour of the estimators in a simulation Study. The improved estimators are applied to goodness-of-fit testing for copulas.",WOS:000268605000015,ANNALS OF STATISTICS,"['SEMIPARAMETRIC ESTIMATION', 'DENSITY', 'MODELS']",IMPROVED KERNEL ESTIMATION OF COPULAS: WEAK CONVERGENCE AND GOODNESS-OF-FIT TESTING,2009
582,"Frailty models are getting more and more popular to account for overdispersion and/or clustering in survival data. When the form of the baseline hazard is somehow known in advance, the parametric estimation approach can be used advantageously. Nonetheless, there is no unified widely available software that deals with the parametric frailty model. The new parfm package remedies that lack by providing a wide range of parametric frailty models in R. The gamma, inverse Gaussian, and positive stable frailty distributions can be specified, together with five different baseline hazards. Parameter estimation is done by maximising the marginal log-likelihood, with right-censored and possibly left-truncated data. In the multivariate setting, the inverse Gaussian may encounter numerical difficulties with a huge number of events in at least one cluster. The positive stable model shows analogous difficulties but an ad-hoc solution is implemented, whereas the gamma model is very resistant due to the simplicity of its Laplace transform.",WOS:000312289700001,JOURNAL OF STATISTICAL SOFTWARE,"['PENALIZED LIKELIHOOD ESTIMATION', 'SURVIVAL-DATA', 'HETEROGENEITY', 'REGRESSION', 'TRIALS']",parfm: Parametric Frailty Models in R,2012
583,"The software POPS performs inference of population genetic structure using multilocus genotypic data. Based on a hierarchical Bayesian framework for latent regression models, POPS implements algorithms that improve estimation of individual admixture proportions and cluster membership probabilities by using geographic and environmental information. In addition, POPS defines ancestry distribution models allowing its users to forecast admixture proportion and cluster membership geographic variation under changing environmental conditions. We illustrate a typical use of POPS using data for an alpine plant species, for which POPS predicts changes in spatial population structure assuming a particular scenario of climate change.",WOS:000384910300001,JOURNAL OF STATISTICAL SOFTWARE,"['CLIMATE-CHANGE', 'LANDSCAPE GENETICS', 'INDIVIDUALS', 'ASSOCIATION', 'OUTCOMES', 'PROGRAM', 'ECOLOGY']",POPS: A Software for Prediction of Population Genetic Structure Using Latent Regression Models,2015
584,"An important aspect of multiple hypothesis testing is controlling the significance level, or the level of Type I error. When the test statistics are not independent it can be particularly challenging to deal with this problem, without resorting to very conservative procedures. In this paper we show that, in the context of contemporary multiple testing problems, where the number of tests is often very large, the difficulties caused by dependence are less serious than in classical cases. This is particularly true when the null distributions of test statistics are relatively light-tailed, for example, when they can be based on Normal or Student's t approximations. There, if the test statistics can fairly be viewed as being generated by a linear process, an analysis founded on the incorrect assumption of independence is asymptotically correct as the number of hypotheses diverges. In particular, the point process representing the null distribution of the indices at which statistically significant test results occur is approximately Poisson, just as in the case of independence. The Poisson process also has the same mean as in the independence case, and of course exhibits no clustering of false discoveries. However, this result can fail if the null distributions are particularly heavy-tailed. There clusters of statistically significant results can occur, even when the null hypothesis is correct. We give an intuitive explanation for these disparate properties in light- and heavy-tailed cases, and provide rigorous theory underpinning the intuition.",WOS:000263129000012,ANNALS OF STATISTICS,"['STATISTICAL CONCLUSION VALIDITY', 'FALSE DISCOVERY RATE', 'REHABILITATION RESEARCH', 'BONFERRONI PROCEDURE', 'LARGE DEVIATION', 'STEP-DOWN', 'P-VALUES', 'TRIALS', 'ERRORS']",ROBUSTNESS OF MULTIPLE TESTING PROCEDURES AGAINST DEPENDENCE,2009
585,,WOS:000312899000008,ANNALS OF STATISTICS,['LASSO'],REJOINDER: LATENT VARIABLE GRAPHICAL MODEL SELECTION VIA CONVEX OPTIMIZATION,2012
586,"Principal component analysis (PCA) is a classical method for dimensionality reduction based oil extracting the dominant eigenvectors of the Sample covariance matrix. However, PCA is well known to behave poorly inw the ""large p, small n"" setting, in which the problem dimension p is comparable to or larger than the sample size n. This paper studies PICA ill this high-dimensional regime, but under the additional assumption that the maximal eigenvector is sparse, say, with at most k nonzero components. We consider a spiked covariance model in which a base matrix is perturbed by adding a k-sparse maximal eigenvector, and we analyze two computationally tractable methods for recovering the Support set of this maximal eigenvector, as follows: (a) a simple diagonal thresholding method. which transitions from success to failure as a function of the resealed sample size theta(dia)(n, p, k) = n/[k(2) log(p - k)]; and (b) a more sophisticated semidefinite programming (SDP) relaxation, which succeeds once the resealed sample Size theta(sdp)(n, p, k) = n/[k log(p - k)] is larger than a critical threshold. In addition, we prove that no method, including the best method which has exponential-time complexity, can Succeed in recovering the Support if the order parameter theta(sdp)(n, p, k) is below a threshold. Our results thus highlight,in interesting trade-oft between computational and statistical efficiency in high-dimensional inference.",WOS:000268605000011,ANNALS OF STATISTICS,"['COVARIANCE MATRICES', 'SELECTION', 'LASSO', 'MODEL', 'NORM']",HIGH-DIMENSIONAL ANALYSIS OF SEMIDEFINITE RELAXATIONS FOR SPARSE PRINCIPAL COMPONENTS,2009
587,"Object oriented data analysis is the statistical analysis of populations of complex objects. In the special case of functional data analysis, these data objects are curves, where standard Euclidean approaches, such as principal component analysis, have been very successful. Recent developments in medical image analysis motivate the statistical analysis of populations of more complex data objects which are elements of mildly non-Euclidean spaces, such as Lie groups and symmetric spaces, or of strongly non-Euclidean spaces, such as spaces of tree-structured data objects. These new contexts for object oriented data analysis create several potentially large new interfaces between mathematics and statistics. This point is illustrated through the careful development of a novel mathematical framework for statistical analysis of populations of tree-structured objects.",WOS:000251096100001,ANNALS OF STATISTICS,,Object oriented data analysis: Sets of trees,2007
588,"The estimation of the extremal dependence structure is spoiled by the impact of the bias, which increases with the number of observations used for the estimation. Already known in the univariate setting, the bias correction procedure is studied in this paper under the multivariate framework. New families of estimators of the stable tail dependence function are obtained. They are asymptotically unbiased versions of the empirical estimator introduced by Huang [Statistics of bivariate extremes (1992) Erasmus Univ.]. Since the new estimators have a regular behavior with respect to the number of observations, it is possible to deduce aggregated versions so that the choice of the threshold is substantially simplified. An extensive simulation study is provided as well as an application on real data.",WOS:000352757100016,ANNALS OF STATISTICS,"['TAIL DEPENDENCE', 'PICKANDS DEPENDENCE', 'REGULAR VARIATION', 'ESTIMATORS', 'INDEX', 'DISTRIBUTIONS', 'PROBABILITY', 'STATISTICS', 'MODELS']",BIAS CORRECTION IN MULTIVARIATE EXTREMES,2015
589,"The BUGS language offers a very flexible way of specifying complex statistical models for the purposes of Gibbs sampling, while its JAGS variant offers very convenient R integration via the rjags package. However, including smoothers in JAGS models can involve some quite tedious coding, especially for multivariate or adaptive smoothers. Further, if an additive smooth structure is required then some care is needed, in order to centre smooths appropriately, and to find appropriate starting values. R package mgcv implements a wide range of smoothers, all in a manner appropriate for inclusion in JAGS code, and automates centring and other smooth setup tasks. The purpose of this note is to describe an interface between mgcv and JAGS, based around an R function, jagam, which takes a generalized additive model (GAM) as specified in mgcv and automatically generates the JAGS model code and data required for inference about the model via Gibbs sampling. Although the auto-generated JAGS code can be run as is, the expectation is that the user would wish to modify it in order to add complex stochastic model components readily specified in JAGS. A simple interface is also provided for visualisation and further inference about the estimated smooth components using standard mgcv functionality. The methods described here will be un-necessarily inefficient if all that is required is fully Bayesian inference about a standard GAM, rather than the full flexibility of JAGS. In that case the BayesX package would be more efficient.",WOS:000392704800001,JOURNAL OF STATISTICAL SOFTWARE,"['MIXED MODELS', 'REGRESSION', 'SPLINE']",Just Another Gibbs Additive Modeler: Interfacing JAGS and mgcv,2016
590,"Genetic algorithms (GAs) are stochastic search algorithms inspired by the basic principles of biological evolution and natural selection. GAs simulate the evolution of living organisms, where the fittest individuals dominate over the weaker ones, by mimicking the biological mechanisms of evolution, such as selection, crossover and mutation. GAs have been successfully applied to solve optimization problems, both for continuous (whether differentiable or not) and discrete functions.
This paper describes the R package G A, a collection of general purpose functions that provide a flexible set of tools for applying a wide range of genetic algorithm methods. Several examples are discussed, ranging from mathematical functions in one and two dimensions known to be hard to optimize with standard derivative-based methods, to some selected statistical problems which require the optimization of user de fined objective functions. (This paper contains animations that can be viewed using the Adobe Acrobat PDF viewer.)",WOS:000318237000001,JOURNAL OF STATISTICAL SOFTWARE,['OPTIMIZATION'],GA: A Package for Genetic Algorithms in R,2013
591,,WOS:000352757100017,ANNALS OF STATISTICS,,"STRONG ORACLE OPTIMALITY OF FOLDED CONCAVE PENALIZED ESTIMATION (vol 42, pg 819, 2014)",2015
592,"We propose MC+, a fast, continuous, nearly unbiased and accurate method of penalized variable selection in high-dimensional linear regression. The LASSO is fast and continuous, but biased. The bias of the LASSO may prevent consistent variable selection. Subset selection is unbiased but computationally costly. The MC+ has two elements: a minimax concave penalty (MCP) and a penalized linear unbiased selection (PLUS) algorithm. The MCP provides the convexity of the penalized loss in sparse regions to the greatest extent given certain thresholds for variable selection and unbiasedness. The PLUS computes multiple exact local minimizers of a possibly nonconvex penalized loss function in a certain main branch of the graph of critical points of the penalized loss. Its output is a continuous piecewise linear path encompassing from the origin for infinite penalty to a least squares solution for zero penalty. We prove that at a universal penalty level, the MC+ has high probability of matching the signs of the unknowns, and thus correct selection, without assuming the strong irrepresentable condition required by the LASSO. This selection consistency applies to the case of p >> n, and is proved to hold for exactly the MC+ solution among possibly many local minimizers. We prove that the MC+ attains certain minimax convergence rates in probability for the estimation of regression coefficients in e, balls. We use the SURE method to derive degrees of freedom and C-p-type risk estimates for general penalized LSE, including the LASSO and MC+ estimators, and prove their unbiasedness. Based on the estimated degrees of freedom, we propose an estimator of the noise level for proper choice of the penalty level. For full rank designs and general sub-quadratic penalties, we provide necessary and sufficient conditions for the continuity of the penalized LSE. Simulation results overwhelmingly support our claim Of Superior variable selection properties and demonstrate the computational efficiency of the proposed method.",WOS:000275510800012,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'GENERALIZED LINEAR-MODELS', 'STATISTICAL ESTIMATION', 'ORACLE PROPERTIES', 'DANTZIG SELECTOR', 'ADAPTIVE LASSO', 'REGRESSION', 'SHRINKAGE', 'SPARSITY', 'LARGER']",NEARLY UNBIASED VARIABLE SELECTION UNDER MINIMAX CONCAVE PENALTY,2010
593,"We describe an R package focused on Bayesian analysis of dynamic linear models. The main features of the package are its flexibility to deal with a variety of constant or time-varying, univariate or multivariate models, and the numerically stable singular value decomposition-based algorithms used for filtering and smoothing. In addition to the examples of ""out-of-the-box"" use, we illustrate how the package can be used in advanced applications to implement a Gibbs sampler for a user-specified model.",WOS:000284004600001,JOURNAL OF STATISTICAL SOFTWARE,['STATE-SPACE MODELS'],An R Package for Dynamic Linear Models,2010
594,"This paper considers estimating a covariance matrix of p variables from n observations by either banding or tapering the sample covariance matrix, or estimating a banded version of the inverse of the covariance. We show that these estimates are consistent in the operator norm as long as (log p)/n -> 0, and obtain explicit rates. The results are uniform over some fairly natural well-conditioned families of covariance matrices. We also introduce an analogue of the Gaussian white noise model and show that if the population covariance is embeddable in that model and well-conditioned, then the banded approximations produce consistent estimates of the eigenvalues and associated eigenvectors of the covariance matrix. The results can be extended to smooth versions of banding and to non-Gaussian distributions with sufficiently short tails. A resampling approach is proposed for choosing the banding parameter in practice. This approach is illustrated numerically on both simulated and real data.",WOS:000253390000008,ANNALS OF STATISTICS,"['EIGENVALUE', 'SELECTION', 'LIMIT']",Regularized estimation of large covariance matrices,2008
595,"We consider the problem of estimating the mean vector of a p-variate normal (theta, Sigma) distribution under invariant quadratic loss, (delta - theta)'Sigma(-1) (delta - theta), when the covariance is unknown. We propose a new class of estimators that dominate the usual estimator delta(0)(X) = X. The proposed estimators of theta depend upon X and an independent Wishart matrix S with n degrees of freedom, however, S is singular almost surely when p > n. The proof of domination involves the development of some new unbiased estimators of risk for the p > n setting. We also find some relationships between the amount of domination and the magnitudes of n and p.",WOS:000321845400014,ANNALS OF STATISTICS,"['ARBITRARY QUADRATIC LOSS', 'SHRINKAGE ESTIMATION', 'WISHART DISTRIBUTION', 'MINIMAX ESTIMATORS', 'SINGULAR WISHART', 'MATRIX', 'DISTRIBUTIONS', 'VECTOR', 'VARIABLES', 'INVERSES']",IMPROVED MULTIVARIATE NORMAL MEAN ESTIMATION WITH UNKNOWN COVARIANCE WHEN p IS GREATER THAN n,2012
596,"We consider specification and inference for the stochastic scale of discretely-observed pure-jump semimartingales with locally stable Levy densities in the setting where both the time span of the data set increases, and the mesh of the observation grid decreases. The estimation is based on constructing a nonparametric estimate for the empirical Laplace transform of the stochastic scale over a given interval of time by aggregating high-frequency increments of the observed process on that time interval into a statistic we call realized Laplace transform. The realized Laplace transform depends on the activity of the driving pure-jump martingale, and we consider both cases when the latter is known or has to be inferred from the data.",WOS:000307608000022,ANNALS OF STATISTICS,"['LEVY PROCESSES', 'STABLE LAWS', 'VOLATILITY', 'PARAMETERS', 'MODELS']",REALIZED LAPLACE TRANSFORMS FOR PURE-JUMP SEMIMARTINGALES,2012
597,"maxent is a package with tools for data classification using multinomial logistic regression, also known as maximum entropy. The focus of this maximum entropy classifier is to minimize memory consumption on very large datasets, particularly sparse document-term matrices represented by the tm text mining package.",WOS:000313197700008,R JOURNAL,,maxent: An R Package for Low-memory Multinomial Logistic Regression with Support for Semi-automated Text Classification,2012
598,"Model selection and sparse recovery are two important problems for which many regularization methods have been proposed. We study the properties of regularization methods in both problems under the unified framework of regularized least squares with concave penalties. For model selection, we establish conditions under which a regularized least squares estimator enjoys a nonasymptotic property, called the weak oracle property, where the dimensionality can grow exponentially with sample size. For sparse recovery, we present a sufficient condition that ensures the recoverability of the sparsest solution. In particular, we approach both problems by considering a family of penalties that give a smooth homotopy between L(0) and L(1) penalties. We also propose the sequentially and iteratively reweighted squares (SIRS) algorithm for sparse recovery. Numerical studies support our theoretical results and demonstrate the advantage of our new methods for model selection and sparse recovery.",WOS:000271673500014,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'VARIABLE SELECTION', 'L(1) MINIMIZATION', 'ORACLE PROPERTIES', 'DANTZIG SELECTOR', 'REGRESSION', 'LASSO', 'SHRINKAGE', 'NOISE']",A UNIFIED APPROACH TO MODEL SELECTION AND SPARSE RECOVERY USING REGULARIZED LEAST SQUARES,2009
599,,WOS:000290231500016,ANNALS OF STATISTICS,,"A CLASS OF RENYI INFORMATION ESTIMATORS FOR MULTIDIMENSIONAL DENSITIES (vol 36, pg 2153, 2008)",2010
600,"PracTools is an R package with functions that compute sample sizes for various types of finite population sampling designs when totals or means are estimated. One-, two-, and three-stage designs are covered as well as allocations for stratified sampling and probability proportional to size sampling. Sample allocations can be computed that minimize the variance of an estimator subject to a budget constraint or that minimize cost subject to a precision constraint. The package also contains some specialized functions for estimating variance components and design effects. Several finite populations are included that are useful for classroom instruction.",WOS:000368551800013,R JOURNAL,"['R PACKAGE', 'MODEL']",PracTools: Computations for Design of Finite Population Samples,2015
601,"An R package BSGS is provided for the integration of Bayesian variable and sparse group selection separately proposed by Chen et al. (2011) and Chen et al. (in press) for variable selection problems, even in the cases of large p and small n. This package is designed for variable selection problems including the identification of the important groups of variables and the active variables within the important groups. This article introduces the functions in the BSGS package that can be used to perform sparse group selection as well as variable selection through simulation studies and real data.",WOS:000368551800010,R JOURNAL,['VARIABLE-SELECTION'],BSGS: Bayesian Sparse Group Selection,2015
602,"The Stata package ebalance implements entropy balancing, a multivariate reweighting method described in Hainmueller (2012) that allows users to reweight a dataset such that the covariate distributions in the reweighted data satisfy a set of specified moment conditions. This can be useful to create balanced samples in observational studies with a binary treatment where the control group data can be reweighted to match the covariate moments in the treatment group. Entropy balancing can also be used to reweight a survey sample to known characteristics from a target population.",WOS:000324371500001,JOURNAL OF STATISTICAL SOFTWARE,"['TRAINING-PROGRAMS', 'CAUSAL INFERENCE', 'PROPENSITY SCORE']",ebalance: A Stata Package for Entropy Balancing,2013
603,"This paper presents a software package designed to estimate Poole and Rosenthal W-NOMINATE scores in R. The package uses a logistic regression model to analyze political choice data, usually (though not exclusively) from a legislative setting. In contrast to other scaling methods, W-NOMINATE explicitly assumes probabilistic voting based on a spatial utility function, where the parameters of the utility function and the spatial coordinates of the legislators and the votes can all be estimated on the basis of observed voting behavior. Building on software written by Poole in Fortran, the new wnominate package in R facilitates easier data input and manipulation, generates boots trapped standard errors, and includes a new suite of graphics functions to display the results. We demonstrate the functionality of this package by conducting a natural experiment using roll calls - an experiment which is greatly simplified by the data manipulation capabilities of the wnominate package in R.",WOS:000292098400001,JOURNAL OF STATISTICAL SOFTWARE,,Scaling Roll Call Votes with wnominate in R,2011
604,"Nomograms have become very useful tools among clinicians as they provide individualized predictions based on the characteristics of the patient. For complex design survey data with survival outcome, Binder (1992) proposed methods for fitting survey-weighted Cox models, but to the best of our knowledge there is no available software to build a nomogram based on such models. This paper introduces an R package, SvyNom, to accomplish this goal and illustrates its use on a gastric cancer dataset. Validation and calibration routines are also included.",WOS:000352917200001,JOURNAL OF STATISTICAL SOFTWARE,"['PROPORTIONAL HAZARDS MODELS', 'PROSTATE-CANCER', 'REGRESSION', 'TABLES']",Building a Nomogram for Survey-Weighted Cox Models Using R,2015
605,"Methods for clustering in unsupervised learning are an important part of the statistical toolbox in numerous scientific disciplines. Tewari, Giering, and Raghunathan (2011) proposed to use so-called Gaussian mixture copula models (GMCM) for general unsupervised learning based on clustering. Li, Brown, Huang, and Bickel (2011) independently discussed a special case of these GMCMs as a novel approach to meta-analysis in high-dimensional settings. GMCMs have attractive properties which make them highly flexible and therefore interesting alternatives to other well-established methods. However, parameter estimation is hard because of intrinsic identifiability issues and intractable likelihood functions. Both aforementioned papers discuss similar expectation-maximization-like algorithms as their pseudo maximum likelihood estimation procedure. We present and discuss an improved implementation in R of both classes of GMCMs along with various alternative optimization routines to the EM algorithm. The software is freely available in the R package GMCM. The implementation is fast, general, and optimized for very large numbers of observations. We demonstrate the use of package GMCM through different applications.",WOS:000373921700001,JOURNAL OF STATISTICAL SOFTWARE,"['GENECHIP DATA', 'MICROARRAY', 'REPRODUCIBILITY', 'MECHANISMS']",GMCM: Unsupervised Clustering and Meta-Analysis Using Gaussian Mixture Copula Models,2016
606,"A set of FORTRAN subprograms is presented to compute density and cumulative distribution functions and critical values for the range ratio statistics of Dixon ( 1951, The Annals of Mathematical Statistics) These statistics are useful for detection of outliers in small samples.",WOS:000239139500001,JOURNAL OF STATISTICAL SOFTWARE,"['REJECTION', 'RULES']",Programs to compute distribution functions and critical values for extreme value ratios for outlier detection,2006
607,"In many fields of science, we observe a response variable together with a large number of potential explanatory variables, and would like to be able to discover which variables are truly associated with the response. At the same time, we need to know that the false discovery rate (FDR) the expected fraction of false discoveries among all discoveries is not too high, in order to assure the scientist that most of the discoveries are indeed true and replicable. This paper introduces the knockoff filter, a new variable selection procedure controlling the FDR in the statistical linear model whenever there are at least as many observations as variables. This method achieves exact FDR control in finite sample settings no matter the design or covariates, the number of variables in the model, or the amplitudes of the unknown regression coefficients, and does not require any knowledge of the noise level. As the name suggests, the method operates by manufacturing knockoff variables that are cheap their construction does not require any new data and are designed to mimic the correlation structure found within the existing variables, in a way that allows for accurate FDR control, beyond what is possible with permutation-based methods. The method of knockoffs is very general and flexible, and can work with a broad class of test statistics. We test the method in combination with statistics from the Lasso for sparse regression, and obtain empirical results showing that the resulting method has far more power than existing selection rules when the proportion of null variables is high.",WOS:000362697700007,ANNALS OF STATISTICS,"['CONFIDENCE-INTERVALS', 'VARIABLE SELECTION', 'DRUG-RESISTANCE', 'MODEL SELECTION', 'REGRESSION', 'LASSO', 'TESTS']",CONTROLLING THE FALSE DISCOVERY RATE VIA KNOCKOFFS,2015
608,"We provide a new approach, along with extensions, to results in two important papers of Worsley, Siegmund and coworkers closely tied to the statistical analysis of fMRI (functional magnetic resonance imaging) brain data. These papers studied approximations for the exceedence probabilities of scale and rotation space random fields, the latter playing an important role in the statistical analysis of fMRI data. The techniques used there came either from the Euler characteristic heuristic or via tube formulae, and to a large extent were carefully attuned to the specific examples of the paper.
This paper treats the same problem, but via calculations based on the so-called Gaussian kinematic formula. This allows for extensions of the Worsley-Siegmund results to a wide class of non-Gaussian cases. In addition, it allows one to obtain results for rotation space random fields in any dimension via reasonably straightforward Riemannian geometric calculations. Previously only the two-dimensional case could be covered, and then only via computer algebra.
By adopting this more structured approach to this particular problem, a solution path for other, related problems becomes clearer.",WOS:000321845400006,ANNALS OF STATISTICS,"['UNKNOWN LOCATION', 'SIGNALS', 'FMRI']",ROTATION AND SCALE SPACE RANDOM FIELDS AND THE GAUSSIAN KINEMATIC FORMULA,2012
609,"In this paper, we study the asymptotic variance of sample path averages for inhomogeneous Markov chains that evolve alternatingly according to two different 7-reversible Markov transition kernels P and Q. More specifically, our main result allows us to compare directly the asymptotic variances of two inhomogeneous Markov chains associated with different kernels Pi and Q(i), i is an element of {0, 1}, as soon as the kernels of each pair (P-0, P-1) and (Q(0), Q(1)) can be ordered in the sense of lag-one autocovariance. As an important application, we use this result for comparing different data-augmentation-type Metropolis Hastings algorithms. In particular, we compare some pseudo-marginal algorithms and propose a novel exact algorithm, referred to as the random refreshment algorithm, which is more efficient, in terms of asymptotic variance, than the Grouped Independence Metropolis Hastings algorithm and has a computational complexity that does not exceed that of the Monte Carlo Within Metropolis algorithm.",WOS:000342481700009,ANNALS OF STATISTICS,,COMPARISON OF ASYMPTOTIC VARIANCES OF INHOMOGENEOUS MARKOV CHAINS WITH APPLICATION TO MARKOV CHAIN MONTE CARLO METHODS,2014
610,"We consider the classical problem of estimating a vector mu = (mu(1,) ..., mu(n)) based on independent observations Yi similar to N(mu(i), 1), i = 1, ..., n.
Suppose mu(i), i = 1, ..., n are independent realizations from a completely unknown G. We suggest an easily computed estimator (mu) over cap, such that the ratio of its risk E((mu) over cap - mu)(2) with that of the Bayes procedure approaches 1. A related compound decision result is also obtained.
Our asymptotics is of a triangular array; that is, we allow the distribution G to depend on n. Thus, our theoretical asymptotic results are also meaningful in situations where the vector mu is sparse and the proportion of zero coordinates approaches 1.
We demonstrate the performance of our estimator in simulations, emphasizing sparse setups. In ""moderately-sparse"" situations, our procedure performs very well compared to known procedures tailored for sparse setups. It also adapts well to nonsparse situations.",WOS:000268113500003,ANNALS OF STATISTICS,,NONPARAMETRIC EMPIRICAL BAYES AND COMPOUND DECISION APPROACHES TO ESTIMATION OF A HIGH-DIMENSIONAL VECTOR OF NORMAL MEANS,2009
611,"The third special volume in the ""Foometrics in R"" series of the Journal of Statistical Software collects a number of contributions describing statistical methodology and corresponding implementations related to ecology and ecological modelling. The scope of the papers ranges from theoretical ecology and ecological modelling to statistical methodology relevant for data analyses in ecological applications.",WOS:000252429500001,JOURNAL OF STATISTICAL SOFTWARE,['STAT'],"Introduction to the special volume on ""ecology and ecological modelling in R""",2007
612,"Gifi was the nom de plume for a group of researchers led by Jan de Leeuw at the University of Leiden. Between 1970 and 1990 the group produced a stream of theoretical papers and computer programs in the area of nonlinear multivariate analysis that were very innovative. In an informal way this paper discusses the so-called Gifi system of nonlinear multivariate analysis, that entails homogeneity analysis (which is closely related to multiple correspondence analysis) and generalizations. The history is discussed, giving attention to the scientific philosophy of this group, and links to machine learning are indicated.",WOS:000389126800001,JOURNAL OF STATISTICAL SOFTWARE,,Looking Back at the Gifi System of Nonlinear Multivariate Analysis,2016
613,"Heavy-tailed high-dimensional data are commonly encountered in various scientific fields and pose great challenges to modern statistical analysis. A natural procedure to address this problem is to use penalized quantile regression with weighted L-1-penalty, called weighted robust Lasso (WR-Lasso), in which weights are introduced to ameliorate the bias problem induced by the L-1-penalty. In the ultra-high dimensional setting, where the dimensionality can grow exponentially with the sample size, we investigate the model selection oracle property and establish the asymptotic normality of the WR-Lasso. We show that only mild conditions on the model error distribution are needed. Our theoretical results also reveal that adaptive choice of the weight vector is essential for the WR-Lasso to enjoy these nice asymptotic properties. To make the WR-Lasso practically feasible, we propose a two-step procedure, called adaptive robust Lasso (AR-Lasso), in which the weight vector in the second step is constructed based on the L-1-penalized quantile regression estimate from the first step. This two-step procedure is justified theoretically to possess the oracle property and the asymptotic normality. Numerical studies demonstrate the favorable finite-sample performance of the AR-Lasso.",WOS:000334256100013,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'QUANTILE REGRESSION', 'MODEL SELECTION', 'ORACLE PROPERTIES', 'DANTZIG SELECTOR', 'QUASI-LIKELIHOOD', 'LASSO', 'SHRINKAGE']",ADAPTIVE ROBUST VARIABLE SELECTION,2014
614,This paper proposes new tests of conditional independence of two random variables given a single-index involving an unknown finite-dimensional parameter. The tests employ Rosenblatt transforms and are shown to be distribution-free while retaining computational convenience. Some results from Monte Carlo simulations are presented and discussed.,WOS:000271673700011,ANNALS OF STATISTICS,"['CENTRAL-LIMIT-THEOREM', 'SERIAL INDEPENDENCE', 'NONPARAMETRIC TEST', 'REGRESSION', 'ENTROPY', 'BOOTSTRAP', 'MODELS', 'CHECKS']",TESTING CONDITIONAL INDEPENDENCE VIA ROSENBLATT TRANSFORMS,2009
615,"We investigate sampling laws for particle algorithms and the influence of these laws on the efficiency of particle approximations of marginal likelihoods in hidden Markov models. Among a broad class of candidates we characterize the essentially unique family of particle system transition kernels which is optimal with respect to an asymptotic-in-time variance growth rate criterion. The sampling structure of the algorithm defined by these optimal transitions turns out to be only subtly different from standard algorithms and yet the fluctuation properties of the estimates it provides can be dramatically different. The structure of the optimal transition suggests a new class of algorithms, which we term ""twisted"" particle filters and which we validate with asymptotic analysis of a more traditional nature, in the regime where the number of particles tends to infinity.",WOS:000334256100005,ANNALS OF STATISTICS,"['MONTE-CARLO METHODS', 'CENTRAL-LIMIT-THEOREM', 'NONLINEAR FILTERS', 'LARGE DEVIATIONS', 'MARKOV-CHAINS', 'STABILITY', 'APPROXIMATION', 'ALGORITHMS', 'SIMULATION', 'SIGNALS']",TWISTED PARTICLE FILTERS,2014
616,"Let {X-i}(i)(infinity)=-infinity be a sequence of random vectors and Y-in= f(in)(X-i,X-l) be zero mean block-variables where X-i,X-l = (X-i,X-...,X- Xi+l- 1), i >= 1, are overlapping blocks of length e and where fin are Borel measurable functions. This paper establishes valid joint asymptotic expansions of general orders for the joint distribution of the sums Sigma(n)(i=1) X-i and Sigma(n)(i-1) Y-in under weak dependence conditions on the sequence {X-i}(i=-infinity)(infinity) when the block length l grows to infinity. In contrast to the classical Edgeworth expansion results where the terms in the expansions are given by powers of n(-1/2), the expansions derived here are mixtures of two series, one in powers of n(-1/2) and the other in powers of [n/l](-1/2). Applications of the main results to (i) expansions for 7 Studentized statistics of time series data and (ii) second order correctness of the blocks of blocks bootstrap method are given.",WOS:000248692700015,ANNALS OF STATISTICS,"['CENTRAL LIMIT-THEOREM', 'EDGEWORTH EXPANSION', 'STATIONARY OBSERVATIONS', 'RANDOM VECTORS', 'BOOTSTRAP', 'CONVERGENCE', 'STATISTICS', 'SEQUENCE']",Asymptotic expansions for sums of block-variables under weak dependence,2007
617,"This paper discusses a Fortran 90 program referred to as BIEMS (Bayesian inequality and equality constrained model selection) that can be used for calculating Bayes factors of multivariate normal linear models with equality and/or inequality constraints between the model parameters versus a model containing no constraints, which is referred to as the unconstrained model. The prior that is used under the unconstrained model is the conjugate expected-constrained posterior prior and the prior under the constrained model is proportional to the unconstrained prior truncated in the constrained space. This results in Bayes factors that appropriately balance between model fit and complexity for a broad class of constrained models. When the set of equality and/or inequality constraints in the model represents a hypothesis that applied researchers have in, for instance, (M)AN(C)OVA, (multivariate) regression, or repeated measurements, the obtained Bayes factor can be used to determine how much evidence is provided by the data in favor of the hypothesis in comparison to the unconstrained model. If several hypotheses are under investigation, the Bayes factors between the constrained models can be calculated using the obtained Bayes factors from BIEMS. Furthermore, posterior model probabilities of constrained models are provided which allows the user to compare the models directly with each other.",WOS:000301068800001,JOURNAL OF STATISTICAL SOFTWARE,['SELECTION'],BIEMS: A Fortran 90 Program for Calculating Bayes Factors for Inequality and Equality Constrained Models,2012
618,"Principal Component Analysis (PCA) is an important tool of dimension reduction especially when the dimension (or the number of variables) is very high. Asymptotic studies where the sample size is fixed, and the dimension grows [i.e., High Dimension, Low Sample Size (HDLSS)] are becoming increasingly relevant. We investigate the asymptotic behavior of the Principal Component (PC) directions. HDLSS asymptotics are used to study consistency, strong inconsistency and subspace consistency. We show that if the first few eigenvalues of a population covariance matrix are large enough compared to the others, then the corresponding estimated PC directions are consistent or converge to the appropriate subspace (subspace consistency) and most other PC directions are strongly inconsistent. Broad sets of sufficient conditions for each of these cases are specified and the main theorem gives a catalogue of possible combinations. In preparation for these results, we show that the geometric representation of HDLSS data holds under general conditions, which includes a rho-mixing condition and a broad range of sphericity measures of the covariance matrix.",WOS:000271673700014,ANNALS OF STATISTICS,"['GEOMETRIC REPRESENTATION', 'COVARIANCE MATRICES', 'LARGEST EIGENVALUE']","PCA CONSISTENCY IN HIGH DIMENSION, LOW SAMPLE SIZE CONTEXT",2009
619,,WOS:000344632400015,ANNALS OF STATISTICS,,"A SIGNIFICANCE TEST FOR THE LASSO (vol 42, pg 518, 2014)",2014
620,"In this paper we consider a novel statistical inverse problem on the Poincare, or Lobachevsky, upper (complex) half plane. Here the Riemannian structure is hyperbolic and a transitive group action comes from the space of 2 x 2 real matrices of determinant one via Mobius transformations. Our approach is based on a deconvolution technique which relies on the Helgason-Fourier calculus adapted to this hyperbolic space. This gives a minimax nonparametric density estimator of a hyperbolic density that is corrupted by a random Mains transform. A motivation for this work comes from the reconstruction of impedances of capacitors where the above scenario on the Poincare plane exactly describes the physical system that is of statistical interest.",WOS:000280359400017,ANNALS OF STATISTICS,"['STATISTICAL INVERSE PROBLEMS', 'EXTRINSIC SAMPLE MEANS', 'NONPARAMETRIC DECONVOLUTION', 'PARAMETER', 'MANIFOLDS', 'SELECTION', 'REGULARIZATION', 'CONVERGENCE', 'RATES']",MOBIUS DECONVOLUTION ON THE HYPERBOLIC PLANE WITH APPLICATION TO IMPEDANCE DENSITY ESTIMATION,2010
621,"The R2WinBUGS package provides convenient functions to call WinBUGS from R. It automatically writes the data and scripts in a format readable by WinBUGS for processing in batch mode, which is possible since version 1.4. After the WinBUGS process has finished, it is possible either to read the resulting data into R by the package itself-which gives a compact graphical summary of inference and convergence diagnostics-or to use the facilities of the coda package for further analyses of the output. Examples are given to demonstrate the usage of this package.",WOS:000232806500001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION', 'EXPOSURE']",R2WinBUGS: A package for running WinBUGS from R,2005
622,"We consider parallel computation for Gaussian process calculations to overcome computational and memory constraints on the size of datasets that can be analyzed. Using a hybrid parallelization approach that uses both threading (shared memory) and message-passing (distributed memory), we implement the core linear algebra operations used in spatial statistics and Gaussian process regression in an R package called bigGP that relies on C and MPI. The approach divides the covariance matrix into blocks such that the computational load is balanced across processes while communication between processes is limited. The package provides an API enabling R programmers to implement Gaussian process-based methods by using the distributed linear algebra operations without any C or MPI coding. We illustrate the approach and software by analyzing an astrophysics dataset with n = 67; 275 observations.",WOS:000349846200001,JOURNAL OF STATISTICAL SOFTWARE,"['LARGE SPATIAL DATASETS', 'COMPUTER-MODELS', 'LINEAR ALGEBRA', 'DATA SETS', 'SYSTEMS']",Parallelizing Gaussian Process Calculations in R,2015
623,"The sample frequency spectrum (SFS) is a widely-used summary statistic of genomic variation in a sample of homologous DNA sequences. It provides a highly efficient dimensional reduction of large-scale population genomic data and its mathematical dependence on the underlying population demography is well understood, thus enabling the development of efficient inference algorithms. However, it has been recently shown that very different population demographies can actually generate the same SFS for arbitrarily large sample sizes. Although in principle this nonidentifiability issue poses a thorny challenge to statistical inference, the population size functions involved in the counterexamples are arguably not so biologically realistic. Here, we revisit this problem and examine the identifiability of demographic models under the restriction that the population sizes are piecewise-defined where each piece belongs to some family of biologically-motivated functions. Under this assumption, we prove that the expected SFS of a sample uniquely determines the underlying demographic model, provided that the sample is sufficiently large. We obtain a general bound on the sample size sufficient for identifiability; the bound depends on the number of pieces in the demographic model and also on the type of population size function in each piece. In the cases of piecewise-constant, piecewise-exponential and piecewise-generalized-exponential models, which are often assumed in population genomic inferences, we provide explicit formulas for the bounds as simple functions of the number of pieces. Lastly, we obtain analogous results for the ""folded"" SFS, which is often used when there is ambiguity as to which allelic type is ancestral. Our results are proved using a generalization of Descartes' rule of signs for polynomials to the Laplace transform of piecewise continuous functions.",WOS:000345884900011,ANNALS OF STATISTICS,"['ALLELE FREQUENCY-SPECTRUM', 'GENETIC-VARIATION', 'GROWTH', 'INFERENCE', 'HISTORY', 'VARIANTS', 'IMPACT', 'STRATIFICATION', 'DISTRIBUTIONS', 'ASSOCIATION']",DESCARTES' RULE OF SIGNS AND THE IDENTIFIABILITY OF POPULATION DEMOGRAPHIC MODELS FROM GENOMIC VARIATION DATA,2014
624,"The problem of comparing a new solution method against existing ones to find statistically significant differences arises very often in sciences and engineering. When the problem instance being solved is defined by several parameters, assessing a number of methods with respect to many problem configurations simultaneously becomes a hard task. Some visualization technique is required for presenting a large number of statistical significance results in an easily interpretable way. Here we review an existing color-based approach called Statistical Ranking Color Scheme (SRCS) for displaying the results of multiple pairwise statistical comparisons between several methods assessed separately on a number of problem configurations. We introduce an R package implementing SRCS, which performs all the pairwise statistical tests from user data and generates customizable plots. We demonstrate its applicability on two examples from the areas of dynamic optimization and machine learning, in which several algorithms are compared on many problem instances, each defined by a combination of parameters.",WOS:000368551800008,R JOURNAL,"['ALGORITHMS', 'TESTS', 'EVOLUTIONARY', 'INTELLIGENCE']",SRCS: Statistical Ranking Color Scheme for Visualizing Parameterized Multiple Pairwise Comparisons with R,2015
625,"In this paper we provide a short tutorial illustrating the new functions in the package ggm that deal with ancestral, summary and ribbonless graphs. These are mixed graphs (containing three types of edges) that are important because they capture the modified independence structure after marginalisation over, and conditioning on, nodes of directed acyclic graphs. We provide functions to verify whether a mixed graph implies that A is independent of B given C for any disjoint sets of nodes and to generate maximal graphs inducing the same independence structure of non-maximal graphs. Finally, we provide functions to decide on the Markov equivalence of two graphs with the same node set but different types of edges.",WOS:000313198000009,R JOURNAL,,Graphical Markov Models with Mixed Graphs in R,2012
626,"The ability to remove a large amount of noise and the ability to preserve most structure are desirable properties of an image smoother. Unfortunately, they usually seem to be at odds with each other; one can only improve one property at the cost of the other. By combining M-smoothing and least-squares-trimming, the TM-smoother is introduced as a means to unify corner-preserving properties and outlier robustness. To identify edge- and corner-preserving properties, a new theory based on differential geometry is developed. Further, robustness concepts are transferred to image processing. In two examples, the TM-smoother outperforms other comer-preserving smoothers. A software package containing both the TM- and the M-smoother can be downloaded from the Internet.",WOS:000247498100007,ANNALS OF STATISTICS,"['REGRESSION', 'ESTIMATORS', 'SMOOTHERS']",Outlier robust corner-preserving methods for reconstructing noisy images,2007
627,This article suggests an implementation of the compendium concept by combining Sweave and the LAT(E)X literate programming environment DOCSTRIP.,WOS:000208590200004,R JOURNAL,,Implementing the Compendium Concept with Sweave and DOCSTRIP,2011
628,"A stochastic algorithm for the recursive approximation of the location theta of a maximum of a regression function was introduced by Kiefer and Wolfowitz [Ann. Math. Statist. 23 (1952) 462-466] in the univariate framework, and by Blum [Ann. Math. Statist. 25 (1954) 737-744] in the multivariate case. The aim of this paper is to provide a companion algorithm to the Kiefer-Wolfowitz-Blum algorithm, which allows one to simultaneously recursively approximate the size p of the maximum of the regression function. A precise study of the joint weak convergence rate of both algorithms is given; it turns out that, unlike the location of the maximum, the size of the maximum can be approximated by an algorithm which converges at the parametric rate. Moreover, averaging leads to an asymptotically efficient algorithm for the approximation of the couple (theta, mu).",WOS:000249568000016,ANNALS OF STATISTICS,"['REGULARLY VARYING SEQUENCES', 'CONVERGENCE', 'MINIMA']",A companion for the Kiefer-Wolfowitz-Blum stochastic approximation algorithm,2007
629,"We introduce a principal support vector machine (PSVM) approach that can be used for both linear and nonlinear sufficient dimension reduction. The basic idea is to divide the response variables into slices and use a modified form of support vector machine to find the optimal hyperplanes that separate them. These optimal hyperplanes are then aligned by the principal components of their normal vectors. It is proved that the aligned normal vectors provide an unbiased, root n-consistent, and asymptotically normal estimator of the sufficient dimension reduction space. The method is then generalized to nonlinear sufficient dimension reduction using the reproducing kernel Hilbert space. In that context, the aligned normal vectors become functions and it is proved that they are unbiased in the sense that they are functions of the true nonlinear sufficient predictors. We compare PSVM with other sufficient dimension reduction methods by simulation and in real data analysis, and through both comparisons firmly establish its practical advantages.",WOS:000311639700002,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'VARIABLE SELECTION', 'EQUATIONS', 'MATRIX']",PRINCIPAL SUPPORT VECTOR MACHINES FOR LINEAR AND NONLINEAR SUFFICIENT DIMENSION REDUCTION,2011
630,"The problem of constructing confidence sets in the high-dimensional linear model with n response variables and p parameters, possibly p >= n, is considered. Full honest adaptive inference is possible if the rate of sparse estimation does not exceed n(-1/4), otherwise sparse adaptive confidence sets exist only over strict subsets of the parameter spaces for which sparse estimators exist. Necessary and sufficient conditions for the existence of confidence sets that adapt to a fixed sparsity level of the parameter vector are given in terms of minimal l(2)-separation conditions on the parameter space. The design conditions cover common coherence assumptions used in models for sparsity, including (possibly correlated) sub-Gaussian designs.",WOS:000330204900006,ANNALS OF STATISTICS,"['GAUSSIAN REGRESSION', 'DANTZIG SELECTOR', 'ESTIMATORS', 'BALLS', 'BANDS']",CONFIDENCE SETS IN SPARSE REGRESSION,2013
631,"This paper demonstrates how state space models can be fitted in EViews. We first briefly introduce EViews as an econometric software package. Next we fit a local level model to the Nile data. We then show how a multivariate ""latent risk"" model can be developed, making use of the EViews programming environment. We conclude by summarizing the possibilities and limitations of the software package when it comes to state space modeling.",WOS:000290527400001,JOURNAL OF STATISTICAL SOFTWARE,['TRAFFIC SAFETY'],Fitting State Space Models with EViews,2011
632,"This paper describes the implementation of Heckman-type sample selection models in R. We discuss the sample selection problem as well as the Heckman solution to it, and argue that although modern econometrics has non- and semiparametric estimation methods in its toolbox, Heckman models are an integral part of the modern applied analysis and econometrics syllabus. We describe the implementation of these models in the package sampleSelection and illustrate the usage of the package on several simulation and real data examples. Our examples demonstrate the effect of exclusion restrictions, identification at infinity and misspecification. We argue that the package can be used both in applied research and teaching.",WOS:000258206600001,JOURNAL OF STATISTICAL SOFTWARE,"['SPECIFICATION ERROR', 'BIAS']",Sample selection models in R: Package sampleSelection,2008
633,"Population quantiles and their functions are important parameters in many applications. For example, the lower quantiles often serve as crucial quality indices for forestry products. Given several independent samples from populations satisfying the density ratio model, we investigate the properties of empirical likelihood (EL) based inferences. The induced EL quantile estimators are shown to admit a Bahadur representation that leads to asymptotically valid confidence intervals for functions of quantiles. We rigorously prove that EL quantiles based on all the samples are more efficient than empirical quantiles based on individual samples. A simulation study shows that the EL quantiles and their functions have superior performance when the density ratio model assumption is satisfied and when it is mildly violated. An example is used to demonstrate the new method and the potential cost savings.",WOS:000323271500011,ANNALS OF STATISTICS,"['GOODNESS-OF-FIT', 'EMPIRICAL LIKELIHOOD', 'SAMPLE QUANTILES', 'REPRESENTATION']",QUANTILE AND QUANTILE-FUNCTION ESTIMATIONS UNDER DENSITY RATIO MODEL,2013
634,"Currently, a part of the R statistical software is developed in order to deal with spatial models. More specifically, some available packages allow the user to analyse categorical spatial random patterns. However, only the spMC package considers a viewpoint based on transition probabilities between locations. Through the use of this package it is possible to analyse the spatial variability of data, make inference, predict and simulate the categorical classes in unobserved sites. An example is presented by analysing the well-known Swiss Jura data set.",WOS:000330193300003,R JOURNAL,"['MAXIMUM-ENTROPY APPROACH', 'CATEGORICAL VARIABLES', 'ALGORITHM', 'GEOSTATISTICS', 'PREDICTION']",spMC: Modelling Spatial Random Fields with Continuous Lag Markov Chains,2013
635,Bernstein-von Mises theorems for nonparametric Bayes priors in the Gaussian white noise model are proved. It is demonstrated how such results justify Bayes methods as efficient frequentist inference procedures in a variety of concrete nonparametric problems. Particularly Bayesian credible sets are constructed that have asymptotically exact 1 - alpha frequentist coverage level and whose L-2-diameter shrinks at the minimax rate of convergence (within logarithmic factors) over Holder balls. Other applications include general classes of linear and nonlinear functionals and credible bands for auto-convolutions. The assumptions cover nonconjugate product priors defined on general orthonormal bases of L-2 satisfying weak conditions.,WOS:000326991200011,ANNALS OF STATISTICS,"['POSTERIOR DISTRIBUTIONS', 'DENSITY ESTIMATORS', 'EXPONENTIAL-FAMILIES', 'ASYMPTOTIC NORMALITY', 'CONVERGENCE-RATES', 'LIMIT-THEOREMS', 'PRIORS', 'CONTRACTION', 'FUNCTIONALS', 'PARAMETERS']",NONPARAMETRIC BERNSTEIN-VON MISES THEOREMS IN GAUSSIAN WHITE NOISE,2013
636,"The Journal of Statistical Software was founded by Jan de Leeuw in 1996, the year before the Comprehensive R Archive Network (CRAN) first made R and contributed R packages widely available on the Internet. Within a few years, R came increasingly to dominate contributions to JSS. We trace the continuing development of R and CRAN, and the representation of R and other statistical software in the pages of JSS.",WOS:000389126600001,JOURNAL OF STATISTICAL SOFTWARE,,R and the Journal of Statistical Software,2016
637,"We consider high-dimensional generalized linear models with Lipschitz loss functions, and prove a nonasymptotic oracle inequality for the empirical risk minimizer with Lasso penalty. The penalty is based on the coefficients in the linear predictor, after normalization with the empirical norm. The examples include logistic regression, density estimation and classification with hinge loss. Least squares regression is also discussed.",WOS:000254502700005,ANNALS OF STATISTICS,"['LARGE UNDERDETERMINED SYSTEMS', 'EMPIRICAL PROCESSES', 'SELECTION', 'INEQUALITIES', 'AGGREGATION', 'CLASSIFIERS', 'EQUATIONS', 'SPARSITY']",High-dimensional generalized linear models and the lasso,2008
638,"We describe, in the detection of multi-sample aligned sparse signals, the critical boundary separating detectable from nondetectable signals, and construct tests that achieve optimal detectability: penalized versions of the Berk-Jones and the higher-criticism test statistics evaluated over pooled scans, and an average likelihood ratio over the critical boundary. We show in our results an inter-play between the scale of the sequence length to signal length ratio, and the sparseness of the signals. In particular the difficulty of the detection problem is not noticeably affected unless this ratio grows exponentially with the number of sequences. We also recover the multiscale and sparse mixture testing problems as illustrative special cases.",WOS:000362697700001,ANNALS OF STATISTICS,"['OF-FIT TESTS', 'HIGHER CRITICISM', 'MIXTURES', 'STATISTICS', 'DISCOVERY', 'SEQUENCES', 'NUMBER', 'POINT', 'SCAN']",OPTIMAL DETECTION OF MULTI-SAMPLE ALIGNED SPARSE SIGNALS,2015
639,"We introduce a useful tool for analyzing boosting algorithms called the ""smooth margin function,"" a differentiable approximation of the usual margin for boosting algorithms. We present two boosting algorithms based on this smooth margin, ""coordinate ascent boosting"" and ""approximate coordinate ascent boosting,"" which are similar to Freund and Schapire's AdaBoost algorithm and Breiman's arc-gv algorithm. We give convergence rates to the maximum margin solution for both of our algorithms and for arc-gv. We then study AdaBoost's convergence properties using the smooth margin function. We precisely bound the margin attained by AdaBoost when the edges of the weak classifiers fall within a specified range. This shows that a previous bound proved by Ratsch and Warmuth is exactly tight. Furthermore, we use the smooth margin to capture explicit properties of AdaBoost in cases where cyclic behavior occurs.",WOS:000253077800021,ANNALS OF STATISTICS,"['LOGISTIC-REGRESSION', 'ADABOOST', 'CONVERGENCE']",Analysis of boosting algorithms using the smooth margin function,2007
640,"etasFLP is an R package which fits an epidemic type aftershock sequence (ETAS) model to an earthquake catalog; non-parametric background seismicity can be estimated through a forward predictive likelihood approach, while parametric components of triggered seismicity are estimated through maximum likelihood; estimation steps are alternated until convergence is obtained and for each event the probability of being a background event is estimated. The package includes options which allow its wide use. Methods for plot, summary and profile are defined for the main output class object. The paper provides examples of the package's use with description of the underlying R and Fortran routines.",WOS:000392705800001,JOURNAL OF STATISTICAL SOFTWARE,"['TIME POINT-PROCESSES', 'SEISMIC CATALOGS', 'PROCESS MODELS', 'OCCURRENCES', 'DIAGNOSTICS']",Mixed Non-Parametric and Parametric Estimation Techniques in R Package etasFLP for Earthquakes' Description,2017
641,"Outlying data can heavily influence standard clustering methods. At the same time, clustering principles can be useful when robustifying statistical procedures. These two reasons motivate the development of feasible robust model-based clustering approaches. With this in mind, an R package for performing non-hierarchical robust clustering, called tclust, is presented here. Instead of trying to ""fit"" noisy data, a proportion alpha of the most outlying observations is trimmed. The tclust package efficiently handles different cluster scatter constraints. Graphical exploratory tools are also provided to help the user make sensible choices for the trimming proportion as well as the number of clusters to search for.",WOS:000305065300001,JOURNAL OF STATISTICAL SOFTWARE,"['COVARIANCE DETERMINANT ESTIMATOR', 'K-MEANS', 'ALGORITHM', 'MIXTURES']",tclust: An R Package for a Trimming Approach to Cluster Analysis,2012
642,A sequence of null hypotheses regarding the number of negligible effects (zero effects) in orthogonal saturated designs is formulated. Two step-up simultaneous testing procedures are proposed to identify active effects (nonzero effects) under the commonly used assumption of effect sparsity. It is shown that each procedure controls the experimentwise error rate at a given alpha level in the strong sense.,WOS:000247498100017,ANNALS OF STATISTICS,"['FACTORIALS', 'ERROR']",Step-up simultaneous tests for identifying active effects in orthogonal saturated designs,2007
643,"It is well-known that Wilcoxon procedures out perform least squares procedures when the data deviate from normality and/or contain outliers. These procedures can be generalized by introducing weights; yielding so-called weighted Wilcoxon (WW) techniques. In this paper we demonstrate how WW-estimates can be calculated using an L-1 regression routine. More importantly, we present a collection of functions that can be used to implement a robust analysis of a linear model based on WW-estimates. For instance, estimation, tests of linear hypotheses, residual analyses, and diagnostics to detect differences in fits for various weighting schemes are discussed. We analyze a regression model, designed experiment, and autoregressive time series model for the sake of illustration. We have chosen to implement the suite of functions using the R statistical software package. Because R is freely available and runs on multiple platforms, WW-estimation and associated inference is now universally accessible.",WOS:000232891100001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION', 'DISPERSION', 'OUTLIERS', 'ROBUST', 'FITS']",Rank-based analyses of linear models using R,2005
644,"The R package sns implements the stochastic Newton sampler (SNS), a Metropolis-Hastings Markov chain Monte Carlo (MCMC) algorithm where the proposal density function is a multivariate Gaussian based on a local, second-order Taylor-series expansion of log-density. The mean of the proposal function is the full Newton step in the Newton-Raphson optimization algorithm. Taking advantage of the local, multivariate geometry captured in log-density Hessian allows SNS to be more efficient than univariate samplers, approaching independent sampling as the density function increasingly resembles a multivariate Gaussian. SNS requires the log-density Hessian to be negative-definite everywhere in order to construct a valid proposal function. This property holds, or can be easily checked, for many GLM-like models. When the initial point is far from density peak, running SNS in non-stochastic mode by taking the Newton step - augmented with line search - allows the MCMC chain to converge to high-density areas faster. For high-dimensional problems, partitioning the state space into lower-dimensional subsets, and applying SNS to the subsets within a Gibbs sampling framework can significantly improve the mixing of SNS chains. In addition to the above strategies for improving convergence and mixing, sns offers utilities for diagnostics and visualization, sample-based calculation of Bayesian predictive posterior distributions, numerical differentiation, and log-density validation.",WOS:000392512500001,JOURNAL OF STATISTICAL SOFTWARE,"['HAMILTONIAN MONTE-CARLO', 'MARKOV-CHAINS', 'MODELS']",Stochastic Newton Sampler: The R Package sns,2016
645,"We analyze the performance of cross-validation (CV) in the density estimation framework with two purposes: (i) risk estimation and (ii) model selection. The main focus is given to the so-called leave-p-out CV procedure (Lpo), where p denotes the cardinality of the test set. Closed-form expressions are settled for the Lpo estimator of the risk of projection estimators. These expressions provide a great improvement upon V-fold cross-validation in terms of variability and computational complexity.
From a theoretical point of view, closed-form expressions also enable to study the Lpo performance in terms of risk estimation. The optimality of leave-one-out (Loo), that is Lpo with p = 1, is proved among CV procedures used for risk estimation. Two model selection frameworks are also considered: estimation, as opposed to identification. For estimation with finite sample size n, optimality is achieved for p large enough [with p/n = o(1)] to balance the overfitting resulting from the structure of the model collection. For identification, model selection consistency is settled for Lpo as long as p I n is conveniently related to the rate of convergence of the best estimator in the collection: (i) p/n -> 1 as n -> +infinity with a parametric rate, and (ii) p/n = o(1) with some nonparametric estimators. These theoretical results are validated by simulation experiments.",WOS:000344632400007,ANNALS OF STATISTICS,"['GAUSSIAN MODEL SELECTION', 'LEARNING-TESTING METHODS', 'REGRESSION', 'CHOICE', 'COMPLEXITIES', 'INEQUALITIES', 'PENALTIES', 'CP']",OPTIMAL CROSS-VALIDATION IN DENSITY ESTIMATION WITH THE L-2-LOSS,2014
646,"This paper describes the STARS ArcGIS geoprocessing toolset,which is used to calcul ate the spatial information needed to fit spatial statistical models to stream network data using the SSN package. The STARS toolset is designed for use with a landscape network (LSN),which is at opological data model produced by the FLoWS ArcGIS geoprocessing toolset. An overview of the FLoWS LSN structure and a few particularly useful tools is also provided so that users will have a clear understanding of the underlying data structure that the STARS toolset depends on. This document may be used as an introduction to new users. The methods used to calculate the spatial information and format the final.ssn object are also explicitly described so that users may create their own.ssn object using other data models and software.",WOS:000332107900001,JOURNAL OF STATISTICAL SOFTWARE,,STARS: An ArcGIS Toolset Used to Calculate the Spatial Information Needed to Fit Spatial Statistical Models to Stream Network Data,2014
647,"The R package micompr implements a procedure for assessing if two or more multivariate samples are drawn from the same distribution. The procedure uses principal component analysis to convert multivariate observations into a set of linearly uncorrelated statistical measures, which are then compared using a number of statistical methods. This technique is independent of the distributional properties of samples and automatically selects features that best explain their differences. The procedure is appropriate for comparing samples of time series, images, spectrometric measures or similar high-dimension multivariate observations.",WOS:000395669800027,R JOURNAL,['MODEL'],micompr: An R Package for Multivariate Independent Comparison of Observations,2016
648,"The case-control study is an important design for testing association between genetic markers and a disease. The Cochran-Armitage trend test (CATT) is one of the most commonly used statistics for the analysis of case-control genetic association studies. The asymptotically optimal CATT can be used when the underlying genetic model (mode of inheritance) is known. However, for most complex diseases, the underlying genetic models are unknown. Thus, tests robust to genetic model misspecification are preferable to the model-dependant CATT. Two robust tests, MAX3 and the genetic model selection (GMS), were recently proposed. Their asymptotic null distributions are often obtained by Monte-Carlo simulations, because they either have not been fully studied or involve multiple integrations. In this article, we study how components of each robust statistic are correlated, and find a linear dependence among the components. Using this new finding, we propose simple algorithms to calculate asymptotic null distributions for MAX3 and GMS, which greatly reduce the computing intensity. Furthermore, we have developed the R package Rassoc implementing the proposed algorithms to calculate the empirical and asymptotic p values for MAX3 and GMS as well as other commonly used tests in case-control association studies. For illustration, Rassoc is applied to the analysis of case-control data of 17 most significant SNPs reported in four genome-wide association studies.",WOS:000275203900001,JOURNAL OF STATISTICAL SOFTWARE,"['GENOME-WIDE ASSOCIATION', 'TREND TESTS', 'SAMPLE-SIZE', 'RISK', 'DISEASES', 'CANCER', 'SCORES', 'TABLES', 'MAX']",Simple Algorithms to Calculate Asymptotic Null Distributions of Robust Tests in Case-Control Genetic Association Studies in R,2010
649,"Under the assumption that the true density is decreasing, it is well known that the Grenander estimator converges at rate n(1/3) if the true density is curved [Sankhya Ser. A 31 (1969) 23-36] and at rate n(1/2) if the density is flat [Ann. Probab. 11 (1983) 328-345; Canad. J. Statist. 27 (1999) 557-566]. In the case that the true density is misspecified, the results of Patilea [Ann. Statist. 29 (2001) 94-123] tell us that the global convergence rate is of order n1/3 in Hellinger distance. Here, we show that the local convergence rate is n(1/2) at a point where the density is misspecified. This is not in contradiction with the results of Patilea [Ann. Statist. 29 (2001) 94-123]: the global convergence rate simply comes from locally curved well-specified regions. Furthermore, we study global convergence under misspecification by considering linear functionals. The rate of convergence is n(1/2) and we show that the limit is made up of two independent terms: a mean-zero Gaussian term and a second term (with nonzero mean) which is present only if the density has well-specified locally flat regions.",WOS:000336888400012,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'LOG-CONCAVE DENSITY', 'ASYMPTOTIC NORMALITY', 'LIMIT DISTRIBUTION', 'BROWNIAN-MOTION', 'MAJORANT', 'MODELS', 'ERROR']",CONVERGENCE OF LINEAR FUNCTIONALS OF THE GRENANDER ESTIMATOR UNDER MISSPECIFICATION,2014
650,"In this paper we describe MIDAS: a SAS macro for multiple imputation using distance aided selection of donors which implements an iterative predictive mean matching hot deck for imputing missing data. This is a flexible multiple imputation approach that can handle data in a variety of formats: continuous, ordinal, and scaled. Because the imputation models are implicit, it is not necessary to specify a parametric distribution for each variable to be imputed. MIDAS also allows the user to address the sensitivity of their inferences to different assumptions concerning the missing data mechanism. An example using MIDAS to impute missing data is presented and MIDAS is compared to existing missing data software.",WOS:000263825300001,JOURNAL OF STATISTICAL SOFTWARE,"['APPROXIMATE BAYESIAN BOOTSTRAP', 'UNRESOLVED ENUMERATION STATUS', 'LOGISTIC-REGRESSION MODELS', 'MISSING-DATA', 'UNDERCOUNT ESTIMATION', 'PERFORMANCE']",MIDAS: A SAS Macro for Multiple Imputation Using Distance-Aided Selection of Donors,2009
651,"A new time series bootstrap scheme, the time frequency toggle (TFT)-bootstrap, is proposed. Its basic idea is to bootstrap the Fourier coefficients of the observed time series, and then to back-transform them to obtain a bootstrap sample in the time domain. Related previous proposals, such as the ""surrogate data"" approach, resampled only the phase of the Fourier coefficients and thus had only limited validity. By contrast, we show that the appropriate resampling of phase and magnitude, in addition to some smoothing of Fourier coefficients, yields a bootstrap scheme that mimics the correct second-order moment structure for a large class of time series processes. As a main result we obtain a functional limit theorem for the TFT-bootstrap under a variety of popular ways of frequency domain bootstrapping. Possible applications of the TFT-bootstrap naturally arise in change-point analysis and unit-root testing where statistics are frequently based on functionals of partial sums. Finally, a small simulation study explores the potential of the TFT-bootstrap for small samples showing that for the discussed tests in change-point analysis as well as unit-root testing, it yields better results than the corresponding asymptotic tests if measured by size and power.",WOS:000293716500004,ANNALS OF STATISTICS,"['UNIT-ROOT TESTS', 'REGRESSION', 'STATISTICS', 'ESTIMATORS', 'DEPENDENCE', 'INFERENCE', 'MODELS']",TFT-BOOTSTRAP: RESAMPLING TIME SERIES IN THE FREQUENCY DOMAIN TO OBTAIN REPLICATES IN THE TIME DOMAIN,2011
652,"This paper revisits a meta-analysis method proposed by Pearson [Biometrika 26 (1934) 425-442] and first used by David [Biometrika 26 (1934) 1-11]. It was thought to be inadmissible for over fifty years, dating back to a paper of Birnbaum [J. Amer Statist. Assoc. 49 (1954) 559-574]. It turns out that the method Birnbaum analyzed is not the one that Pearson proposed. We show that Pearson's proposal is admissible. Because it is admissible, it has better power than the standard test of Fisher [Statistical Methods for Research Workers (1932) Oliver and Boyd] at some alternatives, and worse power at others. Pearson's method has the advantage when all or most of the nonzero parameters share the same sign. Pearson's test has proved useful in a genomic setting, screening for age-related genes. This paper also presents an FFT-based method for getting hard upper and lower bounds on the CDF of a sum of nonnegative random variables.",WOS:000271673700006,ANNALS OF STATISTICS,"['GOODNESS-OF-FIT', 'INDEPENDENT TESTS', 'PROBABILITY', 'HYPOTHESES']",KARL PEARSON'S META-ANALYSIS REVISITED,2009
653,"Data mining delivers insights, patterns, and descriptive and predictive models from the large amounts of data available today in many organisations. The data miner draws heavily on methodologies, techniques and algorithms from statistics, machine learning, and computer science. R increasingly provides a powerful platform for data mining. However, scripting and programming is sometimes a challenge for data analysts moving into data mining. The Rattle package provides a graphical user interface specifically for data mining using R. It also provides a stepping stone toward using R as a programming language for data analysis.",WOS:000208589800008,R JOURNAL,,Rattle: A Data Mining GUI for R,2009
654,"This paper develops a new direct approach to approximating suprema of general empirical processes by a sequence of suprema of Gaussian processes, without taking the route of approximating whole empirical processes in the sup-norm. We prove an abstract approximation theorem applicable to a wide variety of statistical problems, such as construction of uniform confidence bands for functions. Notably, the bound in the main approximation theorem is nonasymptotic and the theorem allows for functions that index the empirical process to be unbounded and have entropy divergent with the sample size. The proof of the approximation theorem builds on a new coupling inequality for maxima of sums of random vectors, the proof of which depends on an effective use of Stein's method for normal approximation, and some new empirical process techniques. We study applications of this approximation theorem to local and series empirical processes arising in nonparametric estimation via kernel and series methods, where the classes of functions change with the sample size and are non-Donsker. Importantly, our new technique is able to prove the Gaussian approximation for the supremum type statistics under weak regularity conditions, especially concerning the bandwidth and the number of series functions, in those examples.",WOS:000342481700012,ANNALS OF STATISTICS,"['MULTIVARIATE NORMAL APPROXIMATION', 'KERNEL DENSITY ESTIMATORS', 'INVARIANCE-PRINCIPLES', 'UNIFORM CONSISTENCY', 'EXCHANGEABLE PAIRS', 'RANDOM-VARIABLES', 'LIMIT-THEOREMS', 'REGRESSION', 'RATES', 'CONVERGENCE']",GAUSSIAN APPROXIMATION OF SUPREMA OF EMPIRICAL PROCESSES,2014
655,"Normal mixture distributions are arguably the most important mixture models. and also the most technically challenging. The likelihood function of the normal mixture model is unbounded based oil a set of random samples, unless an artificial bound is placed oil its component variance parameter. Moreover, the model is not strongly identifiable so it is hard to differentiate between over dispersion caused by the presence of a mixture and that caused by a large variance, and it has infinite Fisher information with respect to mixing proportions. There has been extensive research oil finite normal mixture models, but much of it addresses merely consistency of the point estimation or useful practical procedures, and many, results require undesirable restrictions oil the parameter space. We show that an EM-test for homogeneity is effective at overcoming many challenges in the context of finite normal mixtures. We find that the limiting, distribution of the EM-test is a simple function of the 0.5 chi(2)(0) + 0.5 chi(1)(2) and chi(2)(1) distributions when the mixing variances are equal but unknown and the chi(2)(2) when variances are unequal and unknown. Simulations unknown and the show that the limiting distributions approximate the finite sample distribution satisfactorily. Two genetic examples are used to illustrate the application of the EM-test.",WOS:000268604900016,ANNALS OF STATISTICS,"['LIKELIHOOD RATIO TEST', 'STRUCTURAL PARAMETER', 'VARIABLE SELECTION', 'HOMOGENEITY', 'GENE', 'ASYMPTOTICS', 'POPULATION', 'COMPONENTS', 'NUMBER']",HYPOTHESIS TEST FOR NORMAL MIXTURE MODELS: THE EM APPROACH,2009
656,"Quantum state tomography aims to determine the state of a quantum system as represented by a density matrix. It is a fundamental task in modern scientific studies involving quantum systems. In this paper, we study estimation of high-dimensional density matrices based on Pauli measurements. In particular, under appropriate notion of sparsity, we establish the minimax optimal rates of convergence for estimation of the density matrix under both the spectral and Frobenius norm losses; and show how these rates can be achieved by a common thresholding approach. Numerical performance of the proposed estimator is also investigated.",WOS:000372594300009,ANNALS OF STATISTICS,"['HIGH-DIMENSIONAL MATRICES', 'LOW-RANK MATRICES', 'OPTIMAL RATES', 'COMPLETION', 'PENALIZATION', 'CONVERGENCE', 'COMPUTATION', 'ESTIMATORS', 'SELECTION', 'RECOVERY']",OPTIMAL LARGE-SCALE QUANTUM STATE TOMOGRAPHY WITH PAULI MEASUREMENTS,2016
657,"Fluorescence Lifetime Imaging Microscopy ( FLIM) allows fluorescence lifetime images of biological objects to be collected at 250 nm spatial resolution and at ( sub-) nanosecond temporal resolution. Often n(comp) kinetic processes underlie the observed fluorescence at all locations, but the intensity of the fluorescence associated with each process varies per-location, i. e., per-pixel imaged. Then the statistical challenge is global analysis of the image: use of the fluorescence decay in time at all locations to estimate the ncomp lifetimes associated with the kinetic processes, as well as the amplitude of each kinetic process at each location. Given that typical FLIM images represent on the order of 10(2) timepoints and 10(3) locations, meeting this challenge is computationally intensive. Here the utility of the TIMP package for R to solve parameter estimation problems arising in FLIM image analysis is demonstrated. Case studies on simulated and real data evidence the applicability of the partitioned variable projection algorithm implemented in TIMP to the problem domain, and showcase options included in the package for the visual validation of models for FLIM data.",WOS:000244068000001,JOURNAL OF STATISTICAL SOFTWARE,"['NONLINEAR LEAST-SQUARES', 'PROTEIN', 'DECAYS', 'DOMAIN', 'CYAN']",Fluorescence Lifetime Imaging Microscopy (FLIM) data analysis with TIMP,2007
658,"Improved procedures, in terms of smaller missed discovery rates (MDR), for performing multiple hypotheses testing with weak and strong control of the family-wise error rate (FWER) or the false discovery rate (FDR) are developed and studied. The improvement over existing procedures such as the Sidak procedure for FWER control and the Benjamini-Hochberg (BH) procedure for FDR control is achieved by exploiting possible differences in the powers of the individual tests. Results signal the need to take into account the powers of the individual tests and to have multiple hypotheses decision functions which are not limited to simply using the individual p-values, as is the case, for example, with the Sidak, Bonferroni, or BH procedures. They also enhance understanding of the role of the powers of individual tests, or more precisely the receiver operating characteristic (ROC) functions of decision processes, in the search for better multiple hypotheses testing procedures. A decision-theoretic framework is utilized, and through auxiliary randomizers the procedures could be used with discrete or mixed-type data or with rank-based nonparametric tests. This is in contrast to existing p-value based procedures whose theoretical validity is contingent on each of these p-value statistics being stochastically equal to or greater than a standard uniform variable under the null hypothesis. Proposed procedures are relevant in the analysis of high-dimensional ""large M, small n"" data sets arising in the natural, physical, medical, economic and social sciences, whose generation and creation is accelerated by advances in high-throughput technology, notably, but not limited to, microarray technology.",WOS:000288183800018,ANNALS OF STATISTICS,"['MICROARRAY EXPERIMENTS', 'EMPIRICAL BAYES', 'HYPOTHESIS', 'SIZE', 'NULL', 'OPTIMALITY', 'PROPORTION', 'TESTS']",POWER-ENHANCED MULTIPLE DECISION FUNCTIONS CONTROLLING FAMILY-WISE ERROR AND FALSE DISCOVERY RATES,2011
659,"Sequential Monte Carlo is a family of algorithms for sampling from a sequence of distributions. Some of these algorithms, such as particle filters, are widely used in the physics and signal processing researches. More recent developments have established their application in more general inference problems such as Bayesian modeling. These algorithms have attracted considerable attentions in recent years as they admit natural and scalable parallelizations. However, these algorithms are perceived to be difficult to implement. In addition, parallel programming is often unfamiliar to many researchers though conceptually appealing, especially for sequential Monte Carlo related fields.
A C++ template library is presented for the purpose of implementing general sequential Monte Carlo algorithms on parallel hardware. Two examples are presented: a simple particle filter and a classic Bayesian modeling problem.",WOS:000349844000001,JOURNAL OF STATISTICAL SOFTWARE,,vSMC: Parallel Sequenced Monte Carlo in C plus,2014
660,"This paper proposes consistent estimators for transformation parameters in semiparametric models. The problem is to find the optimal transformation into the space of models with a predetermined regression structure like additive or multiplicative separability. We give results for the estimation of the transformation when the rest of the model is estimated non- or semi-parametrically and fulfills some consistency conditions. We propose two methods for the estimation of the transformation parameter maximizing a profile likelihood function or minimizing the mean squared distance from independence. First the problem of identification of such models is discussed. We then state asymptotic results for a general class of nonparametric estimators. Finally, we give some particular examples of nonparametric estimators of transformed separable models. The small sample performance is studied in several simulations.",WOS:000254502700008,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'ADDITIVE-MODELS', 'CENSORED-DATA', 'INTEGRATION', 'LIKELIHOOD', 'CONVERGENCE', 'SMOOTH', 'RATES']",Estimation of a semiparametric transformation model,2008
661,"The UK National River Flow Archive (NRFA) stores several types of hydrological data and metadata: daily river flow and catchment rainfall time series, gauging station and catchment information. Data are served through the NRFA web services via experimental RESTful APIs. Obtaining NRFA data can be unwieldy due to complexities in handling HTTP GET requests and parsing responses in JSON and XML formats. The rnrfa package provides a set of functions to programmatically access, filter, and visualize NRFA data using simple R syntax. This paper describes the structure of the rnrfa package, including examples using the main functions gdf() and cmr() for flow and rainfall data, respectively. Visualization examples are also provided with a shiny web application and functions provided in the package. Although this package is regional specific, the general framework and structure could be applied to similar databases.",WOS:000395669800008,R JOURNAL,,"rnrfa: An R package to Retrieve, Filter and Visualize Data from the UK National River Flow Archive",2016
662,"Motivated by practical applications, chiefly clinical trials, we study the regret achievable for stochastic bandits under the constraint that the employed policy must split trials into a small number of batches. We propose a simple policy, and show that a very small number of batches gives close to minimax optimal regret bounds. As a byproduct, we derive optimal policies with low switching cost for stochastic bandits.",WOS:000372594300008,ANNALS OF STATISTICS,"['2 ARMED BANDIT', 'REGRET BOUNDS', 'SELECTING 1', 'ALLOCATION', 'TESTS', 'MODEL']",BATCHED BANDIT PROBLEMS,2016
663,"In this paper, we describe the R package mediation for conducting causal mediation analysis in applied empirical research. In many scientific disciplines, the goal of researchers is not only estimating causal effects of a treatment but also understanding the process in which the treatment causally affects the outcome. Causal mediation analysis is frequently used to assess potential causal mechanisms. The mediation package implements a comprehensive suite of statistical tools for conducting such an analysis. The package is organized into two distinct approaches. Using the model-based approach, researchers can estimate causal mediation effects and conduct sensitivity analysis under the standard research design. Furthermore, the design-based approach provides several analysis tools that are applicable under different experimental designs. This approach requires weaker assumptions than the model-based approach. We also implement a statistical method for dealing with multiple (causally dependent) mediators, which are often encountered in practice. Finally, the package also offers a methodology for assessing causal mediation in the presence of treatment noncompliance, a common problem in randomized trials.",WOS:000341793300001,JOURNAL OF STATISTICAL SOFTWARE,"['SENSITIVITY-ANALYSIS', 'IDENTIFICATION', 'MECHANISMS', 'MODELS']",mediation: R Package for Causal Mediation Analysis,2014
664,"High-dimensional data sets are commonly collected in many contemporary applications arising in various fields of scientific research. We present two views of finite samples in high dimensions: a probabilistic one and a nonprobabilistic one. With the probabilistic view, we establish the concentration property and robust spark bound for large random design matrix generated from elliptical distributions, with the former related to the sure screening property and the latter related to sparse model identifiability. An interesting concentration phenomenon in high dimensions is revealed. With the nonprobabilistic view, we derive general bounds on dimensionality with some distance constraint on sparse models. These results provide new insights into the impacts of high dimensionality in finite samples.",WOS:000326991200020,ANNALS OF STATISTICS,"['FEATURE SPACE', 'REPRESENTATION', 'CLASSIFICATION', 'DISTRIBUTIONS', 'SELECTION', 'GEOMETRY', 'MODELS', 'LASSO']",IMPACTS OF HIGH DIMENSIONALITY IN FINITE SAMPLES,2013
665,"We consider the problem of estimating a density f(X) using a sample Y(l),....Y(n) from f(Y) = f(X) star f(is an element of), where f(is an element of) is an unknown density. We assume that all additional sample is an element of(l),...,is an element of(m) from f(is an element of) is observed. Estimators of f(X) and its derivatives are constructed by using nonparametric estimators of f(X) and f(is an element of) and by applying a spectral cut-off in the Fourier domain. We derive the rate of convergence of the estimators ill case of a known and unknown error density Where it is assumed that f(X) satisfies a polynomial, logarithmic or general source condition. It is shown that the proposed estimators are asymptotically optimal ill a minimax sense ill the models with known or unknown error density, if the density f(X) belongs to a Sobolev space H(p) and f(is an element of) is ordinary smooth or supersmooth.",WOS:000268604900008,ANNALS OF STATISTICS,"['HILBERT SCALES', 'NONPARAMETRIC DECONVOLUTION', 'DENSITY-ESTIMATION', 'TIKHONOV REGULARIZATION', 'OPTIMAL RATES', 'CONVERGENCE', 'ESTIMATOR', 'OPERATORS', 'VARIANCE', 'MODELS']",DECONVOLUTION WITH UNKNOWN ERROR DISTRIBUTION,2009
666,"We consider a multidimensional Ito semimartingale regularly sampled on [0, t] at high frequency 1/Delta(n), with Delta(n) going to zero. The goal of this paper is to provide an estimator for the integral over [0, t] of a given function of the volatility matrix. To approximate the integral, we simply use a Riemann sum based on local estimators of the pointwise volatility. We show that although the accuracy of the pointwise estimation is at most Delta(1/4)(n), this procedure reaches the parametric rate Delta(1/2)(n), as it is usually the case in integrated functionals estimation. After a suitable bias correction, we obtain an unbiased central limit theorem for our estimator and show that it is asymptotically efficient within some classes of sub models.",WOS:000323271500004,ANNALS OF STATISTICS,,QUARTICITY AND OTHER FUNCTIONALS OF VOLATILITY: EFFICIENT ESTIMATION,2013
667,"This paper introduces and studies multivariate spacings. The spacings are developed using the order statistics derived from data depth. Specifically, the spacing between two consecutive order statistics is the region which bridges the two order statistics, in the sense that the region contains all the points whose depth values fall between the depth values of the two consecutive order statistics. These multivariate spacings can be viewed as a data-driven realization of the so-called ""statistically equivalent blocks."" These spacings assume a form of center-outward layers of ""shells"" (""rings"" in the two-dimensional case), where the shapes of the shells follow closely the underlying probabilistic geometry. The properties and applications of these spacings are studied. In particular, the spacings are used to construct tolerance regions. The construction of tolerance regions is nonparametric and completely data driven, and the resulting tolerance region reflects the true geometry of the underlying distribution. This is different from most existing approaches which require that the shape of the tolerance region be specified in advance. The proposed tolerance regions are shown to meet the prescribed specifications, in terms of beta-content and beta-expectation. They are also asymptotically minimal under elliptical distributions. Finally, a simulation and comparison study on the proposed tolerance regions is presented.",WOS:000256504400011,ANNALS OF STATISTICS,"['SAMPLE SPACINGS', 'TESTS', 'LIMITS', 'CONVERGENCE', 'STATISTICS', 'CONTOURS', 'FIT']",Multivariate spacings based on data depth: I. Construction of nonparametric multivariate tolerance regions,2008
668,"A Fortran 95 program has been written to calculate critical values for the step-up and step-down FDR procedures developed by Somerville (2004). The program allows for arbitrary selection of number of hypotheses, FDR rate, one- or two-sided hypotheses, common correlation coefficient of the test statistics and degrees of freedom. An MCV (minimum critical value) may be specified, or the program will calculate a specified number of critical values or steps in an FDR procedure.
The program can also be used to efficiently ascertain an upper bound to the number of hypotheses which the procedure will reject, given either the values of the test statistics, or their p values. Limiting the number of steps in an FDR procedure can be used to control the number or proportion of false discoveries (Somerville and Hemmelmann 2007). Using the program to calculate the largest critical values makes possible efficient use of the FDR procedures for very large numbers of hypotheses.",WOS:000252428700001,JOURNAL OF STATISTICAL SOFTWARE,['FALSE DISCOVERY RATE'],Calculation of critical values for Somerville's FDR procedures,2007
669,"Giving a UseR! talk at the the international R user conference is a balancing act in which you have to try to impart some new ideas, provide sufficient background and keep the audience interested, all in a very short period of time.",WOS:000208590100012,R JOURNAL,,Giving a useR! Talk,2011
670,"Response-adaptive designs have been extensively studied and used in clinical trials. However, there is a lack of a comprehensive study of responseadaptive designs that include covariates, despite their importance in clinical trials. Because the allocation scheme and the estimation of parameters are affected by both the responses and the covariates, covariate-adjusted responseadaptive (CARA) designs are very complex to formulate. In this paper, we overcome the technical hurdles and lay out a framework for general CARA designs for the allocation of subjects to K (>= 2) treatments. The asymptotic properties are studied under certain widely satisfied conditions. The proposed CARA designs can be applied to generalized linear models. Two important special cases, the linear model and the logistic regression model, are considered in detail.",WOS:000248692700010,ANNALS OF STATISTICS,"['BIASED-COIN DESIGNS', 'SEQUENTIAL CLINICAL-TRIALS', 'PLAY-WINNER RULE', 'PROGNOSTIC FACTORS', 'RANDOMIZATION', 'INFORMATION', 'ALLOCATION', 'LIKELIHOOD']",Asymptotic properties of covariate-adjusted response-adaptive designs,2007
671,,WOS:000290231500018,ANNALS OF STATISTICS,"['LIMITS', 'EFFICIENCY', 'BOUNDS']",ON CONSTRUCTION OF THE SMALLEST ONE-SIDED CONFIDENCE INTERVAL FOR THE DIFFERENCE OF TWO PROPORTIONS,2010
672,"The flexibility and scope of the R programming environment has made it a popular choice for statistical modeling and scientific prototyping in a number of fields. In the field of chemistry, R provides several tools for a variety of problems related to statistical modeling of chemical information. However, one aspect common to these tools is that they do not have direct access to the information that is available from chemical structures, such as contained in molecular descriptors. We describe the rcdk package that provides the R user with access to the CDK, a Java framework for cheminformatics. As a result, it is possible to read in a variety of molecular formats, calculate molecular descriptors and evaluate fingerprints. In addition, we describe the rpubchem that will allow access to the data in PubChem, a public repository of molecular structures and associated assay data for approximately 8 million compounds. Currently, the package allows access to structural information as well as some simple molecular properties from PubChem. In addition the package allows access to bio-assay data from the PubChem FTP servers.",WOS:000244067600001,JOURNAL OF STATISTICAL SOFTWARE,"['COEFFICIENTS', 'PREDICTION']",Chemical Informatics functionality in R,2007
673,"Palaeoecology is an important branch of ecology that uses the subfossil remains of organisms preserved in lake, ocean and bog sediments to inform on changes in ecosystems and the environment through time. The analogue package contains functions to perform modern analogue technique ( MAT) transfer functions, which can be used to predict past changes in the environment, such as climate or lake-water pH from species data. A related technique is that of analogue matching, which is concerned with identifying modern sites that are floristically and faunistically similar to fossil samples. These techniques, and others, are increasingly being used to inform public policy on environmental pollution and conservation practices. These methods and other functionality in analogue are illustrated using the Surface Waters Acidification Project diatom: pH training set and diatom counts on samples of a sediment core from the Round Loch of Glenhead, Galloway, Scotland. The paper is aimed at palaeoecologists who are familiar with the techniques described but not with R.",WOS:000252429600001,JOURNAL OF STATISTICAL SOFTWARE,"['POLLEN SPECTRA', 'DISSIMILARITY', 'DISTANCE', 'METRICS', 'DIATOM']",Analogue methods in palaeoecology: Using the analogue package,2007
674,"Identifying the number of factors in a high-dimensional factor model has attracted much attention in recent years and a general solution to the problem is still lacking. A promising ratio estimator based on singular values of lagged sample auto-covariance matrices has been recently proposed in the literature with a reasonably good performance under some specific assumption on the strength of the factors. Inspired by this ratio estimator and as a first main contribution, this paper proposes a complete theory of such sample singular values for both the factor part and the noise part under the large-dimensional scheme where the dimension and the sample size proportionally grow to infinity. In particular, we provide an exact description of the phase transition phenomenon that determines whether a factor is strong enough to be detected with the observed sample singular values. Based on these findings and as a second main contribution of the paper, we propose a new estimator of the number of factors which is strongly consistent for the detection of all significant factors (which are the only theoretically detectable ones). In particular, factors are assumed to have the minimum strength above the phase transition boundary which is of the order of a constant; they are thus not required to grow to infinity together with the dimension (as assumed in most of the existing papers on high-dimensional factor models). Empirical Monte-Carlo study as well as the analysis of stock returns data attest a very good performance of the proposed estimator. In all the tested cases, the new estimator largely outperforms the existing estimator using the same ratios of singular values.",WOS:000396804900008,ANNALS OF STATISTICS,"['DYNAMIC-FACTOR MODEL', 'APPROXIMATE FACTOR MODELS', 'SPIKED POPULATION-MODEL', 'EIGENVALUES']",IDENTIFYING THE NUMBER OF FACTORS FROM SINGULAR VALUES OF A LARGE SAMPLE AUTO-COVARIANCE MATRIX,2017
675,"This paper introduces rpartScore (Galimberti, Soffritti, and Di Maso 2012), a new R package for building classification trees for ordinal responses, that can be employed whenever a set of scores is assigned to the ordered categories of the response. This package has been created to overcome some problems that produced unexpected results from the package rpartOrdinal (Archer 2010). Explanations for the causes of these unexpected results are provided. The main functionalities of rpartScore are described, and its use is illustrated through some examples.",WOS:000305065000001,JOURNAL OF STATISTICAL SOFTWARE,['DECISION TREES'],Classification Trees for Ordinal Responses in R: The rpartScore Package,2012
676,"Researchers are often faced with analyzing data sets that are not complete. To properly analyze such data sets requires the knowledge of the missing data mechanism. If data are missing completely at random (MCAR), then many missing data analysis techniques lead to valid inference. Thus, tests of MCAR are desirable. The package MissMech implements two tests developed by Jamshidian and Jalal (2010) for this purpose. These tests can be run using a function called TestMCARNormality. One of the tests is valid if data are normally distributed, and another test does not require any distributional assumptions for the data. In addition to testing MCAR, in some special cases, the function TestMCARNormality is also able to test whether data have a multivariate normal distribution. As a bonus, the functions in MissMech can also be used for the following additional tasks: (i) test of homoscedasticity for several groups when data are completely observed, (ii) perform the k-sample test of Anderson-Darling to determine whether k groups of univariate data come from the same distribution, (iii) impute incomplete data sets using two methods, one where normality is assumed and one where no specific distributional assumptions are made, (iv) obtain normal-theory maximum likelihood estimates for mean and covariance matrix when data are incomplete, along with their standard errors, and finally (v) perform the Neyman's test of uniformity. All of these features are explained in the paper, including examples.",WOS:000332109200001,JOURNAL OF STATISTICAL SOFTWARE,,"MissMech: An R Package for Testing Homoscedasticity, Multivariate Normality, and Missing Completely at Random (MCAR)",2014
677,"We study generalized additive partial linear models, proposing the use of polynomial spline smoothing for estimation of nonparametric functions, and deriving quasi-likelihood based estimators for the linear parameters. We establish asymptotic normality for the estimators of the parametric components. The procedure avoids solving large systems of equations as in kernel-based procedures and thus results in gains in computational simplicity. We further develop a class of variable selection procedures for the linear parameters by employing a nonconcave penalized quasi-likelihood, which is shown to have an asymptotic oracle property. Monte Carlo simulations and an empirical example are presented for illustration.",WOS:000296995500001,ANNALS OF STATISTICS,"['SEMIPARAMETRIC ESTIMATION', 'PENALIZED LIKELIHOOD', 'ORACLE PROPERTIES', 'REGRESSION', 'SPLINES', 'LASSO']",ESTIMATION AND VARIABLE SELECTION FOR GENERALIZED ADDITIVE PARTIAL LINEAR MODELS,2011
678,"In this paper, we study inference for high-dimensional data characterized by small sample sizes relative to the dimension of the data. In particular, we provide an infinite-dimensional framework to study statistical models that involve situations in which (i) the number of parameters increase with the sample size (that is, allowed to be random) and (ii) there is a possibility of missing data. Under a variety of tail conditions on the components of the data, we provide precise conditions for the joint consistency of the estimators of the mean. In the process, we clarify and improve some of the recent consistency results that appeared in the literature. An important aspect of the work presented is the development of asymptotic normality results for these models. As a consequence, we construct different test statistics for one-sample and two-sample problems concerning the mean vector and obtain their asymptotic distributions as a corollary of the infinite-dimensional results. Finally, we use these theoretical results to develop an asymptotically justifiable methodology for data analyses. Simulation results presented here describe situations where the methodology can be successfully applied. They also evaluate its robustness under a variety of conditions, some of which are substantially different from the technical conditions. Comparisons to other methods used in the literature are provided. Analyses of real-life data is also included.",WOS:000275510800010,ANNALS OF STATISTICS,"['EXPRESSION', 'CLASSIFICATION', 'INEQUALITIES', 'MICROARRAY']",ASYMPTOTIC INFERENCE FOR HIGH-DIMENSIONAL DATA,2010
679,"We describe a simple way to construct new statistical models for spatial point pattern data. Taking two or more existing models (finite Gibbs spatial point processes) we multiply the probability densities together and renormalise to obtain a new probability density. We call the resulting model a hybrid. We discuss stochastic properties of hybrids, their statistical implications, statistical inference, computational strategies and software implementation in the R package spatstat. Hybrids are particularly useful for constructing models which exhibit interaction at different spatial scales. The methods are demonstrated on a real data set on human social interaction. Software and data are provided.",WOS:000328130700001,JOURNAL OF STATISTICAL SOFTWARE,"['LIKELIHOOD INFERENCE', 'PERFECT SIMULATION', 'PATTERNS', 'INTENSITY']",Hybrids of Gibbs Point Process Models and Their Implementation,2013
680,"Bayesian analysis of data from the general linear mixed model is challenging because any nontrivial prior leads to an intractable posterior density. However, if a conditionally conjugate prior density is adopted, then there is a simple Gibbs sampler that can be employed to explore the posterior density. A popular default among the conditionally conjugate priors is an improper prior that takes a product form with a flat prior on the regression parameter, and so-called power priors on each of the variance components. In this paper, a convergence rate analysis of the corresponding Gibbs sampler is undertaken. The main result is a simple, easily-checked sufficient condition for geometric ergodicity of the Gibbs-Markov chain. This result is close to the best possible result in the sense that the sufficient condition is only slightly stronger than what is required to ensure posterior propriety. The theory developed in this paper is extremely important from a practical standpoint because it guarantees the existence of central limit theorems that allow for the computation of valid asymptotic standard errors for the estimates computed using the Gibbs sampler.",WOS:000321845400003,ANNALS OF STATISTICS,"['CHAIN MONTE-CARLO', 'WIDTH OUTPUT ANALYSIS', 'HIERARCHICAL-MODELS', 'VARIANCE', 'ESTIMATORS']",CONVERGENCE ANALYSIS OF THE GIBBS SAMPLER FOR BAYESIAN GENERAL LINEAR MIXED MODELS WITH IMPROPER PRIORS,2012
681,"Logistic regression is one of the most popular techniques used to describe the relationship between a binary dependent variable and a set of independent variables. However, the application of logistic regression to small data sets is often hindered by the complete or quasicomplete separation. Under the separation scenario, results obtained via maximum likelihood should not be trusted, since at least one parameter estimate diverges to innity. Firth's approach to logistic regression is a theoretically sound procedure, which is guaranteed to arrive at finite estimates even in a separation case. Firth's procedure was also proved to significantly reduce the small sample bias of maximum likelihood estimates. The main goal of the paper is to introduce the STATISTICA macro, which performs Firth-type logistic regression.",WOS:000326871100001,JOURNAL OF STATISTICAL SOFTWARE,['MODELS'],Separation-Resistant and Bias-Reduced Logistic Regression: STATISTICA Macro,2012
682,"In the present paper, we consider the application of overcomplete dictionaries to the solution of general ill-posed linear inverse problems. In the context of regression problems, there has been an enormous amount of effort to recover an unknown function using an overcomplete dictionary. One of the most popular methods, Lasso and its variants, is based on maximizing the likelihood, and relies on stringent assumptions on the dictionary, the so-called compatibility conditions, for a proof of its convergence rates. While these conditions may be satisfied for the original dictionary functions, they usually do not hold for their images due to contraction properties imposed by the linear operator.
In what follows, we bypass this difficulty by a novel approach, which is based on inverting each of the dictionary functions and matching the resulting expansion to the true function, thus, avoiding unrealistic assumptions on the dictionary and using Lasso in a predictive setting. We examine both the white noise and the observational model formulations, and also discuss how exact inverse images of the dictionary functions can be replaced by their approximate counterparts. Furthermore, we show how the suggested methodology can be extended to the problem of estimation of a mixing density in a continuous mixture. For all the situations listed above, we provide sharp oracle inequalities for the risk in a non-asymptotic setting.",WOS:000379972900013,ANNALS OF STATISTICS,"['INVERSE PROBLEMS', 'ORACLE INEQUALITIES', 'DENSITY-ESTIMATION', 'MODEL SELECTION', 'MIXTURE-MODELS', 'MIXING DENSITY', 'DECOMPOSITION', 'SPARSITY', 'LASSO']",SOLUTION OF LINEAR ILL-POSED PROBLEMS USING OVERCOMPLETE DICTIONARIES,2016
683,"The growing availability of network data and of scientific interest in distributed systems has led to the rapid development of statistical models of network structure. Typically, however, these are models for the entire network, while the data consists only of a sampled sub-network. Parameters for the whole network, which is what is of interest, are estimated by applying the model to the sub-network. This assumes that the model is consistent under sampling, or, in terms of the theory of stochastic processes, that it defines a projective family. Focusing on the popular class of exponential random graph models (ERGMs), we show that this apparently trivial condition is in fact violated by many popular and scientifically appealing models, and that satisfying it drastically limits ERGM's expressive power. These results are actually special cases of more general results about exponential families of dependent random variables, which we also prove. Using such results, we offer easily checked conditions for the consistency of maximum likelihood estimation in ERGMs, and discuss some possible constructive responses.",WOS:000320488200005,ANNALS OF STATISTICS,"['SOCIAL NETWORKS', 'MARKOV GRAPHS', 'LOGIT-MODELS', 'FEATHER', 'BIRDS', 'TIME']",CONSISTENCY UNDER SAMPLING OF EXPONENTIAL RANDOM GRAPH MODELS,2013
684,"Within the nonparametric regression model with unknown regression function l and independent, symmetric errors, a new multiscale signed rank statistic is introduced and a conditional multiple test of the simple hypothesis l = 0 against a nonparametric alternative is proposed. This test is distribution-free and exact for finite samples even in the heteroscedastic case. It adapts in a certain sense to the unknown smoothness of the regression function under the alternative, and it is uniformly consistent against alternatives whose sup-norm tends to zero at the fastest possible rate. The test is shown to be asymptotically optimal in two senses: It is rate-optimal adaptive against Holder classes. Furthermore, its relative asymptotic efficiency with respect to an asymptotically minimax optimal test under sup-norm loss is close to I in case of homoscedastic Gaussian errors within a broad range of Holder classes simultaneously.",WOS:000256504400013,ANNALS OF STATISTICS,"['DATA-DRIVEN VERSION', 'REGRESSION-MODELS', 'HYPOTHESES']",Adaptive goodness-of-fit tests based on signed ranks,2008
685,"Complex medical disorders, such as heart disease and diabetes, are thought to involve a number of genes which act in conjunction with lifestyle and environmental factors to increase disease susceptibility. Associations between complex traits and single nucleotide polymorphisms ( SNPs) in candidate genomic regions can provide a useful tool for identifying genetic risk factors. However, analysis of trait associations with single SNPs ignores the potential for extra information from haplotypes, combinations of variants at multiple SNPs along a chromosome inherited from a parent. When haplotype-trait associations are of interest and haplotypes of individuals can be determined, generalized linear models (GLMs) may be used to investigate haplotype associations while adjusting for the effects of non-genetic cofactors or attributes. Unfortunately, haplotypes cannot always be determined cost-effectively when data is collected on unrelated subjects. Uncertain haplotypes may be inferred on the basis of data from single SNPs. However, subsequent analyses of risk factors must account for the resulting uncertainty in haplotype assignment in order to avoid potential errors in interpretation. To account for such uncertainty, we have developed hapassoc, software for R implementing a likelihood approach to inference of haplotype and non-genetic effects in GLMs of trait associations. We provide a description of the underlying statistical method and illustrate the use of hapassoc with examples that highlight the flexibility to specify dominant and recessive effects of genetic risk factors, a feature not shared by other software that restricts users to additive effects only. Additionally, hapassoc can accommodate missing SNP genotypes for limited numbers of subjects.",WOS:000237294600001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED LINEAR-MODELS', 'MAXIMUM-LIKELIHOOD', 'INCOMPLETE DATA', 'LINKAGE PHASE', 'EM ALGORITHM', 'TESTS']",Hapassoc: Software for likelihood inference of trait associations with SNP haplotypes and other attributes,2006
686,"Subspace clustering refers to the task of finding a multi-subspace representation that best fits a collection of points taken from a high-dimensional space. This paper introduces an algorithm inspired by sparse subspace clustering (SSC) [In IEEE Conference on Computer Vision and Pattern Recognition, CVPR (2009) 2790-2797] to cluster noisy data, and develops some novel theory demonstrating its correctness. In particular, the theory uses ideas from geometric functional analysis to show that the algorithm can accurately recover the underlying subspaces under minimal requirements on their orientation, and on the number of samples per subspace. Synthetic as well as real data experiments complement our theoretical study, illustrating our approach and demonstrating its effectiveness.",WOS:000336888400014,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'HIGH-DIMENSIONAL REGRESSION', 'SWITCHED ARX SYSTEMS', 'MOTION SEGMENTATION', 'MIXED DATA', 'ALGORITHM', 'RECOVERY', 'LASSO', 'GRAPHS']",ROBUST SUBSPACE CLUSTERING,2014
687,"We describe the R np package via a series of applications that may be of interest to applied econometricians. The np package implements a variety of nonparametric and semiparametric kernel-based estimators that are popular among econometricians. There are also procedures for nonparametric tests of significance and consistent model specification tests for parametric mean regression models and parametric quantile regression models, among others. The np package focuses on kernel methods appropriate for the mix of continuous, discrete, and categorical data often found in applied settings. Data-driven methods of bandwidth selection are emphasized throughout, though we caution the user that data-driven bandwidth selection methods can be computationally demanding.",WOS:000258205800001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION-FUNCTIONS', 'CROSS-VALIDATION', 'DENSITY-FUNCTION', 'MODELS', 'BOOTSTRAP', 'DISTRIBUTIONS', 'VARIABLES']",Nonparametric econometrics: The np package,2008
688,"We develop a theory of local asymptotic normality in the quantum domain based on a novel quantum analogue of the log-likelihood ratio. This formulation is applicable to any quantum statistical model satisfying a mild smoothness condition. As an application, we prove the asymptotic achievability of the Holevo bound for the local shift parameter.",WOS:000326991200018,ANNALS OF STATISTICS,,QUANTUM LOCAL ASYMPTOTIC NORMALITY BASED ON A NEW QUANTUM LIKELIHOOD RATIO,2013
689,"We consider evaluation of proper posterior distributions obtained from improper prior distributions. Our context is estimating a bounded function phi of a parameter when the loss is quadratic. If the posterior mean of 0 is admissible for all bounded phi, the posterior is strongly admissible. We give sufficient conditions for strong admissibility. These conditions involve the recurrence of a Markov chain associated with the estimation problem. We develop general sufficient conditions for recurrence of general state space Markov chains that are also of independent interest. Our main example concerns the p-dimensional multivariate normal distribution with mean vector 0 when the prior distribution has the form g(parallel to theta parallel to(2)) d theta on the parameter space RP. Conditions on g for strong admissibility of the posterior are provided.",WOS:000260554100016,ANNALS OF STATISTICS,['ADMISSIBILITY'],EVALUATION OF FORMAL POSTERIOR DISTRIBUTIONS VIA MARKOV CHAIN ARGUMENTS,2008
690,"Quantile regression provides a framework for modeling statistical quantities of interest other than the conditional mean. The regression methodology is well developed for linear models, but less so for nonparametric models. We consider conditional quantiles with varying coefficients and propose a methodology for their estimation and assessment using polynomial splines. The proposed estimators are easy to compute via standard quantile regression algorithms and a stepwise knot selection algorithm. The proposed Rao-score-type test that assesses the model against a linear model is also easy to implement. We provide asymptotic results on the convergence of the estimators and the null distribution of the test statistic. Empirical results are also provided, including an application of the methodology to forced expiratory volume (FEV) data.",WOS:000247498100005,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'CONDITIONAL QUANTILES', 'POLYNOMIAL SPLINES', 'SMOOTHING SPLINES', 'MEDIAN REGRESSION', 'LONGITUDINAL DATA', 'MODELS', 'ESTIMATORS', 'INTERVALS']",Quantile regression with varying coefficients,2007
691,"A new thresholding method, based on L-statistics and called order thresholding, is proposed as a technique for improving the power when testing against high-dimensional alternatives. The new method allows great flexibility in the choice of the threshold parameter. This results in improved power over the soft and hard thresholding methods. Moreover, order thresholding is not restricted to the normal distribution. An extension of the basic order threshold statistic to high-dimensional ANOVA is presented. The performance of the basic order threshold statistic and its extension is evaluated with extensive simulations.",WOS:000280359400013,ANNALS OF STATISTICS,"['ASYMPTOTIC-DISTRIBUTION', 'LINEAR COMBINATIONS', 'OF-FIT', 'STATISTICS', 'SHRINKAGE', 'TESTS']",ORDER THRESHOLDING,2010
692,"We introduce new nonparametric predictors for homogeneous pooled data in the context of group testing for rare abnormalities and show that they achieve optimal rates of convergence. In particular, when the level of pooling is moderate, then despite the cost savings, the method enjoys the same convergence rate as in the case of no pooling. In the setting of ""over-pooling"" the convergence rate differs from that of an optimal estimator by no more than a logarithmic factor. Our approach improves on the random-pooling nonparametric predictor, which is currently the only nonparametric method available, unless there is no pooling, in which case the two approaches are identical.",WOS:000304684900006,ANNALS OF STATISTICS,"['MODELS', 'PREVALENCE', 'DISEASE', 'SAMPLES', 'WATER']",NONPARAMETRIC REGRESSION WITH HOMOGENEOUS GROUP TESTING DATA,2012
693,"This paper describes a graphical user interface (GUI) for the tourr package in R. The tour is a dynamic graphical method for viewing multivariate data. The GUI allows users to interact with a tour in order to explore the data for structures like clustering, outliers, nonlinear dependence. Users can pause the tour, choose a subset of variables, color points by other variables, and switch between several different types of tours.",WOS:000305991000001,JOURNAL OF STATISTICAL SOFTWARE,"['GRAND-TOUR', 'PURSUIT']",tourrGui: A gWidgets GUI for the Tour to Explore High-Dimensional Data Using Low-Dimensional Projections,2012
694,"We present an R package for the simulation of simple and complex survival data. It covers different situations, including recurrent events and multiple events. The main simulation routine allows the user to introduce an arbitrary number of distributions, each corresponding to a new event or episode, with its parameters, choosing between the Weibull (and exponential as a particular case), log-logistic and log-normal distributions.",WOS:000341792500001,JOURNAL OF STATISTICAL SOFTWARE,,The R Package survsim for the Simulation of Simple and Complex Survival Data,2014
695,We address the problem of parameter estimation for diffusion driven stochastic volatility models through Markov chain Monte Carlo (MCMC). To avoid degeneracy issues we introduce an innovative reparametrization defined through transformations that operate on the time scale of the diffusion. A novel MCMC scheme which overcomes the inherent difficulties of time change transformations is also presented. The algorithm is fast to implement and applies to models with stochastic volatility. The methodology is tested through simulation based experiments and illustrated on data consisting of US treasury bill rates.,WOS:000275510800008,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'TERM INTEREST-RATE', 'MULTIVARIATE DIFFUSIONS', 'PARTICLE FILTERS', 'DISTRIBUTIONS', 'SIMULATION', 'OPTIONS']",INFERENCE FOR STOCHASTIC VOLATILITY MODELS USING TIME CHANGE TRANSFORMATIONS,2010
696,"The Rcpp package simplifies integrating C++ code with R. It provides a consistent C++ class hierarchy that maps various types of R objects (vectors, matrices, functions, environments, ...) to dedicated C++ classes. Object interchange between R and C++ is managed by simple, flexible and extensible concepts which include broad support for C++ Standard Template Library idioms. C++ code can both be compiled, linked and loaded on the fly, or added via packages. Flexible error and exception code handling is provided. Rcpp substantially lowers the barrier for programmers wanting to combine C++ code with R.",WOS:000289457800001,JOURNAL OF STATISTICAL SOFTWARE,,Rcpp: Seamless R and C plus plus Integration,2011
697,"We develop a mixture procedure to monitor parallel streams of data for a change-point that affects only a subset of them, without assuming a spatial structure relating the data streams to one another. Observations are assumed initially to be independent standard normal random variables After a change-point the observations in a subset of the streams of data have nonzero mean values. The subset and the post-change means are unknown. The procedure we study uses stream specific generalized likelihood ratio statistics, which are combined to form an overall detection statistic in a mixture model that hypothesizes an assumed fraction pp of affected data streams. An analytic expression is obtained for the average run length (ARL) when there is no change and is shown by simulations to be very accurate. Similarly, an approximation for the expected detection delay (EDD) after a change-point is also obtained. Numerical examples are given to compare the suggested procedure to other procedures for unstructured problems and in one case where the problem is assumed to have a well-defined geometric structure. Finally we discuss sensitivity of the procedure to the assumed value of p(0) and suggest a generalization.",WOS:000320488200010,ANNALS OF STATISTICS,,SEQUENTIAL MULTI-SENSOR CHANGE-POINT DETECTION,2013
698,"The concept of Pareto frontiers is well-known in economics. Within the database community there exist many different solutions for the specification and calculation of Pareto frontiers, also called Skyline queries in the database context. Slight generalizations like the combination of the Pareto operator with the lexicographical order have been established under the term database preferences. In this paper we present the rPref package which allows to efficiently deal with these concepts within R. With its help, database preferences can be specified in a very similar way as in a state-of-the-art database management system. Our package provides algorithms for an efficient calculation of the Pareto-optimal set and further functionalities for visualizing and analyzing the induced preference order.",WOS:000395669800026,R JOURNAL,,Computing Pareto Frontiers and Database Preferences with the rPref Package,2016
699,"When testing for reduction of the mean value structure in linear mixed models, it is common to use an asymptotic chi(2) test. Such tests can, however, be very poor for small and moderate sample sizes. The pbkrtest package implements two alternatives to such approximate chi(2) tests: The package implements (1) a Kenward-Roger approximation for performing F tests for reduction of the mean structure and (2) parametric bootstrap methods for achieving the same goal. The implementation is focused on linear mixed models with independent residual errors. In addition to describing the methods and aspects of their implementation, the paper also contains several examples and a comparison of the various methods.",WOS:000341806600001,JOURNAL OF STATISTICAL SOFTWARE,['LIKELIHOOD'],Kenward-Roger Approximation and Parametric Bootstrap Methods for Tests in Linear Mixed Models - The R Package pbkrtest,2014
700,"Sufficient dimension reduction methods often require stringent conditions on the joint distribution of the predictor, or, when such conditions are not satisfied, rely on marginal transformation or reweighting to fulfill them approximately. For example, a typical dimension reduction method would require the predictor to have elliptical or even multivariate normal distribution. In this paper, we reformulate the commonly used dimension reduction methods, via the notion of ""central solution space,"" so as to circumvent the requirements of such strong assumptions, while at the same time preserve the desirable properties of the classical methods, such as root n-consistency and asymptotic normality. Imposing elliptical distributions or even stronger assumptions on predictors is often considered as the necessary tradeoff for overcoming the ""curse of dimensionality,"" but the development of this paper shows that this need not be the case. The new methods will be compared with existing methods by simulation and applied to a data set.",WOS:000265619700007,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'CENTRAL SUBSPACE', 'GRAPHICS']",DIMENSION REDUCTION FOR NONELLIPTICALLY DISTRIBUTED PREDICTORS,2009
701,"Many model search strategies involve trading off model fit with model complexity in a penalized goodness of fit measure. Asymptotic properties for these types of procedures in settings like linear regression and ARMA time series have been studied, but these do not naturally extend to nonstandard situations such as mixed effects models, where simple definition of the sample size is not meaningful. This paper introduces a new class of strategies, known as fence methods, for mixed model selection, which includes linear and generalized linear mixed models. The idea involves a procedure to isolate a subgroup of what are known as correct models (of which the optimal model is a member). This is accomplished by constructing a statistical fence, or barrier, to carefully eliminate incorrect models. Once the fence is constructed, the optimal model is selected from among those within the fence according to a criterion which can be made flexible. In addition, we propose two variations of the fence. The first is a stepwise procedure to handle situations of many predictors; the second is an adaptive approach for choosing a tuning constant. We give sufficient conditions for consistency of fence and its variations, a desirable property for a good model selection procedure. The methods are illustrated through simulation studies and real data analysis.",WOS:000258243000010,ANNALS OF STATISTICS,"['ASYMPTOTIC PROPERTIES', 'VARIANCE', 'REGRESSION', 'VARIABLES']",Fence methods for mixed model selection,2008
702,"In this work we show that, using the eigen-decomposition of the adjacency matrix, we can consistently estimate feature maps for latent position graphs with positive definite link function kappa, provided that the latent positions are i.i.d. from some distribution F. We then consider the exploitation task of vertex classification where the link function kappa belongs to the class of universal kernels and class labels are observed for a number of vertices tending to infinity and that the remaining vertices are to be classified. We show that minimization of the empirical phi-risk for some convex surrogate phi of 0-1 loss over a class of linear classifiers with increasing complexities yields a universally consistent classifier, that is, a classification rule with error converging to Bayes optimal for any distribution F.",WOS:000323271500002,ANNALS OF STATISTICS,"['SUPPORT VECTOR MACHINES', 'STOCHASTIC BLOCKMODELS', 'INTEGRAL-OPERATORS', 'KERNEL-PCA', 'MATRIX', 'LAPLACIANS', 'SPECTRA', 'BOUNDS', 'ERROR']",UNIVERSALLY CONSISTENT VERTEX CLASSIFICATION FOR LATENT POSITIONS GRAPHS,2013
703,"In research of medicines, the comparison of treatments, test articles, conditions, administrations, etc., is very common. Studies are completed, and the data are then most often analyzed with a default mixture of equal variance t tests, analysis of variance,and multiple comparison procedures. But even for an implicit, presumed one-factor linear model to compare groups,more often than not there is the added need to accommodate data which is better suited for expression of multiplicative effects, potential outliers, and limits of detection. Base R and contributed packages provide all the pieces to develop a comprehensive strategy to account for these needs. Such an approach includes exploration of the data, fitting models, formal analysis to gauge the magnitude of effects, and checking of assumptions. The cg package is developed with those goals in mind, using a flow of wrapper functions to guide the full analysis and interpretation of the data. Examples from our non-clinical world of research will be used to illustrate the package and strategy.",WOS:000314392600001,JOURNAL OF STATISTICAL SOFTWARE,,The cg Package for Comparison of Groups,2013
704,"Research and development on atmospheric and topographic correction methods for multispectral satellite data such as Landsat images has far outpaced the availability of those methods in geographic information systems software. As Landsat and other data become more widely available, demand for these improved correction methods will increase. Open source R statistical software can help bridge the gap between research and implementation. Sophisticated spatial data routines are already available, and the ease of program development in R makes it straightforward to implement new correction algorithms and to assess the results. Collecting radiometric, atmospheric, and topographic correction routines into the landsat package will make them readily available for evaluation for particular applications",WOS:000293390400001,JOURNAL OF STATISTICAL SOFTWARE,"['TM DATA', 'RADIOMETRIC CALIBRATION', 'IMAGES', 'CLASSIFICATION', 'TRANSFORMATION', 'NORMALIZATION', 'REFLECTANCE', 'ETM+']",Analyzing Remote Sensing Data in R: The landsat Package,2011
705,"We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, maximum mean discrepancies (MMD), that is, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. In the case where the energy distance is computed with a semimetric of negative type, a positive definite kernel, termed distance kernel, may be defined such that the MMD corresponds exactly to the energy distance. Conversely, for any positive definite kernel, we can interpret the MMD as energy distance with respect to some negative-type semimetric. This equivalence readily extends to distance covariance using kernels on the product space. We determine the class of probability distributions for which the test statistics are consistent against all alternatives. Finally, we investigate the performance of the family of distance kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.",WOS:000327746100001,ANNALS OF STATISTICS,"['INTEGRAL PROBABILITY METRICS', '2-SAMPLE TEST', 'COVARIANCE', 'DEPENDENCE']",EQUIVALENCE OF DISTANCE-BASED AND RKHS-BASED STATISTICS IN HYPOTHESIS TESTING,2013
706,"We consider the asymptotic consistency of maximum likelihood parameter estimation for dynamical systems observed with noise. Under suitable conditions on the dynamical systems and the observations, we show that maximum likelihood parameter estimation is consistent. Our proof involves ideas from both information theory and dynamical systems. Furthermore, we show how some well-studied properties of dynamical systems imply the general statistical properties related to maximum likelihood estimation. Finally, we exhibit classical families of dynamical systems for which maximum likelihood estimation is consistent. Examples include shifts of finite type with Gibbs measures and Axiom A attractors with SRB measures.",WOS:000349738500001,ANNALS OF STATISTICS,"['HIDDEN MARKOV-MODELS', 'STATISTICAL STABILITY', 'PROBABILISTIC FUNCTIONS', 'ASYMPTOTIC PROPERTIES', 'INFERENCE', 'NOISE', 'DIFFEOMORPHISMS', 'NORMALITY', 'CHAINS', 'STATES']",CONSISTENCY OF MAXIMUM LIKELIHOOD ESTIMATION FOR SOME DYNAMICAL SYSTEMS,2015
707,"Modeling of and inference on multivariate data that have been measured in space, such as temperature and pressure, are challenging tasks in environmental sciences, physics and materials science. We give an overview over and some background on modeling with cross-covariance models. The R package RandomFields supports the simulation, the parameter estimation and the prediction in particular for the linear model of coregionalization, the multivariate Matern models, the delay model, and a spectrum of physically motivated vector valued models. An example on weather data is considered, illustrating the use of RandomFields for parameter estimation and prediction.",WOS:000349845700001,JOURNAL OF STATISTICAL SOFTWARE,"['GAUSSIAN STOCHASTIC SIMULATIONS', 'CROSS-COVARIANCE FUNCTIONS', 'MARKOV RANDOM-FIELDS', 'VECTOR-FIELDS', 'R PACKAGE', 'MATRIX', 'MODELS']","Analysis, Simulation and Prediction of Multivariate Random Fields with Package Random Fields",2015
708,"In this paper we show how complete hierarchical multinomial marginal (HMM) models for categorical variables can be defined, estimated and tested using the R package hmmm. Models involving equality and inequality constraints on marginal parameters are needed to define hypotheses of conditional independence, stochastic dominance or notions of positive dependence, or when the parameters are allowed to depend on covariates. The hmmm package also serves the need of estimating and testing HMM models under equality and inequality constraints on marginal interactions.",WOS:000341806900001,JOURNAL OF STATISTICAL SOFTWARE,"['CONTINGENCY-TABLES', 'CATEGORICAL-DATA', 'INEQUALITY CONSTRAINTS']",hmmm: An R Package for Hierarchical Multinomial Marginal Models,2014
709,"We define a conjugate prior for the reversible Markov chain of order r. The prior arises from a partially exchangeable reinforced random walk, in the same way that the Beta distribution arises from the exchangeable Polya urn. An extension to variable-order Markov chains is also derived. We show the utility of this prior in testing the order and estimating the parameters of a reversible Markov model.",WOS:000291183300006,ANNALS OF STATISTICS,,"BAYESIAN ANALYSIS OF VARIABLE-ORDER, REVERSIBLE MARKOV CHAINS",2011
710,"Inference in quantile analysis has received considerable attention in the recent years. Linear quantile mixed models (Geraci and Bottai 2014) represent a flexible statistical tool to analyze data from sampling designs such as multilevel, spatial, panel or longitudinal, which induce some form of clustering. In this paper, I will show how to estimate conditional quantile functions with random effects using the R package lqmm. Modeling, estimation and inference are discussed in detail using a real data example. A thorough description of the optimization algorithms is also provided.",WOS:000341021100001,JOURNAL OF STATISTICAL SOFTWARE,"['LONGITUDINAL DATA', 'EM ALGORITHM', 'LIKELIHOOD', 'COUNTS']",Linear Quantile Mixed Models: The lqmm Package for Laplace Quantile Regression,2014
711,"This paper introduces two R packages available on the Comprehensive R Archive network. The main application concerns the study of computer code output. Package DiceDesign is dedicated to numerical design of experiments, fromt he construction to the study of the design properties. Package DiceEval deals with the fit, the validation and the comparison of metamodels.
After a brief presentation of the context, we focus on the architecture of these two packages. A two-dimensional test function will be a running example to illustrate the main functionalities of these packages and an industrial case study in five dimensions will also be detailed.",WOS:000365975200001,JOURNAL OF STATISTICAL SOFTWARE,"['OPTIMIZATION', 'DISCREPANCY', 'SEQUENCES']",DiceDesign and DiceEval: Two R Packages for Design and Analysis of Computer Experiments,2015
712,"Statistical techniques play a major role in contemporary methods for analyzing magnetic resonance imaging (MRI) data. In addition to the central role that classical statistical methods play in research using MRI, statistical modeling and machine learning techniques are key to many modern data analysis pipelines. Applications for these techniques cover a broad spectrum of research, including many preclinical and clinical studies, and in some cases these methods are working their way into widespread routine use.
In this manuscript we describe a software tool called TractoR (for ""Tractography with R""), a collection of packages for the R language and environment, along with additional infrastructure for straightforwardly performing common image processing tasks. TractoR provides general purpose functions for reading, writing and manipulating MR images, as well as more specific code for fitting signal models to diffusion MRI data and performing tractography, a technique for visualizing neural connectivity.",WOS:000296718800001,JOURNAL OF STATISTICAL SOFTWARE,"['DIFFUSION TENSOR', 'MRI', 'BRAIN', 'MODEL']",TractoR: Magnetic Resonance Imaging and Tractography with R,2011
713,"In computerized testing, the test takers' responses as well as their response times on the items are recorded. The relationship between response times and response accuracies is complex and varies over levels of observation. For example, it takes the form of a tradeoff between speed and accuracy at the level of a fixed person but may become a positive correlation for a population of test takers. In order to explore such relationships and test hypotheses about them, a conjoint model is proposed. Item responses are modeled by a two-parameter normal-ogive IRT model and response times by a lognormal model. The two models are combined using a hierarchical framework based on the fact that response times and responses are nested within individuals. All parameters can be estimated simultaneously using an MCMC estimation approach. A R package for the MCMC algorithm is presented and explained.",WOS:000247011600001,JOURNAL OF STATISTICAL SOFTWARE,['DISTRIBUTIONS'],Modeling of responses and response times with the package cirt,2007
714,"Correspondence analysis (CA) is a popular method that can be used to analyse relationships between categorical variables. Like principal component analysis, CA solutions can be rotated both orthogonally and obliquely to simple structure without affecting the total amount of explained inertia. We describe a MATLAB package for computing CA. The package includes orthogonal and oblique rotation of axes. It is designed not only for advanced users of MATLAB but also for beginners. Analysis can be done using a user-friendly interface, or by using command lines. We illustrate the use of CAR with one example.",WOS:000269819400001,JOURNAL OF STATISTICAL SOFTWARE,,CAR: A MATLAB Package to Compute Correspondence Analysis with Rotations,2009
715,In this article we implement the smoothing-spline-based functional mixed effects models (Guo 2002) by a SAS macro by exploiting the connection between mixed effects models and smoothing splines. The macro can handle flexible design matrices and is easy to use. Input parameters and output results are described and explained. A numeric example and a real data example are used for illustration.,WOS:000208805900001,JOURNAL OF STATISTICAL SOFTWARE,,fmixed: A SAS Macro for Smoothing-Spline-Based Functional Mixed Effects Models,2011
716,"The cross-entropy (CE) method is a simple and versatile technique for optimization, based on Kullback-Leibler (or cross-entropy) minimization. The method can be applied to a wide range of optimization tasks, including continuous, discrete, mixed and constrained optimization problems. The new package CEoptim provides the R implementation of the CE method for optimization. We describe the general CE methodology for optimization and well as some useful modifications. The usage and efficacy of CEoptim is demonstrated through a variety of optimization examples, including model fitting, combinatorial optimization, and maximum likelihood estimation.",WOS:000398466500001,JOURNAL OF STATISTICAL SOFTWARE,"['DIFFERENTIAL-EQUATIONS', 'GLOBAL OPTIMIZATION']",CEoptim: Cross-Entropy R Package for Optimization,2017
717,"In this paper we describe flexible competing risks regression models using the comp.risk () function available in the time re g package for R based on Scheike et al. (2008). Regression models are specified for the transition probabilities, that is the cumulative incidence in the competing risks setting. The model contains the Fine and Gray (1999) model as a special case. This can be used to do goodness-of-fit test for the subdistribution hazards' proportionality assumption (Scheike and Zhang 2008). The program can also construct confidence bands for predicted cumulative incidence curves.
We apply the methods to data on follicular cell lymphoma from Pintilie (2007), where the competing risks are disease relapse and death without relapse. There is important non-proportionality present in the data, and it is demonstrated how one can analyze these data using the flexible regression models.",WOS:000285981200001,JOURNAL OF STATISTICAL SOFTWARE,"['PROPORTIONAL HAZARDS MODEL', 'MULTISTATE MODELS', 'REGRESSION', 'PACKAGE']",Analyzing Competing Risk Data Using the R timereg Pac,2011
718,"This manuscript overviews exact testing of goodness of fit for log-linear models using the R package exactLoglinTest. This package evaluates model fit for Poisson log-linear models by conditioning on minimal sufficient statistics to remove nuisance parameters. A Monte Carlo algorithm is proposed to estimate P values from the resulting conditional distribution. In particular, this package implements a sequentially rounded normal approximation and importance sampling to approximate probabilities from the conditional distribution. Usually, this results in a high percentage of valid samples. However, in instances where this is not the case, a Metropolis Hastings algorithm can be implemented that makes more localized jumps within the reference set. The manuscript details how some conditional tests for binomial logit models can also be viewed as conditional Poisson log-linear models and hence can be performed via exactLoglinTest. A diverse battery of examples is considered to highlight use, features and extensions of the software. Notably, potential extensions to evaluating disclosure risk are also considered.",WOS:000242546000001,JOURNAL OF STATISTICAL SOFTWARE,"['EXACT CONDITIONAL TESTS', 'CONTINGENCY-TABLES', 'NETWORK ALGORITHM', 'LOGISTIC-MODELS', 'DISTRIBUTIONS', 'INFERENCE']",Exact hypothesis tests for log-linear models with exactLoglinTest,2006
719,"We propose kernel estimator for the distribution function of unobserved errors in autoregressive time series, based on residuals computed by estimating the autoregressive coefficients with the Yule Walker method. Under mild assumptions, we establish oracle efficiency of the proposed estimator, that is, it is asymptotically as efficient as the kernel estimator of the distribution function based on the unobserved error sequence itself. Applying the result of Wang, Cheng and Yang [J. Nonparametr. Stat. 25 (2013) 395-407], the proposed estimator is also asymptotically indistinguishable from the empirical distribution function based on the unobserved errors. A smooth simultaneous confidence band (SCB) is then constructed based on the proposed smooth distribution estimator and Kolmogorov distribution. Simulation examples support the asymptotic theory.",WOS:000336888400013,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION-MODELS', 'BICKEL-ROSENBLATT TEST', 'TIME-SERIES MODELS', 'CONVERGENCE', 'DENSITY', 'TESTS']",ORACALLY EFFICIENT ESTIMATION OF AUTOREGRESSIVE ERROR DISTRIBUTION WITH SIMULTANEOUS CONFIDENCE BAND,2014
720,"TIMP is an R package for modeling multiway spectroscopic measurements. The package allows for the simultaneous analysis of datasets collected under different experimental conditions in terms of a wide variety of parametric models. Models arising in spectroscopy data analysis often have some parameters that are intrinstically nonlinear, and some parameters that are conditionally linear on estimates of the nonlinear parameters. TIMP fits such separable nonlinear models using partitioned variable projection, a variant of the variable projection algorithm that is described here for the first time. The of the partitioned variable projection algorithm allows fitting many models for spectroscopy datasets using much less memory as compared to under the standard variable projection algorithm that is implemented in nonlinear optimization routines ( e. g., the plinear option of the R function nls), as is shown here. An overview of modeling with TIMP is also given that includes several case studies in the application of the package.",WOS:000244067300001,JOURNAL OF STATISTICAL SOFTWARE,"['RESOLVED SPECTRA', 'TARGET ANALYSIS', 'GLOBAL ANALYSIS']",TIMP: An R package for modeling multi-way spectroscopic measurements,2007
721,"We consider nonparametric estimation of a regression curve when the data are observed with multiplicative distortion which depends on an observed confounding variable. We suggest several estimators, ranging from a relatively simple one that relies on restrictive assumptions usually made in the literature, to a sophisticated piecewise approach that involves reconstructing a smooth curve from an estimator of a constant multiple of its absolute value, and which can be applied in much more general scenarios. We show that, although our nonparametric estimators are constructed from predictors of the unobserved undistorted data, they have the same first-order asymptotic properties as the standard estimators that could be computed if the undistorted data were available. We illustrate the good numerical performance of our methods on both simulated and real datasets.",WOS:000384397200017,ANNALS OF STATISTICS,"['VARYING COEFFICIENT MODELS', 'ERRORS-IN-VARIABLES', 'CONVERGENCE', 'DISEASE', 'RATES']",NONPARAMETRIC COVARIATE-ADJUSTED REGRESSION,2016
722,"Blind people have experienced access issues to many software applications since the advent of the Windows operating system; statistical software has proven to follow the rule and not be an exception. The ability to use R within minutes of download with next to no adaptation has opened doors for accessible production of statistical analyses for this author (himself blind) and blind students around the world. This article shows how little is required to make R the most accessible statistical software available today. There is any number of ramifications that this opportunity creates for blind students, especially in terms of their future research and employment prospects. There is potential for making R even better for blind users. The extensibility of R makes this possible through added functionality being made available in an add-on package called BrailleR. Functions in this package are intended to make graphical information available in text form.",WOS:000321944400008,R JOURNAL,,Statistical Software from a Blind Person's Perspective,2013
723,"We examine the behaviour of the pseudo-marginal random walk Metropolis algorithm, where evaluations of the target density for the accept/reject probability are estimated rather than computed precisely. Under relatively general conditions on the target distribution, we obtain limiting formulae for the acceptance rate and for the expected squared jump distance, as the dimension of the target approaches infinity, under the assumption that the noise in the estimate of the log-target is additive and is independent of the position. For targets with independent and identically distributed components, we also obtain a limiting diffusion for the first component.
We then consider the overall efficiency of the algorithm, in terms of both speed of mixing and computational time. Assuming the additive noise is Gaussian and is inversely proportional to the number of unbiased estimates that are used, we prove that the algorithm is optimally efficient when the variance of the noise is approximately 3.283 and the acceptance rate is approximately 7.001%. We also find that the optimal scaling is insensitive to the noise and that the optimal variance of the noise is insensitive to the scaling. The theory is illustrated with a simulation study using the particle marginal random walk Metropolis.",WOS:000349738500009,ANNALS OF STATISTICS,"['CHAIN MONTE-CARLO', 'TARGET DISTRIBUTIONS', 'DIFFUSION LIMITS', 'PARTICLE FILTERS', 'HIGH DIMENSIONS']",ON THE EFFICIENCY OF PSEUDO-MARGINAL RANDOM WALK METROPOLIS ALGORITHMS,2015
724,"In longitudinal and spatial studies, observations often demonstrate strong correlations that are stationary in time or distance lags, and the times or locations of these data being sampled may not be homogeneous. We propose a nonparametric estimator of the correlation function in such data, using kernel methods. We develop a pointwise asymptotic normal distribution for the proposed estimator, when the number of subjects is fixed and the number of vectors or functions within each subject goes to infinity. Based on the asymptotic theory, we propose a weighted block bootstrapping method for making inferences about the correlation function, where the weights account for the inhomogeneity of the distribution of the times or locations. The method is applied to a data set from a colon carcinogenesis study, in which colonic crypts were sampled from a piece of colon segment from each of the 12 rats in the experiment and the expression level of p27, an important cell cycle protein, was then measured for each cell within the sampled crypts. A simulation study is also provided to illustrate the numerical performance of the proposed method.",WOS:000249568000011,ANNALS OF STATISTICS,"['STATIONARY RANDOM-FIELDS', 'COVARIANCE FUNCTIONS', 'STATISTICS']","Nonparametric estimation of correlation functions in longitudinal and spatial data, with application to colon carcinogenesis experiments",2007
725,"The R package blm provides functions for fitting a family of additive regression models to binary data. The included models are the binomial linear model, in which all covariates have additive effects, and the linear-expit (lexpit) model, which allows some covariates to have additive effects and other covariates to have logisitc effects. Additive binomial regression is a model of event probability, and the coefficients of linear terms estimate covariate-adjusted risk differences. Thus, in contrast to logistic regression, additive binomial regression puts focus on absolute risk and risk differences. In this paper, we give an overview of the methodology we have developed to fit the binomial linear and lexpit models to binary outcomes from cohort and population-based case-control studies. We illustrate the blm package's methods for additive model estimation, diagnostics, and inference with risk association analyses of a bladder cancer nested case-control study in the NIH-AARP Diet and Health Study.",WOS:000323909300001,JOURNAL OF STATISTICAL SOFTWARE,"['RISK DIFFERENCES', 'ASSOCIATION', 'ESTIMATORS', 'DESIGN', 'RATIOS', 'TESTS']",Fitting Additive Binomial Regression Models with the R Package blm,2013
726,"We consider nonparametric Bayesian estimation inference using a rescaled smooth Gaussian field as a prior for a multidimensional function. The rescaling is achieved using a Gamma variable and the procedure can be viewed as choosing all inverse Gamma bandwidth. The procedure is studied from a frequentist perspective in three statistical settings involving replicated observations (density estimation, regression and classification). We prove that the resulting posterior distribution shrinks to the distribution that generates the data at a speed which is minimax-optimal up to a logarithmic factor, whatever the regularity level of the data-generating distribution. Thus the hierachical Bayesian procedure, with a fixed prior, is shown to be fully adaptive.",WOS:000268605000003,ANNALS OF STATISTICS,"['NONPARAMETRIC BINARY REGRESSION', 'DENSITY-ESTIMATION', 'CONVERGENCE-RATES', 'POSTERIOR DISTRIBUTIONS', 'STOCHASTIC-PROCESSES', 'MODEL SELECTION', 'METRIC ENTROPY', 'PROCESS PRIORS', 'CONSISTENCY', 'INEQUALITY']",ADAPTIVE BAYESIAN ESTIMATION USING A GAUSSIAN RANDOM FIELD WITH INVERSE GAMMA BANDWIDTH,2009
727,"Classical supervised learning enjoys the luxury of accessing the true known labels for the observations in a modeled dataset. Real life, however, poses an abundance of problems, where the labels are only partially defined, i.e., are uncertain and given only for a subset of observations. Such partial labels can occur regardless of the knowledge source. For example, an experimental assessment of labels may have limited capacity and is prone to measurement errors. Also expert knowledge is often restricted to a specialized area and is thus unlikely to provide trustworthy labels for all observations in the dataset. Partially supervised mixture modeling is able to process such sparse and imprecise input. Here, we present an R package called bgmm, which implements two partially supervised mixture modeling methods: soft-label and belief-based modeling. For completeness, we equipped the package also with the functionality of unsupervised, semi-and fully supervised mixture modeling. On real data we present the usage of bgmm for basic model-fitting in all modeling variants. The package can be applied also to selection of the best-fitting from a set of models with different component numbers or constraints on their structures. This functionality is presented on an artificial dataset, which can be simulated in bgmm from a distribution defined by a given model.",WOS:000303803800001,JOURNAL OF STATISTICAL SOFTWARE,,The R Package bgmm: Mixture Modeling with Uncertain Knowledge,2012
728,,WOS:000336888400002,ANNALS OF STATISTICS,"['CONFIDENCE-INTERVALS', 'SELECTION', 'REGRESSION', 'PARAMETERS', 'ESTIMATORS', 'INFERENCE']","DISCUSSION: ""A SIGNIFICANCE TEST FOR THE LASSO""",2014
729,"Unidimensional item response theory(IRT) models are useful when each item is designed to measure some facet of a unified latent trait. In practical applications, items are not necessarily measuring the same underlying trait, and hence the more general multiunidimensional model should be considered. This paper provides the requisite information and description of software that implements the Gibbs sampler for such models with two item parameters and a normal ogive form. The software developed is written in the MATLAB package IRTmu2no. The package is flexible enough to allow a user the choice to simulate binary response data with multiple dimensions, set the number of total or burn-in iterations, specify starting values or prior distributions for model parameters, check convergence of the Markov chain, as well as obtain Bayesian fit statistics. Illustrative examples are provided to demonstrate and validate the use of the software package.",WOS:000261526700001,JOURNAL OF STATISTICAL SOFTWARE,"['ITEM RESPONSE THEORY', 'POSTERIOR PREDICTIVE ASSESSMENT', 'BAYES FACTORS', 'DISTRIBUTIONS', 'ALGORITHM']",A MATLAB Package for Markov Chain Monte Carlo with a Multi-Unidimensional IRT Model,2008
730,"Finite mixture models are being used increasingly to model a wide variety of random phenomena for clustering, classification and density estimation. mclust is a powerful and popular package which allows modelling of data as a Gaussian finite mixture with different covariance structures and different numbers of mixture components, for a variety of purposes of analysis. Recently, version 5 of the package has been made available on CRAN. This updated version adds new covariance structures, dimension reduction capabilities for visualisation, model selection criteria, initialisation strategies for the EM algorithm, and bootstrap-based inference, making it a full-featured R package for data analysis via finite mixture modelling.",WOS:000385276100022,R JOURNAL,"['REGIONAL GEOCHEMICAL DATA', 'DISCRIMINANT-ANALYSIS', 'R PACKAGE', 'EM ALGORITHM', 'LIKELIHOOD', 'COMPONENTS', 'BOOTSTRAP', 'SCIENCES']","mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models",2016
731,"Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity measure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to implement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.",WOS:000349842900001,JOURNAL OF STATISTICAL SOFTWARE,"['FORECAST DENSITIES', 'SIMILARITY SEARCH', 'SPECTRAL DENSITY', 'COMPRESSION', 'MODELS', 'REPRESENTATION', 'DISCRIMINANT', 'WAVELETS']",TSclust: An R Package for Time series clustering,2014
732,"E-optimal experimental designs for a second-order response surface model with k >= 1 predictors are investigated. If the design space is the k-dimensional unit cube, Galil and Kiefer [J. Statist. Plann. Inference 1 (1977a) 121-132] determined optimal designs in a restricted class of designs (defined by the multiplicity of the minimal eigenvalue) and stated their universal optimality as a conjecture. In this paper, we prove this claim and show that these designs are in fact E-optimal in the class of all approximate designs. Moreover, if the design space is the unit ball, E-optimal designs have not been found so far and we also provide a complete solution to this optimal design problem.
The main difficulty in the construction of E-optimal designs for the second-order response surface model consists in the fact that for the multiplicity of the minimum eigenvalue of the ""optimal information matrix"" is larger than one (in contrast to the case k = 1) and as a consequence the corresponding optimality criterion is not differentiable at the optimal solution. These difficulties are solved by considering nonlinear Chebyshev approximation problems, which arise from a corresponding equivalence theorem. The extremal polynomials which solve these Chebyshev problems are constructed explicitly leading to a complete solution of the corresponding E-optimal design problems.",WOS:000342481700014,ANNALS OF STATISTICS,"['POLYNOMIAL REGRESSION', 'CUBE']",E-OPTIMAL DESIGNS FOR SECOND-ORDER RESPONSE SURFACE MODELS,2014
733,"Designs for event-related functional magnetic resonance imaging (ER-fMRI) that help to efficiently achieve the statistical goals while taking into account the psychological constraints and customized requirements are in great demand. This is not only because of the popularity of ER-fMRI but also because of the high cost of ER-fMRI experiments; being able to collect highly informative data is crucial. In this paper, we develop a MATLAB program which can accommodate many user-specified experimental conditions to efficiently find ER-fMRI optimal designs.",WOS:000267708300001,JOURNAL OF STATISTICAL SOFTWARE,"['EVENT-RELATED FMRI', 'MULTIPLE TRIAL TYPES', 'OPTIMIZATION', 'ENTROPY', 'POWER']",Multi-Objective Optimal Experimental Designs for ER-fMRI Using MATLAB,2009
734,"There is a clear growing interest, at least in the statistical literature, in competing risks and multi-state models. With the rising interest in competing risks and multi-state models a number of software packages have been developed for the analysis of such models. The present special issue of the Journal of Statistical Software introduces a selection of R packages devoted to competing risks and multi-state models. This introduction to the special issue contains some background and highlights the contents of the contributions.",WOS:000285981100001,JOURNAL OF STATISTICAL SOFTWARE,['ILLNESS-DEATH MODEL'],Special Issue about Competing Risks and Multi-State Models,2011
735,"In the asymptotic theory of quantum hypothesis testing, the minimal error probability of the first kind jumps sharply from zero to one when the error exponent of the second kind passes by the point of the relative entropy of the two states in an increasing way. This is well known as the direct part and strong converse of quantum Stein's lemma.
Here we look into the behavior of this sudden change and have make it clear how the error of first kind grows smoothly according to a lower order of the error exponent of the second kind, and hence we obtain the second-order asymptotics for quantum hypothesis testing. This actually implies quantum Stein's lemma as a special case. Meanwhile, our analysis also yields tight bounds for the case of finite sample size. These results have potential applications in quantum information theory.
Our method is elementary, based on basic linear algebra and probability theory. It deals with the achievability part and the optimality part in a unified fashion.",WOS:000334256100007,ANNALS OF STATISTICS,"['INFORMATION-SPECTRUM APPROACH', 'RELATIVE ENTROPY', 'STRONG CONVERSE', 'THEOREM', 'CAPACITY', 'FORMULA']",SECOND-ORDER ASYMPTOTICS FOR QUANTUM HYPOTHESIS TESTING,2014
736,"When dealing with high dimensional and low sample size data, feature selection is often needed to help reduce the dimension of the variable space while optimizing the classification task. Few tools exist for selecting variables in such data sets, especially when classes are numerous ( > 2). We have developed ofw, an R package that implements, in the context of classification, the meta algorithm ""optimal feature weighting"". We focus on microarray data, although the method can be applied to any p >> n problems with continuous variables. The aim is to select relevant variables and to numerically evaluate the resulting variable selection. Two versions are proposed with the application of supervised multiclass classifiers such as classification and regression trees and support vector machines. Furthermore, a weighted approach can be chosen to deal with unbalanced multiclasses, a common characteristic in microarray data sets.",WOS:000261526600001,JOURNAL OF STATISTICAL SOFTWARE,['GENE-EXPRESSION'],ofw: An R Package to Select Continuous Variables for Multiclass Classification with a Stochastic Wrapper Method,2008
737,"Many self-report measures of attitudes, beliefs, personality, and pathology include items that can be easily manipulated by respondents. For example, an individual may deliberately attempt to manipulate or distort responses to simulate grossly exaggerated physical or psychological symptoms in order to reach specific goals such as, for example, obtaining financial compensation, avoiding being charged with a crime, avoiding military duty, or obtaining drugs. This article introduces the package sgr that can be used to perform fake data analysis according to the sample generation by replacement approach. The package includes functions for making simple inferences about discrete/ordinal fake data. The package allows to quantify uncertainty in inferences based on possible fake data as well as to study the implications of fake data for empirical results.",WOS:000343788100018,R JOURNAL,"['SOCIAL DESIRABILITY', 'CONSTRUCT-VALIDITY', 'PERSONALITY', 'FAKING', 'SENSITIVITY', 'MODELS', 'SCORES']",sgr: A Package for Simulating Conditional Fake Ordinal Data,2014
738,"The paper aims at finding widely and smoothly defined nonparametric location and scatter functionals. As a convenient vehicle, maximum likelihood estimation of the location vector mu and scatter matrix Sigma of an elliptically symmetric t distribution off R(d) with degrees of freedom nu > 1 extends to an M-functional defined off all probability distributions P in a weakly open, weakly dense domain U. Here U consists of P Putting not too much mass in hyperplanes of dimension < d, as shown for empirical measures by Kent and Tyler [Ann. Statist. 19 (1991) 2102-2119]. It will be seen here that (mu, Sigma) is analytic on U for the bounded Lipschitz norm, of for d = 1 for the sup norm on distribution functions. For k = 1, 2,..., and other norms, depending on k and more directly adapted to t functionals, one has continuous differentiability of order k, allowing the delta-method to be applied to (mu, Sigma) for any P in U, which can be arbitrarily heavy-tailed. These results imply asymptotic normality of the corresponding M-estimators (mu(n), Sigma(n)). In dimension d = 1 only, the t(nu) functional (mu, sigma) extends to be defined and weakly continuous at all P.",WOS:000265500500014,ANNALS OF STATISTICS,"['DESCRIPTIVE STATISTICS', 'MULTIVARIATE LOCATION', 'NONPARAMETRIC MODELS', 'CAUCHY DISTRIBUTION', 'DONSKER CLASSES', 'EM ALGORITHM', 'LIKELIHOOD', 'UNIMODALITY']",DIFFERENTIABILITY OF t-FUNCTIONALS OF LOCATION AND SCATTER,2009
739,"The behavioral, educational, and social sciences are undergoing a paradigmatic shift in methodology, from disciplines that focus on the dichotomous outcome of null hypothesis significance tests to disciplines that report and interpret effect sizes and their corresponding confidence intervals. Due to the arbitrariness of many measurement instruments used in the behavioral, educational, and social sciences, some of the most widely reported effect sizes are standardized. Although forming confidence intervals for standardized effect sizes can be very beneficial, such confidence interval procedures are generally difficult to implement because they depend on noncentral t, F, and chi(2) distributions. At present, no main-stream statistical package provides exact confidence intervals for standardized effects without the use of specialized programming scripts. Methods for the Behavioral, Educational, and Social Sciences (MBESS) is an R package that has routines for calculating confidence intervals for noncentral t, F, and chi(2) distributions, which are then used in the calculation of exact confidence intervals for standardized effect sizes by using the confidence interval transformation and inversion principles. The present article discusses the way in which confidence intervals are formed for standardized effect sizes and illustrates how such confidence intervals can be easily formed using MBESS in R.",WOS:000247011700001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIPLE CORRELATION-COEFFICIENT', 'SAMPLE-SIZE', 'NONCENTRAL DISTRIBUTIONS', 'MEAN DIFFERENCE', 'PSYCHOLOGY']","Confidence intervals for standardized effect sizes: Theory, application, and implementation",2007
740,"Several different hierarchical Bayesian models can be used for the estimation of spatial risk patterns based on spatially aggregated count data. Typically, the resulting posterior distributions of the model parameters cannot be expressed in closed forms, and MCMC approaches are required for inference. However, implementations of hierarchical Bayesian models for such areal data are error-prone. Also, different implementation methods exist, and a surprisingly large variability may develop between the methods as well as between the different MCMC runs of one method. This paper has four main goals: (1) to present a point by point annotated code of two commonly used models for areal count data, namely the BYM and the Leroux models (2) to discuss technical variations in the implementation of a formula-driven sampler and to assess the variability in the posterior results from various alternative implementations (3) to give graphical tools to compare sample(r)s which complement existing convergence diagnostics and (4) to give various practical tips for implementing samplers.",WOS:000349848100001,JOURNAL OF STATISTICAL SOFTWARE,"['NESTED LAPLACE APPROXIMATIONS', 'R PACKAGE', 'REGRESSION', 'INFERENCE', 'FIELDS', 'JOINT']",Pitfalls in the Implementation of Bayesian Hierarchical Modeling of Areal Count Data: An Illustration Using BYM and Leroux Models,2015
741,"In objective Bayesian model selection, no single criterion has emerged as dominant in defining objective prior distributions. Indeed, many criteria have been separately proposed and utilized to propose differing prior choices. We first formalize the most general and compelling of the various criteria that have been suggested, together with a new criterion. We then illustrate the potential of these criteria in determining objective model selection priors by considering their application to the problem of variable selection in normal linear models. This results in a new model selection objective prior with a number of compelling properties.",WOS:000310650900010,ANNALS OF STATISTICS,"['PRIORS', 'CONSISTENCY', 'HYPOTHESES']",CRITERIA FOR BAYESIAN MODEL CHOICE WITH APPLICATION TO VARIABLE SELECTION,2012
742,"Graphics are good for showing the information in datasets and for complementing modelling. Sometimes graphics show information models miss, sometimes graphics help to make model results more understandable, and sometimes models show whether information from graphics has statistical support or not. It is the interplay of the two approaches that is valuable. Graphics could be used a lot more in R examples and we explore this idea with some datasets available in R packages.",WOS:000321944400013,R JOURNAL,,Let Graphics Tell the Story - Datasets in R,2013
743,"This paper presents a unified geometric framework for the statistical analysis of a general ill-posed linear inverse model which includes as special cases noisy compressed sensing, sign vector recovery, trace regression, orthogonal matrix estimation and noisy matrix completion. We propose computationally feasible convex programs for statistical inference including estimation, confidence intervals and hypothesis testing. A theoretical framework is developed to characterize the local estimation rate of convergence and to provide statistical inference guarantees. Our results are built based on the local conic geometry and duality. The difficulty of statistical inference is captured by the geometric characterization of the local tangent cone through the Gaussian width and Sudakov estimate.",WOS:000379972900006,ANNALS OF STATISTICS,"['CONFIDENCE-INTERVALS', 'MATRIX ESTIMATION', 'DANTZIG SELECTOR', 'STATISTICAL ESTIMATION', 'MODELS', 'LASSO', 'PENALIZATION', 'MINIMIZATION', 'COMPLETION', 'REGRESSION']",GEOMETRIC INFERENCE FOR GENERAL HIGH-DIMENSIONAL LINEAR INVERSE PROBLEMS,2016
744,"An important issue in survival analysis is the investigation and the modeling of hazard rates. Within a Bayesian nonparametric framework, a natural and popular approach is to model hazard rates as kernel mixtures with respect to a completely random measure. In this paper we provide a comprehensive analysis of the asymptotic behavior of such models. We investigate consistency of the posterior distribution and derive fixed sample size central limit theorems for both linear and quadratic functionals of the posterior hazard rate. The general results are then specialized to various specific kernels and mixing measures yielding consistency under minimal conditions and neat central limit theorems for the distribution of functionals.",WOS:000268113500010,ANNALS OF STATISTICS,"['BAYESIAN CONSISTENCY', 'STOCHASTIC INTEGRALS', 'SURVIVAL ANALYSIS', 'MODELS', 'DISTRIBUTIONS', 'ESTIMATORS', 'MIXTURES']",ASYMPTOTICS FOR POSTERIOR HAZARDS,2009
745,"Do-calculus is concerned with estimating the interventional distribution of an action from the observed joint probability distribution of the variables in a given causal structure. All identifiable causal effects can be derived using the rules of do-calculus, but the rules themselves do not give any direct indication whether the effect in question is identifiable or not. Shpitser and Pearl (2006b) constructed an algorithm for identifying joint interventional distributions in causal models, which contain unobserved variables and induce directed acyclic graphs. This algorithm can be seen as a repeated application of the rules of do-calculus and known properties of probabilities, and it ultimately either derives an expression for the causal distribution, or fails to identify the effect, in which case the effect is non-identifiable. In this paper, the R package causaleffect is presented, which provides an implementation of this algorithm. Functionality of causaleffect is also demonstrated through examples.",WOS:000398467200001,JOURNAL OF STATISTICAL SOFTWARE,['DIAGRAMS'],Identifying Causal Effects with the R Package causaleffect,2017
746,"While estimation of the marginal (total) causal effect of a point exposure on an outcome is arguably the most common objective of experimental and observational studies in the health and social sciences, in recent years, investigators have also become increasingly interested in mediation analysis. Specifically, upon evaluating the total effect of the exposure, investigators routinely wish to make inferences about the direct or indirect pathways of the effect of the exposure, through a mediator variable or not, that occurs subsequently to the exposure and prior to the outcome. Although powerful semiparametric methodologies have been developed to analyze observational studies that produce double robust and highly efficient estimates of the marginal total causal effect, similar methods for mediation analysis are currently lacking. Thus, this paper develops a general semiparametric framework for obtaining inferences about so-called marginal natural direct and indirect causal effects, while appropriately accounting for a large number of pre-exposure confounding factors for the exposure and the mediator variables. Our analytic framework is particularly appealing, because it gives new insights on issues of efficiency and robustness in the context of mediation analysis. In particular, we propose new multiply robust locally efficient estimators of the marginal natural indirect and direct causal effects, and develop a novel double robust sensitivity analysis framework for the assumption of ignorability of the mediator variable.",WOS:000310650900020,ANNALS OF STATISTICS,"['INCOMPLETE DATA', 'MODELS', 'INFERENCE']","SEMIPARAMETRIC THEORY FOR CAUSAL MEDIATION ANALYSIS: EFFICIENCY BOUNDS, MULTIPLE ROBUSTNESS AND SENSITIVITY ANALYSIS",2012
747,"With the international R user conference, useR! 2011, approaching, many participants may be contemplating how to put their thoughts together for presentation. This paper provides some suggestions for giving presentations and making posters.",WOS:000208590100013,R JOURNAL,,Tips for Presenting Your Work,2011
748,"This paper focuses on generalizing quantiles from the ordering point of view. We propose the concept of partial quantiles, which are based on a given partial order. We establish that partial quantiles are equivariant under order-preserving transformations of the data, robust to outliers, characterize the probability distribution if the partial order is sufficiently rich, generalize the concept of efficient frontier, and can measure dispersion from the partial order perspective.
We also study several statistical aspects of partial quantiles. We provide estimators, associated rates of convergence, and asymptotic distributions that hold uniformly over a continuum of quantile indices. Furthermore, we provide procedures that can restore monotonicity properties that might have been disturbed by estimation error, establish computational complexity bounds, and point out a concentration of measure phenomenon (the latter under independence and the componentwise natural order).
Finally, we illustrate the concepts by discussing several theoretical examples and simulations. Empirical applications to compare intake nutrients within diets, to evaluate the performance of investment funds, and to study the impact of policies on tobacco awareness are also presented to illustrate the concepts and their use.",WOS:000291183300016,ANNALS OF STATISTICS,"['OUTPUT REGRESSION QUANTILES', 'HALF-SPACE DEPTH', 'L-1 OPTIMIZATION', 'EMPIRICAL PROCESSES', 'CONDITIONAL QUANTILE', 'DETERMINING SETS', 'MODELS', 'PERFORMANCE', 'ASYMPTOTICS', 'REGIONS']",ON MULTIVARIATE QUANTILES UNDER PARTIAL ORDERS,2011
749,,WOS:000336888400003,ANNALS OF STATISTICS,,"DISCUSSION: ""A SIGNIFICANCE TEST FOR THE LASSO""",2014
750,This paper considers the problem of constructing optimal discriminating experimental designs for competing regression models on the basis of the T-optimality criterion introduced by Atkinson and Fedorov [Biometrika 62 (1975a) 57-70]. T-optimal designs depend on unknown model parameters and it is demonstrated that these designs are sensitive with respect to misspecification. As a solution to this problem we propose a Bayesian and standardized maximin approach to construct robust and efficient discriminating designs on the basis of the T-optimality criterion. It is shown that the corresponding Bayesian and standardized maximin optimality criteria are closely related to linear optimality criteria. For the problem of discriminating between two polynomial regression models which differ in the degree by two the robust T-optimal discriminating designs can be found explicitly. The results are illustrated in several examples.,WOS:000326991200001,ANNALS OF STATISTICS,"['2 RIVAL MODELS', 'POLYNOMIAL REGRESSION', 'CRITERIA', 'FIT']",ROBUST T-OPTIMAL DISCRIMINATING DESIGNS,2013
751,"Consider estimation of the regression function based on a model with equidistant design and measurement errors generated from a fractional Gaussian noise process. In previous literature, this model has been heuristically linked to an experiment, where the anti-derivative of the regression function is continuously observed under additive perturbation by a fractional Brownian motion. Based on a reformulation of the problem using reproducing kernel Hilbert spaces, we derive abstract approximation conditions on function spaces under which asymptotic equivalence between these models can be established and show that the conditions are satisfied for certain Sobolev balls exceeding some minimal smoothness. Furthermore, we construct a sequence space representation and provide necessary conditions for asymptotic equivalence to hold.",WOS:000345884900014,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'NONPARAMETRIC REGRESSION', 'BROWNIAN-MOTION', 'DENSITY-ESTIMATION', 'WAVELET SHRINKAGE', 'INVERSE PROBLEMS', 'RANDOM DESIGN', 'INTEGRATION', 'VOLATILITY']",ASYMPTOTIC EQUIVALENCE FOR REGRESSION UNDER FRACTIONAL NOISE,2014
752,"Nonparametric density and regression estimation methods for circular data are included in the R package NPCirc. Specifically, a circular kernel density estimation procedure is provided, jointly with different alternatives for choosing the smoothing parameter. In the regression setting, nonparametric estimation for circular-linear, circular-circular and linear-circular data is also possible via the adaptation of the classical Nadaraya-Watson and local linear estimators. In order to assess the significance of the features observed in the smooth curves, both for density and regression with a circular covariate and a linear response, a SiZer technique is developed for circular data, namely CircSiZer. Some data examples are also included in the package, jointly with a routine that allows generating mixtures of different circular distributions.",WOS:000349841500001,JOURNAL OF STATISTICAL SOFTWARE,"['MISES-FISHER DISTRIBUTIONS', 'DENSITY-ESTIMATION', 'DIRECTIONAL-DATA', 'BANDWIDTH SELECTION', 'REGRESSION-MODEL', 'FITTING MIXTURES', 'STATISTICS', 'WIND', 'RULE']",NPCirc: An R Package for Nonarametric Circular Methods,2014
753,"Methodology extending nonparametric goodness-of-fit tests to discrete null distributions has existed for several decades. However, modern statistical software has generally failed to provide this methodology to users. We offer a revision of R's ks. test () function and a new cvm. test () function that fill this need in the R language for two of the most popular nonparametric goodness-of-fit tests. This paper describes these contributions and provides examples of their usage. Particular attention is given to various numerical issues that arise in their implementation.",WOS:000208590200007,R JOURNAL,,Nonparametric Goodness-of-Fit Tests for Discrete Null Distributions,2011
754,"In this paper, we study the approximation and estimation of s-concave densities via Renyi divergence. We first show that the approximation of a probability measure Q by an s-concave density exists and is unique via the procedure of minimizing a divergence functional proposed by [Ann. Statist. 38 (2010) 2998-3027] if and only if Q admits full-dimensional support and a first moment. We also show continuity of the divergence functional in Q: if Q(n) -> Q in the Wasserstein metric, then the projected densities converge in weighted L-1 metrics and uniformly on closed subsets of the continuity set of the limit. Moreover, directional derivatives of the projected densities also enjoy local uniform convergence. This contains both on-the-model and off-the-model situations, and entails strong consistency of the divergence estimator of an s-concave density under mild conditions. One interesting and important feature for the Renyi divergence estimator of an s-concave density is that the estimator is intrinsically related with the estimation of log-concave densities via maximum likelihood methods. In fact, we show that for d = 1 at least, the Renyi divergence estimators for s-concave densities converge to the maximum likelihood estimator of a log-concave density as s NE arrow 0. The Renyi divergence estimator shares similar characterizations as the MLE for log-concave distributions, which allows us to develop pointwise asymptotic distribution theory assuming that the underlying density is s-concave.",WOS:000375175200015,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'LIMIT DISTRIBUTION-THEORY', 'REGRESSION', 'BOUNDS']",APPROXIMATION AND ESTIMATION OF s-CONCAVE DENSITIES VIA RENYI DIVERGENCES,2016
755,"In this paper we describe the SimultAn package dedicated to simultaneous analysis. Simultaneous analysis is a new factorial methodology developed for the joint treatment of a set of several data tables. Since the first stage of simultaneous analysis requires a correspondence analysis of each table the package comprises two parts, one for correspondence analysis and one for simultaneous analysis. The package can be used to perform classical correspondence analysis of frequency/contingency tables as well as to perform simultaneous analysis of a set of frequency/contingency tables. In this package, functions for computation, summaries and graphical visualization in two dimensions are provided, including options to display partial rows and supplementary points.",WOS:000289932900001,JOURNAL OF STATISTICAL SOFTWARE,,Simultaneous Analysis in S-PLUS: The Simult An Package,2011
756,"In longitudinal studies measurements are often collected on different types of outcomes for each subject. These may include several longitudinally measured responses (such as blood values relevant to the medical condition under study) and the time at which an event of particular interest occurs (e. g., death, development of a disease or dropout from the study). These outcomes are often separately analyzed; however, in many instances, a joint modeling approach is either required or may produce a better insight into the mechanisms that underlie the phenomenon under study. In this paper we present the R package JM that fits joint models for longitudinal and time-to-event data.",WOS:000281588100001,JOURNAL OF STATISTICAL SOFTWARE,"['SURVIVAL-DATA', 'LIKELIHOOD APPROACH', 'ERROR']",JM: An R Package for the Joint Modelling of Longitudinal and Time-to-Event Data,2010
757,,WOS:000389620800006,ANNALS OF STATISTICS,"['HIGHER CRITICISM', 'FEATURE-SELECTION', 'VARIABLE SELECTION', 'CLASSIFICATION', 'SPACE', 'GRAPH', 'RARE', 'WEAK']","REJOINDER: ""INFLUENTIAL FEATURES PCA FOR HIGH DIMENSIONAL CLUSTERING""",2016
758,"We consider a bivariate process X(t) = (X(t)(1), X(t)(2)), which is observed on a finite time interval [0, T] at discrete times 0, Delta(n), 2 Delta(n), .... Assuming that its two components X(1) and X(2) have jumps on [0, T], we derive tests to decide whether they have at least one jump occurring at the same time (""common jumps"") or not (""disjoint jumps""). There are two different tests for the two possible null hypotheses (common jumps or disjoint jumps). Those tests have a prescribed asymptotic level, as the mesh Delta(n) goes to 0. We show on some simulations that these tests perform reasonably well even in the finite sample case, and we also put them in use for some exchange rates data.",WOS:000268113500007,ANNALS OF STATISTICS,,TESTING FOR COMMON ARRIVALS OF JUMPS FOR DISCRETELY OBSERVED MULTIDIMENSIONAL PROCESSES,2009
759,,WOS:000357441000003,ANNALS OF STATISTICS,"['INVERSE PROBLEMS', 'GAUSSIAN PRIORS', 'WHITE-NOISE', 'RATES']","DISCUSSION OF ""FREQUENTIST COVERAGE OF ADAPTIVE NONPARAMETRIC BAYESIAN CREDIBLE SETS""",2015
760,"spam is an R package for sparse matrix algebra with emphasis on a Cholesky factorization of sparse positive definite matrices. The implemantation of spam is based on the competing philosophical maxims to be competitively fast compared to existing tools and to be easy to use, modify and extend. The first is addressed by using fast Fortran routines and the second by assuring S3 and S4 compatibility. One of the features of spam is to exploit the algorithmic steps of the Cholesky factorization and hence to perform only a fraction of the workload when factorizing matrices with the same sparsity structure. Simulations show that exploiting this break-down of the factorization results in a speed-up of about a factor 5 and memory savings of about a factor 10 for large matrices and slightly smaller factors for huge matrices. The article is motivated with Markov chain Monte Carlo methods for Gaussian Markov random fields, but many other statistical applications are mentioned that profit from an efficient Cholesky factorization as well.",WOS:000282056300001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIFRONTAL METHOD', 'ALGORITHM']",spam: A Sparse Matrix R Package with Emphasis on MCMC Methods for Gaussian Markov Random Fields,2010
761,"Mathematical simulation models are commonly applied to analyze experimental or environmental data and eventually to acquire predictive capabilities. Typically these models depend on poorly defined, unmeasurable parameters that need to be given a value. Fitting a model to data, so-called inverse modelling, is often the sole way of finding reasonable values for these parameters. There are many challenges involved in inverse model applications, e. g., the existence of non-identifiable parameters, the estimation of parameter uncertainties and the quantification of the implications of these uncertainties on model predictions.
The R package F M E is a modeling package designed to confront a mathematical model with data. It includes algorithms for sensitivity and Monte Carlo analysis, parameter identifiability, model fitting and provides a Markov-chain based method to estimate parameter confidence intervals. Although its main focus is on mathematical systems that consist of differential equations, F M E can deal with other types of models. In this paper, F M E is applied to a model describing the dynamics of the HIV virus.",WOS:000275203400001,JOURNAL OF STATISTICAL SOFTWARE,"['IDENTIFIABILITY ANALYSIS', 'ADAPTIVE MCMC']","Inverse Modelling, Sensitivity and Monte Carlo Analysis in R Using Package FME",2010
762,"String processing is not glamorous, but it is frequently used in data cleaning and preparation. The existing string functions in R are powerful, but not friendly. To remedy this, the stringr package provides string functions that are simpler and more consistent, and also fixes some functionality that R is missing compared to other programming languages.",WOS:000208590000006,R JOURNAL,,"stringr: modern, consistent string processing",2010
763,"A library of common geometric shapes can be used to train our brains for understanding data structure in high-dimensional Euclidean space. This article describes the methods for producing cubes, spheres, simplexes, and tori in multiple dimensions. It also describes new ways to define and generate high-dimensional tori. The algorithms are described, critical code chunks are given, and a large collection of generated data are provided. These are available in the R package geozoo, and selected movies and images, are available on the GeoZoo web site (http://schloerke.github.io/ geozoo/).",WOS:000395669800016,R JOURNAL,,Escape from Boxland,2016
764,"Many methods for causal inference generate directed acyclic graphs (DAGs) that formalize causal relations between n variables. Given the joint distribution on all these variables, the DAG contains all information about how intervening on one variable changes the distribution of the other n - 1 variables. However, quantifying the causal influence of one variable on another one remains a nontrivial question.
Here we propose a set of natural, intuitive postulates that a measure of causal strength should satisfy. We then introduce a communication scenario, where edges in a DAG play the role of channels that can be locally corrupted by interventions. Causal strength is then the relative entropy distance between the old and the new distribution.
Many other measures of causal strength have been proposed, including average causal effect, transfer entropy, directed information, and information flow. We explain how they fail to satisfy the postulates on simple DAGs of <= 3 nodes. Finally, we investigate the behavior of our measure on time-series, supporting our claims with experiments on simulated data.",WOS:000327746100003,ANNALS OF STATISTICS,['NETWORKS'],QUANTIFYING CAUSAL INFLUENCES,2013
765,"Quantile regression has become a valuable tool to analyze heterogeneous covaraite-response associations that are often encountered in practice. The development of quantile regression methodology for high-dimensional covariates primarily focuses on the examination of model sparsity at a single or multiple quantile levels, which are typically prespecified ad hoc by the users. The resulting models may be sensitive to the specific choices of the quantile levels, leading to difficulties in interpretation and erosion of confidence in the results. In this article, we propose a new penalization framework for quantile regression in the high-dimensional setting. We employ adaptive L-1 penalties, and more importantly, propose a uniform selector of the tuning parameter for a set of quantile levels to avoid some of the potential problems with model selection at individual quantile levels. Our proposed approach achieves consistent shrinkage of regression quantile estimates across a continuous range of quantiles levels, enhancing the flexibility and robustness of the existing penalized quantile regression methods. Our theoretical results include the oracle rate of uniform convergence and weak convergence of the parameter estimators. We also use numerical studies to confirm our theoretical findings and illustrate the practical utility of our proposal.",WOS:000362697700013,ANNALS OF STATISTICS,"['TUNING PARAMETER SELECTION', 'CLIPPED ABSOLUTE DEVIATION', 'VARIABLE SELECTION', 'MODEL SELECTION', 'PENALIZED LIKELIHOOD', 'LINEAR-REGRESSION', 'LASSO', 'CRITERIA', 'NUMBER', 'SPACES']",GLOBALLY ADAPTIVE QUANTILE REGRESSION WITH ULTRA-HIGH DIMENSIONAL DATA,2015
766,"We present the qgraph package for R, which provides an interface to visualize data through network modeling techniques. For instance, a correlation matrix can be represented as a network in which each variable is a node and each correlation an edge; by varying the width of the edges according to the magnitude of the correlation, the structure of the correlation matrix can be visualized. A wide variety of matrices that are used in statistics can be represented in this fashion, for example matrices that contain (implied) covariances, factor loadings, regression parameters and p values. qgraph can also be used as a psychometric tool, as it performs exploratory and confirmatory factor analysis, using sem and lavaan; the output of these packages is automatically visualized in qgraph, which may aid the interpretation of results. In this article, we introduce qgraph by applying the package functions to data from the NEO-PI-R, a widely used personality questionnaire.",WOS:000305117400001,JOURNAL OF STATISTICAL SOFTWARE,['PERSONALITY'],qgraph: Network Visualizations of Relationships in Psychometric Data,2012
767,"The complexity of semiparametric models poses new challenges to statistical inference and model selection that frequently arise from real applications. In this work, we propose new estimation and variable selection procedures for the semiparametric varying-coefficient partially linear model. We first study quantile regression estimates for the nonparametric varying-coefficient functions and the parametric regression coefficients. To achieve nice efficiency properties, we further develop a semiparametric composite quantile regression procedure. We establish the asymptotic normality of proposed estimators for both the parametric and nonparametric parts and show that the estimators achieve the best convergence rate. Moreover, we show that the proposed method is much more efficient than the least-squares-based method for many non-normal errors and that it only loses a small amount of efficiency for normal errors. In addition, it is shown that the loss in efficiency is at most 11.1% for estimating varying coefficient functions and is no greater than 13.6% for estimating parametric components. To achieve sparsity with high-dimensional covariates, we propose adaptive penalization methods for variable selection in the semiparametric varying-coefficient partially linear model and prove that the methods possess the oracle property. Extensive Monte Carlo simulation studies are conducted to examine the finite-sample performance of the proposed procedures. Finally, we apply the new methods to analyze the plasma beta-carotene level data.",WOS:000288183800010,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'COMPOSITE QUANTILE REGRESSION', 'ABSOLUTE DEVIATION METHOD', 'ASYMPTOTICS']",NEW EFFICIENT ESTIMATION AND VARIABLE SELECTION METHODS FOR SEMIPARAMETRIC VARYING-COEFFICIENT PARTIALLY LINEAR MODELS,2011
768,"We consider the problem of model selection and estimation in situations where the number of parameters diverges with the sample size. When the dimension is high, an ideal method should have the oracle property [J Amer. Statist. Assoc. 96 (2001) 1348-1360] and [Ann. Statist. 32 (2004) 928-961] which ensures the optimal large sample performance. Furthermore, the high-dimensionality often induces the collinearity problem, which should be properly handled by the ideal method. Many existing variable selection methods fail to achieve both goals simultaneously. In this paper, we propose the adaptive elastic-net that combines the strengths of the quadratic regularization and the adaptively weighted lasso shrinkage. Under weak regularity conditions, we establish the oracle property of the adaptive elastic-net. We show by simulations that the adaptive elastic-net deals with the collinearity problem better than the other oracle-like methods, thus enjoying much improved finite sample performance.",WOS:000268113500005,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'VARIABLE SELECTION', 'WAVELET SHRINKAGE', 'ORACLE PROPERTIES', 'LASSO', 'REGRESSION', 'CONSISTENCY', 'ESTIMATORS', 'MODEL']",ON THE ADAPTIVE ELASTIC-NET WITH A DIVERGING NUMBER OF PARAMETERS,2009
769,,WOS:000336888400005,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'MODEL SELECTION', 'VARIABLE SELECTION', 'DANTZIG SELECTOR', 'REGRESSION']","DISCUSSION: ""A SIGNIFICANCE TEST FOR THE LASSO""",2014
770,"We present an R package, msSurv, to calculate the marginal (that is, not conditional on any covariates) state occupation probabilities, the state entry and exit time distributions, and the marginal integrated transition hazard for a general, possibly non-Markov, multistate system under left-truncation and right censoring. For a Markov model, msSurv also calculates and returns the transition probability matrix between any two states. Dependent censoring is handled via modeling the censoring hazard through observable covariates. Pointwise confidence intervals for the above mentioned quantities are obtained and returned for independent censoring from closed-form variance estimators and for dependent censoring using the bootstrap.",WOS:000309535800001,JOURNAL OF STATISTICAL SOFTWARE,"['STAGE OCCUPATION PROBABILITIES', 'INTEGRATED TRANSITION HAZARDS', 'COMPETING RISKS', 'CONFIDENCE-INTERVALS', 'CENSORED DATA', 'INFERENCE', 'MATRIX', 'ENTRY', 'BANDS']",msSurv: An R Package for Nonparametric Estimation of Multistate Models,2012
771,"The grofit package was developed to fit many growth curves obtained under different conditions in order to derive a conclusive dose-response curve, for instance for a compound that potentially affects growth. grofit fits data to different parametric models and in addition provides a model free spline method to circumvent systematic errors that might occur within application of parametric methods. This amendment increases the reliability of the characteristic parameters (e. g., lag phase, maximal growth rate, stationary phase) derived from a single growth curve. By relating obtained parameters to the respective condition (e. g., concentration of a compound) a dose response curve can be derived that enables the calculation of descriptive pharma-/toxicological values like half maximum effective concentration (EC50). Bootstrap and cross-validation techniques are used for estimating confidence intervals of all derived parameters.",WOS:000275203800001,JOURNAL OF STATISTICAL SOFTWARE,['CELL-GROWTH'],grofit: Fitting Biological Growth Curves with R,2010
772,"By making use of martingale representations, we derive the asymptotic normality of particle filters in hidden Markov models and a relatively simple formula for their asymptotic variances. Although repeated resamplings result in complicated dependence among the sample paths, the asymptotic variance formula and martingale representations lead to consistent estimates of the standard errors of the particle filter estimates of the hidden states.",WOS:000330204900007,ANNALS OF STATISTICS,"['MONTE-CARLO METHODS', 'INFERENCE']",A GENERAL THEORY OF PARTICLE FILTERS IN HIDDEN MARKOV MODELS AND SOME APPLICATIONS,2013
773,"We estimate the quantum state of a light beam from results of quantum homodyne measurements performed on identically prepared quantum Systems. The state is represented through the Wigner function, a generalized probability density on R(2) which may take negative values and must respect intrinsic positivity constraints imposed by quantum physics. The effect of the losses due to detection inefficiencies, which are always present in a real experiment, is the addition to the tomographic data of independent Gaussian noise.
We construct a kernel estimator for the Wigner function, prove that it is minimax efficient for the pointwise risk over a class of infinitely differentiable functions, and implement it for numerical results. We construct adaptive estimators, that is, which do not depend on the smoothness parameters, and prove that in some setups they attain the minimax rates for the corresponding smoothness class.",WOS:000248987600001,ANNALS OF STATISTICS,"['POSITRON EMISSION TOMOGRAPHY', 'DENSITY-MATRIX', 'QUASIPROBABILITY DISTRIBUTIONS', 'STATES', 'LIGHT', 'RECONSTRUCTION', 'COHERENT']",Minimax and adaptive estimation of the wigner function in quantum homodyne tomography with noisy data,2007
774,"Statistical analysis based on random fields has become a widely used approach in order to better understand real processes in many fields such as engineering, environmental sciences, etc. Data analysis based on random fields can be sometimes problematic to carry out from the inferential prospective. Examples are when dealing with: large dataset, counts or binary responses and extreme values data. This article explains how to perform, with the R package CompRandFld, challenging statistical analysis based on Gaussian, binary and max-stable random fields. The software provides tools for performing the statistical inference based on the composite likelihood in complex problems where standard likelihood methods are difficult to apply. The principal features are illustrated by means of simulation examples and an application of Irish daily wind speeds.",WOS:000349845800001,JOURNAL OF STATISTICAL SOFTWARE,"['MAX-STABLE PROCESSES', 'SPATIAL DATA SETS', 'COMPOSITE LIKELIHOOD APPROACH', 'SPACE-TIME DATA', 'COVARIANCE FUNCTIONS', 'MODELS', 'INFERENCE', 'EXTREMES', 'INDEPENDENCE', 'DEPENDENCE']",Analysis of Random Fields Using CompRandFld,2015
775,"This special volume collates ten issues under the rubric ""Spectroscopy and Chemometrics in R"". In so doing, it provides an overview of the breadth, depth and state of the art of R-based software projects for spectroscopy and chemometrics applications. Just as the authors have contributed to R their documentation and source code, so has R contributed to the quality, standardization and dissemination of their software, as this volume attests. We hope that the volume is inspiring to both computational statisticians interested in applications of their methodologies and to spectroscopists or chemometricians in need of solutions to their data analysis problems.",WOS:000244067000001,JOURNAL OF STATISTICAL SOFTWARE,,"An introduction to the special volume ""Spectroscopy and Chemometrics in R""",2007
776,"Clustering of variables is as a way to arrange variables into homogeneous clusters, i.e., groups of variables which are strongly related to each other and thus bring the same information. These approaches can then be useful for dimension reduction and variable selection. Several specific methods have been developed for the clustering of numerical variables. However concerning qualitative variables or mixtures of quantitative and qualitative variables, far fewer methods have been proposed. The R package ClustOfVar was specifically developed for this purpose. The homogeneity criterion of a cluster is defined as the sum of correlation ratios (for qualitative variables) and squared correlations (for quantitative variables) to a synthetic quantitative variable, summarizing ""as good as possible"" the variables in the cluster. This synthetic variable is the first principal component obtained with the PCAMIX method. Two clustering algorithms are proposed to optimize the homogeneity criterion: iterative relocation algorithm and ascendant hierarchical clustering. We also propose a bootstrap approach in order to determine suitable numbers of clusters. We illustrate the methodologies and the associated package on small datasets.",WOS:000309535500001,JOURNAL OF STATISTICAL SOFTWARE,,ClustOfVar: An R Package for the Clustering of Variables,2012
777,"The package nacopula provides procedures for constructing nested Archimedean copulas in any dimensions and with any kind of nesting structure, generating vectors of random variates from the constructed objects, computing function values and probabilities of falling into hypercubes, as well as evaluation of characteristics such as Kendall's tau and the tail-dependence coefficients. As by-products, algorithms for various distributions, including exponentially tilted stable and Sibuya distributions, are implemented. Detailed examples are given.",WOS:000288205600001,JOURNAL OF STATISTICAL SOFTWARE,['DISTRIBUTIONS'],Nested Archimedean Copulas Meet R: The nacopula Package,2011
778,"Packaged statistical software for analyzing categorical, repeated measures marginal models on sample survey data with binary covariates does not appear to be available. Consequently, this report describes a customized SAS program which accomplishes such an analysis on survey data with jackknifed replicate weights for which the primary sampling unit information has been suppressed for respondent confidentiality. First, the program employs the Macro Language and the Output Delivery System (ODS) to estimate the means and covariances of indicator variables for the response variables, taking the design into account. Then, it uses PROC CATMOD and ODS, ignoring the survey design, to obtain the design matrix and hypothesis test specifications. Finally, it enters these results into another run of CATMOD, which performs automated direct input of the survey design specifications and accomplishes the appropriate analysis. This customized SAS program can be employed, with minor editing, to analyze general categorical, repeated measures marginal models on sample surveys with replicate weights. Finally, the results of our analysis accounting for the survey design are compared to the results of two alternate analyses of the same data. This comparison confirms that such alternate analyses, which do not properly account for the design, do not produce useful results.",WOS:000236151500001,JOURNAL OF STATISTICAL SOFTWARE,['CATEGORICAL DATA'],Analyzing repeated measures marginal models on sample surveys with resampling methods,2006
779,"We introduce a general framework to handle structured models (sparse and block-sparse with possibly overlapping blocks). We discuss new methods for their recovery from incomplete observation, corrupted with deterministic and stochastic noise, using block-l(1) regularization. While the current theory provides promising bounds for the recovery errors under a number of different, yet mostly hard to verify conditions, our emphasis is on verifiable conditions on the problem parameters (sensing matrix and the block structure) which guarantee accurate recovery. Verifiability of our conditions not only leads to efficiently computable bounds for the recovery error but also allows us to optimize these error bounds with respect to the method parameters, and therefore construct estimators with improved statistical properties. To justify our approach, we also provide an oracle inequality, which links the properties of the proposed recovery algorithms and the best estimation performance. Furthermore, utilizing these verifiable conditions, we develop a computationally cheap alternative to block-l(1) minimization, the non-Euclidean Block Matching Pursuit algorithm. We close by presenting a numerical study to investigate the effect of different block regularizations and demonstrate the performance of the proposed recoveries.",WOS:000321845400012,ANNALS OF STATISTICS,"['DANTZIG SELECTOR', 'GROUP LASSO', 'REGRESSION', 'REPRESENTATIONS', 'L(1)-RECOVERY', 'UNION']",ACCURACY GUARANTIES FOR l(1) RECOVERY OF BLOCK-SPARSE SIGNALS,2012
780,"We introduce nonparametric regularization of the eigenvalues of a sample covariance matrix through splitting of the data (NERCOME), and prove that NERCOME enjoys asymptotic optimal nonlinear shrinkage of eigenvalues with respect to the Frobenius norm. One advantage of NERCOME is its computational speed when the dimension is not too large. We prove that NERCOME is positive definite almost surely, as long as the true covariance matrix is so, even when the dimension is larger than the sample size. With respect to the Stein's loss function, the inverse of our estimator is asymptotically the optimal precision matrix estimator. Asymptotic efficiency loss is defined through comparison with an ideal estimator, which assumed the knowledge of the true covariance matrix. We show that the asymptotic efficiency loss of NERCOME is almost surely 0 with a suitable split location of the data. We also show that all the aforementioned optimality holds for data with a factor structure. Our method avoids the need to first estimate any unknowns from a factor model, and directly gives the covariance or precision matrix estimator, which can be useful when factor analysis is not the ultimate goal. We compare the performance of our estimators with other methods through extensive simulations and real data analysis.",WOS:000375175200002,ANNALS OF STATISTICS,"['DIMENSIONAL TIME-SERIES', 'CONVERGENCE', 'SELECTION', 'NUMBER', 'LASSO', 'RATES']",NONPARAMETRIC EIGENVALUE-REGULARIZED PRECISION OR COVARIANCE MATRIX ESTIMATOR,2016
781,,WOS:000321842400008,ANNALS OF STATISTICS,,"BACKFITTING AND SMOOTH BACKFITTING FOR ADDITIVE QUANTILE MODELS (vol 38, pg 2857, 2010)",2012
782,"Standard blockwise empirical likelihood (BEL) for stationary, weakly dependent time series requires specifying a fixed block length as a tuning parameter for setting confidence regions. This aspect can be difficult and impacts coverage accuracy. As an alternative, this paper proposes a new version of BEL based on a simple, though nonstandard, data-blocking rule which uses a data block of every possible length. Consequently, the method does not involve the usual block selection issues and is also anticipated to exhibit better coverage performance. Its nonstandard blocking scheme, however, induces nonstandard asymptotics and requires a significantly different development compared to standard BEL. We establish the large-sample distribution of log-ratio statistics from the new BEL method for calibrating confidence regions for mean or smooth function parameters of time series. This limit law is not the usual chi-square one, but is distribution-free and can be reproduced through straightforward simulations. Numerical studies indicate that the proposed method generally exhibits better coverage accuracy than standard BEL.",WOS:000330204900013,ANNALS OF STATISTICS,"['WEAKLY DEPENDENT PROCESSES', 'CONFIDENCE-INTERVALS', 'HETEROSKEDASTICITY', 'HYPOTHESES', 'STATISTICS', 'SEQUENCES', 'BOOTSTRAP', 'VARIANCE', 'MODELS']",A NONSTANDARD EMPIRICAL LIKELIHOOD FOR TIME SERIES,2013
783,"The gridGraphics package provides a function, grid. echo(), that can be used to convert a plot drawn with the graphics package to a visually identical plot drawn using grid. This conversion provides access to a variety of grid tools for making customisations and additions to the plot that are not possible with the graphics package.",WOS:000357431900013,R JOURNAL,,The gridGraphics Package,2015
784,"In longitudinal studies of disease, patients can experience several events across a follow-up period. Analysis of such studies can be successfully performed by multi-state models. In the multi-state framework, issues of interest include the study of the relationship between covariates and disease evolution, estimation of transition probabilities, and survival rates. This paper introduces p3state.msm, a software application for R which performs inference in an illness-death model. It describes the capabilities of the program for estimating semi-parametric regression models and for implementing nonparametric estimators for several quantities. The main feature of the package is its ability for obtaining non-Markov estimates for the transition probabilities. Moreover, the methods can also be used in progressive three-state models. In such a model, estimators for other quantities, such as the bivariate distribution function (for sequentially ordered events), are also given. The software is illustrated using data from the Stanford Heart Transplant Study.",WOS:000285981300001,JOURNAL OF STATISTICAL SOFTWARE,"['EMPIRICAL TRANSITION MATRIX', 'MULTISTATE MODELS', 'COMPETING RISKS', 'PACKAGE', 'PROBABILITIES']",p3state.msm: Analyzing Survival Data from an Illness-Death Model,2011
785,"The problem of constructing Bayesian optimal discriminating designs for a class of regression models with respect to the T-optimality criterion introduced by Atkinson and Fedorov [Biometrika 62 (1975a) 57-70] is considered. It is demonstrated that the discretization of the integral with respect to the prior distribution leads to locally T-optimal discriminating design problems with a large number of model comparisons. Current methodology for the numerical construction of discrimination designs can only deal with a few comparisons, but the discretization of the Bayesian prior easily yields to discrimination design problems for more than 100 competing models. A new efficient method is developed to deal with problems of this type. It combines some features of the classical exchange type algorithm with the gradient methods. Convergence is proved, and it is demonstrated that the new method can find Bayesian optimal discriminating designs in situations where all currently available procedures fail.",WOS:000362697700004,ANNALS OF STATISTICS,"['REGRESSION-MODELS', 'ALGORITHM']",BAYESIAN T-OPTIMAL DISCRIMINATING DESIGNS,2015
786,"This paper describes CADFtest, an R package for testing for the presence of a unit root in a time series using the covariate-augmented Dickey-Fuller (CADF) test proposed in Hansen (1995b). The procedures presented here are user friendly, allow fully automatic model specification, and allow computation of the asymptotic p values of the test.",WOS:000270821600001,JOURNAL OF STATISTICAL SOFTWARE,"['TIME-SERIES', 'COINTEGRATION TESTS', 'PANEL-DATA', 'POWER', 'MODEL', 'STATISTICS', 'SELECTION', 'ORDER']",Unit Root CADF Testing with R,2009
787,"We describe here an online software suite VOStat written mainly for the Virtual Observatory, a novel structure in which astronomers share terabyte scale data. Written mostly in the public-domain statistical computing language and environment R, it can do a variety of statistical analysis on multidimensional, multi-epoch data with errors. Included are techniques which allow astronomers to start with multi-color data in the form of low-resolution spectra and select special kinds of sources in a variety of ways including color outliers. Here we describe the tool and demonstrate it with an example from Palomar-QUEST, a synoptic sky survey.",WOS:000244068500001,JOURNAL OF STATISTICAL SOFTWARE,,Using R-based VOStat as a low-resolution spectrum analysis tool,2007
788,,WOS:000348651700017,R JOURNAL,,Conference Report: Polish Academic R User Meeting,2014
789,"The structure, or Hasse, diagram described by Taylor and Hilton ( 1981, American Statistician) provides a visual display of the relationships between factors for balanced complete experimental designs. Using the Hasse diagram, rules exist for determining the appropriate linear model, ANOVA table, expected means squares, and F-tests in the case of balanced designs. This procedure has been implemented in Lisp-Stat using a software representation of the experimental design. The user can interact with the Hasse diagram to add, change, or delete factors and see the effect on the proposed analysis. The system has potential uses in teaching and consulting.",WOS:000232831400001,JOURNAL OF STATISTICAL SOFTWARE,,Visualizing experimental designs for balanced ANOVA models using Lisp-Stat,2005
790,"In this article, operational details of the R package OrdNor that is designed for the concurrent generation of correlated ordinal and normal data are described, and examples of some important functions are given. The package provides needed tools that have been lacking for generating multivariate data with a mixture of ordinal and normal components.",WOS:000366015200001,JOURNAL OF STATISTICAL SOFTWARE,"['MARGINAL DISTRIBUTIONS', 'BINARY VARIABLES', 'ASSOCIATION', 'ALGORITHM']",OrdNor: An R Package for Concurrent Generation of Correlated Ordinal and Normal Data,2015
791,"Suppose we observe samples of a subset of a collection of random variables. No additional information is provided about the number of latent variables, nor of the relationship between the latent and observed variables. Is it possible to discover the number of latent components, and to learn a statistical model over the entire collection of variables? We address this question in the setting in which the latent and observed variables are jointly Gaussian, with the conditional statistics of the observed variables conditioned on the latent variables being specified by a graphical model. As a first step we give natural conditions under which such latent-variable Gaussian graphical models are identifiable given marginal statistics of only the observed variables. Essentially these conditions require that the conditional graphical model among the observed variables is sparse, while the effect of the latent variables is ""spread out"" over most of the observed variables. Next we propose a tractable convex program based on regularized maximum-likelihood for model selection in this latent-variable setting; the regularizer uses both the l(1) norm and the nuclear norm. Our modeling framework can be viewed as a combination of dimensionality reduction (to identify latent variables) and graphical modeling (to capture remaining statistical structure not attributable to the latent variables), and it consistently estimates both the number of latent components and the conditional graphical model structure among the observed variables. These results are applicable in the high-dimensional setting in which the number of latent/observed variables grows with the number of samples of the observed variables. The geometric properties of the algebraic varieties of sparse matrices and of low-rank matrices play an important role in our analysis.",WOS:000312899000001,ANNALS OF STATISTICS,"['COVARIANCE-MATRIX ESTIMATION', 'PRINCIPAL COMPONENTS', 'LASSO', 'DECOMPOSITION', 'ALGORITHM', 'EQUATIONS']",LATENT VARIABLE GRAPHICAL MODEL SELECTION VIA CONVEX OPTIMIZATION,2012
792,"Applying the Benjamini and Hochberg (B H) method to multiple Student's t tests is a popular technique for gene selection in microarray data analysis. Given the nonnormality of the population, the true p-values of the hypothesis tests are typically unknown. Hence it is common to use the standard normal distribution N(0, 1), Student's t distribution t(n-1) or the bootstrap method to estimate the p-values. In this paper, we prove that when the population has the finite 4th moment and the dimension m and the sample size n satisfy log m = o(n(1/3)), the B-H method controls the false discovery rate (FDR) and the false discovery proportion (FDP) at a given level a asymptotically with p-values estimated from N(0, 1) or t(n-1) distribution. However, a phase transition phenomenon occurs when log m >= c(0)n(1/3). In this case, the FDR and the FDP of the B-H method may be larger than a or even converge to one. In contrast, the bootstrap calibration is accurate for log m = o(n(1/2)) as long as the underlying distribution has the sub-Gaussian tails. However, such a light-tailed condition cannot generally be weakened. The simulation study shows that the bootstrap calibration is very conservative for the heavy tailed distributions. To solve this problem, a regularized bootstrap correction is proposed and is shown to be robust to the tails of the distributions. The simulation study shows that the regularized bootstrap method performs better than its usual counterpart.",WOS:000344632400011,ANNALS OF STATISTICS,"['STUDENTS-T', 'LARGE DEVIATION', 'HYPOTHESIS']",PHASE TRANSITION AND REGULARIZED BOOTSTRAP IN LARGE-SCALE t-TESTS WITH FALSE DISCOVERY RATE CONTROL,2014
793,"Coefficient estimation and variable selection in multiple linear regression is routinely done in the (penalized) least squares (LS) framework. The concept of model selection oracle introduced by Fan and Li [J. Amer. Statist. Assoc. 96 (2001) 1348-1360] characterizes the optimal behavior of a model selection procedure. However, the least-squares oracle theory breaks down if the error variance is infinite. In the current paper we propose a new regression method called composite quantile regression (CQR). We show that the oracle model selection theory using the CQR oracle works beautifully even when the error variance is infinite. We develop a new oracular procedure to achieve the optimal properties of the CQR oracle. When the error variance is finite, CQR still enjoys great advantages in terms of estimation efficiency. We show that the relative efficiency of CQR compared to the least squares is greater than 70% regardless the error distribution. Moreover, CQR could be much more efficient and sometimes arbitrarily more efficient than the least squares. The same conclusions hold when comparing a CQR-oracular estimator with a LS-oracular estimator.",WOS:000256504400004,ANNALS OF STATISTICS,['LASSO'],Composite quantile regression and the oracle model selection theory,2008
794,"We study the asymptotic behavior of kernel estimators of asymptotic variances (or long-run variances) for a class of adaptive Markov chains. The convergence is studied both in L(P) and almost surely. The results also apply to Markov chains and improve on the existing literature by imposing weaker conditions. We illustrate the results with applications to the GARCH(1, 1) Markov model and to an adaptive MCMC algorithm for Bayesian logistic regression.",WOS:000291183300011,ANNALS OF STATISTICS,"['COVARIANCE-MATRIX ESTIMATION', 'MCMC ALGORITHMS', 'HETEROSKEDASTICITY', 'CONSISTENCY', 'ERGODICITY']",KERNEL ESTIMATORS OF ASYMPTOTIC VARIANCE FOR ADAPTIVE MARKOV CHAIN MONTE CARLO,2011
795,"We present the frailtyHL package for fitting semi-parametric frailty models using h-likelihood. This package allows lognormal or gamma frailties for random-effect distribution, and it fits shared or multilevel frailty models for correlated survival data. Functions are provided to format and summarize the frailtyHL results. The estimates of fixed effects and frailty parameters and their standard errors are calculated. We illustrate the use of our package with three well-known data sets and compare our results with various alternative R-procedures.",WOS:000313198000005,R JOURNAL,"['GENERALIZED LINEAR-MODELS', 'HIERARCHICAL LIKELIHOOD', 'SURVIVAL ANALYSIS', 'INFORMATION']",frailtyHL: A Package for Fitting Frailty Models with H-likelihood,2012
796,"Although R is still predominantly applied for statistical analysis and graphical representation, it is rapidly becoming more suitable for mathematical computing. One of the fields where considerable progress has been made recently is the solution of differential equations. Here we give a brief overview of differential equations that can now be solved by R.",WOS:000208590000002,R JOURNAL,"['ODE SOLVER', 'SELECTION']",Solving Differential Equations in R,2010
797,"Distributed lag non-linear models (DLNMs) represent a modeling framework to flexibly describe associations showing potentially non-linear and delayed effects in time series data. This methodology rests on the definition of a crossbasis, a bi-dimensional functional space expressed by the combination of two sets of basis functions, which specify the relationships in the dimensions of predictor and lags, respectively. This framework is implemented in the R package dlnm, which provides functions to perform the broad range of models within the DLNM family and then to help interpret the results, with an emphasis on graphical representation. This paper offers an overview of the capabilities of the package, describing the conceptual and practical steps to specify and interpret DLNMs with an example of application to real data.",WOS:000293391200001,JOURNAL OF STATISTICAL SOFTWARE,"['AIR-POLLUTION', 'TIME-SERIES', 'MORTALITY DISPLACEMENT', 'AMBIENT-TEMPERATURE', 'HEALTH', 'DEATHS', 'CITIES', 'LONDON']",Distributed Lag Linear and Non-Linear Models in R: The Package dlnm,2011
798,"We propose a new class of estimators for Pickands dependence function which is based on the concept of minimum distance estimation. An explicit integral representation of the function A* (t), which minimizes a weighted L(2)-distance between the logarithm of the copula C(y(1-t), y(t)) and functions of the form A (t) log(y) is derived. If the unknown copula is an extreme-value copula, the function A* (t) coincides with Pickands dependence function. Moreover, even if this is not the case, the function A* (t) always satisfies the boundary conditions of a Pickands dependence function. The estimators are obtained by replacing the unknown copula by its empirical counterpart and weak convergence of the corresponding process is shown. A comparison with the commonly used estimators is performed from a theoretical point of view and by means of a simulation study. Our asymptotic and numerical results indicate that some of the new estimators outperform the estimators, which were recently proposed by Genest and Segers [Ann. Statist. 37 (2009) 2990-3022]. As a by-product of our results, we obtain a simple test for the hypothesis of an extreme-value copula, which is consistent against all positive quadrant dependent alternatives satisfying weak differentiability assumptions of first order.",WOS:000296995500005,ANNALS OF STATISTICS,"['VALUE DISTRIBUTIONS', 'NONPARAMETRIC-ESTIMATION', 'VALUE COPULAS', 'MODELS']",NEW ESTIMATORS OF THE PICKANDS DEPENDENCE FUNCTION AND A TEST FOR EXTREME-VALUE DEPENDENCE,2011
799,"Subject dropout is very common in practical applications of crossover designs. However, there is very limited design literature taking this into account. Optimality results have not yet been well established due to the complexity of the problem. This paper establishes feasible, as well as necessary and sufficient conditions for a crossover design to be universally optimal in approximate design theory in the presence of subject dropout. These conditions are essentially linear equations with respect to proportions of all possible treatment sequences being applied to subjects and hence they can be easily solved. A general algorithm is proposed to derive exact designs which are shown to be efficient and robust.",WOS:000317451200003,ANNALS OF STATISTICS,['MODEL'],UNIVERSALLY OPTIMAL CROSSOVER DESIGNS UNDER SUBJECT DROPOUT,2013
800,"Zou [J. Amer. Statist. Assoc. 101 (2006) 1418-1429] proposed the Adaptive LASSO (ALASSO) method for simultaneous variable selection and estimation of the regression parameters, and established its oracle property. In this paper, we investigate the rate of convergence of the ALASSO estimator to the oracle distribution when the dimension of the regression parameters may grow to infinity with the sample size. It is shown that the rate critically depends on the choices of the penalty parameter and the initial estimator, among other factors, and that confidence intervals (CIs) based on the oracle limit law often have poor coverage accuracy. As an alternative, we consider the residual bootstrap method for the ALASSO estimators that has been recently shown to be consistent; cf. Chatterjee and Lahiri [J. Amer. Statist. Assoc. 106 (2011a) 608-625]. We show that the bootstrap applied to a suitable studentized version of the ALASSO estimator achieves second-order correctness, even when the dimension of the regression parameters is unbounded. Results from a moderately large simulation study show marked improvement in coverage accuracy for the bootstrap CIs over the oracle based CIs.",WOS:000321847600007,ANNALS OF STATISTICS,"['DIMENSIONAL REGRESSION-MODELS', 'VARIABLE SELECTION', 'ASYMPTOTIC PROPERTIES', 'DANTZIG SELECTOR', 'INFERENCE', 'SPARSITY']",RATES OF CONVERGENCE OF THE ADAPTIVE LASSO ESTIMATORS TO THE ORACLE DISTRIBUTION AND HIGHER ORDER REFINEMENTS BY THE BOOTSTRAP,2013
801,"In linear models and multivariate normal situations, prior information in linear inequality form may be encountered, or linear inequality hypotheses may be subjected to statistical tests. R package ic.infer has been developed to support inequality-constrained estimation and testing for such situations. This article gives an overview of the principles underlying inequality-constrained inference that are far less well-known than methods for unconstrained or equality-constrained models, and describes their implementation in the package.",WOS:000275204100001,JOURNAL OF STATISTICAL SOFTWARE,"['RELATIVE IMPORTANCE', 'TESTS', 'HYPOTHESES', 'REGRESSION', 'VARIANCE', 'POWERFUL']",Inference with Linear Equality and Inequality Constraints Using R: The Package ic.infer,2010
802,"This article introduces the R package ExtremeBounds to perform extreme bounds analysis (EBA), a sensitivity test that examines how robustly the dependent variable of a regression model is related to a variety of possible determinants. ExtremeBounds supports Leamer's EBA that focuses on the upper and lower extreme bounds of regression coefficients, as well as Sala-i-Martin's EBA which considers their entire distribution. In contrast to existing alternatives, it can estimate models of a variety of user-defined sizes, use regression models other than ordinary least squares, incorporate non-linearities in the model specification, and apply custom weights and standard errors. To alleviate concerns about the multicollinearity and conceptual overlap of examined variables, ExtremeBounds allows users to specify sets of mutually exclusive variables, and can restrict the analysis to coefficients from regression models that yield a variance inflation factor within a pre-specified limit.",WOS:000389073200001,JOURNAL OF STATISTICAL SOFTWARE,"['CONSISTENT COVARIANCE-MATRIX', 'LONG-TERM GROWTH', 'SENSITIVITY-ANALYSIS', 'DETERMINANTS', 'REGRESSION', 'HETEROSKEDASTICITY', 'ROBUST', 'ECONOMETRICS', 'CON']",ExtremeBounds: Extreme Bounds Analysis in R,2016
803,"Ordinary differential equations (ODEs) are commonly used to model dynamic behavior of a system. Because many parameters are unknown and have to be estimated from the observed data, there is growing interest in statistics to develop efficient estimation procedures for these parameters. Among the proposed methods in the literature, the generalized profiling estimation method developed by Ramsay and colleagues is particularly promising for its computational efficiency and good performance. In this approach, the ODE solution is approximated with a linear combination of basis functions. The coefficients of the basis functions are estimated by a penalized smoothing procedure with an ODE-defined penalty. However, the statistical properties of this procedure are not known. In this paper, we first give an upper bound on the uniform norm of the difference between the true solutions and their approximations. Then we use this bound to prove the consistency and asymptotic normality of this estimation procedure. We show that the asymptotic covariance matrix is the same as that of the maximum likelihood estimation. Therefore, this procedure is asymptotically efficient. For a fixed sample and fixed basis functions, we study the limiting behavior of the approximation when the smoothing parameter tends to infinity. We propose an algorithm to choose the smoothing parameters and a method to compute the deviation of the spline approximation from solution without Solving the ODEs.",WOS:000273800100013,ANNALS OF STATISTICS,"['DYNAMIC-MODELS', 'SYSTEMS', 'INFERENCE', 'NETWORKS', 'BOUNDS']",ASYMPTOTIC EFFICIENCY AND FINITE-SAMPLE PROPERTIES OF THE GENERALIZED PROFILING ESTIMATION OF PARAMETERS IN ORDINARY DIFFERENTIAL EQUATIONS,2010
804,"For statistical analysis of functional magnetic resonance imaging (fMRI) data sets, we propose a data-driven approach based on independent component analysis (ICA) implemented in a new version of the AnalyzeFMRI R package. For fMRI data sets, spatial dimension being much greater than temporal dimension, spatial ICA is the computationally tractable approach generally proposed. However, for some neuroscientific applications, temporal independence of source signals can be assumed and temporal ICA becomes then an attractive exploratory technique. In this work, we use a classical linear algebra result ensuring the tractability of temporal ICA. We report several experiments on synthetic data and real MRI data sets that demonstrate the potential interest of our R package.",WOS:000296718900001,JOURNAL OF STATISTICAL SOFTWARE,"['MR IMAGE-ANALYSIS', 'ICA', 'CORTEX', 'TIME', 'IMPLEMENTATION', 'STATISTICS', 'ALGORITHMS', 'SEPARATION', 'ISSUES', 'BRAIN']",Temporal and Spatial Independent Component Analysis for fMRI Data Sets Embedded in the AnalyzeFMRI R Package,2011
805,"A Bernstein-von Mises theorem is derived for general semiparametric functionals. The result is applied to a variety of semiparametric problems in i.i.d. and non-i.i.d. situations. In particular, new tools are developed to handle semiparametric bias, in particular for nonlinear functionals and in cases where regularity is possibly low. Examples include the squared L-2-norm in Gaussian white noise, nonlinear functionals in density estimation, as well as functionals in autoregressive models. For density estimation, a systematic study of BvM results for two important classes of priors is provided, namely random histograms and Gaussian process priors.",WOS:000363437900002,ANNALS OF STATISTICS,"['OPTIMAL ADAPTIVE ESTIMATION', 'GAUSSIAN PROCESS PRIORS', 'POSTERIOR DISTRIBUTIONS', 'ASYMPTOTIC NORMALITY', 'WHITE-NOISE', 'RATES', 'DENSITY', 'CONTRACTION', 'REGRESSION']",A BERNSTEIN-VON MISES THEOREM FOR SMOOTH FUNCTIONALS IN SEMIPARAMETRIC MODELS,2015
806,"The dbmss package for R provides an easy-to-use toolbox to characterize the spatial structure of point patterns. Our contribution presents the state of the art of distance-based methods employed in economic geography and which are also used in ecology. Topographic functions such as Ripley's K, absolute functions such as Duranton and Overman's K-d and relative functions such as Marcon and Puech's M are implemented. Their confidence envelopes (including global ones) and tests against counterfactuals are included in the package.",WOS:000365986400001,JOURNAL OF STATISTICAL SOFTWARE,"['MICRO-GEOGRAPHIC DATA', 'MANUFACTURING-INDUSTRIES', '2ND-ORDER ANALYSIS', 'AGGLOMERATION', 'LOCALIZATION', 'STATISTICS']",Tools to Characterize Point Patterns: dbmss for R,2015
807,"Given a sample from some unknown continuous density f : R -> R, we construct adaptive confidence bands that are honest for all densities in a ""generic"" subset of the union of t-Holder balls, 0 < t <= r, where r is a fixed but arbitrary integer. The exceptional (""nongeneric"") set of densities for which our results do not hold is shown to be nowhere dense in the relevant Holder-norm topologies. In the course of the proofs we also obtain limit theorems for maxima of linear wavelet and kernel density estimators, which are of independent interest.",WOS:000275510800019,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'GAUSSIAN-PROCESSES', 'CONVERGENCE', 'INTERVALS', 'RATES', 'SETS', 'APPROXIMATION', 'INEQUALITIES', 'ADAPTATION', 'REGIONS']",CONFIDENCE BANDS IN DENSITY ESTIMATION,2010
808,"Sliced inverse regression (Duan and Li [Ann. Statist. 19 (1991) 505-530], Li [J. Amer. Statist. Assoc. 86 (1991) 316-342]) is an appealing dimension reduction method for regression models with multivariate covariates. It has been extended by Ferro and Yao [Statistics 37 (2003) 475-488, Statist. Sinica 15 (2005) 665-683] and Hsing and Ren [Ann. Statist. 37 (2009) 726-755] to functional covariates where the whole trajectories of random functional covariates are completely observed. The focus of this paper is to develop sliced inverse regression for intermittently and sparsely measured longitudinal covariates. We develop asymptotic theory for the new procedure and show, under some regularity conditions, that the estimated directions attain the optimal rate of convergence. Simulation studies and data analysis are also provided to demonstrate the performance of our method.",WOS:000336888400010,ANNALS OF STATISTICS,"['PROJECTION PURSUIT REGRESSION', 'DIMENSION-REDUCTION', 'MODELS', 'LINK']",INVERSE REGRESSION FOR LONGITUDINAL DATA,2014
809,"This user guide describes a Python package, PyMC, that allows users to efficiently code a probabilistic model and draw samples from its posterior distribution using Markov chain Monte Carlo techniques.",WOS:000281587500001,JOURNAL OF STATISTICAL SOFTWARE,,PyMC: Bayesian Stochastic Modelling in Python,2010
810,"Microarrays enable the expression levels of thousands of genes to be measured simultaneously. However, only a small fraction of these genes are expected to be expressed under different experimental conditions. Nowadays, filtering has been introduced as a step in the microarray preprocessing pipeline. Gene filtering aims at reducing the dimensionality of data by filtering redundant features prior to the actual statistical analysis. Previous filtering methods focus on the Affymetrix platform and can not be easily ported to the Illumina platform. As such, we developed a filtering method for Illumina bead arrays. We developed an R package, beadarrayFilter, to implement the latter method. In this paper, the main functions in the package are highlighted and using many examples, we illustrate how beadarrayFilter can be used to filter bead arrays.",WOS:000321944400017,R JOURNAL,,beadarrayFilter: An R Package to Filter Beads,2013
811,"This paper studies a class of exponential family models whose canonical parameters are specified as linear functionals of an unknown infinite-dimensional slope function. The optimal minimax rates of convergence for slope function estimation are established. The estimators that achieve the optimal rates are constructed by constrained maximum likelihood estimation with parameters whose dimension grows with sample size. A change-of-measure argument, inspired by Le Cam's theory of asymptotic equivalence, is used to eliminate the bias caused by the nonlinearity of exponential family models.",WOS:000321844300003,ANNALS OF STATISTICS,"['LINEAR-REGRESSION', 'MODELS', 'LIKELIHOOD', 'CONVERGENCE', 'PREDICTION', 'PARAMETERS', 'RATES']",ESTIMATION IN FUNCTIONAL REGRESSION FOR GENERAL EXPONENTIAL FAMILIES,2012
812,"This paper considers regularizing a covariance matrix of p variables estimated from it observations, by hard thresholding. We show that the thresholded estimate is consistent in the operator norm as long as the true covariance matrix is sparse in a suitable sense, the variables are Gaussian or sub-Gaussian, and (log p)/n -> 0, and obtain explicit rates. The results are uniform over families of covariance matrices which satisfy a fairly natural notion of sparsity. We discuss an intuitive resampling scheme for threshold selection and prove a general cross-validation result that justifies this approach. We also compare thresholding to other covariance estimators in simulations and on an example from climate data.",WOS:000262731400003,ANNALS OF STATISTICS,"['LARGEST EIGENVALUE', 'MATRICES', 'SELECTION', 'BAYES', 'LIKELIHOOD', 'LASSO', 'MODEL']",COVARIANCE REGULARIZATION BY THRESHOLDING,2008
813,"Let Y be a Gaussian vector whose components are independent with a common unknown variance. We consider the problem of estimating the mean p of Y by model selection. More precisely, we start with a collection S = {S(m), m is an element of M} of linear subspaces of R(n) and associate to each of these the least-squares estimator of mu on S(m). Then, we use a data driven penalized criterion in order to select one estimator among these. Our first objective is to analyze the performance of estimators associated to classical criteria such as FPE, AIC, BIC and AMDL. Our second objective is to propose better penalties that are versatile enough to take into account both the complexity of the collection S and the sample size. Then we apply those to solve various statistical problems such as variable selection, change point detections and signal estimation among others. Our results are based on a nonasymptotic risk bound with respect to the Euclidean loss for the selected estimator. Some analogous results are also established for the Kullback loss.",WOS:000265500500004,ANNALS OF STATISTICS,"['REGRESSION', 'SHRINKAGE', 'LASSO']",GAUSSIAN MODEL SELECTION WITH AN UNKNOWN VARIANCE,2009
814,"We propose an algorithm for evaluation of the cumulative bivariate normal distribution, building upon Marsaglia's ideas for evaluation of the cumulative univariate normal distribution. The algorithm delivers competitive performance and can easily be extended to arbitrary precision.",WOS:000316316400001,JOURNAL OF STATISTICAL SOFTWARE,,Recursive Numerical Evaluation of the Cumulative Bivariate Normal Distribution,2013
815,"We present the R package stellaR, which is designed to access and manipulate publicly available stellar evolutionary tracks and isochrones from the Pisa low-mass database. The procedures for extracting important stages in the evolution of a star from the database, for constructing isochrones from stellar tracks and for interpolating among tracks are discussed and demonstrated.
Due to the advance in the instrumentation, nowadays astronomers can deal with a huge amount of high-quality observational data. In the last decade impressive improvements of spectroscopic and photometric observational capabilities made available data which stimulated the research in the globular clusters field. The theoretical effort of recovering the evolutionary history of the clusters benefits from the computation of extensive databases of stellar tracks and isochrones, such as Pietrinferni et al. (2006); Dotter et al. (2008); Bertelli et al. (2008). We recently computed a large data set of stellar tracks and isochrones, ""The Pisa low-mass database"" (Dell'Omodarme et al., 2012), with up to date physical and chemical inputs, and made available all the calculations to the astrophysical community at the Centre de Donnes astronomiques de Strasbourg (CDS)(1), a data center dedicated to the collection and worldwide distribution of astronomical data.
In most databases, the management of the information and the extraction of the relevant evolutionary properties from libraries of tracks and/ or isochrones is the responsibility of the end users. Due to its extensive capabilities of data manipulation and analysis, however, R is an ideal choice for these tasks. Nevertheless R is not yet well known in astrophysics; up to December 2012 only seven astronomical or astrophysical-oriented packages have been published on CRAN (see the CRAN Task View Chemometrics and Computational Physics).
The package stellaR (Dell'Omodarme and Valle, 2012) is an effort to make available to the astrophysical community a basic tool set with the following capabilities: retrieve the required calculations from CDS; plot the information in a suitable form; construct by interpolation tracks or isochrones of compositions different to the ones available in the database; construct isochrones for age not included in the database; extract relevant evolutionary points from tracks or isochrones.",WOS:000321944400012,R JOURNAL,"['M-CIRCLE-DOT', 'DATABASE', 'MODELS']",stellaR: A Package to Manage Stellar Evolution Tracks and Isochrones,2013
816,"The dynamic properties and independence structure of stochastic kinetic models (SKMs) are analyzed. An SKM is a highly multivariate jump process used to model chemical reaction networks, particularly those in biochemical and cellular systems. We identify SKM subprocesses with the corresponding counting processes and propose a directed, cyclic graph (the kinetic independence graph or KIG) that encodes the local independence structure of their conditional intensities. Given a partition [A, D, B] of the vertices, the graphical separation A perpendicular to B vertical bar D in the undirected KIG has an intuitive chemical interpretation and implies that A is locally independent of B given A boolean OR D. It is proved that this separation also results in global independence of the internal histories of A and B conditional on a history of the jumps in D which, under conditions we derive, corresponds to the internal history of D. The results enable mathematical definition of a modularization of an SKM using its implied dynamics. Graphical decomposition methods are developed for the identification and efficient computation of nested modularizations. Application to an SKM of the red blood cell advances understanding of this biochemical system.",WOS:000280359400011,ANNALS OF STATISTICS,"['COUPLED CHEMICAL-REACTIONS', 'RED-BLOOD-CELL', 'METABOLIC NETWORKS', 'APPROXIMATIONS', 'EVOLUTION', 'SYSTEMS']","STOCHASTIC KINETIC MODELS: DYNAMIC INDEPENDENCE, MODULARITY AND GRAPHS",2010
817,"We investigate a method for extracting nonlinear principal components (NPCs). These NPCs maximize variation subject to smoothness and orthogonality constraints; but we allow for a general class of constraints and multivariate probability densities, including densities without compact support and even densities with algebraic tails. We provide primitive sufficient conditions for the existence of these NPCs. By exploiting the theory of continuous-time, reversible Markov diffusion processes, we give a different interpretation of these NPCs and the smoothness constraints. When the diffusion matrix is used to enforce smoothness, the NPCs maximize long-run variation relative to the overall variation subject to orthogonality constraints. Moreover, the NPCs behave as scalar autoregressions with heteroskedastic innovations; this supports semiparametric identification and estimation of a multivariate reversible diffusion process and tests of the overidentifying restrictions implied by Such a process from low-frequency data. We also explore implications for stationary, possibly nonreversible diffusion processes. Finally, we suggest a sieve method to estimate the NPCs from discretely-sampled data.",WOS:000271673700020,ANNALS OF STATISTICS,"['MULTIPLE TIME-SERIES', 'CANONICAL-ANALYSIS', 'FUNCTIONAL PRINCIPAL', 'SCALAR DIFFUSIONS', 'MARKOV-PROCESSES', 'VARIABLES']",NONLINEAR PRINCIPAL COMPONENTS AND LONG-RUN IMPLICATIONS OF MULTIVARIATE DIFFUSIONS,2009
818,"We propose a new sparsity-smoothness penalty for high-dimensional generalized additive models. The combination of sparsity and smoothness is crucial for mathematical theory as well as performance for finite-sample data. We present a computationally efficient algorithm, with provable numerical convergence properties, for optimizing the penalized likelihood. Furthermore, we provide oracle results which yield asymptotic optimality of our estimator for high dimensional but sparse additive models. Finally, an adaptive version of Our sparsity-smoothness penalized approach yields large additional performance gains.",WOS:000271673700003,ANNALS OF STATISTICS,"['DANTZIG SELECTOR', 'LASSO', 'REGRESSION', 'SPARSITY']",HIGH-DIMENSIONAL ADDITIVE MODELING,2009
819,"The mixtools package for R provides a set of functions for analyzing a variety of finite mixture models. These functions include both traditional methods, such as EM algorithms for univariate and multivariate normal mixtures, and newer methods that reflect some recent research in finite mixture models. In the latter category, mixtools provides algorithms for estimating parameters in a wide range of different mixture-of-regression contexts, in multinomial mixtures such as those arising from discretizing continuous multivariate data, in nonparametric situations where the multivariate component densities are completely unspecified, and in semiparametric situations such as a univariate location mixture of symmetric but otherwise unspecified densities. Many of the algorithms of the mixtools package are EM algorithms or are based on EM-like ideas, so this article includes an overview of EM algorithms for finite mixture models.",WOS:000271534100001,JOURNAL OF STATISTICAL SOFTWARE,"['NONPARAMETRIC-ESTIMATION', 'MULTIVARIATE MIXTURES', 'SWITCHING REGRESSIONS', 'EM ALGORITHM', 'DISTRIBUTIONS', 'INFERENCE', 'LIKELIHOOD']",mixtools: An R Package for Analyzing Finite Mixture Models,2009
820,,WOS:000357441000007,ANNALS OF STATISTICS,"['INVERSE PROBLEMS', 'GAUSSIAN PRIORS']","REJOINDER TO DISCUSSIONS OF ""FREQUENTIST COVERAGE OF ADAPTIVE NONPARAMETRIC BAYESIAN CREDIBLE SETS""",2015
821,"The POWERLIB SAS/IML software provides convenient power calculations for a wide range of multivariate linear models with Gaussian errors. The software includes the Box, Geisser-Greenhouse, Huynh-Feldt, and uncorrected tests in the ""univariate"" approach to repeated measures (UNIREP), the Hotelling Lawley Trace, Pillai-Bartlett Trace, and Wilks Lambda tests in ""multivariate"" approach (MULTIREP), as well as a limited but useful range of mixed models. The familiar univariate linear model with Gaussian errors is an important special case. For estimated covariance, the software provides confidence limits for the resulting estimated power. All power and confidence limits values can be output to a SAS dataset, which can be used to easily produce plots and tables for manuscripts.",WOS:000266310700001,JOURNAL OF STATISTICAL SOFTWARE,"['SAMPLE-SIZE', 'INTERNAL PILOTS', 'MIXED MODELS']",POWERLIB: SAS/IML Software for Computing Power in Multivariate Linear Models,2009
822,"Independence screening is a variable selection method that uses a ranking criterion to select significant variables, particularly for statistical models with nonpolynomial dimensionality or ""large p, small n"" paradigms when p can be as large as an exponential of the sample size n. In this paper we propose a robust rank correlation screening (RRCS) method to deal with ultra-high dimensional data. The new procedure is based on the Kendall tau correlation coefficient between response and predictor variables rather than the Pearson correlation of existing methods. The new method has four desirable features compared with existing independence screening methods. First, the sure independence screening property can hold only under the existence of a second order moment of predictor variables, rather than exponential tails or alikeness, even when the number of predictor variables grows as fast as exponentially of the sample size. Second, it can be used to deal with semiparametric models such as transformation regression models and single-index models under monotonic constraint to the link function without involving nonparametric estimation even when there are nonparametric functions in the models. Third, the procedure can be largely used against outliers and influence points in the observations. Last, the use of indicator functions in rank correlation screening greatly simplifies the theoretical derivation due to the boundedness of the resulting statistics, compared with previous studies on variable screening. Simulations are carried out for comparisons with existing methods and a real data example is analyzed.",WOS:000310650900021,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'GENERALIZED LINEAR-MODELS', 'SLICED INVERSE REGRESSION', 'DIMENSIONAL FEATURE SPACE', 'VARIABLE SELECTION', 'NP-DIMENSIONALITY', 'ORACLE PROPERTIES', 'DIVERGING NUMBER', 'LASSO', 'TRANSFORMATIONS']",ROBUST RANK CORRELATION BASED SCREENING,2012
823,"The core of the wavelet approach to nonparametric regression is thresholding of wavelet coefficients. This paper reviews a cross-validation method for the selection of the thresholding value in wavelet shrinkage of Oh, Kim, and Lee (2006), and introduces the R package CVThresh implementing details of the calculations for the procedures.
This procedure is implemented by coupling a conventional cross-validation with a fast imputation method, so that it overcomes a limitation of data length, a power of 2. It can be easily applied to the classical leave-one-out cross-validation and K-fold cross-validation. Since the procedure is computationally fast, a level-dependent cross-validation can be developed for wavelet shrinkage of data with various sparseness according to levels.",WOS:000236800700001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED LINEAR-MODELS', 'WAVELET SHRINKAGE']",CVThresh: R package for level-dependent cross-validation thresholding,2006
824,"Generalized likelihood ratio (GLR) test statistics are often used in the detection of spatial clustering in case-control and case-population datasets to check for a significantly large proportion of cases within some scanning window. The traditional spatial scan test statistic takes the supremum GLR value over all windows, whereas the average likelihood ratio (ALR) test statistic that we consider here takes an average of the GLR values. Numerical experiments in the literature and in this paper show that the ALR test statistic has more power compared to the spatial scan statistic. We develop in this paper accurate tail probability approximations of the ALR test statistic that allow us to by-pass computer intensive Monte Carlo procedures to estimate p-values. In models that adjust for covariates, these Monte Carlo evaluations require an initial fitting of parameters that can result in very biased p-value estimates.",WOS:000271673700010,ANNALS OF STATISTICS,"['INHOMOGENEOUS POPULATIONS', 'SEQUENTIAL-ANALYSIS', 'RENEWAL THEORY', 'DISEASE']",DETECTION OF SPATIAL CLUSTERING WITH AVERAGE LIKELIHOOD RATIO TEST STATISTICS,2009
825,"Motivated by the latest effort to employ banded matrices to estimate a high-dimensional covariance Sigma, we propose a test for Sigma being banded with possible diverging bandwidth. The test is adaptive to the ""large p, small n"" situations without assuming a specific parametric distribution for the data. We also formulate a consistent estimator for the bandwidth of a banded high-dimensional covariance matrix. The properties of the test and the bandwidth estimator are investigated by theoretical evaluations and simulation studies, as well as an empirical analysis on a protein mass spectroscopy data.",WOS:000310650900001,ANNALS OF STATISTICS,"['LARGEST EIGENVALUE', 'REGULARIZATION', 'DISTRIBUTIONS', 'INDEPENDENCE', 'LASSO']",TEST FOR BANDEDNESS OF HIGH-DIMENSIONAL COVARIANCE MATRICES AND BANDWIDTH ESTIMATION,2012
826,"Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if it encourages truthful reporting. It is local of order k if the score depends on the predictive density only through its value and the values of its derivatives of order up to k at the realizing event. Complementing fundamental recent work by Parry, Dawid and Lauritzen, we characterize the local proper scoring rules of order 2 relative to a broad class of Lebesgue densities on the real line, using a different approach. In a data example, we use local and nonlocal proper scoring rules to assess statistically postprocessed ensemble weather forecasts.",WOS:000304684900023,ANNALS OF STATISTICS,"['ASSESSING PROBABILISTIC FORECASTS', 'MODEL OUTPUT STATISTICS', 'MULTIVARIATE QUANTITIES', 'ENSEMBLE PREDICTIONS', 'SURFACE WINDS', 'DISTRIBUTIONS', 'INFORMATION']",LOCAL PROPER SCORING RULES OF ORDER TWO,2012
827,"Given a finite collection of estimators or classifiers, we study the problem of model selection type aggregation, that is, we construct a new estimator or classifier, called aggregate, which is nearly as good as the best among them with respect to a given risk criterion. We define our aggregate by a simple recursive procedure which solves an auxiliary stochastic linear programming problem related to the original nonlinear one and constitutes a special case of the mirror averaging algorithm. We show that the aggregate satisfies sharp oracle inequalities under some general assumptions. The results are applied to several problems including regression, classification and density estimation.",WOS:000260554100007,ANNALS OF STATISTICS,"['DENSITY-ESTIMATION', 'MODEL SELECTION', 'AGGREGATION', 'REGRESSION', 'PREDICTION']",LEARNING BY MIRROR AVERAGING,2008
828,"Analysis of stochastic models of networks is quite important in light of the huge influx of network data in social, information and bio sciences, but a proper statistical analysis of features of different stochastic models of networks is still underway. We propose bootstrap subsampling methods for finding empirical distribution of count features or ""moments"" (Bickel, Chen and Levina [Ann. Statist. 39 (2011) 2280-2301]) and smooth functions of these features for the networks. Using these methods, we cannot only estimate the variance of count features but also get good estimates of such feature counts, which are usually expensive to compute numerically in large networks. In our paper, we prove theoretical properties of the bootstrap estimates of variance of the count features as well as show their efficacy through simulation. We also use the method on some real network data for estimation of variance and expectation of some count features.",WOS:000363437900003,ANNALS OF STATISTICS,"['SOCIAL NETWORKS', 'MOTIFS']",SUBSAMPLING BOOTSTRAP OF COUNT FEATURES OF NETWORKS,2015
829,"In the low-dimensional case, the generalized additive coefficient model (GACM) proposed by Xue and Yang [Statist. Sinica 16 (2006) 1423-1446] has been demonstrated to be a powerful tool for studying nonlinear interaction effects of variables. In this paper, we propose estimation and inference procedures for the GACM when the dimension of the variables is high. Specifically, we propose a groupwise penalization based procedure to distinguish significant covariates for the ""large p small n"" setting. The procedure is shown to be consistent for model structure identification. Further, we construct simultaneous confidence bands for the coefficient functions in the selected model based on a refined two-step spline estimator. We also discuss how to choose the tuning parameters. To estimate the standard deviation of the functional estimator, we adopt the smoothed bootstrap method. We conduct simulation experiments to evaluate the numerical performance of the proposed methods and analyze an obesity data set from a genome-wide association study as an illustration.",WOS:000362697700009,ANNALS OF STATISTICS,"['VARIABLE SELECTION', 'POLYNOMIAL SPLINE', 'PENALIZED LIKELIHOOD', 'REGRESSION-MODELS', 'LOCAL ASYMPTOTICS', 'ORACLE PROPERTIES', 'CONFIDENCE BANDS', 'DIVERGING NUMBER', 'INDEX MODELS', 'OBESITY']",ESTIMATION AND INFERENCE IN GENERALIZED ADDITIVE COEFFICIENT MODELS FOR NONLINEAR INTERACTIONS WITH HIGH-DIMENSIONAL COVARIATES,2015
830,"This paper studies the multiplicity-correction effect of standard Bayesian variable-selection priors in linear regression. Our first goal is to clarify when, and how, multiplicity correction happens automatically in Bayesian analysis, and to distinguish this correction from the Bayesian Ockham's-razor effect. Our second goal is to contrast empirical-Bayes and fully Bayesian approaches to variable selection through examples, theoretical results and simulations. Considerable differences between the two approaches are found. In particular, we prove a theorem that characterizes a surprising aymptotic discrepancy between fully Bayes and empirical Bayes. This discrepancy arises from a different source than the failure to account for hyperparameter uncertainty in the empirical-Bayes estimate. Indeed, even at the extreme, when the empirical-Bayes estimate converges asymptotically to the true variable-inclusion probability, the potential for a serious difference remains.",WOS:000282402800001,ANNALS OF STATISTICS,"['GAUSSIAN GRAPHICAL MODELS', 'GROWTH', 'PRIORS']",BAYES AND EMPIRICAL-BAYES MULTIPLICITY ADJUSTMENT IN THE VARIABLE-SELECTION PROBLEM,2010
831,,WOS:000362697700016,ANNALS OF STATISTICS,,"Inverse regression for longitudinal data (vol 42, pg 563, 2014)",2015
832,"The present paper deals with the problem of allocating patients to two competing treatments in the presence of covariates or prognostic factors in order to achieve a good trade-off among ethical concerns, inferential precision and randomness in the treatment allocations. In particular we suggest a multipurpose design methodology that combines efficiency and ethical gain when the linear homoscedastic model with both treatment/covariate interactions and interactions among covariates is adopted. The ensuing compound optimal allocations of the treatments depend on the covariates and their distribution on the population of interest, as well as on the unknown parameters of the model. Therefore, we introduce the reinforced doubly adaptive biased coin desien, namely a general class of covariate-adjusted response-adaptive procedures that includes both continuous and discontinuous randomization functions, aimed to target any desired allocation proportion. The properties of this proposal are described both theoretically and through simulations.",WOS:000310650900002,ANNALS OF STATISTICS,"['PROGNOSTIC-FACTORS', 'ASYMPTOTIC PROPERTIES', 'OPTIMAL ALLOCATION', 'NORMAL RESPONSES', 'EQUIVALENCE']",MULTI-OBJECTIVE OPTIMAL DESIGNS IN COMPARATIVE CLINICAL TRIALS WITH COVARIATES: THE REINFORCED DOUBLY ADAPTIVE BIASED COIN DESIGN,2012
833,"The structure of the package vars and its implementation of vector autoregressive, structural vector autoregressive and structural vector error correction models are explained in this paper. In addition to the three cornerstone functions VAR (), SVAR () and SVEC () for estimating such models, functions for diagnostic testing, estimation of a restricted models, prediction, causality analysis, impulse response analysis and forecast error variance decomposition are provided too. It is further possible to convert vector error correction models into their level VAR representation. The different methods and functions are elucidated by employing a macroeconomic data set for Canada. However, the focus in this writing is on the implementation part rather than the usage of the tools at hand.",WOS:000258205500001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION RESIDUALS', 'TIME-SERIES', 'EFFICIENT TESTS', 'NORMALITY', 'HOMOSCEDASTICITY', 'COINTEGRATION', 'INDEPENDENCE', 'STATISTICS']","VAR, SVAR and SVEC models: Implementation within R package vars",2008
834,"We introduce a new adjusted residual maximum likelihood method (REML) in the context of producing an empirical Bayes (EB) confidence interval for a normal mean, a problem of great interest in different small area applications. Like other rival empirical Bayes confidence intervals such as the well-known parametric bootstrap empirical Bayes method, the proposed interval is second-order correct, that is, the proposed interval has a coverage error of order O(m(-3/2)). Moreover, the proposed interval is carefully constructed so that it always produces an interval shorter than the corresponding direct confidence interval, a property not analytically proved for other competing methods that have the same coverage error of order 0(m-3/2). The proposed method is not simulation-based and requires only a fraction of computing time needed for the corresponding parametric bootstrap empirical Bayes confidence interval. A Monte Carlo simulation study demonstrates the superiority of the proposed method over other competing methods.",WOS:000342481700001,ANNALS OF STATISTICS,"['MEAN SQUARED ERROR', 'SMALL-AREA ESTIMATORS', 'PREDICTION INTERVALS', 'MODEL', 'MAXIMUM']",A SECOND-ORDER EFFICIENT EMPIRICAL BAYES CONFIDENCE INTERVAL,2014
835,"Cook's [J. Roy. Statist. Soc. Ser. B 48 (1986) 133-169] local influence approach based on normal curvature is an important diagnostic tool for assessing local influence of minor perturbations to a statistical model. However, no rigorous approach has been developed to address two fundamental issues: the selection of an appropriate perturbation and the development of influence measures for objective functions at a point with a nonzero first derivative. The aim of this paper is to develop a differential-geometrical framework of a perturbation model (called the perturbation manifold) and utilize associated metric tensor and affine curvatures to resolve these issues. We will show that the metric tensor of the perturbation manifold provides important information about selecting an appropriate perturbation of a model. Moreover, we will introduce new influence measures that are applicable to objective functions at any point. Examples including linear regression models and linear mixed models are examined to demonstrate the effectiveness of using new influence measures for the identification of influential observations.",WOS:000253077800015,ANNALS OF STATISTICS,"['MULTIVARIATE ADAPTIVE SPLINES', 'LINEAR MIXED MODELS', 'NORMAL CURVATURE', 'CHILDREN BORN', 'DIAGNOSTICS', 'MALTREATMENT', 'ESTIMATORS', 'REGRESSION', 'LEVERAGE', 'MOTHERS']",Perturbation selection and influence measures in local influence analysis,2007
836,"The R package coin implements a unified approach to permutation tests providing a huge class of independence tests for nominal, ordered, numeric, and censored data as well as multivariate data at mixed scales. Based on a rich and flexible conceptual framework that embeds different permutation test procedures into a common theory, a computational framework is established in coin that likewise embeds the corresponding R functionality in a common S 4 class structure with associated generic functions. As a consequence, the computational tools in coin inherit the flexibility of the underlying theory and conditional inference functions for important special cases can be set up easily. Conditional versions of classical tests-such as tests for location and scale problems in two or more samples, independence in two- or three- way contingency tables, or association problems for censored, ordered categorical or multivariate data-can easily be implemented as special cases using this computational toolbox by choosing appropriate transformations of the observations. The paper gives a detailed exposition of both the internal structure of the package and the provided user interfaces along with examples on how to extend the implemented functionality.",WOS:000261526300001,JOURNAL OF STATISTICAL SOFTWARE,['STATISTICS'],Implementing a Class of Permutation Tests: The coin Package,2008
837,"Multivariate time series present many challenges, especially when they are high dimensional. The paper's focus is twofold. First, we address the subject of consistently estimating the autocovariance sequence; this is a sequence of matrices that we conveniently stack into one huge matrix. We are then able to show consistency of an estimator based on the so-called flat-top tapers; most importantly, the consistency holds true even when the time series dimension is allowed to increase with the sample size. Second, we revisit the linear process bootstrap (LPB) procedure proposed by McMurry and Politis [J. Time Series Anal. 31 (2010) 471-482] for univariate time series. Based on the aforementioned stacked autocovariance matrix estimator, we are able to define a version of the LPB that is valid for multivariate time series. Under rather general assumptions, we show that our multivariate linear process bootstrap (MLPB) has asymptotic validity for the sample mean in two important cases: (a) when the time series dimension is fixed and (b) when it is allowed to increase with sample size. As an aside, in case (a) we show that the MLPB works also for spectral density estimators which is a novel result even in the univariate case. We conclude with a simulation study that demonstrates the superiority of the MLPB in some important cases.",WOS:000355768700007,ANNALS OF STATISTICS,"['AUTOREGRESSIVE SIEVE BOOTSTRAP', 'AUTOCOVARIANCE MATRICES', 'RANDOM-VARIABLES']",COVARIANCE MATRIX ESTIMATION AND LINEAR PROCESS BOOTSTRAP FOR MULTIVARIATE TIME SERIES OF POSSIBLY INCREASING DIMENSION,2015
838,"In high-dimensional linear regression, the goal pursued here is to estimate an unknown regression function using linear combinations of a suitable set of covariates. One of the key assumptions for the success of any statistical procedure in this setup is to assume that the linear combination is sparse in some sense, for example, that it involves only few covariates. We consider a general, nonnecessarily linear, regression with Gaussian noise and study a related question, that is, to find a linear combination of approximating functions, which is at the same time sparse and has small mean squared error (MSE). We introduce a new estimation procedure, called Exponential Screening, that shows remarkable adaptation properties. It adapts to the linear combination that optimally balances MSE and sparsity, whether the latter is measured in terms of the number of nonzero entries in the combination (l(0) norm) or in terms of the global weight of the combination (l(1) norm). The power of this adaptation result is illustrated by showing that Exponential Screening solves optimally and simultaneously all the problems of aggregation in Gaussian regression that have been discussed in the literature. Moreover, we show that the performance of the Exponential Screening estimator cannot be improved in a minimax sense, even if the optimal sparsity is known in advance. The theoretical and numerical superiority of Exponential Screening compared to state-of-the-art sparse procedures is also discussed.",WOS:000291183300003,ANNALS OF STATISTICS,"['RESTRICTED ISOMETRY PROPERTY', 'ORACLE INEQUALITIES', 'DANTZIG SELECTOR', 'VARIABLE SELECTION', 'LASSO', 'AGGREGATION', 'REGRESSION', 'SHRINKAGE', 'RISK', 'BOUNDS']",EXPONENTIAL SCREENING AND OPTIMAL RATES OF SPARSE ESTIMATION,2011
839,,WOS:000253077800006,ANNALS OF STATISTICS,"['MODEL SELECTION', 'REGRESSION', 'NOISE']","Discussion: A tale of three cousins: Lasso, L2Boosting and Dantzig",2007
840,We find lower and upper bounds for the risk of estimating a manifold in Hausdorff distance under several models. We also show that there are close connections between manifold estimation and the problem of deconvolving a singular measure.,WOS:000307608000012,ANNALS OF STATISTICS,"['CONVERGENCE', 'RATES']",MANIFOLD ESTIMATION AND SINGULAR DECONVOLUTION UNDER HAUSDORFF LOSS,2012
841,"For a random matrix following a Wishart distribution, we derive formulas for the expectation and the covariance matrix of compound matrices. The compound matrix of order m is populated by all m x m-minors of the Wishart matrix. Our results yield first and second moments of the minors of the sample covariance matrix for multivariate normal observations. This work is motivated by the fact that such minors arise in the expression of constraints on the covariance matrix in many classical multivariate problems.",WOS:000260554100010,ANNALS OF STATISTICS,,MOMENTS OF MINORS OF WISHART MATRICES,2008
842,"In this article, I summarise Peter Hall's contributions to high-dimensional data, including their geometric representations and variable selection methods based on ranking. I also discuss his work on classification problems, concluding with some personal reflections on my own interactions with him. This article complements [Ann. Statist. 44 (2016) 1821-1836; Ann. Statist. 44 (2016) 1837-1853; Ann. Statist. 44 (2016) 1854-1866 and Ann. Statist. 44 (2016) 1867-1887], which focus on other aspects of Peter's research.",WOS:000384397200006,ANNALS OF STATISTICS,"['NEAREST-NEIGHBOR CLASSIFIERS', 'NONPARAMETRIC CLASSIFICATION', 'STABILITY SELECTION', 'VARIABLE SELECTION', 'REGRESSION', 'CHOICE']",PETER HALL'S WORK ON HIGH-DIMENSIONAL DATA AND CLASSIFICATION,2016
843,"Consider d dependent change point tests, each based on a CUSUM-statistic. We provide an asymptotic theory that allows us to deal with the maximum over all test statistics as both the sample size n and d tend to infinity. We achieve this either by a consistent bootstrap or an appropriate limit distribution. This allows for the construction of simultaneous confidence bands for dependent change point tests, and explicitly allows us to determine the location of the change both in time and coordinates in high-dimensional time series. If the underlying data has sample size greater or equal n for each test, our conditions explicitly allow for the large d small n situation, that is, where n/d -> 0. The setup for the high-dimensional time series is based on a general weak dependence concept. The conditions are very flexible and include many popular multivariate linear and nonlinear models from the literature, such as ARMA, GARCH and related models. The construction of the tests is completely nonparametric, difficulties associated with parametric model selection, model fitting and parameter estimation are avoided. Among other things, the limit distribution for max(1 <= h <= d) sup(0 <= t <= 1) vertical bar W-t,W-h - tW(1,h)vertical bar is established, where {W-t,W-h}(1 <= h <= d) denotes a sequence of dependent Brownian motions. As an application, we analyze all S&P 500 companies over a period of one year.",WOS:000363437900005,ANNALS OF STATISTICS,"['TIME-SERIES', 'COVARIANCE STRUCTURE', 'AUTOREGRESSIVE PROCESSES', 'STATIONARY-SEQUENCES', 'NORMALIZED SUMS', 'THEOREM', 'MATRICES', 'MAXIMUM', 'MODELS']",UNIFORM CHANGE POINT TESTS IN HIGH DIMENSION,2015
844,"High-frequency data observed on the prices of financial assets are commonly modeled by diffusion processes with micro-structure noise, and realized volatility-based methods are often used to estimate integrated volatility. For problems involving a large number of assets, the estimation objects we face are volatility matrices of large size. The existing volatility estimators work well for a small number of assets but perform poorly when the number of assets is very large. In fact, they are inconsistent when both the number, p, of the assets and the average sample size, n, of the price data on the p assets go to infinity. This paper proposes a new type of estimators for the integrated volatility matrix and establishes asymptotic theory for the proposed estimators in the framework that allows both n and p to approach to infinity. The theory shows that the proposed estimators achieve high convergence rates under a sparsity assumption on the integrated volatility matrix. The numerical studies demonstrate that the proposed estimators perform well for large p and complex price and volatility models. The proposed method is applied to real high-frequency financial data.",WOS:000275510800013,ANNALS OF STATISTICS,"['COVARIANCE MATRICES', 'INTEGRATED VOLATILITY', 'MICROSTRUCTURE NOISE', 'ECONOMETRIC-ANALYSIS', 'REALIZED VOLATILITY', 'LARGEST EIGENVALUE', 'MODELS']",VAST VOLATILITY MATRIX ESTIMATION FOR HIGH-FREQUENCY FINANCIAL DATA,2010
845,"We develop methodology for a multistage decision problem with flexible number of stages in which the rewards are survival times that are subject to censoring. We present a novel Q-learning algorithm that is adjusted for censored data and allows a flexible number of stages. We provide finite sample bounds on the generalization error of the policy learned by the algorithm, and show that when the optimal Q-function belongs to the approximation space, the expected survival time for policies obtained by the algorithm converges to that of the optimal policy. We simulate a multistage clinical trial with flexible number of stages and apply the proposed censored-Q-learning algorithm to find individualized treatment regimens. The methodology presented in this paper has implications in the design of personalized medicine trials in cancer and in other life-threatening diseases.",WOS:000304684900020,ANNALS OF STATISTICS,"['2-STAGE RANDOMIZATION DESIGNS', 'KAPLAN-MEIER ESTIMATOR', 'DYNAMIC TREATMENT REGIMES', 'CLINICAL-TRIALS', 'SURVIVAL DISTRIBUTIONS', 'TREATMENT STRATEGIES', 'REGRESSION', 'CANCER']",Q-LEARNING WITH CENSORED DATA,2012
846,"Image segmentation is a long-studied and important problem in image processing. Different solutions have been proposed, many of which follow the information theoretic paradigm. While these information theoretic segmentation methods often produce excellent empirical results, their theoretical properties are still largely unknown. The main goal of this paper is to conduct a rigorous theoretical study into the statistical consistency properties of such methods. To be more specific, this paper investigates if these methods can accurately recover the true number of segments together with their true boundaries in the image as the number of pixels tends to infinity. Our theoretical results show that both the Bayesian information criterion (BIC) and the minimum description length (MDL) principle can be applied to derive statistically consistent segmentation methods, while the same is not true for the Akaike information criterion (AIC). Numerical experiments were conducted to illustrate and support our theoretical findings.",WOS:000300383200005,ANNALS OF STATISTICS,"['CHANGE-POINTS', 'MODEL', 'VALIDATION', 'NUMBER']",ON IMAGE SEGMENTATION USING INFORMATION THEORETIC CRITERIA,2011
847,"While adaptive sensing has provided improved rates of convergence in sparse regression and classification, results in nonparametric regression have so far been restricted to quite specific classes of functions. In this, paper, we describe an adaptive-sensing algorithm which is applicable to general nonparametric-regression problems. The algorithm is spatially adaptive, and achieves improved rates of convergence over spatially inhomogeneous functions. Over standard function classes, it likewise retains the spatial adaptivity properties of a uniform design.",WOS:000317451200002,ANNALS OF STATISTICS,"['WAVELET SHRINKAGE', 'VARIABLE BANDWIDTH', 'ADAPTATION', 'DESIGN', 'SELECTION']",SPATIALLY-ADAPTIVE SENSING IN NONPARAMETRIC REGRESSION,2013
848,"We study maximum likelihood estimation in Gaussian graphical models from a geometric point of view. An algebraic elimination criterion allows us to find exact lower bounds on the number of observations needed to ensure that the maximum likelihood estimator (MLE) exists with probability one. This is applied to bipartite graphs, grids and colored graphs. We also study the ML degree, and we present the first instance of a graph for which the MLE exists with probability one, even when the number of observations equals the treewidth.",WOS:000304684900010,ANNALS OF STATISTICS,,GEOMETRY OF MAXIMUM LIKELIHOOD ESTIMATION IN GAUSSIAN GRAPHICAL MODELS,2012
849,"Relative importance is a topic that has seen a lot of interest in recent years, particularly in applied work. The R package relaimpo implements six different metrics for assessing relative importance of regressors in the linear model, two of which are recommended averaging over orderings of regressors and a newly proposed metric (Feldman 2005) called pmvd. Apart from delivering the metrics themselves, relaimpo also provides ( exploratory) bootstrap confidence intervals. This paper offers a brief tutorial introduction to the package. The methods and relaimpo's functionality are illustrated using the data set swiss that is generally available in R. The paper targets readers who have a basic understanding of multiple linear regression. For the background of more advanced aspects, references are provided.",WOS:000241807900001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIPLE-REGRESSION', 'PREDICTORS']",Relative importance for linear regression in R: The package relaimpo,2006
850,"Statistical, data manipulation, and presentation tools make R an ideal integrated package for research in the fields of health policy and healthcare management and evaluation. However, the technical documentation accompanying most data sets used by researchers in these fields does not include syntax examples for analysts to make the transition from another statistical package to R. This paper describes the steps required to import health policy data into R, to prepare that data for analysis using the two most common complex survey variance calculation techniques, and to produce the principal set of statistical estimates sought by health policy researchers. Using data from the Medical Expenditure Panel Survey Household Component (MEPS-HC), this paper outlines complex survey data analysis techniques in R, with side-by-side comparisons to the SAS, Stata, and SUDAAN statistical software packages.",WOS:000208589800007,R JOURNAL,,"Transitioning to R: Replicating SAS, Stata, and SUDAAN Analysis Techniques in Health Policy Data",2009
851,"Since version 2.10.0, R includes expanded support for source references in R code and '.Rd' files. This paper describes the origin and purposes of source references, and current and future support for them.",WOS:000208590000003,R JOURNAL,,Source References,2010
852,"Let X vertical bar mu similar to N-p (mu, upsilon I-x) and Y vertical bar mu similar to N-p (mu, upsilon I-y) be independent p-dimensional multivariate normal vectors with common unknown mean A. Based on observing X = x, we consider the problem of estimating the true predictive density p(y vertical bar mu) of Y under expected Kullback-Leibler loss. Our focus here is the characterization of admissible procedures for this problem. We show that the class of all generalized Bayes rules is a complete class, and that the easily interpretable conditions of Brown and Hwang [Statistical Decision Theory and Related Topics (1982) III 205-230] are sufficient for a formal Bayes rule to be admissible.",WOS:000256504400006,ANNALS OF STATISTICS,,Admissible predictive density estimation,2008
853,"We consider estimation of covariance matrices and their inverses (a.k.a. precision matrices) for high-dimensional stationary and locally stationary time series. In the latter case the covariance matrices evolve smoothly in time, thus forming a covariance matrix function. Using the functional dependence measure of Wu [Proc. Natl. Acad. Sci. USA 102 (2005) 14150-14154 (electronic)], we obtain the rate of convergence for the thresholded estimate and illustrate how the dependence affects the rate of convergence. Asymptotic properties are also obtained for the precision matrix estimate which is based on the graphical Lasso principle. Our theory substantially generalizes earlier ones by allowing dependence, by allowing nonstationarity and by relaxing the associated moment conditions.",WOS:000330204900011,ANNALS OF STATISTICS,"['PRINCIPAL-COMPONENTS-ANALYSIS', 'OPTIMAL RATES', 'MODELS', 'CONVERGENCE', 'SELECTION', 'LASSO', 'LIKELIHOOD', 'VARIABLES']",COVARIANCE AND PRECISION MATRIX ESTIMATION FOR HIGH-DIMENSIONAL TIME SERIES,2013
854,"We consider semiparametric location-scatter models for which the p-variate observation is obtained as X = Lambda Z + mu, where mu is a p-vector, Lambda is a full-rank p x p matrix and the (unobserved) random p-vector Z has marginals that are centered and mutually independent but are otherwise unspecified. As in blind source separation and independent component analysis (ICA), the parameter of interest throughout the paper is Lambda. On the basis of n i.i.d. copies of X, we develop, under a symmetry assumption on Z, signed-rank one-sample testing and estimation procedures for Lambda. We exploit the uniform local and asymptotic normality (ULAN) of the model to define signed-rank procedures that are semiparametrically efficient under correctly specified densities. Yet, as is usual in rank-based inference, the proposed procedures remain valid (correct asymptotic size under the null, for hypothesis testing, and root-n consistency, for point estimation) under a very broad range of densities. We derive the asymptotic properties of the proposed procedures and investigate their finite-sample behavior through simulations.",WOS:000299186500010,ANNALS OF STATISTICS,"['ADAPTIVE ESTIMATION', 'SHAPE']",SEMIPARAMETRICALLY EFFICIENT INFERENCE BASED ON SIGNED RANKS IN SYMMETRIC INDEPENDENT COMPONENT MODELS,2011
855,"The pls package implements principal component regression ( PCR) and partial least squares regression ( PLSR) in R ( R Development Core Team 2006b), and is freely available from the Comprehensive R Archive Network ( CRAN), licensed under the GNU General Public License ( GPL). The user interface is modelled after the traditional formula interface, as exemplified by 1m. This was done so that people used to R would not have to learn yet another interface, and also because we believe the formula interface is a good way of working interactively with models. It thus has methods for generic functions like predict, update and coef. It also has more specialised functions like scores, loadings and RMSEP, and a flexible cross-validation system. Visual inspection and assessment is important in chemometrics, and the pls package has a number of plot functions for plotting scores, loadings, predictions, coefficients and RMSEP estimates. The package implements PCR and several algorithms for PLSR. The design is modular, so that it should be easy to use the underlying algorithms in other functions. It is our hope that the package will serve well both for interactive data analysis and as a building block for other functions or packages using PLSR or PCR. We will here describe the package and how it is used for data analysis, as well as how it can be used as a part of other packages. Also included is a section about formulas and data frames, for people not used to the R modelling idioms.",WOS:000244067100001,JOURNAL OF STATISTICAL SOFTWARE,"['SPECTRA', 'MODELS', 'ERROR']",The pls package: Principal component and partial least squares regression in R,2007
856,"We consider a general supervised learning problem with strongly convex and Lipschitz loss and study the problem of model selection aggregation. In particular, given a finite dictionary functions (learners) together with the prior, we generalize the results obtained by Dai, Rigollet and Zhang [Ann. Statist. 40 (2012) 1878-1905] for Gaussian regression with squared loss and fixed design to this learning setup. Specifically, we prove that the Q-aggregation procedure outputs an estimator that satisfies optimal oracle inequalities both in expectation and with high probability. Our proof techniques somewhat depart from traditional proofs by making most of the standard arguments on the Laplace transform of the empirical process to be controlled.",WOS:000334256100009,ANNALS OF STATISTICS,"['EMPIRICAL RISK MINIMIZATION', 'SHARP ORACLE INEQUALITIES', 'PAC-BAYESIAN BOUNDS', 'OPTIMAL RATES', 'SPARSE ESTIMATION', 'REGRESSION', 'CLASSIFICATION']",OPTIMAL LEARNING WITH Q-AGGREGATION,2014
857,"The large sample theory of estimators for density modes is well understood. In this paper we consider density ridges, which are a higher-dimensional extension of modes. Modes correspond to zero-dimensional, local high-density regions in point clouds. Density ridges correspond to s-dimensional, local high-density regions in point clouds. We establish three main results. First we show that under appropriate regularity conditions, the local variation of the estimated ridge can be approximated by an empirical process. Second, we show that the distribution of the estimated ridge converges to a Gaussian process. Third, we establish that the bootstrap leads to valid confidence sets for density ridges.",WOS:000362697700002,ANNALS OF STATISTICS,"['METRIC GRAPH RECONSTRUCTION', 'LEVEL SET TREES', 'CURVE RECONSTRUCTION', 'UNIFORM CONSISTENCY', 'MANIFOLD ESTIMATION', 'FUNCTION ESTIMATORS', 'MODE', 'APPROXIMATION']",ASYMPTOTIC THEORY FOR DENSITY RIDGES,2015
858,"This paper discusses the simultaneous inference of mean parameters in a family of distributions with quadratic variance function. We first introduce a class of semiparametric/parametric shrinkage estimators and establish their asymptotic optimality properties. Two specific cases, the location-scale family and the natural exponential family with quadratic variance function, are then studied in detail. We conduct a comprehensive simulation study to compare the performance of the proposed methods with existing shrinkage estimators. We also apply the method to real data and obtain encouraging results.",WOS:000372594300005,ANNALS OF STATISTICS,"['EMPIRICAL BAYES ESTIMATION', 'EXPONENTIAL-FAMILIES', 'CROSS-VALIDATION', 'POISSON', 'ADMISSIBILITY', 'PREDICTION', 'VECTOR']",OPTIMAL SHRINKAGE ESTIMATION OF MEAN PARAMETERS IN FAMILY OF DISTRIBUTIONS WITH QUADRATIC VARIANCE,2016
859,"The YUIMA Project is an open source and collaborative effort aimed at developing the R package yuima for simulation and inference of stochastic differential equations. In the yuima package stochastic differential equations can be of very abstract type, multidimensional, driven by Wiener process or fractional Brownian motion with general Hurst parameter, with or without jumps specified as Levy noise. The yuima package is intended to offer the basic infrastructure on which complex models and inference procedures can be built on. This paper explains the design of the yuima package and provides some examples of applications.",WOS:000334020300001,JOURNAL OF STATISTICAL SOFTWARE,"['OBSERVED DIFFUSION-PROCESSES', 'REGRESSION', 'LASSO']",The YUIMA Project: A Computational Framework for Simulation and Inference of Stochastic Differential Equations,2014
860,"This paper presents the MATLAB package DeCo (density combination) which is based on the paper by Billio, Casarin, Ravazzolo, and van Dijk (2013) where a constructive Bayesian approach is presented for combining predictive densities originating from different models or other sources of information. The combination weights are time-varying and may depend on past predictive forecasting performances and other learning mechanisms. The core algorithm is the function DeCo which applies banks of parallel sequential Monte Carlo algorithms to filter the time-varying combination weights. The DeCo procedure has been implemented both for standard CPU computing and for graphical process unit (GPU) parallel computing. For the GPU implementation we use the MATLAB parallel computing toolbox and show how to use general purpose GPU computing almost effortlessly. This GPU implementation provides a speed-up of the execution time of up to seventy times on a standard CPU MATLAB implementation on a multicore CPU. We show the use of the package and the computational gain of the GPU version through some simulation experiments and empirical applications.",WOS:000366014000001,JOURNAL OF STATISTICAL SOFTWARE,"['PARTICLE FILTERS', 'FORECASTS', 'MODELS', 'PREDICTION']",Parallel Sequential Monte Carlo for Efficient Density Combination: The DeCo MATLAB Toolbox,2015
861,"This document describes classes and methods designed to deal with different types of spatio-temporal data in R implemented in the R package spa c e time, and provides examples for analyzing them. It builds upon the classes and methods for spatial data from package sp, and for time series data from package xts. The goal is to cover a number of useful representations for spatio-temporal sensor data, and results from predicting (spatial and/or temporal interpolation or smoothing), aggregating, or subsetting them, and to represent trajectories. The goals of this paper is to explore how spatio-temporal data can be sensibly represented in classes, and to find out which analysis and visualisation methods are useful and feasible. We discuss the time series convention of representing time intervals by their starting time only. This document is the main reference for the R package spacetime, and is available (in updated form) as a vignette in this package.",WOS:000312289200001,JOURNAL OF STATISTICAL SOFTWARE,['PACKAGE'],spacetime: Spatio-Temporal Data in R,2012
862,"We consider the problem of combining a (possibly uncountably infinite) set of affine estimators in nonparametric regression model with heteroscedastic Gaussian noise. Focusing on the exponentially weighted aggregate, we prove a PAC-Bayesian type inequality that leads to sharp oracle inequalities in discrete but also in continuous settings. The framework is general enough to cover the combinations of various procedures such as least square regression, kernel ridge regression, shrinking estimators and many other estimators used in the literature on statistical inverse problems. As a consequence, we show that the proposed aggregate provides an adaptive estimator in the exact minimax sense without discretizing the range of tuning parameters or splitting the set of observations. We also illustrate numerically the good performance achieved by the exponentially weighted aggregate.",WOS:000321842400007,ANNALS OF STATISTICS,"['PAC-BAYESIAN BOUNDS', 'NONPARAMETRIC REGRESSION', 'WAVELET SHRINKAGE', 'INVERSE PROBLEMS', 'MODEL SELECTION', 'LINEAR-MODELS', 'OPTIMAL RATES', 'ADAPTATION', 'SPARSITY', 'WEIGHTS']",SHARP ORACLE INEQUALITIES FOR AGGREGATION OF AFFINE ESTIMATORS,2012
863,"eco is a publicly available R package that implements the Bayesian and likelihood methods proposed in Imai, Lu, and Strauss (2008b) for ecological inference in 2 x 2 tables as well as the method of bounds introduced by (Duncan and Davis 1953). The package fits both parametric and nonparametric models using either the Expectation-Maximization algorithms (for likelihood models) or the Markov chain Monte Carlo algorithms (for Bayesian models). For all models, the individual-level data can be directly incorporated into the estimation whenever such data are available. Along with in-sample and out-of-sample predictions, the package also provides a functionality which allows one to quantify the effect of data aggregation on parameter estimation and hypothesis testing under the parametric likelihood models. This paper illustrates the usage of eco with several real data examples that are also part of the package.",WOS:000292097000001,JOURNAL OF STATISTICAL SOFTWARE,"['INCOMPLETE-DATA', 'REGRESSIONS', 'INDIVIDUALS', 'LIKELIHOOD', 'ALGORITHM', 'BEHAVIOR', 'MODELS', 'EM']",eco: R Package for Ecological Inference in 2 x 2 Tables,2011
864,"We study an instance of high-dimensional inference in which the goal is to estimate a matrix circle minus* is an element of R(m1xm2) on the basis of N noisy observations. The unknown matrix circle minus* is assumed to be either exactly low rank, or ""near"" low-rank, meaning that it can be well-approximated by a matrix with low rank. We consider a standard M-estimator based on regularization by the nuclear or trace norm over matrices, and analyze its performance under high-dimensional scaling. We define the notion of restricted strong convexity (RSC) for the loss function, and use it to derive nonasymptotic bounds on the Frobenius norm error that hold for a general class of noisy observation models, and apply to both exactly low-rank and approximately low rank matrices. We then illustrate consequences of this general theory for a number of specific matrix models, including low-rank multivariate or multi-task regression, system identification in vector autoregressive processes and recovery of low-rank matrices from random projections. These results involve nonasymptotic random matrix theory to establish that the RSC condition holds, and to determine an appropriate choice of regularization parameter. Simulation results show excellent agreement with the high-dimensional scaling of the error predicted by our theory.",WOS:000291183300014,ANNALS OF STATISTICS,"['VARIABLE SELECTION', 'COVARIANCE ESTIMATION', 'PRINCIPAL COMPONENTS', 'NORM MINIMIZATION', 'LASSO', 'REGRESSION', 'REGULARIZATION', 'APPROXIMATION', 'RECOVERY', 'MODEL']",ESTIMATION OF (NEAR) LOW-RANK MATRICES WITH NOISE AND HIGH-DIMENSIONAL SCALING,2011
865,We apply the techniques of stochastic integration with respect to fractional Brownian motion and the theory of regularity and supremum estimation for stochastic processes to study the maximum likelihood estimator (MLE) for the drift parameter of stochastic processes satisfying stochastic equations driven by a fractional Brownian motion with arty level of Holder-regularity (any Hurst parameter). We prove existence and strong consistency of the MLE for linear and nonlinear equations. We also prove that a version of the MLE using only discrete observations is still a strongly consistent estimator.,WOS:000248692700011,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'DIFFERENTIAL-EQUATIONS', 'DIFFUSION-PROCESSES', 'INFERENCE', 'MODELS', 'NOISE']",Statistical aspects of the fractional stochastic calculus,2007
866,"Supervised learning can be used to segment/identify regions of interest in images using both color and morphological information. A novel object identification algorithm was developed in Java to locate immune and cancer cells in images of immunohistochemically-stained lymph node tissue from a recent study published by Kohrt et al. (2005). The algorithms are also showing promise in other domains. The success of the method depends heavily on the use of color, the relative homogeneity of object appearance and on interactivity. As is often the case in segmentation, an algorithm specifically tailored to the application works bette than using broader methods that work passably well on any problem. Our main innovation is the interactive feature extraction from color images. We also enable the user to improve the classification with an interactive visualization system. This is then coupled with the statistical learning algorithms and intensive feedback from the user over many classification-correction iterations, resulting in highly accurate and user-friendly solution. The system ultimately provides the locations of every cell recognized in the entire tissue in a text file tailored to be easily imported into R (Ihaka and Gentleman 1996; R Development Core Team 2009) for further statistical analyses. This data is invaluable in the study of spatial and multidimentional relationships between cell populations and tumor structure. This system is available at http://www.GemIdent.com/ together with three demonstration videos and a manual.",WOS:000267708100001,JOURNAL OF STATISTICAL SOFTWARE,['TRANSFORM'],An Interactive Java Statistical Image Segmentation System: GemIdent,2009
867,"Resolvable designs with two blocks per replicate are studied from an optimality perspective. Because in practice the number of replicates is typically less than the number of treatments, arguments can be based on the dual of the information matrix and consequently given in terms of block concurrences. Equalizing block concurrences for given block sizes is often, but not always, the best strategy. Sufficient conditions are established for various strong optimalities and a detailed study of E-optimality is offered, including a characterization of the E-optimal class. Optimal designs are found to correspond to balanced arrays and an affine-like generalization.",WOS:000248987600012,ANNALS OF STATISTICS,"['BALANCED ARRAYS', 'VARIETY TRIALS', 'CONSTRUCTION', 'EFFICIENCY', 'OPTIMALITY', 'SQUARES', 'LIST']",Resolvable designs with large blocks,2007
868,"Package checking errors are more common on Solaris than Linux. In many cases, these errors are due to non-portable C++ code. This article reviews some commonly recurring problems in C++ code found in R packages and suggests solutions.",WOS:000208590200011,R JOURNAL,,Portable C plus plus for R Packages,2011
869,"The analysis of matrix population models has become a fundamental tool in ecology, conservation biology, and life history theory. In this paper, I present demogR, a package for analyzing age-structured population models in R. The package includes tools for the construction and analysis of matrix population models. In addition to the standard analyses commonly used in evolutionary demography and conservation biology, demogR contains a variety of tools from classical demography. This includes the construction of period life tables, and the generation of model mortality and fertility schedules for human populations. The tools in demogR are generally applicable to age-structured populations but are particularly useful for analyzing problems in human ecology. I illustrate some of the capabilities of the package by doing an evolutionary demographic analysis of several human populations.",WOS:000252430400001,JOURNAL OF STATISTICAL SOFTWARE,"['POPULATION-GROWTH RATE', 'LIFE-HISTORY EVOLUTION', 'ELASTICITY ANALYSIS', 'LOOP ANALYSIS', 'RED DEER', 'SELECTION', 'CONSERVATION', 'SENSITIVITY', 'SENESCENCE', 'FREQUENCY']",demogR: A package for the construction and analysis of age-structured demographic models in R,2007
870,"For survival data with a large number of explanatory variables, lasso penalized Cox regression is a popular regularization strategy. However, a penalized Cox model may not always provide the best fit to data and can be difficult to estimate in high dimension because of its intrinsic nonlinearity. The semiparametric additive hazards model is a flexible alternative which is a natural survival analogue of the standard linear regression model. Building on this analogy, we develop a cyclic coordinate descent algorithm for fitting the lasso and elastic net penalized additive hazards model. The algorithm requires no nonlinear optimization steps and offers excellent performance and stability. An implementation is available in the R package ahaz. We demonstrate this implementation in a small timing study and in an application to real data.",WOS:000303805200001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED LINEAR-MODELS', 'GENE-EXPRESSION DATA', 'VARIABLE SELECTION', 'RISK MODEL', 'REGULARIZATION PATHS', 'ORACLE PROPERTIES', 'LASSO', 'REGRESSION']",Coordinate Descent Methods for the Penalized Semiparametric Additive Hazards Model,2012
871,"Testing genetic markers for Hardy-Weinberg equilibrium is an important issue in genetic association studies. The HardyWeinberg package offers the classical tests for equilibrium, functions for power computation and for the simulation of marker data under equilibrium and disequilibrium. Functions for testing equilibrium in the presence of missing data by using multiple imputation are provided. The package also supplies various graphical tools such as ternary plots with acceptance regions, log-ratio plots and Q-Q plots for exploring the equilibrium status of a large set of diallelic markers. Classical tests for equilibrium and graphical representations for diallelic marker data are reviewed. Several data sets illustrate the use of the package.",WOS:000352911000001,JOURNAL OF STATISTICAL SOFTWARE,"['HARDY-WEINBERG EQUILIBRIUM', 'POPULATION', 'TESTS', 'DISTRIBUTIONS']",Exploring Diallelic Genetic Markers: The HardyWeinberg Package,2015
872,"The wgaim (whole genome average interval mapping) package developed in the R system for statistical computing (R Development Core Team 2011) builds on linear mixed modelling techniques by incorporating a whole genome approach to detecting significant quantitative trait loci (QTL) in bi-parental populations. Much of the sophistication is inherited through the well established linear mixed modelling package ASReml-R (Butler et al. 2009). As wgaim uses an extension of interval mapping to incorporate the whole genome into the analysis, functions are provided which allow conversion of genetic data objects created with the qtl package of Broman and Wu (2010) available in R. Results of QTL analyses are available using summary and print methods as well as diagnostic summaries of the selection method. In addition, the package features a flexible linkage map plotting function that can be easily manipulated to provide an aesthetic viewable genetic map. As a visual summary, QTL obtained from one or more models can also be added to the linkage map.",WOS:000289228900001,JOURNAL OF STATISTICAL SOFTWARE,"['QUANTITATIVE TRAIT LOCI', 'EXPERIMENTAL CROSSES', 'INFORMATION', 'MARKERS', 'TRIALS', 'SIZES']",R Package wgaim: QTL Analysis in Bi-Parental Populations Using Linear Mixed Models,2011
873,"This paper outlines how to conduct a simple meta-analysis of neuroimaging foci of activation in R. In particular, the first part of this paper reviews the nature of fMRI data, and presents a brief overview of the existing packages that can be used to analyze fMRI data in R. The second part illustrates how to handle fMRI data by showing how to visualize the results of different neuroimaging studies in a so-called orthographic view, where the spatial distribution of the foci of activation from different fMRI studies can be inspected visually.
Functional MRI (fMRI) is one of the most important and powerful tools of neuroscientific research. Although not as commonly used for fMRI analysis as some specific applications such as SPM (Friston et al., 2006), AFNI (Cox and Hyde, 1997), or FSL (Smith et al., 2004), R does provide several packages that can be employed in neuroimaging research. These packages deal with a variety of topics, ranging from reading and manipulating fMRI datasets, to implementing sophisticated statistical models.
The goal of this paper is to provide a brief introduction to fMRI analysis, and the various R packages that can be used to carry it out. As an example, it will show how to use simple R commands to read fMRI images and plot results from previous studies, which can then be visually compared. This is a special form of meta-analysis, and a common way to compare results from the existing literature.",WOS:000348651700002,R JOURNAL,"['ROSTRAL PREFRONTAL CORTEX', 'PACKAGE', 'BRAIN', 'LOCALIZATION']",Coordinate-Based Meta-Analysis of fMRI Studies with R,2014
874,"Mixmod is a well-established software package for fitting mixture models of multivariate Gaussian or multinomial probability distribution functions to a given dataset with either a clustering, a density estimation or a discriminant analysis purpose. The Rmixmod S4 package provides an interface from the R statistical computing environment to the C++ core library of Mixmod (mixmodLib). In this article, we give an overview of the model-based clustering and classification methods implemented, and we show how the R package Rmixmod can be used for clustering and discriminant analysis.",WOS:000365982900001,JOURNAL OF STATISTICAL SOFTWARE,"['DISCRIMINANT-ANALYSIS', 'MIXTURE MODEL', 'FINITE MIXTURES', 'EM ALGORITHM', 'LIKELIHOOD', 'CLUSTERS', 'PARAMETERS', 'VARIABLES', 'CRITERION', 'NUMBER']","Rmixmod: The R Package of the Model-Based Unsupervised, Supervised, and Semi-Supervised Classification Mixmod Library",2015
875,"There are many different ways in which change point analysis can be performed, from purely parametric methods to those that are distribution free. The ecp package is designed to perform multiple change point analysis while making as few assumptions as possible. While many other change point methods are applicable only for univariate data, this R package is suitable for both univariate and multivariate observations. Hierarchical estimation can be based upon either a divisive or agglomerative algorithm. Divisive estimation sequentially identifiers change points via a bisection algorithm. The agglomerative algorithm estimates change point location by determining an optimal segmentation. Both apporaches are able to detect any type of distributional change within the data. This provides an advantage over many existing change point algorithm which are only able to detect changes within the marginal distributions.",WOS:000349843600001,JOURNAL OF STATISTICAL SOFTWARE,['VARIANCE'],ecp: An R Package for Nonparametric Multiple Change Point Analysis of Multivariate Data,2014
876,"Functional covariates are common in many medical, biodemographic and neuroimaging studies. The aim of this paper is to study functional Cox models with right-censored data in the presence of both functional and scalar covariates. We study the asymptotic properties of the maximum partial likelihood estimator and establish the asymptotic normality and efficiency of the estimator of the finite-dimensional estimator. Under the framework of reproducing kernel Hilbert space, the estimator of the coefficient function for a functional covariate achieves the minimax optimal rate of convergence under a weighted L-2-risk. This optimal rate is determined jointly by the censoring scheme, the reproducing kernel and the covariance kernel of the functional covariates. Implementation of the estimation approach and the selection of the smoothing parameter are discussed in detail. The finite sample performance is illustrated by simulated examples and a real application.",WOS:000379972900012,ANNALS OF STATISTICS,"['LINEAR-REGRESSION', 'LARGE-SAMPLE', 'INFORMATION', 'LIKELIHOOD']",OPTIMAL ESTIMATION FOR THE FUNCTIONAL COX MODEL,2016
877,"PySSM is a Python package that has been developed for the analysis of time series using linear Gaussian state space models. PySSM is easy to use; models can be set up quickly and efficiently and a variety of different settings are available to the user. It also takes advantage of scientific libraries NumPy and SciPy and other high level features of the Python language. PySSM is also used as a platform for interfacing between optimized and parallelized Fortran routines. These Fortran routines heavily utilize basic linear algebra and linear algebra Package functions for maximum performance. PySSM contains classes for filtering, classical smoothing as well as simulation smoothing.",WOS:000334020600001,JOURNAL OF STATISTICAL SOFTWARE,['SIMULATION SMOOTHER'],PySSM : APython Module for Bayesian Inference of Linear Gaussian State Space Models,2014
878,"This article extends and amplifies on results from a paper of over forty years ago. It provides software for evaluating the density and distribution functions of the ratio z/w for any two jointly normal variates z, w, and provides details on methods for transforming a general ratio z/w into a standard form, (a+x)/(b+y), with x and y independent standard normal and a, b non-negative constants. It discusses handling general ratios when, in theory, none of the moments exist yet practical considerations suggest there should be approximations whose adequacy can be verified by means of the included software. These approximations show that many of the ratios of normal variates encountered in practice can themselves be taken as normally distributed. A practical rule is developed: If a < 2.256 and 4 < b then the ratio (a+x)/(b+y) is itself approximately normally distributed with mean mu = a/(1.01b -.2713) and variance sigma(2) = (a(2) + 1)/(b(2) +.108b - 3.795) - mu(2).",WOS:000239139600001,JOURNAL OF STATISTICAL SOFTWARE,,Ratios of normal variables,2006
879,"The beta distribution is a versatile function that accommodates a broad range of probability distribution shapes. Beta regression based on the beta distribution can be used to model a response variable y that takes values in open unit interval (0, 1). Zero/one inflated beta (ZOIB) regression models can be applied when y takes values from closed unit interval [0, 1]. The ZOIB model is based a piecewise distribution that accounts for the probability mass at 0 and 1, in addition to the probability density within (0, 1). This paper introduces an R package - zoib that provides Bayesian inferences for a class of ZOIB models. The statistical methodology underlying the zoib package is discussed, the functions covered by the package are outlined, and the usage of the package is illustrated with three examples of different data and model types. The package is comprehensive and versatile in that it can model data with or without inflation at 0 or 1, accommodate clustered and correlated data via latent variables, perform penalized regression as needed, and allow for model comparison via the computation of the DIC criterion.",WOS:000368551800004,R JOURNAL,['MODELS'],zoib: An R Package for Bayesian Inference for Beta Regression and Zero/One Inflated Beta Regression,2015
880,"Anew package called boolfun is available for R users. The package provides tools to handle Boolean functions, in particular for cryptographic purposes. This document guides the user through some (code) examples and gives a feel of what can be done with the package.",WOS:000208590100008,R JOURNAL,,Cryptographic Boolean Functions with R,2011
881,"A new package crs is introduced for computing nonparametric regression (and quantile) splines in the presence of both continuous and categorical predictors. B-splines are employed in the regression model for the continuous predictors and kernel weighting is employed for the categorical predictors. We also develop a simple R interface to NOMAD, which is a mixed integer optimization solver used to compute optimal regression spline solutions.",WOS:000313198000007,R JOURNAL,['CROSS-VALIDATION'],The crs Package: Nonparametric Regression Splines for Continuous and Categorical Predictors,2012
882,"Large-scale computer experiments are becoming increasingly important in science. A multi-step procedure is introduced to statisticians for modeling such experiments, which builds an accurate interpolator in multiple steps. In practice, the procedure shows substantial improvements in overall accuracy, but its theoretical properties are not well established. We introduce the terms nominal and numeric error and decompose the overall error of an interpolator into nominal and numeric portions. Bounds on the numeric and nominal error are developed to show theoretically that substantial gains in overall accuracy can be attained with the multi-step approach.",WOS:000300383200007,ANNALS OF STATISTICS,"['SPACE-FILLING DESIGNS', 'CENTRAL-LIMIT-THEOREM', 'SCRAMBLED NET QUADRATURE', 'LATIN HYPERCUBE DESIGNS', 'GAUSSIAN PROCESS MODELS', 'ARRAY SAMPLING DESIGNS', 'MONTE-CARLO VARIANCE', 'SPATIAL DATA SETS', 'ORTHOGONAL ARRAYS', 'INPUT VARIABLES']",ACCURATE EMULATORS FOR LARGE-SCALE COMPUTER EXPERIMENTS,2011
883,"When the prevalence of a disease or of some other binary characteristic is small, group testing (also known as pooled testing) is frequently used to estimate the prevalence and/or to identify individuals as positive or negative. We have developed the binGroup package as the first package designed to address the estimation problem in group testing. We present functions to estimate an overall prevalence for a homogeneous population. Also, for this setting, we have functions to aid in the very important choice of the group size. When individuals come from a heterogeneous population, our group testing regression functions can be used to estimate an individual probability of disease positivity by using the group observations only. We illustrate our functions with data from a multiple vector transfer design experiment and a human infectious disease prevalence study.",WOS:000208590000009,R JOURNAL,,binGroup: A Package for Group Testing,2010
884,"A scheme for locally adaptive bandwidth selection is proposed which sensitively shrinks the bandwidth of a kernel estimator at lowest density regions such as the support boundary which are unknown to the statistician. In case of a Holder continuous density, this locally minimax-optimal bandwidth is shown to be smaller than the usual rate, even in case of homogeneous smoothness. Some new type of risk bound with respect to a density-dependent standardized loss of this estimator is established. This bound is fully nonasymptotic and allows to deduce convergence rates at lowest density regions that can be substantially faster than n(-1/2). It is complemented by a weighted minimax lower bound which splits into two regimes depending on the value of the density. The new estimator adapts into the second regime, and it is shown that simultaneous adaptation into the fastest regime is not possible in principle as long as the Holder exponent is unknown. Consequences on plug-in rules for support recovery are worked out in detail. In contrast to those with classical density estimators, the plug-in rules based on the new construction are minimax-optimal, up to some logarithmic factor.",WOS:000368022000009,ANNALS OF STATISTICS,"['ADAPTIVE ESTIMATION', 'NONPARAMETRIC-ESTIMATION', 'ORACLE INEQUALITIES', 'LEVEL SETS', 'MINIMAX', 'REGRESSION', 'BOUNDARIES', 'SELECTION', 'RATES']",ADAPTATION TO LOWEST DENSITY REGIONS WITH APPLICATION TO SUPPORT RECOVERY,2016
885,"We present the package fslr, a set of R functions that interface with FSL (FMRIB Software Library), a commonly-used open-source software package for processing and analyzing neuroimaging data. The fslr package performs operations on 'nifti' image objects in R using command-line functions from FSL, and returns R objects back to the user. fslr allows users to develop image processing and analysis pipelines based on FSL functionality while interfacing with the functionality provided by R. We present an example of the analysis of structural magnetic resonance images, which demonstrates how R users can leverage the functionality of FSL without switching to shell commands.
[GRAPHICS]",WOS:000357431900014,R JOURNAL,"['MULTIPLE-SCLEROSIS', 'BRAIN EXTRACTION', 'MR-IMAGES', 'SEGMENTATION', 'REGISTRATION', 'ROBUST', 'OPTIMIZATION', 'PACKAGE', 'FIELD', 'MODEL']",fslr: Connecting the FSL Software with R,2015
886,"We study semiparametric efficiency bounds and efficient estimation of parameters defined through general moment restrictions with missing data. Identification relies on auxiliary data containing information about the distribution of the missing variables conditional on proxy variables that are observed in both the primary and the auxiliary database, when such distribution is common to the two data sets. The auxiliary sample can be independent of the primary sample, or can be a subset of it. For both cases, we derive bounds when the probability of missing data given the proxy variables is unknown, or known, or belongs to a correctly specified parametric family. We find that the conditional probability is not ancillary when the two samples are independent. For all cases, we discuss efficient semiparametric estimators. An estimator based on a conditional expectation projection is shown to require milder regularity conditions than one based on inverse probability weighting.",WOS:000254502700012,ANNALS OF STATISTICS,"['MEASUREMENT ERROR MODELS', 'LOGISTIC-REGRESSION', 'PROPENSITY SCORE']",Semiparametric efficiency in GMM models with auxiliary data,2008
887,"The SSN package for R provides a set of functions for modeling stream network data. The package can import geographic information systems data or simulate new data as a 'SpatialStreamNetwork', a new object class that builds on the spatial sp classes. Functions are provided that fit spatial linear models (SLMs) for the 'SpatialStreamNetwork' object. The covariance matrix of the SLMs use distance metrics and geostatistical models that are unique to stream networks; these models account for the distances and topological configuration of stream networks, including the volume and direction of flowing water. In addition, traditional models that use Euclidean distance and simple random effects are included, along with Poisson and binomial families, for a generalized linear mixed model framework. Plotting and diagnostic functions are provided. Prediction (kriging) can be performed for missing data or for a separate set of unobserved locations, or block prediction (block kriging) can be used over sets of stream segments. This article summarizes the SSN package for importing, simulating, and modeling of stream network data, including diagnostics and prediction.",WOS:000332108200001,JOURNAL OF STATISTICAL SOFTWARE,"['GEOSTATISTICS', 'PREDICTION']",SSN: An R Package for Spatial Statistical Modeling on Stream Networks,2014
888,"The Minkowski content L-0(G) of a body G C R-d represents the bound ary length (for d = 2) or the surface area (for d = 3) of G. A method for estimating L-0(G) is proposed. It relies on a nonparametric estimator based on the information provided by a random sample (taken on a rectangle containing G) in which we are able to identify whether every point is inside or outside G. Some theoretical properties concerning strong consistency, L-1-error and convergence rates are obtained. A practical application to a problem of image analysis in cardiology is discussed in some detail. A brief simulation study is provided.",WOS:000248692700005,ANNALS OF STATISTICS,"['LEVEL SETS', 'SUPPORT']",A nonparametric approach to the estimation of lengths and surface areas,2007
889,"Since the seminal work of Tukey (1975), depth functions have proved extremely useful in robust data analysis and inference for multivariate data. Many notions of depth have been developed in the last decades. Among others, projection depth appears to be very favorable. It turns out that (Zuo 2003; Zuo, Cui, and He 2004; Zuo 2006), with appropriate choices of univariate location and scale estimators, the projection depth induced estimators usually possess very high breakdown point robustness and finite sample relative efficiency. However, the computation of the projection depth seems hopeless and intimidating if not impossible. This hinders the further inference procedures development in practice. Sporadically algorithms exist in individual papers, though an unified computation package for projection depth has not been documented. To fill the gap, a MATLAB package entitled CompPD is presented in this paper, which is in fact an implementation of the latest developments (Liu, Zuo, and Wang 2013; Liu and Zuo 2014). Illustrative examples are also provided to guide readers through step-by-step usage of package CompPD to demonstrate its utility.",WOS:000365973700001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION DEPTH', 'TRIMMED REGIONS', 'LOCATION DEPTH', 'MULTIVARIATE', 'ESTIMATORS', 'DIMENSION', 'ALGORITHM', 'MEDIANS']",CompPD: A MATLAB Package for Computing Projection Depth,2015
890,"Random forests are one of the most popular statistical learning algorithms, and a variety of methods for fitting random forests and related recursive partitioning approaches is available in R. This paper points out two important features of the random forest implementation cforest available in the party package: The resulting forests are unbiased and thus preferable to the randomForest implementation available in randomForest if predictor variables are of different types. Moreover, a conditional permutation importance measure has recently been added to the party package, which can help evaluate the importance of correlated predictor variables. The rationale of this new measure is illustrated and hands-on advice is given for the usage of recursive partitioning tools in R.",WOS:000208589800003,R JOURNAL,,Party on!,2009
891,"We consider nonparametric estimation of the mean and covariance functions for functional/longitudinal data. Strong uniform convergence rates are developed for estimators that are local-linear smoothers. Our results are obtained in a unified framework in which the number of observations within each curve/cluster can be of any rate relative to the sample size. We show that the convergence rates for the procedures depend on both the number of sample curves and the number of observations on each curve. For sparse functional data, these rates are equivalent to the optimal rates in nonparametric regression. For dense functional data, root-n rates of convergence can be achieved with proper choices of bandwidths. We further derive almost sure rates of convergence for principal component analysis using the estimated covariance function. The results are illustrated with simulation studies.",WOS:000283792900004,ANNALS OF STATISTICS,"['SPARSE FUNCTIONAL DATA', 'LONGITUDINAL DATA', 'MODELS']",UNIFORM CONVERGENCE RATES FOR NONPARAMETRIC REGRESSION AND PRINCIPAL COMPONENT ANALYSIS IN FUNCTIONAL/LONGITUDINAL DATA,2010
892,"Discrete mixture models provide a well-known basis for effective clustering algorithms, although technical challenges have limited their scope. In the context of gene-expression data analysis, a model is presented that mixes over a finite catalog of structures, each one representing equality and inequality constraints among latent expected values. Computations depend on the probability that independent gamma-distributed variables attain each of their possible orderings. Each ordering event is equivalent to an event in independent negative-binomial random variables, and this finding guides a dynamic-programming calculation. The structuring of mixture-model components according to constraints among latent means leads to strict concavity of the mixture log likelihood. In addition to its beneficial numerical properties, the clustering method shows promising results in an empirical study.",WOS:000283792900001,ANNALS OF STATISTICS,"['HIDDEN MARKOV-MODELS', 'FINITE MIXTURES', 'RNA-SEQ', 'DISTRIBUTIONS', 'IDENTIFIABILITY', 'ABUNDANCE', 'RESPONSES', 'STRESS']",GAMMA-BASED CLUSTERING VIA ORDERED MEANS WITH APPLICATION TO GENE-EXPRESSION ANALYSIS,2010
893,"The data augmentation (DA) algorithm is a widely used Markov chain Monte Carlo (MCMC) algorithm that is based on a Markov transition density of the form p(x vertical bar x') = integral y fx vertical bar y (x vertical bar y)fY vertical bar X (y vertical bar x') dy, where fX vertical bar Y and fY vertical bar X are conditional densities. The PX-DA and marginal augmentation algorithms of Liu and Wu [J. Amer. Statist. Assoc. 94 (1999) 1264-1274] and Meng and van Dyk [Biometrika 86 (1999) 301-320] are alternatives to DA that often converge much faster and are only slightly more computationally demanding. The transition densities of these alternative algorithms can be written in the form PR (x vertical bar x') = integral Y integral y fX vertical bar Y (x vertical bar y') R(y, dy')fY vertical bar X (y vertical bar x') dy, where R is a Markov transition function on Y. We prove that when R satisfies certain conditions, the MCMC algorithm driven by PR is at least as good as that driven by p in terms of performance in the central limit theorem and in the operator norm sense. These results are brought to bear on a theoretical comparison of the DA, PX-DA and marginal augmentation algorithms. Our focus is on situations where the group structure exploited by Liu and Wu is available. We show that the PX-DA algorithm based on Haar measure is at least as good as any PX-DA algorithm constructed using a proper prior on the group.",WOS:000254502700002,ANNALS OF STATISTICS,"['CHAIN MONTE-CARLO', 'GIBBS SAMPLER', 'COVARIANCE STRUCTURE', 'DISTRIBUTIONS', 'CONVERGENCE', 'SCHEMES', 'RATES']","A theoretical comparison of the data augmentation, marginal augmentation and PX-DA algorithms",2008
894,"In this paper, we present a test for the maximal rank of the matrix-valued volatility process in the continuous Ito semimartingale framework. Our idea is based upon a random perturbation of the original high frequency observations of an Ito semimartingale, which opens the way for rank testing. We develop the complete limit theory for the test statistic and apply it to various null and alternative hypotheses. Finally, we demonstrate a homoscedasticity test for the rank process.",WOS:000327746100005,ANNALS OF STATISTICS,"['ASYMPTOTIC PROPERTIES', 'SEMIMARTINGALES']",A TEST FOR THE RANK OF THE VOLATILITY PROCESS: THE RANDOM PERTURBATION APPROACH,2013
895,"This paper generalizes a part of the theory of Z-estimation which has been developed mainly in the context of modern empirical processes to the case of stochastic processes, typically, semimartingales. We present a general theorem to derive the asymptotic behavior of the solution to an estimating equation theta (sic) Psi(n) (theta, (h) over cap (n)) = 0 with an abstract nuisance parameter h when the compensator of Psi(n) is random. As its application, we consider the estimation problem in an ergodic diffusion process model where the drift coefficient contains an unknown, finite-dimensional parameter theta and the diffusion coefficient is indexed by a nuisance parameter h from an infinite-dimensional space. An example for the nuisance parameter space is a class of smooth functions. We establish the asymptotic normality and efficiency of a Z-estimator for the drift coefficient. As another application, we present a similar result also in an ergodic time series model.",WOS:000271673500016,ANNALS OF STATISTICS,"['WEAK-CONVERGENCE', 'MARTINGALES', 'JUMPS']",ASYMPTOTIC THEORY OF SEMIPARAMETRIC Z-ESTIMATORS FOR STOCHASTIC PROCESSES WITH APPLICATIONS TO ERGODIC DIFFUSIONS AND TIME SERIES,2009
896,Notions of minimal sufficient causation are incorporated within the directed acyclic graph causal framework. Doing so allows for the graphical representation of sufficient causes and minimal sufficient causes on causal directed acyclic graphs while maintaining all of the properties of causal directed acyclic graphs. This in turn provides a clear theoretical link between two major conceptualizations of causality: one counterfactual-based and the other based on a more mechanistic understanding of causation. The theory developed can be used to draw conclusions about the sign of the conditional covariances among variables.,WOS:000265619700012,ANNALS OF STATISTICS,"['EMPIRICAL-RESEARCH', 'MARKOV PROPERTIES', 'MODELS', 'INDEPENDENCE', 'INFERENCE', 'NETWORKS', 'SEPARATION', 'VARIABLES', 'SYNERGISM', 'DIAGRAMS']",MINIMAL SUFFICIENT CAUSATION AND DIRECTED ACYCLIC GRAPHS,2009
897,,WOS:000357441000006,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'VON-MISES THEOREM']","DISCUSSION OF ""FREQUENTIST COVERAGE OF ADAPTIVE NONPARAMETRIC BAYESIAN CREDIBLE SETS""",2015
898,"Commonly used classification and regression tree methods like the CART algorithm are recursive partitioning methods that build the model in a forward stepwise search. Although this approach is known to be an efficient heuristic, the results of recursive tree methods are only locally optimal, as splits are chosen to maximize homogeneity at the next step only. An alternative way to search over the parameter space of trees is to use global optimization methods like evolutionary algorithms. This paper describes the evtree package, which implements an evolutionary algorithm for learning globally optimal classification and regression trees in R. Computationally intensive tasks are fully computed in C++ while the partykit package is leveraged for representing the resulting trees in R, providing unified infrastructure for summaries, visualizations, and predictions. evtree is compared to the open-source CART implementation rpart, conditional inference trees (ctree), and the open-source C4.5 implementation J48. A benchmark study of predictive accuracy and complexity is carried out in which evtree achieved at least similar and most of the time better results compared to rpart, ctree, and J48. Furthermore, the usefulness of evtree in practice is illustrated in a textbook customer classication task.",WOS:000349840200001,JOURNAL OF STATISTICAL SOFTWARE,"['DECISION TREES', 'GENETIC ALGORITHMS', 'INFERENCE', 'TARGET']",ectree: Evolutionary Learning of Globally Optimal Classification and Regression Trees in R,2014
899,"The Scythe Statistical Library is an open source C++ library for statistical computation. It includes a suite of matrix manipulation functions, a suite of pseudo-random number generators, and a suite of numerical optimization routines. Programs written using Scythe are generally much faster than those written in commonly used interpreted languages, such as R and MATLAB; and can be compiled on any system with the GNU GCC compiler (and perhaps with other C++ compilers). One of the primary design goals of the Scythe developers has been ease of use for non-expert C++ programmers. Ease of use is provided through three primary mechanisms: (1) operator and function over loading, (2) numerous pre-fabricated utility functions, and (3) clear documentation and example programs. Additionally, Scythe is quite flexible and entirely extensible because the source code is available to all users under the GNU General Public License.",WOS:000292098100001,JOURNAL OF STATISTICAL SOFTWARE,,The Scythe Statistical Library: An Open Source C plus plus Library for Statistical Computation,2011
900,"Ultra-high dimensional longitudinal data are increasingly common and the analysis is challenging both theoretically and methodologically. We offer a new automatic procedure for finding a sparse semivarying coefficient model, which is widely accepted for longitudinal data analysis. Our proposed method first reduces the number of covariates to a moderate order by employing a screening procedure, and then identifies both the varying and constant coefficients using a group SCAD estimator, which is subsequently refined by accounting for the within-subject correlation. The screening procedure is based on working independence and B-spline marginal models. Under weaker conditions than those in the literature, we show that with high probability only irrelevant variables will be screened out, and the number of selected variables can be bounded by a moderate order. This allows the desirable sparsity and oracle properties of the subsequent structure identification step. Note that existing methods require some kind of iterative screening in order to achieve this, thus they demand heavy computational effort and consistency is not guaranteed. The refined semivarying coefficient model employs profile least squares, local linear smoothing and nonparametric covariance estimation, and is semiparametric efficient. We also suggest ways to implement the proposed methods, and to select the tuning parameters. An extensive simulation study is summarized to demonstrate its finite sample performance and the yeast cell cycle data is analyzed.",WOS:000344632400005,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'NONCONCAVE PENALIZED LIKELIHOOD', 'PARTIALLY LINEAR-MODELS', 'VARIABLE SELECTION', 'SEMIPARAMETRIC ESTIMATION', 'NP-DIMENSIONALITY', 'ORACLE PROPERTIES', 'COX MODELS', 'REGRESSION', 'LASSO']",NONPARAMETRIC INDEPENDENCE SCREENING AND STRUCTURE IDENTIFICATION FOR ULTRA-HIGH DIMENSIONAL LONGITUDINAL DATA,2014
901,"The random numbers driving Markov chain Monte Carlo (MCMC) simulation are usually modeled as independent U(0, 1) random variables. Tribble [Markov chain Monte Carlo algorithms using completely uniformly distributed driving sequences (2007) Stanford Univ.] reports substantial improvements when those random numbers are replaced by carefully balanced inputs from completely uniformly distributed sequences. The previous theoretical justification for using anything other than lid. U(0, 1) points shows consistency for estimated means, but only applies for discrete stationary distributions. We extend those results to some MCMC algorithms for continuous stationary distributions. The main motivation is the search for quasi-Monte Carlo versions of MCMC. As a side benefit, the results also establish consistency for the usual method of using pseudo-random numbers in place of random ones.",WOS:000291183300001,ANNALS OF STATISTICS,"['ITERATED RANDOM FUNCTIONS', 'METROPOLIS ALGORITHM', 'DISCREPANCY', 'SIMULATION', 'SEQUENCES', 'NUMBERS']",CONSISTENCY OF MARKOV CHAIN QUASI-MONTE CARLO ON CONTINUOUS STATE SPACES,2011
902,"In attempts to further understand the dynamics of complex systems, the application of computer simulation is becoming increasingly prevalent. Whereas a great deal of focus has been placed in the development of software tools that aid researchers develop simulations, similar focus has not been applied in the creation of tools that perform a rigorous statistical analysis of results generated through simulation: vital in understanding how these results offer an insight into the captured system. This encouraged us to develop spartan, a package of statistical techniques designed to assist researchers in understanding the relationship between their simulation and the real system. Previously we have described each technique within spartan in detail, with an accompanying immunology case study examining the development of lymphoid tissue. Here we provide a practical introduction to the package, demonstrating how each technique is run in R, to assist researchers in integrating this package alongside their chosen simulation platform.",WOS:000348651700007,R JOURNAL,,Applying spartan to Understand Parameter Uncertainty in Simulations,2014
903,"The pa package provides tools for conducting performance attribution for long-only, single currency equity portfolios. The package uses two methods: the Brinson-Hood-Beebower model (hereafter referred to as the Brinson model) and a regression-based analysis. The Brinson model takes an ANOVA-type approach and decomposes the active return of any portfolio into asset allocation, stock selection, and interaction effect. The regression-based analysis utilizes estimated coefficients, based on a regression model, to attribute active return to different factors.",WOS:000330193300006,R JOURNAL,,Performance Attribution for Equity Portfolios,2013
904,"Consider a two-class classification problem where the number of features is much larger than the sample size. The features are masked by Gaussian noise with mean zero and covariance matrix Sigma, where the precision matrix Omega = Sigma(-1) is unknown but is presumably sparse. The useful features, also unknown, are sparse and each contributes weakly (i.e., rare and weak) to the classification decision.
By obtaining a reasonably good estimate of Omega, we formulate the setting as a linear regression model. We propose a two-stage classification method where we first select features by the method of Innovated Thresholding (IT), and then use the retained features and Fisher's LDA for classification. In this approach, a crucial problem is how to set the threshold of IT. We approach this problem by adapting the recent innovation of Higher Criticism Thresholding (HCT).
We find that when useful features are rare and weak, the limiting behavior of HCT is essentially just as good as the limiting behavior of ideal threshold, the threshold one would choose if the underlying distribution of the signals is known (if only). Somewhat surprisingly, when Omega is sufficiently sparse, its off-diagonal coordinates usually do not have a major influence over the classification decision.
Compared to recent work in the case where Omega is the identity matrix [Proc. Natl. Acad. Sci. USA 105 (2008) 14790-14795; Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci. 367 (2009) 4449-4470], the current setting is much more general, which needs a new approach and much more sophisticated analysis. One key component of the analysis is the intimate relationship between HCT and Fisher's separation. Another key component is the tight large-deviation bounds for empirical processes for data with unconventional correlation structures, where graph theory on vertex coloring plays an important role.",WOS:000327746100009,ANNALS OF STATISTICS,"['LINEAR DISCRIMINANT-ANALYSIS', 'OPTIMAL PHASE-DIAGRAM', 'HIGH-DIMENSIONAL DATA', 'HIGHER CRITICISM', 'VARIABLE SELECTION', 'COVARIANCE ESTIMATION', 'GENE-EXPRESSION', 'USEFUL FEATURES', 'LASSO', 'ALTERNATIVES']",OPTIMAL CLASSIFICATION IN SPARSE GAUSSIAN GRAPHIC MODEL,2013
905,"Blind source separation (BSS) is a well-known signal processing tool which is used to solve practical data analysis problems in various fields of science. In BSS, we assume that the observed data consists of linear mixtures of latent variables. The mixing system and the distributions of the latent variables are unknown. The aim is to find an estimate of an unmixing matrix which then transforms the observed data back to latent sources. In this paper we present the R packages JADE and BSSasymp. The package JADE offers several BSS methods which are based on joint diagonalization. Package BSSasymp contains functions for computing the asymptotic covariance matrices as well as their data-based estimates for most of the BSS estimators included in package JADE. Several simulated and real datasets are used to illustrate the functions in these two packages.",WOS:000392705700001,JOURNAL OF STATISTICAL SOFTWARE,['STATIONARY TIME-SERIES'],Blind Source Separation Based on Joint Diagonalization in R: The Packages JADE and BSSasymp,2017
906,"Functional integration in the brain refers to distributed interactions among functionally segregated regions. Investigation of effective connectivity in brain networks, i.e, the directed causal influence that one brain region exerts over another region, is being increasingly recognized as an important tool for understanding brain function in neuroimaging studies. Methods for identifying intrinsic relationships among elements in a network are increasingly in demand.
Over the last few decades several techniques such as Bayesian networks, Granger causality, and dynamic causal models have been developed to identify causal relations in dynamic systems. At the same time, established techniques such as structural equation modeling (SEM) are being modified and extended in order to reveal underlying interactions in imaging data. In the R package FIAR, which stands for Functional Integration Analysis in R, we have implemented many of the latest techniques for analyzing brain networks based on functional magnetic resonance imaging (fMRI) data. The package can be used to analyze experimental data, but also to simulate data under certain models.",WOS:000296719700001,JOURNAL OF STATISTICAL SOFTWARE,"['BOLD HEMODYNAMIC-RESPONSES', 'GRANGER CAUSALITY', 'TIME-SERIES', 'FMRI', 'CONNECTIVITY', 'MODEL', 'VARIABILITY']",FIAR: An R Package for Analyzing Functional Integration in the Brain,2011
907,"Classical categorical regression models such as the multinomial logit and proportional odds models are shown to be readily handled by the vector generalized linear and additive model (VGLM/VGAM) framework. Additionally, there are natural extensions, such as reduced-rank VGLMs for dimension reduction, and allowing covariates that have values specific to each linear/additive predictor, e.g., for consumer choice modeling. This article describes some of the framework behind the VGAM R package, its usage and implementation details.",WOS:000273372000001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED LINEAR-MODELS', 'ADDITIVE-MODELS', 'REGRESSION']",The VGAM Package for Categorical Data Analysis,2010
908,"The package diverse provides an easy-to-use interface to calculate and visualize different aspects of diversity in complex systems. In recent years, an increasing number of research projects in social and interdisciplinary sciences, including fields like innovation studies, scientometrics, economics, and network science have emphasized the role of diversification and sophistication of socioeconomic systems. However, so far no dedicated package exists that covers the needs of these emerging fields and interdisciplinary teams. Most packages about diversity tend to be created according to the demands and terminology of particular areas of natural and biological sciences. The package diverse uses interdisciplinary concepts of diversity-like variety, disparity and balance-as well as ubiquity and revealed comparative advantages, that are relevant to many fields of science, but are in particular useful for interdisciplinary research on diversity in socioeconomic systems. The package diverse provides a toolkit for social scientists, interdisciplinary researcher, and beginners in ecology to (i) import data, (ii) calculate different data transformations and normalization like revealed comparative advantages, (iii) calculate different diversity measures, and (iv) connect diverse to other specialized R packages on similarity measures, data visualization techniques, and statistical significance tests. The comprehensiveness of the package, from matrix import and transformations options, over similarity and diversity measures, to data visualization methods, makes it a useful package to explore different dimensions of diversity in complex systems.",WOS:000395669800005,R JOURNAL,"['SPECIES-DIVERSITY', 'FUNCTIONAL DIVERSITY', 'MATHEMATICAL-THEORY', 'HILL NUMBERS', 'EVENNESS', 'COMMUNICATION', 'FRAMEWORK', 'TRAITS']",diverse: an R Package to Analyze Diversity in Complex Systems,2016
909,"We consider the problem of nonparametric density estimation where estimates are constrained to be unimodal. Though several methods have been proposed to achieve this end, each of them has its own drawbacks and none of them have readily-available computer codes. The approach of Braun and Hall (2001), where a kernel density estimator is modified by data sharpening, is one of the most promising options, but optimization difficulties make it hard to use in practice. This paper presents a new algorithm and MATLAB code for finding good unimodal density estimates under the Braun and Hall scheme. The algorithm uses a greedy, feasibility-preserving strategy to ensure that it always returns a unimodal solution. Compared to the incumbent method of optimization, the greedy method is easier to use, runs faster, and produces solutions of comparable quality. It can also be extended to the bivariate case.",WOS:000303804400001,JOURNAL OF STATISTICAL SOFTWARE,['CONSTRAINTS'],A Greedy Algorithm for Unimodal Kernel Density Estimation by Data Sharpening,2012
910,"The ""curse of dimensionality"" has remained a challenge for high-dimensional data analysis in statistics'. The sliced inverse regression (SIR) and canonical correlation (CANCOR) methods aim to reduce the dimensionality of data by replacing the explanatory variables with a small number of composite directions without losing much information. However, the estimated composite directions generally involve all of the variables, making their interpretation difficult. To simplify the direction estimates, Ni, Cook and Tsai [Biometrika 92 (2005) 242-247] proposed the shrinkage sliced inverse regression (SSIR) based on SIR. In this paper, we propose the constrained canonical correlation (C-3) method based on CANCOR, followed by a simple variable filtering method. As a result, each composite direction consists of a subset of the variables for interpretability as well as predictive power. The proposed method aims to identify simple structures without sacrificing the desirable properties of the unconstrained CANCOR estimates. The simulation studies demonstrate the performance advantage of the proposed C-3 method over the SSIR method. We also use the proposed method in two examples for illustration.",WOS:000258243000009,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'NONCONCAVE PENALIZED LIKELIHOOD', 'SELECTION', 'SHRINKAGE']",Dimension reduction based on constrained canonical correlation and variable filtering,2008
911,"In this paper, we are concerned with how to select significant variables in semiparametric modeling. Variable selection for semiparametric regression models consists of two components: model selection for nonparametric components and selection of significant variables for the parametric portion. Thus, semiparametric variable selection is much more challenging than parametric variable selection (e.g., linear and generalized linear models) because traditional variable selection procedures including stepwise regression and the best subset selection now require separate model selection for the nonparametric components for each submodel. This leads to a very heavy computational burden. In this paper, we propose a class of variable selection procedures for semiparametric regression models using nonconcave penalized likelihood. We establish the rate of convergence of the resulting estimate. With proper choices of penalty functions and regularization parameters, we show the asymptotic normality of the resulting estimate and further demonstrate that the proposed procedures perform as well as an oracle procedure. A semiparametric generalized likelihood ratio test is proposed to select significant variables in the nonparametric component. We investigate the asymptotic behavior of the proposed test and demonstrate that its limiting null distribution follows a chi-square distribution which is independent of the nuisance parameters. Extensive Monte Carlo simulation studies are conducted to examine the finite sample performance of the proposed variable selection procedures.",WOS:000253390000010,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'EFFICIENT ESTIMATION', 'LIKELIHOOD', 'INFERENCES']",Variable selection in semiparametric regression modeling,2008
912,,WOS:000208590200013,R JOURNAL,,Conference Report: useR! 2011,2011
913,"This paper presents the R package HDclassif which is devoted to the clustering and the discriminant analysis of high-dimensional data. The classification methods proposed in the package result from a new parametrization of the Gaussian mixture model which combines the idea of dimension reduction and model constraints on the covariance matrices. The supervised classification method using this parametrization is called high dimensional discriminant analysis (HDDA). In a similar manner, the associated clustering method is called high dimensional data clustering (HDDC) and uses the expectation-maximization algorithm for inference. In order to correctly fit the data, both methods estimate the specific subspace and the intrinsic dimension of the groups. Due to the constraints on the covariance matrices, the number of parameters to estimate is significantly lower than other model-based methods and this allows the methods to be stable and efficient in high dimensions. Two introductory examples illustrated with R codes allow the user to discover the hdda and hddc functions. Experiments on simulated and real datasets also compare HDDC and HDDA with existing classification methods on high-dimensional datasets. HDclassif is a free software and distributed under the general public license, as part of the R software project.",WOS:000301071000001,JOURNAL OF STATISTICAL SOFTWARE,"['GAUSSIAN MIXTURE-MODELS', 'EM ALGORITHM', 'MAXIMUM-LIKELIHOOD', 'ANALYZERS', 'SELECTION']",HDclassif: An R Package for Model-Based Clustering and Discriminant Analysis of High-Dimensional Data,2012
914,"We establish a Cramer-type moderate deviation result for self-normalized sums of weakly dependent random variables, where the moment requirement is much weaker than the non-self-normalized counterpart. The range of the moderate deviation is shown to depend on the moment condition and the degree of dependence of the underlying processes. We consider three types of self-normalization: the equal-block scheme, the big-block-small-block scheme and the interlacing scheme. Simulation study shows that the latter can have a better finite-sample performance. Our result is applied to multiple testing and construction of simultaneous confidence intervals for ultra-high dimensional time series mean vectors.",WOS:000379972900008,ANNALS OF STATISTICS,"['HIGH-DIMENSIONAL DATA', 'AR-GARCH MODELS', 'TIME-SERIES', 'STUDENTS-T', 'TEMPORAL DEPENDENCE', 'LIMIT-THEOREMS', 'MARKOV-MODELS', 'CONVERGENCE', 'LIKELIHOOD', 'SEQUENCES']",SELF-NORMALIZED CRAMER-TYPE MODERATE DEVIATIONS UNDER DEPENDENCE,2016
915,"This paper describes an R package which produces tours of multivariate data. The package includes functions for creating different types of tours, including grand, guided, and little tours, which project multivariate data (p-D) down to 1, 2, 3, or, more generally, d (<= p) dimensions. The projected data can be rendered as densities or histograms, scatterplots, anaglyphs, glyphs, scatterplot matrices, parallel coordinate plots, time series or images, and viewed using an R graphics device, passed to GGobi, or saved to disk. A tour path can be stored for visualisation or replay. With this package it is possible to quickly experiment with different, and new, approaches to tours of data.This paper contains animations that can be viewed using the Adobe Acrobat PDF viewer.",WOS:000289228300001,JOURNAL OF STATISTICAL SOFTWARE,"['DATA VISUALIZATION', 'MULTIDIMENSIONAL DATA', 'GRAND-TOUR', 'PURSUIT', 'XGOBI', 'CLASSIFICATION']",tourr: An R Package for Exploring Multivariate Data with Projections,2011
916,"The gapped local alignment score of two random sequences follows a Gumbel distribution. If computers could estimate the parameters of the Gumbel distribution within one second, the use of arbitrary alignment scoring schemes could increase the sensitivity of searching biological sequence databases over the web. Accordingly, this article gives a novel equation for the scale parameter of the relevant Gumbel distribution. We speculate that the equation is exact, although present numerical evidence is limited. The equation involves ascending ladder variates in the global alignment of random sequences. In global alignment simulations, the ladder variates yield stopping times specifying random sequence lengths. Because of the random lengths. and because our trial distribution for importance sampling occurs on a different sample space from our target distribution, our study led to a mapping theorem, which led naturally in turn to an efficient dynamic programming algorithm for the importance sampling weights. Numerical studies using several popular alignment scoring schemes then examined the efficiency and accuracy of the resulting simulations.",WOS:000271673500022,ANNALS OF STATISTICS,"['ACID SUBSTITUTION MATRICES', 'APPROXIMATE P-VALUES', 'STATISTICAL SIGNIFICANCE']",ESTIMATING THE GUMBEL SCALE PARAMETER FOR LOCAL ALIGNMENT OF RANDOM SEQUENCES BY IMPORTANCE SAMPLING WITH STOPPING TIMES,2009
917,"Many statistical applications require an estimate of a covariance matrix and/or its inverse. When the matrix dimension is large compared to the sample size, which happens frequently, the sample covariance matrix is known to perform poorly and may suffer from ill-conditioning. There already exists an extensive literature concerning improved estimators in such situations. In the absence of further knowledge about the structure of the true covariance matrix, the most successful approach so far, arguably, has been shrinkage estimation. Shrinking the sample covariance matrix to a multiple of the identity, by taking a weighted average of the two, turns out to be equivalent to linearly shrinking the sample eigenvalues to their grand mean, while retaining the sample eigenvectors. Our paper extends this approach by considering nonlinear transformations of the sample eigenvalues. We show how to construct an estimator that is asymptotically equivalent to an oracle estimator suggested in previous work. As demonstrated in extensive Monte Carlo simulations, the resulting bona fide estimator can result in sizeable improvements over the sample covariance matrix and also over linear shrinkage.",WOS:000307608000015,ANNALS OF STATISTICS,"['LIMITING SPECTRAL DISTRIBUTION', 'EIGENVALUES', 'MODELS']",NONLINEAR SHRINKAGE ESTIMATION OF LARGE-DIMENSIONAL COVARIANCE MATRICES,2012
918,"Clustering text documents is a fundamental task in modern data analysis, requiring approaches which perform well both in terms of solution quality and computational efficiency. Spherical k-means clustering is one approach to address both issues, employing cosine dissimilarities to perform prototype-based partitioning of term weight representations of the documents.
This paper presents the theory underlying the standard spherical k-means problem and suitable extensions, and introduces the R extension package skmeans which provides a computational environment for spherical k-means clustering featuring several solvers: a fixed-point and genetic algorithm, and interfaces to two external solvers (CLUTO and Gmeans). Performance of these solvers is investigated by means of a large scale benchmark experiment.",WOS:000308909900001,JOURNAL OF STATISTICAL SOFTWARE,['ALGORITHM'],Spherical k-Means Clustering,2012
919,,WOS:000336888400007,ANNALS OF STATISTICS,['INFERENCE'],"DISCUSSION: ""A SIGNIFICANCE TEST FOR THE LASSO""",2014
920,"We introduce MCMCpack, an R package that contains functions to perform Bayesian inference using posterior simulation for a number of statistical models. In addition to code that can be used to fit commonly used models, MCMCpack also contains some useful utility functions, including some additional density functions and pseudo-random number generators for statistical distributions, a general purpose Metropolis sampling algorithm, and tools for visualization.",WOS:000292097600001,JOURNAL OF STATISTICAL SOFTWARE,"['MARGINAL LIKELIHOOD', 'MODELS', 'DENSITIES', 'PACKAGE', 'SYSTEMS', 'OUTPUT', 'FORCE']",MCMCpack: Markov Chain Monte Carlo in R,2011
921,This paper deals with the consistency of the nonparametric least squares estimator of a convex regression function when the predictor is multidimensional. We characterize and discuss the computation of such an estimator via the solution of certain quadratic and linear programs. Mild sufficient conditions for the consistency of this estimator and its subdifferentials in fixed and stochastic design regression settings are provided.,WOS:000293716500011,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'LOG-CONCAVE DENSITY', 'CONSISTENCY', 'MONOTONE']",NONPARAMETRIC LEAST SQUARES ESTIMATION OF A MULTIVARIATE CONVEX REGRESSION FUNCTION,2011
922,"We present fixed domain asymptotic results that establish consistent estimates of the variance and scale parameters for a Gaussian random field with a geometric anisotropic Matern autocovariance in dimension d > 4. When d < 4 this is impossible due to the mutual absolute continuity of Matern Gaussian random fields with different scale and variance (see Zhang [J. Amer. Statist. Assoc. 99 (2004) 250-261]). Informally, when d > 4, we show that one can estimate the coefficient on the principle irregular term accurately enough to get a consistent estimate of the coefficient on the second irregular term. These two coefficients can then be used to separate the scale and variance. We extend our results to the general problem of estimating a variance and geometric anisotropy for more general autocovariance functions. Our results illustrate the interaction between the accuracy of estimation, the smoothness of the random field, the dimension of the observation space and the number of increments used for estimation. As a corollary, our results establish the orthogonality of Matern Gaussian random fields with different parameters when d > 4. The case d = 4 is still open.",WOS:000275510800011,ANNALS OF STATISTICS,"['LEVY-BAXTER THEOREM', 'ASYMPTOTIC OPTIMALITY', 'QUADRATIC VARIATION', 'LINEAR PREDICTIONS', 'BROWNIAN SHEET', 'IDENTIFICATION', 'INCREMENTS', 'VERSION']",ON THE CONSISTENT SEPARATION OF SCALE AND VARIANCE FOR GAUSSIAN RANDOM FIELDS,2010
923,"We introduce a new method for performing clustering with the aim of fitting clusters with different scatters and weights. It is designed by allowing to handle a proportion alpha of contaminating data to guarantee the robustness of the method. As a characteristic feature, restrictions on the ratio between the maximum and the minimum eigenvalues of the groups scatter matrices are introduced. This makes the problem to be well defined and guarantees the consistency of the sample solutions to the population ones.
The method covers a wide range of clustering approaches depending on the strength of the chosen restrictions. Our proposal includes an algorithm for approximately solving the sample problem.",WOS:000256504400012,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD', 'K-MEANS', 'ORTHOGONAL REGRESSION', 'EM ALGORITHM', 'SCALES']",A general trimming approach to robust cluster analysis,2008
924,Structural equation models are multivariate statistical models that are defined by specifying noisy functional relationships among random variables. We consider the classical case of linear relationships and additive Gaussian noise terms. We give a necessary and sufficient condition for global identifiability of the model in terms of a mixed graph encoding the linear structural equations and the correlation structure of the error terms. Global identifiability is understood to mean injectivity of the parametrization of the model and is fundamental in particular for applicability of standard statistical methodology.,WOS:000291183300007,ANNALS OF STATISTICS,['CORRELATED ERRORS'],GLOBAL IDENTIFIABILITY OF LINEAR STRUCTURAL EQUATION MODELS,2011
925,"We analyze and discuss how a generic software to produce biplot graphs should be designed. We describe a data structure appropriate to include the biplot description and we specify the algorithm(s) to be used for several biplot types. We discuss the options the software should offer to the user in two different environments. In a highly interactive environment the user should be able to specify many graphical options and also to change them using the usual interactive tools. The resulting graph needs to be available in several formats, including high quality format for printing. In a web-based environment, the user submits a data file or listing together with some options specified either in a file or using a form. Then the graphic is sent back to the user in one of several possible formats according to the specifications. We review some of the already available software and we present an implementation based in XLISP-STAT. It can be run under Unix or Windows, and it is also part of a service that provides biplot graphs through the web.",WOS:000232831600001,JOURNAL OF STATISTICAL SOFTWARE,,Interactive biplot construction,2005
926,"A recurrent task in applied statistics is the (mostly manual) preparation of model output for inclusion in LATEX, Microsoft Word, or HTML documents - usually with more than one model presented in a single table along with several goodness-of-fit statistics. However, statistical models in R have diverse object structures and summary methods, which makes this process cumbersome. This article first develops a set of guidelines for converting statistical model output to LATEX and HTML tables, then assesses to what extent existing packages meet these requirements, and finally presents the texreg package as a solution that meets all of the criteria set out in the beginning. After providing various usage examples, a blueprint for writing custom model extensions is proposed.",WOS:000328129600001,JOURNAL OF STATISTICAL SOFTWARE,,texreg: Conversion of Statistical Model Output in R to LATEX and HTML Tables,2013
927,"Functional magnetic resonance imaging (fMRI) aims to locate activated regions in human brains when specific tasks are performed. The conventional tool for analyzing fMRI data applies some variant of the linear model, which is restrictive in modeling assumptions. To yield more accurate prediction of the time-course behavior of neuronal responses, the semiparametric inference for the underlying hemodynamic response function is developed to identify significantly activated voxels. Under mild regularity conditions, we demonstrate that a class of the proposed semiparametric test statistics, based on the local linear estimation technique, follow chi(2) distributions under null hypotheses for a number of useful hypotheses. Furthermore, the asymptotic power functions of the constructed tests are derived under the fixed and contiguous alternatives. Simulation evaluations and real fMRI data application suggest that the semiparametric inference procedure provides more efficient detection of activated brain areas than the popular imaging analysis tools AFNI and FSL.",WOS:000258243000011,ANNALS OF STATISTICS,"['STATISTICS', 'MATRICES', 'RATES', 'MODEL']",Semiparametric detection of significant activation for brain fMRI,2008
928,"We introduce here Momocs, a package intended to ease and popularize modern morphometrics with R, and particularly outline analysis, which aims to extract quantitative variables from shapes. It mostly hinges on the functions published in the book entitled Modern Morphometrics Using R by Claude (2008). From outline extraction from raw data to multivariate analysis, Momocs provides an integrated and convenient toolkit to students and researchers who are, or may become, interested in describing the shape and its variation. The methods implemented so far in Momocs are introduced through a simplistic case study that aims to test if two sets of bottles have different shapes.",WOS:000332113200001,JOURNAL OF STATISTICAL SOFTWARE,"['ELLIPTIC FOURIER DESCRIPTORS', 'SHAPE VARIATION', 'MORPHOMETRICS', 'REVOLUTION']",Momocs: Outline Analysis Using R,2014
929,"We describe the R package geo Count for the analysis of geostatistical count data. The package performs Bayesian analysis for the Poisson-lognormal and binomial- logitnormal spatial models, which are subclasses of the class of generalized linear spatial models proposed by Diggle, Tawn, and Moyeed (1998). The package implements the computational intensive tasks in C++ using an R/C++ interface, and has parallel computation capabilities to speed up the computations. geoCount also implements group updating, LangevinHastings algorithms and a data-based parameterization, algorithmic approaches proposed by Christensen, Roberts, and Skold (2006) to improve the efficiency of the Markov chain Monte Carlo algorithms. In addition, the package includes functions for simulation and visualization, as well as three geostatistical count datasets taken from the literature. One of those is used to illustrate the package capabilities. Finally, we provide a side-by-side comparison between geoCount and the R packages geoRglm and INLA",WOS:000349846400001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR MIXED MODELS', 'HIERARCHICAL-MODELS', 'PREDICTION', 'APPROXIMATIONS']",geoCount: An R Package for the Analysis of Geostatistical Count Data,2015
930,"Convex optimization now plays an essential role in many facets of statistics. We briefly survey some recent developments and describe some implementations of these methods in R. Applications of linear and quadratic programming are introduced including quantile regression, the Huber M-estimator and various penalized regression methods. Applications to additively separable convex problems subject to linear equality and inequality constraints such as nonparametric density estimation and maximum likelihood estimation of general nonparametric mixture models are described, as are several cone programming problems. We focus throughout primarily on implementations in the R environment that rely on solution methods linked to R, like MOSEK by the package Rmosek. Code is provided in R to illustrate several of these problems. Other applications are available in the R package REBayes, dealing with empirical Bayes estimation of nonparametric mixture models.",WOS:000345289600001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'LOG-CONCAVE DENSITY', 'MODEL SELECTION', 'REGRESSION', 'ALGORITHM', 'CONSISTENCY', 'RECOVERY', 'LASSO']",Convex Optimization in R,2014
931,"The fastcluster package is a C++ library for hierarchical, agglomerative clustering. It provides a fast implementation of the most efficient, current algorithms when the input is a dissimilarity index. Moreover, it features memory-saving routines for hierarchical clustering of vector data. It improves both asymptotic time complexity (in most cases) and practical performance (in all cases) compared to the existing implementations in standard software: several R packages, MATLAB, Mathematica, Python with SciPy.
The fastcluster package presently has interfaces to R and Python. Part of the functionality is designed as a drop-in replacement for the methods hclust and flashClust in R and scipy. cluster. hierarchy. linkage in Python, so that existing programs can be effortlessly adapted for improved performance.",WOS:000320040800001,JOURNAL OF STATISTICAL SOFTWARE,,"fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for R and Python",2013
932,"Dense spatial data are commonplace nowadays, and they provide the impetus for addressing nonstationarity in a general way. This paper extends the notion of intrinsic random function by allowing the stationary component of the covariance to vary with spatial location. A nonparametric estimation procedure based on gridded data is introduced for the case where the covariance function is regularly varying at any location. An asymptotic theory is developed for the procedure on a fixed domain by letting the grid size tend to zero.",WOS:000384397200012,ANNALS OF STATISTICS,"['FRACTIONAL BROWNIAN MOTIONS', 'GAUSSIAN RANDOM-FIELDS', 'FRACTAL DIMENSION', 'SPATIAL DATA', 'NONSTATIONARY', 'PLANE']",LOCAL INTRINSIC STATIONARITY AND ITS INFERENCE,2016
933,"We present a new R software package lawstat that contains statistical tests and procedures that are utilized in various litigations on securities law, antitrust law, equal employment and discrimination as well as in public policy and biostatistics. Along with the wellknown tests such as the Bartels test, runs test, tests of homogeneity of several sample proportions, the Brunner-Munzel tests, the Lorenz curve, the Cochran-Mantel-Haenszel test and others, the package contains new distribution-free robust tests for symmetry, robust tests for normality that are more sensitive to heavy-tailed departures, measures of relative variability, Levene-type tests against trends in variances etc. All implemented tests and methods are illustrated by simulations and real-life examples from legal cases, economics and biostatistics. Although the packageis called lawstat, it presents implementation and discussion of statistical procedures and tests that are also employed in a variety of other applications, e.g., biostatistics, environmental studies, social sciences and others, in other words, all applications utilizing statistical dataanalysis. Hence, name of the package should not be considered as a restriction to legal statistics. The package will be useful to applied statisticians and ""quantitatively alert practitioners "" of other subjects as well as an asset in teaching statistical courses.",WOS:000259946800001,JOURNAL OF STATISTICAL SOFTWARE,"['DEPENDENT DATA', 'TESTS', 'NORMALITY', 'VARIANCES', 'RANDOMNESS', 'VARIABLES', 'SYMMETRY', 'EQUALITY']","lawstat: An R Package for Law, Public Policy and Biostatistics",2008
934,"Clickstream analysis is a useful tool for investigating consumer behavior, market research and software testing. I present the clickstream package which provides functionality for reading, clustering, analyzing and writing clickstreams in R. The package allows for a modeling of lists of clickstreams as zero-, first- and higher-order Markov chains. I illustrate the application of clickstream for a list of representative clickstreams from an online store.",WOS:000392513300001,JOURNAL OF STATISTICAL SOFTWARE,,R Package clickstream: Analyzing Clickstream Data with Markov Chains,2016
935,"In this paper we detail the reformulation and rewrite of core functions in the spBayes R package. These efforts have focused on improving computational efficiency, flexibility, and usability for point-referenced data models. Attention is given to algorithm and computing developments that result in improved sampler convergence rate and efficiency by reducing parameter space; decreased sampler run-time by avoiding expensive matrix computations, and; increased scalability to large datasets by implementing a class of predictive process models that attempt to overcome computational hurdles by representing spatial processes in terms of lower-dimensional realizations. Beyond these general computational improvements for existing model functions, we detail new functions for modeling data indexed in both space and time. These new functions implement a class of dynamic spatio-temporal models for settings where space is viewed as continuous and time is taken as discrete.",WOS:000349846700001,JOURNAL OF STATISTICAL SOFTWARE,,spBayes for Large Univariate and Multivariate Point-Referenced Spatio-Temporal Data Models,2015
936,Data-driven versions of Sobolev tests of uniformity on compact Riemannian manifolds are proposed. These tests are invariant under isometries; and are consistent against all alternatives. The large-sample asymptotic null distributions are given.,WOS:000256504400009,ANNALS OF STATISTICS,"['OF-FIT TESTS', 'SMOOTH TESTS', 'COMPOSITE HYPOTHESES', 'VERSION']",Data-driven Sobolev tests of uniformity on compact Riemannian manifolds,2008
937,,WOS:000208589700009,R JOURNAL,,EMD: A Package for Empirical Mode Decomposition and Hilbert Spectrum,2009
938,"The most popular multiple testing procedures are stepwise procedures based on P-values for individual test statistics. Included among these are the false discovery rate (FDR) controlling procedures of Benjamini-Hochberg [J. Roy. Statist. Soc. Ser B 57 (1995) 289-300] and their offsprings. Even for models that entail dependent data, P-values based on marginal distributions are used. Unlike such methods, the new method takes dependency into account at all stages. Furthermore, the P-value procedures often lack an intuitive convexity property, which is needed for admissibility. Still further, the new methodology is computationally feasible. If the number of tests is large and the proportion of true alternatives is less than say 25 percent, simulations demonstrate a clear preference for the new methodology. Applications are detailed for models such as testing treatments against control (or any intraclass correlation model), testing for change points and testing means when correlation is successive.",WOS:000265619700015,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'STEP-UP PROCEDURE', 'HYPOTHESES', 'INADMISSIBILITY', 'MICROARRAYS']",A NEW MULTIPLE TESTING METHOD IN THE DEPENDENT CASE,2009
939,"We study nonparametric estimation for current status data with competing risks. Our main interest is in the nonparametric maximum likelihood estimator (MLE), and for comparison we also consider a simpler ""naive estimator."" Groeneboom, Maathuis and Wellner [Ann. Statist. (2008) 36 10311063] proved that both types of estimators converge globally and locally at rate n(1/3). We use these results to derive the local limiting distributions of the estimators. The limiting distribution of the naive estimator is given by the slopes of the convex minorants of correlated Brownian motion processes with parabolic drifts. The limiting distribution of the MLE involves a new self-induced limiting process. Finally, we present a simulation study showing that the MLE is superior to the naive estimator in terms of mean squared error, both for small sample sizes and asymptotically.",WOS:000256504400002,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'BROWNIAN-MOTION']",Current status data with competing risks: Limiting distribution of the MLE,2008
940,"We present the FRB package for R, which implements the fast and robust bootstrap. This method constitutes an alternative to ordinary bootstrap or asymptotic inference procedures when using robust estimators such as S-, MM- or GS-estimators. The package considers three multivariate settings: principal components analysis, Hotelling tests and multivariate regression. It provides both the robust point estimates and uncertainty measures based on the fast and robust bootstrap. In this paper we give some background on the method, discuss the implementation and provide various examples.",WOS:000318236800001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED S-ESTIMATORS', 'REGRESSION']",Fast and Robust Bootstrap for Multivariate Inference: The R Package FRB,2013
941,"This paper is the second in a series of two papers that fully develops two-stage short-run ( X, MR) control charts. This paper describes the development and execution of a computer program that accurately calculates first- and second-stage short-run control chart factors for ( X, MR) charts using the equations derived in the first paper. The software used is Mathcad. The program accepts values for number of subgroups, alpha for the X chart, and alpha for the MR chart both above the upper control limit and below the lower control limit. Tables are generated for specific values of these inputs and the implications of the results are discussed. A numerical example illustrates the use of the program.",WOS:000236800900001,JOURNAL OF STATISTICAL SOFTWARE,,"A computer program to calculate two-stage short-run control chart factors for (X, MR) charts",2006
942,"Regression problems are traditionally analyzed via univariate characteristics like the regression function, scale function and marginal density of regression errors. These characteristics are useful and informative whenever the association between the predictor and the response is relatively simple. More detailed information about the association can be provided by the conditional density of the response given the predictor. For the first time in the literature, this article develops the theory of minimax estimation of the conditional density for regression settings with fixed and random designs of predictors, bounded and unbounded responses and a vast set of anisotropic classes of conditional densities. The study of fixed design regression is of special interest and novelty because the known literature is devoted to the case of random predictors. For the aforementioned models, the paper suggests a universal adaptive estimator which (i) matches performance of an oracle that knows both an underlying model and an estimated conditional density; (ii) is sharp minimax over a vast class of anisotropic conditional densities; (iii) is at least rate minimax when the response is independent of the predictor and thus a bivariate conditional density becomes a univariate density; (iv) is adaptive to an underlying design (fixed or random) of predictors.",WOS:000253077800013,ANNALS OF STATISTICS,['NONPARAMETRIC-ESTIMATION'],Conditional density estimation in a regression setting,2007
943,"The inverse Gaussian distribution is a positively skewed probability model that has received great attention in the last 20 years. Recently, a family that generalizes this model called inverse Gaussian type distributions has been developed. The new R package named ig has been designed to analyze data from inverse Gaussian type distributions. This package contains basic probabilistic functions, lifetime indicators and a random number generator from this model. Also, parameter estimates and diagnostics analysis can be obtained using likelihood methods by means of this package. In addition, goodness-of-fit methods are implemented in order to detect the suitability of the model to the data. The capabilities and features of the ig package are illustrated using simulated and real data sets. Furthermore, some new results related to the inverse Gaussian type distribution are also obtained. Moreover, a simulation study is conducted for evaluating the estimation method implemented in the ig package.",WOS:000257322600001,JOURNAL OF STATISTICAL SOFTWARE,,An R package for a general class of inverse Gaussian distributions,2008
944,"This article describes the R package FrF2 for design and analysis of experiments with 2-level factors. The package offers both regular and non-regular fractional factorial 2-level designs, in the regular case with blocking and split plot facilities and algorithms for ensuring estimability of certain two-factor interactions. Furthermore, simple analysis facilities are on offer, first and foremost plotting functions for half normal plots, main effects and interaction plots, and cube plots. Package FrF2 receives infrastructure support from package DoE.base and heavily builds on a subgraph isomorphism algorithm from package igraph for one of its estimability algorithms.",WOS:000331696000001,JOURNAL OF STATISTICAL SOFTWARE,"['CLEAR 2-FACTOR INTERACTIONS', 'MINIMUM ABERRATION', 'COMPROMISE PLANS', 'RESOLUTION', 'CATALOG', 'RUNS']",R Package FrF2 for Creating and Analyzing Fractional Factorial 2-Level Designs,2014
945,,WOS:000336888400006,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'SELECTION', 'INFERENCE']","DISCUSSION: ""A SIGNIFICANCE TEST FOR THE LASSO""",2014
946,"This paper describes the package PtProcess which uses the R statistical language. The package provides a unified approach to fitting and simulating a wide variety of temporal point process or temporal marked point process models. The models are specified by an intensity function which is conditional on the history of the process. The user needs to provide routines for calculating the conditional intensity function. Then the package enables one to carry out maximum likelihood fitting, goodness of fit testing, simulation and comparison of models. The package includes the routines for the conditional intensity functions for a variety of standard point process models. The package is intended to simplify the fitting of point process models indexed by time in much the same way as generalized linear model programs have simplified the fitting of various linear models. The primary examples used in this paper are earthquake sequances but the package is intended to have a much wider applicability.",WOS:000281587900001,JOURNAL OF STATISTICAL SOFTWARE,"['EARTHQUAKE OCCURRENCES', 'RESIDUAL ANALYSIS', 'LINEAR-MODELS', 'SIMULATION']",PtProcess: An R Package for Modelling Marked Point Process Indexed by Time,2010
947,"We consider the problem of selecting covariates ill spatial linear models with Gaussian process errors. Penalized maximum likelihood estimation (PMLE) that enables simultaneous variable selection and parameter estimation is developed and, for ease of computation, PMLE is approximated by one-step sparse estimation (OSE). To further improve computational efficiency, particularly with large sample sizes, we propose penalized maximum covariance-tapered likelihood estimation (PMLE(T)) and its one-step sparse estimation (OSE(T)). General forms of penalty functions with an emphasis on smoothly clipped absolute deviation are used for penalized maximum likelihood. Theoretical properties of PMLE and USE, as well as their approximations PMLE(T) and OSE(T) using covariance tapering, are derived, including consistency, sparsity, asymptotic normality and the oracle properties. For covariance tapering, a by-product of our theoretical results is consistency and asymptotic normality of maximum covariance-tapered likelihood estimates. Finite-sample properties of the proposed methods are demonstrated in a simulation study and, for illustration, the methods are applied to analyze two real data sets.",WOS:000299186500016,ANNALS OF STATISTICS,"['SPATIAL REGRESSION', 'ORACLE PROPERTIES', 'MODEL SELECTION', 'COVARIANCE', 'LASSO', 'SHRINKAGE']",PENALIZED MAXIMUM LIKELIHOOD ESTIMATION AND VARIABLE SELECTION IN GEOSTATISTICS,2011
948,,WOS:000208589700010,R JOURNAL,,Sample Size Estimation while Controlling False Discovery Rate for Microarray Experiments Using the ssize.fdr Package,2009
949,"The traveling salesperson (or, salesman) problem (TSP) is a well known and important combinatorial optimization problem. The goal is to find the shortest tour that visits each city in a given list exactly once and then returns to the starting city. Despite this simple problem statement, solving the TSP is difficult since it belongs to the class of NP-complete problems. The importance of the TSP arises besides from its theoretical appeal from the variety of its applications. Typical applications in operations research include vehicle routing, computer wiring, cutting wallpaper and job sequencing. The main application in statistics is combinatorial data analysis, e.g., reordering rows and columns of data matrices or identifying clusters. In this paper, we introduce the R package TSP which provides a basic infrastructure for handling and solving the traveling salesperson problem. The package features S3 classes for specifying a TSP and its (possibly optimal) solution as well as several heuristics to find good solutions. In addition, it provides an interface to Concorde, one of the best exact TSP solvers currently available.",WOS:000252431000001,JOURNAL OF STATISTICAL SOFTWARE,['SALESMAN PROBLEM'],TSP - Infrastructure for the traveling salesperson problem,2007
950,"In this paper we introduce a general theory for nonlinear sufficient dimension reduction, and explore its ramifications and scope. This theory subsumes recent work employing reproducing kernel Hilbert spaces, and reveals many parallels between linear and nonlinear sufficient dimension reduction. Using these parallels we analyze the properties of existing methods and develop new ones. We begin by characterizing dimension reduction at the general level of sigma-fields and proceed to that of classes of functions, leading to the notions of sufficient, complete and central dimension reduction classes. We show that, when it exists, the complete and sufficient class coincides with the central class, and can be unbiasedly and exhaustively estimated by a generalized sliced inverse regression estimator (GSIR). When completeness does not hold, this estimator captures only part of the central class. However, in these cases we show that a generalized sliced average variance estimator (GSAVE) can capture a larger portion of the class. Both estimators require no numerical optimization because they can be computed by spectral decomposition of linear operators. Finally, we compare our estimators with existing methods by simulation and on actual data sets.",WOS:000317451200009,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'LINEAR-REGRESSION', 'MODELS']",A GENERAL THEORY FOR NONLINEAR SUFFICIENT DIMENSION REDUCTION: FORMULATION AND ESTIMATION,2013
951,"The profdpm package facilitates inference at the posterior mode for a class of product partition models. Dirichlet process mixtures are currently the only available class members. Several methods are implemented to search for the maximum posterior estimate of the data partition. This article discusses the relevant theory, the R and underlying C implementation, and examples of high level functionality.",WOS:000320040600001,JOURNAL OF STATISTICAL SOFTWARE,"['2 HIERARCHICAL CLUSTERINGS', 'PROCESS MIXTURE MODEL', 'DIRICHLET PROCESS', 'DENSITY-ESTIMATION', 'BAYESIAN MODEL', 'INFERENCE']",profdpm: An R Package for MAP Estimation in a Class of Conjugate Product Partition Models,2013
952,"The kth-nearest neighbor rule is arguably the simplest and most intuitively appealing nonparametric classification procedure. However, application of this method is inhibited by lack of knowledge about its properties, in particular, about the manner in which it is influenced by the value of k; and by the absence of techniques for empirical choice of k. In the present paper we detail the way in which the value of k determines the misclassification error. We consider two models, Poisson and Binomial, for the training samples. Under the first model, data are recorded in a Poisson stream and are ""assigned"" to one or other of the two populations in accordance with the prior probabilities. In particular, the total number of data in both training samples is a Poisson-distributed random variable. Under the Binomial model, however, the total number of data in the training samples is fixed, although again each data value is assigned in a random way. Although the values of risk and regret associated with the Poisson and Binomial models are different, they are asymptotically equivalent to first order, and also to the risks associated with kernel-based classifiers that are tailored to the case of two derivatives. These properties motivate new methods for choosing the value of k.",WOS:000260554100005,ANNALS OF STATISTICS,"['NONPARAMETRIC DISCRIMINATION', 'PATTERN-CLASSIFICATION', 'CONVERGENCE', 'ERROR', 'RATES', 'PROBABILITY', 'CONSISTENCY', 'CLASSIFIERS', 'RULES', 'RISK']",CHOICE OF NEIGHBOR ORDER IN NEAREST-NEIGHBOR CLASSIFICATION,2008
953,"Approximations to the modified signed likelihood ratio statistic are asymptotically standard normal with error of order n(-1), where n is the sample size. Proofs of this fact generally require that the sufficient statistic of the model be written as ((theta) over cap, a), where theta is the maximum likelihood estimator of the parameter theta of the model and a is an ancillary statistic. This condition is very difficult or impossible to verify for many models. However, calculation of the statistics themselves does not require this condition. The goal of this paper is to provide conditions under which these statistics are asymptotically normally distributed to order n(-1) without making any assumption about the sufficient statistic of the model.",WOS:000251096100009,ANNALS OF STATISTICS,"['EDGEWORTH EXPANSIONS', 'INFERENCE']",Higher-order asymptotic normality of approximations to the modified signed likelihood ratio statistic for regular models,2007
954,"This article provides a brief introduction to the state space modeling capabilities in SAS, a well-known statistical software system. SAS provides state space modeling in a few different settings. SAS/ETS, the econometric and time series analysis module of the SAS system, contains many procedures that use state space models to analyze univariate and multivariate time series data. In addition, SAS/IML, an interactive matrix language in the SAS system, provides Kalman filtering and smoothing routines for stationary and nonstationary state space models. SAS/IML also provides support for linear algebra and nonlinear function optimization, which makes it a convenient environment for general-purpose state space modeling.",WOS:000290528100001,JOURNAL OF STATISTICAL SOFTWARE,,State Space Modeling Using SAS,2011
955,"Testing covariance structure is of significant interest in many areas of statistical analysis and construction of compressed sensing matrices is an important problem in signal processing. Motivated by these applications, we study in this paper the limiting laws of the coherence of an n x p random matrix in the high-dimensional setting where p can be much larger than n. Both the law of large numbers and the limiting distribution are derived. We then consider testing the bandedness of the covariance matrix of a high-dimensional Gaussian distribution which includes testing for independence as a special case. The limiting laws of the coherence of the data matrix play a critical role in the construction of the test. We also apply the asymptotic results to the construction of compressed sensing matrices.",WOS:000293716500006,ANNALS OF STATISTICS,"['SAMPLE CORRELATION-MATRICES', 'DIMENSIONAL FEATURE SPACE', 'DANTZIG SELECTOR', 'ASYMPTOTIC-DISTRIBUTION', 'STATISTICAL ESTIMATION', 'MULTIVARIATE-ANALYSIS', 'LARGEST EIGENVALUE', 'STABLE RECOVERY', 'LARGEST ENTRIES', 'SPARSE SIGNALS']",LIMITING LAWS OF COHERENCE OF RANDOM MATRICES WITH APPLICATIONS TO TESTING COVARIANCE STRUCTURE AND CONSTRUCTION OF COMPRESSED SENSING MATRICES,2011
956,"Identifying causal parameters from observational data is fraught with subtleties due to the issues of selection bias and confounding. In addition, more complex questions of interest, such as effects of treatment on the treated and mediated effects may not always be identified even in data where treatment assignment is known and under investigator control, or may be identified under one causal model but not another.
Increasingly complex effects of interest, coupled with a diversity of causal models in use resulted in a fragmented view of identification. This fragmentation makes it unnecessarily difficult to determine if a given parameter is identified (and in what model), and what assumptions must hold for this to be the case. This, in turn, complicates the development of estimation theory and sensitivity analysis procedures.
In this paper, we give a unifying view of a large class of causal effects of interest, including novel effects not previously considered, in terms of a hierarchy of interventions, and show that identification theory for this large class reduces to an identification theory of random variables under interventions from this hierarchy. Moreover, we show that one type of intervention in the hierarchy is naturally associated with queries identified under the Finest Fully Randomized Causally Interpretable Structure Tree Graph (FFRCISTG) model of Robins (via the extended g-formula), and another is naturally associated with queries identified under the Non-Parametric Structural Equation Model with Independent Errors (NPSEM-IE) of Pearl, via a more general functional we call the edge g-formula.
Our results motivate the study of estimation theory for the edge g-formula, since we show it arises both in mediation analysis, and in settings where treatment assignment has unobserved causes, such as models associated with Pearl's front-door criterion.",WOS:000389620800008,ANNALS OF STATISTICS,"['DYNAMIC TREATMENT REGIMES', 'POTENTIAL OUTCOMES', 'MEDIATION ANALYSIS', 'MODELS']",CAUSAL INFERENCE WITH A GRAPHICAL HIERARCHY OF INTERVENTIONS,2016
957,"Regression models to relate a scalar Y to a functional predictor X(t) are becoming increasingly common. Work in this area has concentrated on estimating a coefficient function, beta(t), with Y related to X(t) through integral beta(t)X(t)dt. Regions where beta (t) not equal 0 correspond to places where there is a relationship between X (t) and Y. Alternatively, points where beta(t) = 0 indicate no relationship. Hence, for interpretation put-poses, it is desirable for a regression procedure to be capable of producing estimates of beta(t) thin are exactly zero over regions with no apparent relationship and have simple structures over the remaining regions. Unfortunately, most fitting procedures result in an estimate for beta(t) that is rarely exactly zero and has unnatural wiggles making the curve hard to interpret. In this article we introduce a new approach which uses variable selection ideas, applied to various derivatives of beta(t), to produce estimates that Lire both interpretable, flexible and accurate. We call Our method ""Functional Linear Regression That's Interpretable"" (FLiRTI) and demonstrate it on simulated and real-world data sets. In addition, non-asymptotic theoretical bounds on the estimation error are presented. The bounds provide strong theoretical motivation for our approach.",WOS:000268604900001,ANNALS OF STATISTICS,"['LONGITUDINAL DATA', 'VARIABLE SELECTION', 'DANTZIG SELECTOR', 'MODELS', 'LASSO', 'SHRINKAGE', 'CURVES']",FUNCTIONAL LINEAR REGRESSION THAT'S INTERPRETABLE,2009
958,"Statistical estimation and inference for marginal hazard models with varying coefficients for multivariate failure time data are important subjects in survival analysis. A local pseudo-partial likelihood procedure is proposed for estimating the unknown coefficient functions. A weighted average estimator is also proposed in an attempt to improve the efficiency of the estimator. The consistency and asymptotic normality of the proposed estimators are established and standard error formulas for the estimated coefficients are derived and empirically tested. To reduce the computational burden of the maximum local pseudo-partial likelihood estimator, a simple and useful one-step estimator is proposed. Statistical properties of the one-step estimator are established and simulation studies are conducted to compare the performance of the one-step estimator to that of the maximum local pseudo-partial likelihood estimator. The results show that the one-step estimator can save computational cost without compromising performance both asymptotically and empirically and that an optimal weighted average estimator is more efficient than the maximum local pseudo-partial likelihood estimator. A data set from the Busselton Population Health Surveys is analyzed to illustrate our proposed methodology.",WOS:000247498100013,ANNALS OF STATISTICS,"['COX REGRESSION-MODEL', 'LIKELIHOOD-ESTIMATION', 'ESTIMATING EQUATIONS', 'FRAILTY', 'BUSSELTON', 'MORTALITY', 'TESTS']",Hazard models with varying coefficients for multivariate failure time data,2007
959,"This paper discusses the problem of adaptive estimation Of a univariate object like the value of a regression function at a given point or a linear functional in a linear inverse problem. We consider an adaptive procedure originated from Lepski [Theory Probab. Appl. 35 (1990) 454-466.] that selects in a data-driven way one estimate Out of a given class of estimates ordered by their variability. A serious problem with using this and similar procedures is the choice of some tuning parameters like thresholds. Numerical results show that the theoretically recommended proposals appear to be too conservative and lead to a strong oversmoothing effect. A careful choice of the parameters of the procedure is extremely important for getting the reasonable (quality of estimation. The main contribution of this paper is the new approach for choosing the parameters of the procedure by providing the prescribed behavior of the resulting estimate in the simple parametric situation. We establish a non-asymptotical ""oracle"" bound, which shows that the estimation risk is, Lip to a logarithmic multiplier, equal to the risk of the ""oracle"" estimate that is optimally selected from the given family. A numerical study demonstrates a good performance of the resulting procedure in a number of simulated examples.",WOS:000268605000008,ANNALS OF STATISTICS,"['MINIMUM CONTRAST ESTIMATORS', 'ADAPTIVE ESTIMATION', 'NONPARAMETRIC-ESTIMATION', 'LINEAR FUNCTIONALS', 'HILBERT SCALES', 'WHITE-NOISE', 'CONVERGENCE', 'RATES', 'BOUNDS']",PARAMETER TUNING IN POINTWISE ADAPTATION USING A PROPAGATION APPROACH,2009
960,"Knowledge space theory is part of psychometrics and provides a theoretical framework for the modeling, assessment, and training of knowledge. It utilizes the idea that some pieces of knowledge may imply others, and is based on order and set theory. We introduce the R package D A K S for performing basic and advanced operations in knowledge space theory. This package implements three inductive item tree analysis algorithms for deriving quasi orders from binary data, the original, corrected, and minimized corrected algorithms, in sample as well as population quantities. It provides functions for computing population and estimated asymptotic variances of and one and two sample Z tests for the diff fit measures, and for switching between test item and knowledge state representations. Other features are a function for computing response pattern and knowledge state frequencies, a data (based on a finite mixture latent variable model) and quasi order simulation tool, and a Hasse diagram drawing device. We describe the functions of the package and demonstrate their usage by real and simulated data examples.",WOS:000284597100001,JOURNAL OF STATISTICAL SOFTWARE,"['MODEL', 'HIERARCHIES']",DAKS: An R Package for Data Analysis Methods in Knowledge Space Theory,2010
961,"Genetic recombination is one of the most important mechanisms that can generate and maintain diversity, and recombination information plays an important role in population genetic studies. However, the phenomenon of recombination is extremely complex, and hence simulation methods are indispensable in the statistical inference of recombination. So far there are mainly two classes of simulation models practically in wide use: back-in-time models and spatially moving models. However, the statistical properties shared by the two classes of simulation models have not yet been theoretically studied. Based on our joint research with CAS-MPG Partner Institute for Computational Biology and with Beijing Jiaotong University, in this paper we provide for the first time a rigorous argument that the statistical properties of the two classes of simulation models are identical. That is, they share the same probability distribution on the space of ancestral recombination graphs (ARGs). As a consequence, our study provides a unified interpretation for the algorithms of simulating coalescent with recombination, and will facilitate the study of statistical inference on recombination.",WOS:000342481700005,ANNALS OF STATISTICS,"['SIMULATION', 'SEQUENCES', 'SAMPLES']",MARKOV JUMP PROCESSES IN MODELING COALESCENT WITH RECOMBINATION,2014
962,"This article illustrates usage of the ramps R package, which implements the reparameterized and marginalized posterior sampling (RAMPS) algorithm for complex Bayesian geostatistical models. The RAMPS methodology allows joint modeling of areal and point-source data arising from the same underlying spatial process. A reparametrization of variance parameters facilitates slice sampling based on simplexes, which can be useful in general when multiple variances are present. Prediction at arbitrary points can be made, which is critical in applications where maps are needed. Our implementation takes advantage of sparse matrix operations in the Matrix package and can provide substantial savings in computing time for large datasets. A user-friendly interface, similar to the nlme mixed effects models package, enables users to analyze datasets with little programming effort. Support is provided for numerous spatial and spatiotemporal correlation structures, user-defined correlation structures, and non-spatial random effects. The package features are illustrated via a synthetic dataset of spatially correlated observation distributed across the state of Iowa, USA.",WOS:000255794800001,JOURNAL OF STATISTICAL SOFTWARE,['CHAIN MONTE-CARLO'],Unified geostatistical modeling for data fusion and spatial heteroskedasticity with R package ramps,2008
963,"A modification of an accept-and-reject algorithm to sample from a set of restricted permutations is proposed. By concentrating on a special class of matrices obtained by restriction of the permutation in time, assuming the objects to be permuted to be events in time, the modified algorithm's running time can be shown to be linear instead of geometric in the number of elements. The implementation of the algorithm in the language R is presented in a Literate Programming style.",WOS:000239693800001,JOURNAL OF STATISTICAL SOFTWARE,"['CLINICAL-TRIALS', 'RANDOMIZATION']",An accept-and-reject algorithm to sample from a set of permutations restricted by a time constraint,2006
964,"Based partly on interviews with members of the R Core team, this paper considers the development of the R Project in the context of open-source software development and, more generally, voluntary activities. The paper describes aspects of the social organization of the R Project, including the organization of the R Core team; describes the trajectory of the R Project; seeks to identify factors crucial to the success of R; and speculates about the prospects for R.",WOS:000208589800002,R JOURNAL,,Aspects of the Social Organization and Trajectory of the R Project,2009
965,"Computerized adaptive testing (CAT) is a powerful technique to help improve measurement precision and reduce the total number of items required in educational, psychological, and medical tests. In CATs, tailored test forms are progressively constructed by capitalizing on information available from responses to previous items. CAT applications primarily have relied on unidimensional item response theory (IRT) to help select which items should be administered during the session. However, multidimensional CATs may be constructed to improve measurement precision and further reduce the number of items required to measure multiple traits simultaneously.
A small selection of CAT simulation packages exist for the R environment; namely, catR (Magis and Raiche 2012), catIrt (Nydick 2014), and MAT (Choi and King 2014). However, the ability to generate graphical user interfaces for administering CATs in real-time has not been implemented in R to date, support for multidimensional CATs have been limited to the multidimensional three-parameter logistic model, and CAT designs were required to contain IRT models from the same modeling family. This article describes a new R package for implementing unidimensional and multidimensional CATs using a wide variety of IRT models, which can be unique for each respective test item, and demonstrates how graphical user interfaces and Monte Carlo simulation designs can be constructed with the mirtCAT package.",WOS:000384914900001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'MH-RM ALGORITHM', 'BIFACTOR ANALYSIS', 'EM ALGORITHM', 'MODEL', 'PACKAGE']",Generating Adaptive and Non-Adaptive Test Interfaces for Multidimensional Item Response Theory Applications,2016
966,"Gaussian graphical models are semi-algebraic subsets of the cone of positive definite covariance matrices. Submatrices with low rank correspond to generalizations of conditional independence constraints on collections of random variables. We give a precise graph-theoretic characterization of when submatrices of the covariance matrix have small rank for a general class of mixed graphs that includes directed acyclic and undirected graphs as special cases. Our new trek separation criterion generalizes the familiar d-separation criterion. Proofs are based on the trek rule, the resulting matrix factorizations and classical theorems of algebraic combinatories on the expansions of determinants of path polynomials.",WOS:000277471000014,ANNALS OF STATISTICS,,TREK SEPARATION FOR GAUSSIAN GRAPHICAL MODELS,2010
967,"The R package Three Way is presented and its main features are illustrated. The aim of Three Way is too ff er a suit of functions for handling three- way arrays. In particular, the most relevant available functions are T3 and CP, which implement, respectively, the Tucker3 and Candecomp/ Parafac methods. They are the two most popular tools for summarizing three-way arrays in terms of components. After briefly recalling both techniques from a theoretical point of view, the functions T3 and CP are described by considering three real life examples.",WOS:000340586400001,JOURNAL OF STATISTICAL SOFTWARE,"['CANDECOMP/PARAFAC', 'DEGENERACY', 'ARRAYS', 'RANK', 'DECOMPOSITIONS', 'NUMBERS', 'MODELS']",Three-Way Component Analysis Using the R Package ThreeWay,2014
968,"MIXREGLS is a program which provides estimates for a mixed-effects location scale model assuming a (conditionally) normally-distributed dependent variable. This model can be used for analysis of data in which subjects may be measured at many observations and interest is in modeling the mean and variance structure. In terms of the variance structure, covariates can by specified to have effects on both the between-subject and within-subject variances. Another use is for clustered data in which subjects are nested within clusters (e.g., clinics, hospitals, schools, etc.) and interest is in modeling the between-cluster and within-cluster variances in terms of covariates. MIXREGLS was written in Fortran and uses maximum likelihood estimation, utilizing both the EM algorithm and a Newton-Raphson solution. Estimation of the random effects is accomplished using empirical Bayes methods. Examples illustrating stand-alone usage and features of MIXREGLS are provided, as well as use via the SAS and R software packages.",WOS:000316316900001,JOURNAL OF STATISTICAL SOFTWARE,"['MOMENTARY ASSESSMENT DATA', 'ASSESSMENT EMA DATA', 'INTRAINDIVIDUAL VARIABILITY', 'MODEL', 'PARAMETERS', 'VARIANCES', 'ISSUES']",MIXREGLS: A Program for Mixed-Effects Location Scale Analysis,2013
969,"During the last decade text mining has become a widely used discipline utilizing statistical and machine learning methods. We present the tm package which provides a framework for text mining applications within R. We give a survey on text mining facilities in R and explain how typical application tasks can be carried out using our framework. We present techniques for count- based analysis methods, text clustering, text classification and string kernels.",WOS:000254619800001,JOURNAL OF STATISTICAL SOFTWARE,"['LATENT SEMANTIC ANALYSIS', 'STRING KERNELS', 'CATEGORIZATION']",Text mining infrastructure in R,2008
970,"Markov chain Monte Carlo (MCMC) is the most widely used method of estimating joint posterior distributions in Bayesian analysis. The idea of MCMC is to iteratively produce parameter values that are representative samples from the joint posterior. Unlike frequentist analysis where iterative model fitting routines are monitored for convergence to a single point, MCMC output is monitored for convergence to a distribution. Thus, specialized diagnostic tools are needed in the Bayesian setting. To this end, the R package boa was created. This manuscript presents the user's manual for boa, which out lines the use of and methodology upon which the software is based. Included is a description of the menu system, data management capabilities, and statistical/graphical methods for convergence assessment and posterior inference. Throughout the manual, a linear regression example is used to ilustrate the software.",WOS:000252429300001,JOURNAL OF STATISTICAL SOFTWARE,"['CHAIN MONTE-CARLO', 'INITIALIZATION BIAS', 'SIMULATION OUTPUT']",boa: An R package for MCMC output convergence assessment and posterior inference,2007
971,"This paper discusses the software D-STEM as a statistical tool for the analysis and mapping of environmental space-time variables. The software is based on a flexible hierarchical space-time model which is able to deal with multiple variables, heterogeneous spatial supports, heterogeneous sampling networks and missing data. Model estimation is based on the expectation maximization algorithm and it can be performed using a distributed computing environment to reduce computing time when dealing with large data sets. The estimated model is eventually used to dynamically map the variables over the geographic region of interest. Three examples of increasing complexity illustrate usage and capabilities of D-STEM, both in terms of modeling and implementation, starting from a univariate model and arriving at a multivariate data fusion with tapering.",WOS:000349843500001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'MULTIVARIATE RANDOM-FIELDS', 'CROSS-COVARIANCE FUNCTIONS', 'AIR-QUALITY', 'SPATIAL DATA', 'DATA SETS', 'MODEL']",D-STEM: A Software for the Analysis and Mapping of Environmental Space-Time Variables,2014
972,"Inverse estimation is a classical and well-known problem in regression. In simple terms, it involves the use of an observed value of the response to make inference on the corresponding unknown value of the explanatory variable. To our knowledge, however, statistical software is somewhat lacking the capabilities for analyzing these types of problems. In this paper, we introduce investr (which stands for inverse estimation in R), a package for solving inverse estimation problems in both linear and nonlinear regression models.",WOS:000343788100010,R JOURNAL,['CALIBRATION'],investr: An R Package for Inverse Estimation,2014
973,"The purpose of this paper is to list the recent updates of the R package catR. This package allows for generating response patterns under a computerized adaptive testing (CAT) framework with underlying item response theory (IRT) models. Among the most important updates, well-known polytomous IRT models are now supported by catR; several item selection rules have been added; and it is now possible to perform post-hoc simulations. Some functions were also rewritten or withdrawn to improve the usefulness and performances of the package.",WOS:000392705400001,JOURNAL OF STATISTICAL SOFTWARE,"['ITEM-EXPOSURE CONTROL', 'LATENT ABILITY', 'SHADOW TESTS', 'MODEL', 'INFORMATION', 'CATEGORIES', 'SELECTION', 'RATES']",Computerized Adaptive Testing with R: Recent Updates of the Package catR,2017
974,"Local increases in the mean of a random field are detected (conservatively) by thresholding a field of test statistics at a level u chosen to control the tail probability or p-value of its maximum. This p-value is approximated by the expected Euler characteristic (EC) of the excursion set of the test statistic field above u, denoted E phi (A(u)). Under isotropy, one can use the expansion E phi(A(u)) = Sigma(k) V(k rho k()u), where V-k is an intrinsic volume of the parameter space and rho(k) is an EC density of the field. EC densities are available for a number of processes, mainly those constructed from (multivariate) Gaussian fields via smooth functions. Using saddlepoint methods, we derive an expansion for (rho k)(u) for fields which are only approximately Gaussian, but for which higher-order cumulants are available. We focus on linear combinations of n independent non-Gaussian fields, whence a Central Limit theorem is in force. The threshold u is allowed to grow with the sample size n, in which case our expression has a smaller relative asymptotic error than the Gaussian EC density. Several illustrative examples including an application to ""bubbles"" data accompany the theory.",WOS:000260554100018,ANNALS OF STATISTICS,"['VOXEL-BASED MORPHOMETRY', 'SADDLEPOINT APPROXIMATIONS', 'CONFIDENCE BANDS', 'EXCURSION SETS', 'RECOGNITION', 'STATISTICS', 'FORMULA']","TILTED EULER CHARACTERISTIC DENSITIES FOR CENTRAL LIMIT RANDOM FIELDS, WITH APPLICATION TO ""BUBBLES""",2008
975,"A random-walk Metropolis sampler is geometrically ergodic if its equilibrium density is super-exponentially light and satisfies a curvature condition [Stochastic Process. Appl. 85 (2000) 341-361]. Many applications, including Bayesian analysis with conjugate priors of logistic and Poisson regression and of log-linear models for categorical data result in posterior distributions that are not super-exponentially light. We show how to apply the change-of-variable formula for diffeomorphisms to obtain new densities that do satisfy the conditions for geometric ergodicity. Sampling the new variable and mapping the results back to the old gives a geometrically ergodic sampler for the original variable. This method of obtaining geometric ergodicity has very wide applicability.",WOS:000321845400011,ANNALS OF STATISTICS,"['CENTRAL-LIMIT-THEOREM', 'CHAIN MONTE-CARLO', 'EXPLORING POSTERIOR DISTRIBUTIONS', 'RANDOM EFFECTS MODEL', 'MARKOV-CHAIN', 'GIBBS SAMPLER', 'HIERARCHICAL-MODELS', 'CONVERGENCE-RATES', 'ESTIMATORS', 'HASTINGS']",VARIABLE TRANSFORMATION TO OBTAIN GEOMETRIC ERGODICITY IN THE RANDOM-WALK METROPOLIS ALGORITHM,2012
976,"Seriation, i.e., finding a suitable linear order for a set of objects given data and a loss or merit function, is a basic problem in data analysis. Caused by the problem's combinatorial nature, it is hard to solve for all but very small sets. Nevertheless, both exact solution methods and heuristics are available. In this paper we present the package seriation which provides an infrastructure for seriation with R. The infrastructure comprises data structures to represent linear orders as permutation vectors, a wide array of seriation methods using a consistent interface, a method to calculate the value of various loss and merit functions, and several visualization techniques which build on seriation. To illustrate how easily the package can be applied for a variety of applications, a comprehensive collection of examples is presented.",WOS:000254619600001,JOURNAL OF STATISTICAL SOFTWARE,"['HIERARCHICAL CLUSTER-ANALYSIS', 'TRAVELING-SALESMAN PROBLEM', 'STABILITY', 'ALGORITHM']",Getting things in order: An introduction to the R package seriation,2008
977,"This paper deals with the R-php statistical software, that is an environment for statistical analysis, freely accessible and attainable through the World Wide Web, based on R. Indeed, this software uses, as ""engine"" for statistical analyses, R via PHP and its design has been inspired by a paper of de Leeuw (1997). R-php is based on two modules: a base module and a point-and-click module. R-php base allows the simple editing of R code in a form. R-php point-and-click allows some statistical analyses by means of a graphical user interface (GUI): then, to use this module it is not necessary for the user to know the R environment, but all the allowed analyses can be performed by using the computer mouse. We think that this tool could be particularly useful for teaching purposes: one possible use could be in a University computer laboratory to permit a smooth approach of students to R.",WOS:000241808200001,JOURNAL OF STATISTICAL SOFTWARE,,Using R via PHP for teaching purposes: R-php,2006
978,,WOS:000336888400008,ANNALS OF STATISTICS,,"REJOINDER: ""A SIGNIFICANCE TEST FOR THE LASSO""",2014
979,"It is shown how to set up, conduct, and analyze large simulation studies with the new R package simsalapar (= simulations simplified and launched parallel). A simulation study typically starts with determining a collection of input variables and their values on which the study depends. Computations are desired for all combinations of these variables. If conducting these computations sequentially is too time-consuming, parallel computing can be applied over all combinations of select variables. The final result object of a simulation study is typically an array. From this array, summary statistics can be derived and presented in terms of flat contingency or LATEX tables or visualized in terms of matrix-like figures.
The R package simsalapar provides several tools to achieve the above tasks. Warnings and errors are dealt with correctly, various seeding methods are available, and run time is measured. Furthermore, tools for analyzing the results via tables or graphics are provided. In contrast to rather minimal examples typically found in R packages or vignettes, an end-to-end, not-so-minimal simulation problem from the realm of quantitative risk management is given. The concepts presented and solutions provided by simsalapar may be of interest to students, researchers, and practitioners as a how-to for conducting realistic, large-scale simulation studies in R.",WOS:000373917100001,JOURNAL OF STATISTICAL SOFTWARE,,Parallel and Other Simulations in R Made Easy: An End-to-End Study,2016
980,"The capabilities of the packages exams for automatic generation (statistical) exams in R are extended adding support foe learing mansgemnt systems: As in earlier versions - of the package exam generation is still based on separate sweave files for each exercise - but rather than just producing different types of pdf output files, the package can now render the same exercises into a wide variety of output formats. these include IITML (with exams in learning management systems such as moodle or OLAT. The flexibility is accomplished by a new modular and extensible desgn of the package that alloes for reading all weaved exercises into R and manging associated supplementary files (such as graphics or data files the manuscript discuss , and how new functioality can be built on top of the existing tools.",WOS:000341583500001,JOURNAL OF STATISTICAL SOFTWARE,,"Flexible Generation of E-learning Exams in R: Moodle Quizzes, OLAT Assessments, and Beyond",2014
981,"Meinshausen and Buhlmann [Ann. Statist. 34 (2006) 1436-1462] showed that, for neighborhood selection in Gaussian graphical models, under a neighborhood stability condition, the LASSO is consistent, even when the number of variables is of greater order than the sample size. Zhao and Yu [(2006) J. Machine Learning Research 7 2541-2567] formalized the neighborhood stability condition in the context of linear regression as a strong irrepresentable condition. That paper showed that under this condition, the LASSO selects exactly the set of nonzero regression coefficients, provided that these coefficients are bounded away from zero at a certain rate. In this paper, the regression coefficients outside an ideal model are assumed to be small, but not necessarily zero. Under a sparse Riesz condition on the correlation of design variables, we prove that the LASSO selects a model of the correct order of dimensionality, controls the bias of the selected model at a level determined by the contributions of small regression coefficients and threshold bias, and selects all coefficients of greater order than the bias of the selected model. Moreover, as a consequence of this rate consistency of the LASSO in model selection, it is proved that the sum of error squares for the mean response and the l(alpha)-loss for the regression coefficients converge at the best possible rates under the given conditions. An interesting aspect of our results is that the logarithm of the number of variables can be of the same order as the sample size for certain random dependent designs.",WOS:000258243000006,ANNALS OF STATISTICS,"['STATISTICAL ESTIMATION', 'VARIABLE SELECTION', 'DANTZIG SELECTOR', 'MODEL SELECTION', 'RANDOM MATRICES', 'LARGER', 'RISK']",The sparsity and bias of the lasso selection in high-dimensional linear regression,2008
982,"This article describes the recent package rsm, which was designed to provide R support for standard response-surface methods. Functions are provided to generate central-composite and Box-Behnken designs. For analysis of the resulting data, the package provides for estimating the response surface, testing its lack of fit, displaying an ensemble of contour plots of the fitted surface, and doing follow-up analyses such as steepest ascent, canonical analysis, and ridge analysis. It also implements a coded-data structure to aid in this essential aspect of the methodology. The functions are designed in hopes of providing an intuitive and effective user interface. Potential exists for expanding the package in a variety of ways.",WOS:000271534200001,JOURNAL OF STATISTICAL SOFTWARE,,"Response-Surface Methods in R, Using rsm",2009
983,"We study the asymptotic behaviour of the posterior distribution in a broad class of statistical models where the ""true"" solution occurs on the boundary of the parameter space. We show that in this case Bayesian inference is consistent, and that the posterior distribution has not only Gaussian components as in the case of regular models (the Bernstein-von Mises theorem) but also has Gamma distribution components whose form depends on the behaviour of the prior distribution near the boundary and have a faster rate of convergence. We also demonstrate a remarkable property of Bayesian inference, that for some models, there appears to be no bound on efficiency of estimating the unknown parameter if it is on the boundary of the parameter space. We illustrate the results on a problem from emission tomography.",WOS:000344632400006,ANNALS OF STATISTICS,"['LIKELIHOOD RATIO TESTS', 'BAYESIAN RECONSTRUCTIONS', 'POSTERIOR DISTRIBUTIONS', 'NONSTANDARD CONDITIONS', 'EMISSION-TOMOGRAPHY', 'CONSISTENCY', 'ESTIMATORS']",THE BERNSTEIN-VON MISES THEOREM AND NONREGULAR MODELS,2014
984,We identify principles and practices for writing and publishing statistical software with maximum bene fit to the scholarly community.,WOS:000292096700001,JOURNAL OF STATISTICAL SOFTWARE,,Nineteen Ways of Looking at Statistical Software,2011
985,"We consider a class of doubly weighted rank-based estimating methods for the transformation (or accelerated failure time) model with missing. data as arise, for example, in case-cohort studies. The weights considered may not be predictable its required in a martingale stochastic process formulation. We treat the general problem as a semi parametric estimating equation problem and provide proofs of asymptotic properties for the weighted estimators, with either true weights or estimated Weights. by using empirical process theory where martingale theory may fail. Simulations show that the outcome-dependent weighted method works well for finite samples in case-cohort studies and improves efficiency compared to methods based oil predictable weights. Further. it is seen that the method is even more efficient when estimated Weights are used, as is commonly the case in the missing data literature. The Gehan censored data Wilcoxon weights are found to lie surprisingly efficient in a wide class of problems.",WOS:000268604900010,ANNALS OF STATISTICS,"['CASE-COHORT', 'LINEAR-REGRESSION', 'CENSORED-DATA', 'RESAMPLING METHOD', 'RANK-TESTS', 'COVARIABLES', 'PARAMETERS', 'LIKELIHOOD', 'EFFICIENCY']",ASYMPTOTIC THEORY FOR THE SEMIPARAMETRIC ACCELERATED FAILURE TIME MODEL WITH MISSING DATA,2009
986,"Panel data analysis is an important topic in statistics and econometrics. In such analysis, it is very common to assume the impact of a covariate on the response variable remains constant across all individuals. While the modelling based on this assumption is reasonable when only the global effect is of interest, in general, it may overlook some individual/subgroup attributes of the true covariate impact. In this paper, we propose a data driven approach to identify the groups in panel data with interactive effects induced by latent variables. It is assumed that the impact of a covariate is the same within each group, but different between the groups. An EM based algorithm is proposed to estimate the unknown parameters, and a binary segmentation based algorithm is proposed to detect the grouping. We then establish asymptotic theories to justify the proposed estimation, grouping method, and the modelling idea. Simulation studies are also conducted to compare the proposed method with the existing approaches, and the results obtained favour our method. Finally, the proposed method is applied to analyse a data set about income dynamics, which leads to some interesting findings.",WOS:000375175200011,ANNALS OF STATISTICS,"['GROUPING PURSUIT', 'FACTOR MODELS', 'SELECTION']",STRUCTURE IDENTIFICATION IN PANEL DATA ANALYSIS,2016
987,"The predictive value of a statistical model can often be improved by applying shrinkage methods. This can be achieved, e.g., by regularized regression or empirical Bayes approaches. Various types of shrinkage factors can also be estimated after a maximum likelihood fit has been obtained: while global shrinkage modifies all regression coefficients by the same factor, parameterwise shrinkage factors differ between regression coefficients. The latter ones have been proposed especially in the context of variable selection. With variables which are either highly correlated or associated with regard to contents, such as dummy variables coding a categorical variable, or several parameters describing a nonlinear effect, parameterwise shrinkage factors may not be the best choice. For such cases, we extend the present methodology by so-called 'joint shrinkage factors', a compromise between global and parameterwise shrinkage.
Shrinkage factors are often estimated using leave-one-out resampling. We also discuss a computationally simple and much faster approximation to resampling-based shrinkage factor estimation, can be easily obtained in most standard software packages for regression analyses. This alternative may be relevant for simulation studies and other computer-intensive investigations.
Furthermore, we provide an R package shrink implementing the mentioned shrinkage methods for models fitted by linear, generalized linear, or Cox regression, even if these models involve fractional polynomials or restricted cubic splines to estimate the influence of a continuous variable by a nonlinear function. The approaches and usage of the package shrink are illustrated by means of two examples.",WOS:000373918200001,JOURNAL OF STATISTICAL SOFTWARE,"['LOGISTIC-REGRESSION ANALYSIS', 'FRACTIONAL POLYNOMIALS', 'NONORTHOGONAL PROBLEMS', 'RIDGE REGRESSION', 'SELECTION', 'MODELS', 'PREDICTION', 'RECURRENCE', 'LASSO', 'RISK']","Global, Parameterwise and Joint Shrinkage Factor Estimation",2016
988,"This paper presents an example of online reproducible multivariate data analysis. This example is based on a web page providing an online computing facility on a server. HTML forms contain editable R code snippets that can be executed in any web browser thanks to the Rweb software. The example is based on the multivariate analysis of DNA fingerprints of the internal bacterial flora of the poultry red mite Dermanyssus gallinae. Several multivariate data analysis methods from the ade4 package are used to compare the fingerprints of mite pools coming from various poultry farms. All the computations and graphical displays can be redone interactively and further explored online, using only a web browser. Statistical methods are detailed in the duality diagram framework, and a discussion about online reproducibility is initiated.",WOS:000208589900008,R JOURNAL,,Online Reproducible Research: An Application to Multivariate Analysis of Bacterial DNA Fingerprint Data,2010
989,"Recent advances in the implementation of spatial econometrics model estimation techniques have made it desirable to compare results, which should correspond between implementations across software applications for the same data. These model estimation techniques are associated with methods for estimating impacts (emanating effects), which are also presented and compared. This review constitutes an up-to-date comparison of generalized method of moments and maximum likelihood implementations now available. The comparison uses the cross-sectional US county data set provided by Drukker, Prucha, and Raciborski (2013d). The comparisons will be cast in the context of alternatives using the MATLAB Spatial Econometrics toolbox, Stata's user-written sppack commands, Python with PySAL and R packages including spdep, sphet and McSpatial.",WOS:000349847500001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR-REGRESSION MODELS', 'AUTOREGRESSIVE DISTURBANCES', 'HETEROSKEDASTIC INNOVATIONS', 'SPECIFICATION', 'DEPENDENCE', 'ALGORITHMS', 'VARIABLES', 'COMMAND', 'SYSTEM', 'ERRORS']",Comparing Implementations of Estimation Methods for Spatial Econometrics,2015
990,"The R Commander graphical user interface to R is extensible via plug-in packages, which integrate seamlessly with the R Commander's menu structure, data, and model handling. The paper describes the RcmdrPlugin.survival package, which makes many of the facilities of the survival package for R available through the R Commander, including Cox and parametric survival models. We explain the structure, capabilities, and limitations of this plug-in package and illustrate its use.",WOS:000305991300001,JOURNAL OF STATISTICAL SOFTWARE,,The RcmdrPlugin.survival Package: Extending the R Commander Interface to Survival Analysis,2012
991,"In this paper, we observe a fixed number of unknown 2 pi-periodic functions differing from each other by both phases and amplitude. This semiparametric model appears in literature under the name ""shape invariant model."" While the common shape is unknown, we introduce an asymptotically efficient estimator of the unite-dimensional parameter (phases and amplitude) using the profile likelihood and the Fourier basis. Moreover, this estimation method leads to a consistent and asymptotically linear estimator or the common shape.",WOS:000277471000020,ANNALS OF STATISTICS,"['NONLINEAR-REGRESSION', 'LIKELIHOOD', 'CURVES']",EFFICIENT ESTIMATION FOR A SUBCLASS OF SHAPE INVARIANT MODELS,2010
992,"Record linkage deals with detecting homonyms and mainly synonyms in data. The package RecordLinkage provides means to perform and evaluate different record linkage methods. A stochastic framework is implemented which calculates weights through an EM algorithm. The determination of the necessary thresholds in this model can be achieved by tools of extreme value theory. Furthermore, machine learning methods are utilized, including decision trees (rpart), bootstrap aggregating (bagging), ada boost (ada), neural nets (nnet) and support vector machines (svm). The generation of record pairs and comparison patterns from single data items are provided as well. Comparison patterns can be chosen to be binary or based on some string metrics. In order to reduce computation time and memory usage, blocking can be used. Future development will concentrate on additional and refined methods, performance improvements and input/output facilities needed for real-world application.",WOS:000208590000010,R JOURNAL,,The RecordLinkage Package: Detecting Errors in Data,2010
993,"This paper considers the statistical inference of the class of asymmetric power-transformed GARCH(1, 1) models in presence of possible explosiveness. We study the explosive behavior of volatility when the strict stationarity condition is not met. This allows us to establish the asymptotic normality of the quasi-maximum likelihood estimator (QMLE) of the parameter, including the power but without the intercept, when strict stationarity does not hold. Two important issues can be tested in this framework: asymmetry and stationarity. The tests exploit the existence of a universal estimator of the asymptotic covariance matrix of the QMLE. By establishing the local asymptotic normality (LAN) property in this nonstationary framework, we can also study optimality issues.",WOS:000326991200010,ANNALS OF STATISTICS,"['AUTOREGRESSIVE CONDITIONAL HETEROSCEDASTICITY', 'TIME-SERIES', 'NONPARAMETRIC-ESTIMATION', 'ASYMPTOTIC INFERENCE', 'STRICT STATIONARITY', 'ADAPTIVE ESTIMATION', 'ARCH MODELS', 'UNIT-ROOT', 'VOLATILITY', 'TESTS']",INFERENCE IN NONSTATIONARY ASYMMETRIC GARCH MODELS,2013
994,"Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm.
Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible.
Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.",WOS:000392705600001,JOURNAL OF STATISTICAL SOFTWARE,['MONTE-CARLO'],Stan: A Probabilistic Programming Language,2017
995,"In a series of papers De Leeuw developed a general framework for multivariate analysis with optimal scaling. The basic idea of optimal scaling is to transform the observed variables (categories) in terms of quantifications. In the approach presented here the multivariate data are collected into a multivariable. An aspect of a multivariable is a function that is used to measure how well the multivariable satisfies some criterion. Basically we can think of two different families of aspects which unify many well-known multivariate methods: Correlational aspects based on sums of correlations, eigenvalues and determinants which unify multiple regression, path analysis, correspondence analysis, nonlinear PCA, etc. Non-correlational aspects which linearize bivariate regressions and can be used for SEM preprocessing with categorical data. Additionally, other aspects can be established that do not correspond to classical techniques at all. By means of the R package aspect we provide a unified majorization-based implementation of this methodology. Using various data examples we will show the flexibility of this approach and how the optimally scaled results can be represented using graphical tools provided by the package.",WOS:000273371500001,JOURNAL OF STATISTICAL SOFTWARE,"['MODELS', 'VARIABLES', 'SETS']",A General Framework for Multivariate Analysis with Optimal Scaling: The R Package aspect,2010
996,"Object orientation provides a flexible framework for the implementation of the convolution of arbitrary distributions of real-valued random variables. We discuss an algorithm which is based on the fast Fourier transform. It directly applies to lattice-supported distributions. In the case of continuous distributions an additional discretization to a linear lattice is necessary and the resulting lattice-supported distributions are suitably smoothed after convolution.
We compare our algorithm to other approaches aiming at a similar generality as to accuracy and speed. In situations where the exact results are known, several checks confirm a high accuracy of the proposed algorithm which is also illustrated for approximations of non-central chi(2) distributions.
By means of object orientation this default algorithm is overloaded by more specific algorithms where possible, in particular where explicit convolution formulae are available. Our focus is on R package distr which implements this approach, overloading operator + for convolution; based on this convolution, we define a whole arithmetics of mathematical operations acting on distribution objects, comprising operators +, -, *, /, and <^>.",WOS:000341793000001,JOURNAL OF STATISTICAL SOFTWARE,,General Purpose Convolution Algorithm in S4 Classes by Means of FFT,2014
997,"We consider estimation in a particular semiparametric regression model for the mean of a counting process with ""panel count"" data. The basic model assumption is that the conditional mean function of the counting process is of the form E{N(t)vertical bar Z} = exp(beta(T)(0)Z)Lambda(0)(t) where Z is a vector of covariates and Lambda(0) is the baseline mean function. The ""panel count"" observation scheme involves observation of the counting process N for an individual at a random number K of random time points; both the number and the locations of these time points may differ across individuals.
We study semiparametric maximum pseudo-likelihood and maximum likelihood estimators of the unknown parameters (beta(0), Lambda(0)) derived on the basis of a nonhomogeneous Poisson process assumption. The pseudo-likelihood estimator is fairly easy to compute, while the maximum likelihood estimator poses more challenges from the computational perspective. We study asymptotic properties of both estimators assuming that the proportional mean model holds, but dropping the Poisson process assumption used to derive the estimators. In particular we establish asymptotic normality for the estimators of the regression parameter beta(0) under appropriate hypotheses. The results show that our estimation procedures are robust in the sense that the estimators converge to the truth regardless of the underlying counting process.",WOS:000251096100011,ANNALS OF STATISTICS,"['RECURRENT EVENTS', 'REGRESSION-ANALYSIS', 'TESTS', 'MODEL']",Two likelihood-based semiparametric estimation methods for panel count data with covariates,2007
998,"The data augmentation (DA) algorithm is a widely used Markov chain Monte Carlo algorithm that is easy to implement but often suffers from slow convergence. The sandwich algorithm is an alternative that can converge much faster while requiring roughly the same computational effort per iteration. Theoretically, the sandwich algorithm always converges at least as fast as the corresponding DA algorithm in the sense that parallel to K*parallel to <= parallel to K parallel to, where K and K* are the Markov operators associated with the DA and sandwich algorithms, respectively, and parallel to . parallel to denotes operator norm. In this paper, a substantial refinement of this operator norm inequality is developed. In particular, under regularity conditions implying that K is a trace-class operator, it is shown that K* is also a positive, trace-class operator, and that the spectrum of K* dominates that of K in the sense that the ordered elements of the former are all less than or equal to the corresponding elements of the latter. Furthermore, if the sandwich algorithm is constructed using a group action, as described by Liu and Wu [J. Amer Statist. Assoc. 94 (1999) 1264-1274] and Hobert and Marchev [Ann. Statist. 36 (2008) 532-554], then there is strict inequality between at least one pair of eigenvalues. These results are applied to a new DA algorithm for Bayesian quantile regression introduced by Kozumi and Kobayashi [J. Stat. Comput. Simul. 81 (2011) 1565-1578].",WOS:000299186500015,ANNALS OF STATISTICS,"['INTERWEAVING STRATEGY ASIS', 'BOOSTING MCMC EFFICIENCY', 'GIBBS SAMPLER', 'MARGINAL AUGMENTATION', 'COVARIANCE STRUCTURE', 'MONTE-CARLO', 'SCHEMES']",A SPECTRAL ANALYTIC COMPARISON OF TRACE-CLASS DATA AUGMENTATION ALGORITHMS AND THEIR SANDWICH VARIANTS,2011
999,,WOS:000253077800005,ANNALS OF STATISTICS,,Discussion: The Dantzig selector: Statistical estimation when p is much larger than n,2007
1000,"Panel Data Toolbox is a new package for MATLAB that includes functions to estimate the main econometric methods of balanced and unbalanced panel data analysis. The package includes code for the standard fixed, between and random effects estimation methods, as well as for the existing instrumental panels and a wide array of spatial panels. A full set of relevant tests is also included. This paper describes the methodology and implementation of the functions and illustrates their use with well-known examples. We perform numerical checks against other popular commercial and free software to show the validity of the results.",WOS:000392706600001,JOURNAL OF STATISTICAL SOFTWARE,"['SPATIAL AUTOREGRESSIVE MODEL', 'ROBUST STANDARD ERRORS', 'SIMULTANEOUS-EQUATIONS', 'MATRIX ESTIMATOR', 'LINEAR-MODELS', 'CROSS-SECTION', 'TIME-SERIES', 'COMPONENTS', 'ECONOMETRICS', 'HETEROSKEDASTICITY']",A Panel Data Toolbox for MATLAB,2017
1001,"BARS (DiMatteo, Genovese, and Kass 2001) uses the powerful reversible-jump MCMC engine to perform spline-based generalized nonparametric regression. It has been shown to work well in terms of having small mean-squared error in many examples (smaller than known competitors), as well as producing visually-appealing fits that are smooth (filtering out high-frequency noise) while adapting to sudden changes (retaining high-frequency signal). However, BARS is computationally intensive. The original implementation in S was too slow to be practical in certain situations, and was found to handle some data sets incorrectly. We have implemented BARS in C for the normal and Poisson cases, the latter being important in neurophysiological and other point-process applications. The C implementation includes all needed subroutines for fitting Poisson regression, manipulating B-splines (using code created by Bates and Venables), and finding starting values for Poisson regression (using code for density estimation created by Kooperberg). The code utilizes only freely-available external libraries (LAPACK and BLAS) and is otherwise self-contained. We have also provided wrappers so that BARS can be used easily within S or R.",WOS:000257322300001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR ALGEBRA SUBPROGRAMS', 'SCHWARZ CRITERION', 'MODELS', 'SET']",An implementation of Bayesian adaptive regression splines (BARS) in C with S and R wrappers,2008
1002,"Meta-analysis is a statistical methodology that combines or integrates the results of several independent clinical trials considered by the analyst to be 'combinable' (Huque 1988). However, completeness and user-friendliness are uncommon both in specialised meta-analysis software packages and in mainstream statistical packages that have to rely on user-written commands. We implemented the meta-analysis methodology in a Microsoft Excel add-in which is freely available and incorporates more meta-analysis models (including the iterative maximum likelihood and profile likelihood) than are usually available, while paying particular attention to the user-friendliness of the package.",WOS:000266311000001,JOURNAL OF STATISTICAL SOFTWARE,"['CLINICAL-TRIALS', 'HETEROGENEITY', 'SIMULATION', 'PROMISE']",MetaEasy: A Meta-Analysis Add-In for Microsoft Excel,2009
1003,"Minimax L-2 risks for high-dimensional nonparametric regression are derived under two sparsity assumptions: (1) the true regression surface is a sparse function that depends only on d = O(log n) important predictors among a list of p predictors, with log p = o(n); (2) the true regression surface depends on O(n) predictors but is an additive function where each additive component is sparse but may contain two or more interacting predictors and may have a smoothness level different from other components. For either modeling assumption, a practicable extension of the widely used Bayesian Gaussian process regression method is shown to adaptively attain the optimal minimax rate (up to log n terms) asymptotically as both n, p --> infinity with log p = o(n).",WOS:000352757100007,ANNALS OF STATISTICS,"['SPARSE ADDITIVE-MODELS', 'POSTERIOR DISTRIBUTIONS', 'LINEAR-REGRESSION', 'DANTZIG SELECTOR', 'GAUSSIAN PROCESS', 'RATES', 'LASSO', 'CONVERGENCE', 'RECOVERY', 'APPROXIMATION']",MINIMAX-OPTIMAL NONPARAMETRIC REGRESSION IN HIGH DIMENSIONS,2015
1004,"We consider here the problem of computing the mean vector and covariance matrix for a conditional normal distribution, considering especially a sequence of problems where the conditioning variables are changing. The sweep operator provides one simple general approach that is easy to implement and update. A second, more goal-oriented general method avoids explicit computation of the vector and matrix, while enabling easy evaluation of the conditional density for likelihood computation or easy generation from the conditional distribution. The covariance structure that arises from the special case of an ARMA( p, q) time series can be exploited for substantial improvements in computational efficiency.",WOS:000240206200001,JOURNAL OF STATISTICAL SOFTWARE,,Some algorithms for the conditional mean vector and covariance matrix,2006
1005,"This paper addresses the estimation of the nonparametric conditional moment restricted model that involves an infinite-dimensional parameter g(0). We estimate it in a quasi-Bayesian way, based on the limited information likelihood, and investigate the impact of three types of priors on the posterior consistency: (i) truncated prior (priors supported on a bounded set), (ii) thin-tail prior (a prior that has very thin tail outside a growing bounded set) and (iii) normal prior with nonshrinking variance. In addition, g0 is allowed to be only partially identified in the frequentist sense, and the parameter space does not need to be compact. The posterior is regularized using a slowly growing sieve dimension, and it is shown that the posterior converges to any small neighborhood of the identified region. We then apply our results to the nonparametric instrumental regression model. Finally, the posterior consistency using a random sieve dimension parameter is studied.",WOS:000300383200008,ANNALS OF STATISTICS,"['INSTRUMENTAL VARIABLES ESTIMATION', 'SINGLE-INDEX MODELS', 'CONFIDENCE-REGIONS', 'CONVERGENCE-RATES', 'BAYESIAN-ANALYSIS', 'REGRESSION', 'DISTRIBUTIONS', 'LIKELIHOOD', 'INFERENCE', 'IDENTIFICATION']",POSTERIOR CONSISTENCY OF NONPARAMETRIC CONDITIONAL MOMENT RESTRICTED MODELS,2011
1006,"We investigate the problem of deriving posterior concentration rates under different loss functions in nonparametric Bayes. We first provide a lower bound on posterior coverages of shrinking neighbourhoods that relates the metric or loss under which the shrinking neighbourhood is considered, and an intrinsic pre-metric linked to frequentist separation rates. In the Gaussian white noise model, we construct feasible priors based on a spike and slab procedure reminiscent of wavelet thresholding that achieve adaptive rates of contraction under L-2 or L-infinity metrics when the underlying parameter belongs to a collection of Holder balls and that moreover achieve our lower bound. We analyse the consequences in terms of asymptotic behaviour of posterior credible balls as well as frequentist minimax adaptive estimation. Our results are appended with an upper bound for the contraction rate under an arbitrary loss in a generic regular experiment. The upper bound is attained for certain sieve priors and enables to extend our results to density estimation.",WOS:000362697700014,ANNALS OF STATISTICS,"['NONPARAMETRIC CONFIDENCE-INTERVALS', 'BAYESIAN DENSITY-ESTIMATION', 'LINEAR FUNCTIONALS', 'CONVERGENCE-RATES', 'SPECTRAL DENSITY', 'GAUSSIAN PROCESS', 'DISTRIBUTIONS', 'MIXTURES', 'ADAPTATION', 'INFERENCE']",ON ADAPTIVE POSTERIOR CONCENTRATION RATES,2015
1007,"Orthogonal array based space-filling designs (Owen [Statist. Sinica 2(1992a) 439-452]; Tang [J. Amer. Statist. Assoc. 88 (1993) 1392-1397]) have become popular in computer experiments, numerical integration, stochastic optimization and uncertainty quantification. As improvements of ordinary Latin hypercube designs, these designs achieve stratification in multi-dimensions. If the underlying orthogonal array has strength t, such designs achieve uniformity up to t dimensions. Existing central limit theorems are limited to these designs with only two-dimensional stratification based on strength two orthogonal arrays. We develop a new central limit theorem for these designs that possess stratification in arbitrary multi-dimensions associated with orthogonal arrays of general strength. This result is useful for building confidence statements for such designs in various statistical applications.",WOS:000344632400002,ANNALS OF STATISTICS,"['COMPUTER EXPERIMENTS', 'SAMPLING DESIGNS', 'INTEGRALS', 'VARIANCE', 'CODE']",A CENTRAL LIMIT THEOREM FOR GENERAL ORTHOGONAL ARRAY BASED SPACE-FILLING DESIGNS,2014
1008,"In this work, a novel package called nmfgpu4R is presented, which offers the computation of Non-negative Matrix Factorization (NMF) on Compute Unified Device Architecture (CUDA) platforms within the R environment. Benchmarks show a remarkable speed-up in terms of time per iteration by utilizing the parallelization capabilities of modern graphics cards. Therefore the application of NMF gets more attractive for real-world sized problems because the time to compute a factorization is reduced by an order of magnitude.",WOS:000395669800025,R JOURNAL,"['RECOGNITION', 'ALGORITHMS']",nmfgpu4R: GPU-Accelerated Computation of the Non-Negative Matrix Factorization (NMF) Using CUDA Capable Hardware,2016
1009,The orderbook package provides facilities for exploring and visualizing the data associated with an order book: the electronic collection of the outstanding limit orders for a financial instrument. This article provides an overview of the orderbook package and examples of its use.,WOS:000208590100011,R JOURNAL,,Analyzing an Electronic Limit Order Book,2011
1010,"We consider a nonparametric additive model of a conditional mean function in which the number of variables and additive components may be larger than the sample size but the number of nonzero additive components is ""small"" relative to the sample size. The statistical problem is to determine which additive components are nonzero. The additive components are approximated by truncated series expansions with B-spline bases. With this approximation, the problem of component selection becomes that of selecting the groups of coefficients in the expansion. We apply the adaptive group Lasso to select nonzero components, using the group Lasso to obtain an initial estimator and reduce the dimension of the problem. We give conditions under which the group Lasso selects a model whose number of components is comparable with the underlying model, and the adaptive group Lasso selects the nonzero components correctly with probability approaching one as the sample size increases and achieves the optimal rate of convergence. The results of Monte Carlo experiments show that the adaptive group Lasso procedure works well with samples of moderate size. A data example is used to illustrate the application of the proposed method.",WOS:000280359400012,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'DIMENSIONAL REGRESSION-MODELS', 'COMPONENT SELECTION', 'ORACLE PROPERTIES', 'GENE-EXPRESSION', 'ADAPTIVE LASSO']",VARIABLE SELECTION IN NONPARAMETRIC ADDITIVE MODELS,2010
1011,"We carry out ANOVA comparisons of multiple treatments for longitudinal studies with missing values. The treatment effects are modeled semiparametrically via a partially linear regression which is flexible in quantifying the time effects of treatments. The empirical likelihood is employed to formulate model-robust nonparametric ANOVA tests for treatment effects with respect to covariates, the nonparametric time-effect functions and interactions between covariates and time. The proposed tests can be readily modified for a variety of data and model combinations, that encompasses parametric, semiparametric and nonparametric regression models; cross-sectional and longitudinal data, and with or without missing values.",WOS:000290231500009,ANNALS OF STATISTICS,"['SEMIPARAMETRIC REGRESSION-ANALYSIS', 'VARYING-COEFFICIENT MODEL', 'EMPIRICAL LIKELIHOOD', 'LINEAR-MODELS', 'CONFIDENCE-REGIONS', 'BOOTSTRAP', 'TESTS', 'CURVES']",ANOVA FOR LONGITUDINAL DATA WITH MISSING VALUES,2010
1012,"Modern scientific technology has provided a new class of large-scale simultaneous inference problems, with thousands of hypothesis tests to consider at the same time. Microarrays epitomize this type of technology, but similar situations arise in proteomics, spectroscopy, imaging, and social science surveys. This paper uses false discovery rate methods to carry out both size and power calculations on large-scale problems. A simple empirical Bayes approach allows the false discovery rate (fdr) analysis to proceed with a minimum of frequentist or Bayesian modeling assumptions. Closed-form accuracy formulas are derived for estimated false discovery rates, and used to compare different methodologies: local or tail-area fdr's, theoretical, permutation, or empirical null hypothesis estimates. Two microarray data sets as well as simulations are used to evaluate the methodology, the power diagnostics showing why nonnull cases might easily fail to appear on a list of ""significant"" discoveries.",WOS:000249568000001,ANNALS OF STATISTICS,"['DIFFERENTIAL GENE-EXPRESSION', 'EMPIRICAL BAYES METHODS', 'MICROARRAY DATA', 'MIXTURE MODEL', 'HYPOTHESIS', 'INFERENCE']","Size, power and false discovery rates",2007
1013,"Sequential Monte Carlo (SMC) is a methodology for sampling approximately from a sequence of probability distributions of increasing dimension and estimating their normalizing constants. We propose here an alternative methodology named Sequentially Interacting Markov Chain Monte Carlo (SIMCMC). SIMCMC methods work by generating interacting non-Markovian sequences which behave asymptotically like independent Metropolis-Hastings (MH) Markov chains with the desired limiting distributions. Contrary to SMC, SIMCMC allows us to iteratively improve our estimates in an MCMC-like fashion. We establish convergence results under realistic verifiable assumptions and demonstrate its performance on several examples arising in Bayesian time series analysis.",WOS:000290231500002,ANNALS OF STATISTICS,"['PARTICLE FILTERS', 'ALGORITHMS', 'SIMULATION', 'INFERENCE', 'CONVERGENCE', 'ERGODICITY', 'MODELS']",SEQUENTIALLY INTERACTING MARKOV CHAIN MONTE CARLO METHODS,2010
1014,"We study the merging and the testing of opinions in the context of a prediction model. In the absence of incentive problems, opinions can be tested and rejected, regardless of whether or not data produces consensus among Bayesian agents. In contrast, in the presence of incentive problems, opinions can only be tested and rejected when data produces consensus among Bayesian agents. These results show a strong connection between the testing and the merging of opinions. They also relate the literature on Bayesian learning and the literature on testing strategic experts.",WOS:000338477800007,ANNALS OF STATISTICS,"['REPEATED GAMES', 'NASH EQUILIBRIUM', 'CALIBRATION', 'PROBABILITIES', 'INSPECTIONS', 'CONSISTENCY', 'INFORMATION', 'HYPOTHESIS', 'PREDICTION', 'EXPERTS']",MERGING AND TESTING OPINIONS,2014
1015,"The R package Synth implements synthetic control methods for comparative case studies designed to estimate the causal effects of policy interventions and other events of interest (Abadie and Gardeazabal 2003; Abadie, Diamond, and Hainmueller 2010). These techniques are particularly well-suited to investigate events occurring at an aggregate level (i.e., countries, cities, regions, etc.) and affecting a relatively small number of units. Benefits and features of the Synth package are illustrated using data from Abadie and Gardeazabal (2003), which examined the economic impact of the terrorist conflict in the Basque Country.",WOS:000292098300001,JOURNAL OF STATISTICAL SOFTWARE,,Synth: An R Package for Synthetic Control Methods in Comparative Case Studies,2011
1016,"The support vector machine (SVM) algorithm is well known to the computer learning community for its very good practical results. The goal of the present paper is to study this algorithm from a statistical perspective, using tools of concentration theory and empirical processes.
Our main result builds on the observation made by other authors that the SVM can be viewed as a statistical regularization procedure. From this point of view, it can also be interpreted as a model selection principle using a penalized criterion. It is then possible to adapt general methods related to model selection in this framework to Study two important points: (1) what is the minimum penalty and how does it compare to the penalty actually used in the SVM algorithm; (2) is it possible to obtain ""oracle inequalities"" in that setting, for the specific loss function used in the SVM algorithm? We show that the answer to the latter question is positive and provides relevant insight to the former. Our result shows that it is possible to obtain fast rates of convergence for SVMs.",WOS:000254502700001,ANNALS OF STATISTICS,"['LOCAL RADEMACHER COMPLEXITIES', 'REGULARIZATION NETWORKS', 'GENERALIZATION ERROR', 'UNIFORM-CONVERGENCE', 'RISK MINIMIZATION', 'CLASSIFICATION', 'CLASSIFIERS', 'MARGIN', 'INEQUALITIES', 'ADAPTATION']",Statistical performance of support vector machines,2008
1017,"One of the most powerful features of R is its infrastructure for contributed code. The built-in package manager and complementary repositories provide a great system for development and exchange of code, and have played an important role in the growth of the platform towards the de-facto standard in statistical computing that it is today. However, the number of packages on CRAN and other repositories has increased beyond what might have been foreseen, and is revealing some limitations of the current design. One such problem is the general lack of dependency versioning in the infrastructure. This paper explores this problem in greater detail, and suggests approaches taken by other open source communities that might work for R as well. Three use cases are defined that exemplify the issue, and illustrate how improving this aspect of package management could increase reliability while supporting further growth of the R community.",WOS:000321944400020,R JOURNAL,,Possible Directions for Improving Dependency Versioning in R,2013
1018,"We derive rates of contraction of posterior distributions on nonparametric or semiparametric models based on Gaussian processes. The rate of contraction is shown to depend on the position of the true parameter relative to the reproducing kernel Hilbert space of the Gaussian process and the small ball probabilities of the Gaussian process. We determine these quantities for a range of examples of Gaussian priors and in several statistical settings. For instance, we consider the rate of contraction of the posterior distribution based on sampling from a smooth density model when the prior models the log density as a (fractionally integrated) Brownian motion. We also consider regression with Gaussian errors and smooth classification under a logistic or probit link function combined with various priors.",WOS:000256504400016,ANNALS OF STATISTICS,"['FRACTIONAL BROWNIAN-MOTION', 'CONVERGENCE-RATES', 'STOCHASTIC-PROCESSES', 'SERIES EXPANSION', 'REGRESSION', 'REPRESENTATION', 'DENSITIES']",Rates of contraction of posterior distributions based on Gaussian process priors,2008
1019,"A topological multiple testing scheme for one-dimensional domains is proposed where, rather than testing every spatial or temporal location for the presence of a signal, tests are performed only at the local maxima of the smoothed observed sequence. Assuming unimodal true peaks with finite support and Gaussian stationary ergodic noise, it is shown that the algorithm with Bonferroni or Benjamini-Hochberg correction provides asymptotic strong control of the family wise error rate and false discovery rate, and is power consistent, as the search space and the signal strength get large, where the search space may grow exponentially faster than the signal strength. Simulations show that error levels are maintained for nonasymptotic conditions, and that power is maximized when the smoothing kernel is close in shape and bandwidth to the signal peaks, akin to the matched filter theorem in signal processing. The methods are illustrated in an analysis of electrical recordings of neuronal cell activity.",WOS:000311639700006,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'RANDOM-FIELD THEORY', 'BIOMARKER DISCOVERY', 'MASS-SPECTROMETRY', 'PROTEOMIC DATA', 'DECONVOLUTION', 'INFERENCE', 'SIGNALS', 'ACTIVATION', 'SPIKES']",MULTIPLE TESTING OF LOCAL MAXIMA FOR DETECTION OF PEAKS IN 1D,2011
1020,"splm is an R package for the estimation and testing of various spatial panel data specifications. We consider the implementation of both maximum likelihood and generalized moments estimators in the context of fixed as well as random effects spatial panel data models. This paper is a general description of splm and all functionalities are illustrated using a well-known example taken from Munnell (1990) with productivity data on 48 US states observed over 17 years. We perform comparisons with other available software; and, when this is not possible, Monte Carlo results support our original implementation.",WOS:000303803400001,JOURNAL OF STATISTICAL SOFTWARE,"['ERROR CORRELATION', 'ECONOMETRICS', 'SPECIFICATION', 'AUTOCORRELATION', 'SERIAL']",splm: Spatial Panel Data Models in R,2012
1021,"This article introduces a method for estimating the smoothness of a stationary, isotropic Gaussian random field from irregularly spaced data. This involves novel constructions of higher-order quadratic variations and the establishment of the corresponding fixed-domain asymptotic theory. In particular, we consider:
(i) higher-order quadratic variations using nonequispaced line transect data,
(ii) second-order quadratic variations from a sample of Gaussian random field observations taken along a smooth curve in R-2,
(iii) second-order quadratic variations based on deformed lattice data on R-2.
Smoothness estimators are proposed that are strongly consistent under mild assumptions. Simulations indicate that these estimators perform well for moderate sample sizes.",WOS:000363437900016,ANNALS OF STATISTICS,"['FRACTAL DIMENSION', 'PLANE']",ESTIMATING THE SMOOTHNESS OF A GAUSSIAN RANDOM FIELD FROM IRREGULARLY SPACED DATA VIA HIGHER-ORDER QUADRATIC VARIATIONS,2015
1022,"Being among the most popular and efficient classification and regression methods currently available, implementations of support vector machines exist in almost every popular programming language. Currently four R packages contain SVM related software. The purpose of this paper is to present and compare these implementations.",WOS:000236800500001,JOURNAL OF STATISTICAL SOFTWARE,,Support Vector Machines in R,2006
1023,"Genetic algorithms (GAs) are a popular technology to search for an optimum in a large search space. Using new concepts of forbidden array and weighted mutation, Mandal, Wu, and Johnson (2006) used elements of GAs to introduce a new global optimization technique called sequential elimination of level combinations (SELC), that efficiently finds optimums. A SAS macro, and MATLAB and R functions are developed to implement the SELC algorithm.",WOS:000254619900001,JOURNAL OF STATISTICAL SOFTWARE,"['GENETIC ALGORITHMS', 'OPTIMAL DESIGNS']",Software for Implementing the Sequential Elimination of Level Combinations Algorithm,2008
1024,"A joint optimization plot, shortly JOP, graphically displays the result of a loss function based robust parameter design for multiple responses. Different importance of reaching a target value can be assigned to the individual responses by weights. The JOP method simultaneously runs through a whole range of possible weights. For each weight matrix a parameter setting is derived which minimizes the estimated expected loss. The joint optimization plot displays these settings together with corresponding expected values and standard deviations of the response variable. The R package JOP provides all tools necessary to apply the JOP approach to a given data set. It also returns parameter settings for a desirable compromise of achieved expected responses chosen from the plot.",WOS:000324371700001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED LINEAR-MODELS', 'ROBUST DESIGN', 'QUALITY CHARACTERISTICS', 'DISPERSION']",Simultaneous Optimization of Multiple Responses with the R Package JOP,2013
1025,"We consider L(2)Boosting, a special case of Friedman's generic boosting algorithm applied to linear regression under L-2-loss. We study L(2)Boosting for an arbitrary regularization parameter and derive an exact closed form expression for the number of steps taken along a fixed coordinate direction. This relationship is used to describe L(2)Boosting's solution path, to describe new tools for studying its path, and to characterize some of the algorithm's unique properties, including active set cycling, a property where the algorithm spends lengthy periods of time cycling between the same coordinates when the regularization parameter is arbitrarily small. Our fixed descent analysis also reveals a repressible condition that limits the effectiveness of L(2)Boosting in correlated problems by preventing desirable variables from entering the solution path. As a simple remedy, a data augmentation method similar to that used for the elastic net is used to introduce L-2-penalization and is shown, in combination with decorrelation, to reverse the repressible condition and circumvents L(2)Boosting's deficiencies in correlated problems. In itself, this presents a new explanation for why the elastic net is successful in correlated problems and why methods like LAR and lasso can perform poorly in such settings.",WOS:000307608000017,ANNALS OF STATISTICS,"['REGRESSION', 'SELECTION', 'LASSO']",CHARACTERIZING L(2)BOOSTING,2012
1026,"Pure-jump processes have been increasingly popular in modeling high-frequency financial data, partially due to their versatility and flexibility. In the meantime, several statistical tests have been proposed in the literature to check the validity of using pure-jump models. However, these tests suffer from several drawbacks, such as requiring rather stringent conditions and having slow rates of convergence. In this paper, we propose a different test to check whether the underlying process of high-frequency data can be modeled by a pure-jump process. The new test is based on the realized characteristic function, and enjoys a much faster convergence rate of order O(n(1/2)) (where n is the sample size) versus the usual o(n(1/4)) available for existing tests; it is applicable much more generally than previous tests; for example, it is robust to jumps of infinite variation and flexible modeling of the diffusion component. Simulation studies justify our findings and the test is also applied to some real high-frequency financial data.",WOS:000352757100014,ANNALS OF STATISTICS,"['STOCHASTIC VOLATILITY', 'LEVY PROCESSES', 'MICROSTRUCTURE NOISE', 'EFFICIENT ESTIMATION', 'CURRENCY OPTIONS', 'LIMIT-THEOREMS', 'MODELS', 'SEMIMARTINGALES', 'TRANSFORM', 'VALUATION']",TESTING FOR PURE-JUMP PROCESSES FOR HIGH-FREQUENCY DATA,2015
1027,"We establish minimax lower bounds and maximum likelihood convergence rates of parameter estimation for mean-covariance multivariate Gaussian mixtures, shape-rate Gamma mixtures and some variants of finite mixture models, including the setting where the number of mixing components is bounded but unknown. These models belong to what we call ""weakly identifiable"" classes, which exhibit specific interactions among mixing parameters driven by the algebraic structures of the class of kernel densities and their partial derivatives. Accordingly, both the minimax bounds and the maximum likelihood parameter estimation rates in these models, obtained under some compactness conditions on the parameter space, are shown to be typically much slower than the usual n(-1/2) or n(-1/4) rates of convergence.",WOS:000389620800017,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD', 'MODELS', 'DISTRIBUTIONS', 'INFERENCE', 'DECONVOLUTION', 'COMPONENTS', 'DENSITIES', 'TESTS']",CONVERGENCE RATES OF PARAMETER ESTIMATION FOR SOME WEAKLY IDENTIFIABLE FINITE MIXTURES,2016
1028,"We place Ourselves in the setting of high-dimensional statistical inference where the number of variables p in a dataset of interest is of the same order of magnitude as the number of observations n.
We consider the spectrum of certain kernel random matrices, in particular n x n matrices whose (i, j)th entry is f(X-i' X-j/p) or f(vertical bar vertical bar X-i - X-j vertical bar vertical bar(2)/p) where p is the dimension of the data, and X-i are independent data vectors. Here f is assumed to be a locally smooth function.
The study is motivated by questions arising in statistics and computer science where these matrices are used to perform, among other things, nonlinear versions of principal component analysis. Surprisingly, we show that in high dimensions, and for the models we analyze, the problem becomes essentially linear-which is at odds with heuristics sometimes used to justify the usage of these methods. The analysis also highlights certain Peculiarities of models widely studied in random matrix theory and raises some questions about their relevance as tools to model high-dimensional data encountered in practice.",WOS:000273800100001,ANNALS OF STATISTICS,"['SAMPLE COVARIANCE MATRICES', 'DIMENSIONAL RANDOM MATRICES', 'LARGEST EIGENVALUE', 'SPACING DISTRIBUTIONS', 'NO EIGENVALUES', 'LIMIT', 'ASYMPTOTICS', 'ENSEMBLES', 'SUPPORT']",THE SPECTRUM OF KERNEL RANDOM MATRICES,2010
1029,"In a 2(m) factorial design, search designs are consider for searching and estimating some non-zero interactions based on search linear models. There are some criteria for comparing search designs. Computing these criteria is a heavy task. In this paper, we provide the SD package for the numerical computing environment MATLAB to compute these criteria and also to check Srivastava's condition for a given design. These package is illustrated by an example.",WOS:000331695000001,JOURNAL OF STATISTICAL SOFTWARE,['PROBABILITIES'],A MATLAB Package for Computing Two-Level Search Design Performance Criteria,2014
1030,"Nucleic acid Melting Curve Analysis is a powerful method to investigate the interaction of double stranded nucleic acids. Many researchers rely on closed source software which is not ubiquitously available, and gives only little control over the computation and data presentation. R in contrast, is open source, highly adaptable and provides numerous utilities for data import, sophisticated statistical analysis and presentation in publication quality. This article covers methods, implemented in the MBmca package, for DNA Melting Curve Analysis on microbead surfaces. Particularly, the use of the second derivative melting peaks is suggested as an additional parameter to characterize the melting behavior of DNA duplexes. Examples of microbead surface Melting Curve Analysis on fragments of human genes are presented.",WOS:000330193300005,R JOURNAL,"['POLYMERASE-CHAIN-REACTION', 'PRIMERS']",Surface Melting Curve Analysis with R,2013
1031,"IsoGene is an R package for the analysis of dose-response microarray experiments to identify gene or subsets of genes with a monotone relationship between the gene expression and the doses. Several testing procedures (i.e., the likelihood ratio test, Williams, Marcus, the M, and Modified M), that take into account the order restriction of the means with respect to the increasing doses are implemented in the package. The inference is based on resampling methods, both permutations and the Significance Analysis of Microarrays (SAM).",WOS:000208589900002,R JOURNAL,,IsoGene: An R Package for Analyzing Dose-response Studies in Microarray Experiments,2010
1032,"This paper describes the R package mhsmm which implements estimation and prediction methods for hidden Markov and semi-Markov models for multiple observation sequences. Such techniques are of interest when observed data is thought to be dependent on some unobserved (or hidden) state. Hidden Markov models only allow a geometrically distributed sojourn time in a given state, while hidden semi-Markov models extend this by allowing an arbitrary sojourn distribution. We demonstrate the software with simulation examples and an application involving the modelling of the ovarian cycle of dairy cows.",WOS:000288203800001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD', 'ALGORITHM', 'CHAINS']",Hidden Semi Markov Models for Multiple Observation Sequences: The mhsmm Package for R,2011
1033,,WOS:000385276100001,R JOURNAL,,"A peer-reviewed, open-access publication of the R Foundation for Statistical Computing",2016
1034,"Estimating the unknown number of classes in a population has numerous important applications. In a Poisson mixture model, the problem is reduced to estimating the odds that a class is undetected in a sample. The discontinuity of the odds prevents the existence of locally unbiased and informative estimators and restricts confidence intervals to be one-sided. Confidence intervals for the number of classes are also necessarily one-sided. A sequence of lower bounds to the odds is developed and used to define pseudo maximum likelihood estimators for the number of classes.",WOS:000248987600019,ANNALS OF STATISTICS,"['NONEXISTENCE', 'FUNCTIONALS', 'SAMPLE']",Estimating the number of classes,2007
1035,"We discuss the facilities in base R for spell checking via Aspell, Hunspell or Ispell, which are useful in particular for conveniently checking the spelling of natural language texts in package Rd files and vignettes. Spell checking performance is illustrated using the Rd files in package stats. This example clearly indicates the need for a domain-specific statistical dictionary. We analyze the results of spell checking all Rd files in all CRAN packages and show how these can be employed for building such a dictionary.",WOS:000208590200005,R JOURNAL,,Watch Your Spelling!,2011
1036,"The definition of a distance measure between time series is crucial for many time series data mining tasks, such as clustering and classification. For this reason, a vast portfolio of time series distance measures has been published in the past few years. In this paper, the TSdist package is presented, a complete tool which provides a unified framework to calculate the largest variety of time series dissimilarity measures available in R at the moment, to the best of our knowledge. The package implements some popular distance measures which were not previously available in R, and moreover, it also provides wrappers for measures already included in other R packages. Additionally, the application of these distance measures to clustering and classification tasks is also supported in TSdist, directly enabling the evaluation and comparison of their performance within these two frameworks.",WOS:000395669800030,R JOURNAL,,Distance Measures for Time Series in R: The TSdist Package,2016
1037,"This paper reviews the use of STAMP (Structural Time Series Analyser, Modeler and Predictor) for modeling time series data using state-space methods with unobserved components. STAMP is a commercial, GUI-based program that runs on Windows, Linux and Macintosh computers as part of the larger OxMetrics System. STAMP can estimate a wide-variety of both univariate and multivariate state-space models, provides a wide array of diagnostics, and has a batch mode capability. The use of STAMP is illustrated for the Nile river data which is analyzed throughout this issue, as well as by modeling a variety of oceanographic and climate related data sets. The analyses of the oceanographic and climate data illustrate the breadth of models available in STAMP, and that state-space methods produce results that provide new insights into important scientific problems.",WOS:000290526400001,JOURNAL OF STATISTICAL SOFTWARE,"['PACIFIC', 'NOISE']",The STAMP Software for State Space Models,2011
1038,"A family of variable stage size multistage tests of simple hypotheses is described, based on efficient multistage sampling procedures. Using a loss function that is a linear combination of sampling costs and error probabilities, these tests are shown to minimize the integrated risk to second order as the costs per stage and per observation approach zero. A numerical study shows significant improvement over group sequential tests in a binomial testing problem.",WOS:000251096100010,ANNALS OF STATISTICS,"['GROUP SEQUENTIAL-TESTS', 'SAMPLE-SIZE', 'CLINICAL-TRIALS', 'DESIGN']",Asymptotically optimal multistage tests of simple hypotheses,2007
1039,"The R package structSSI provides an accessible implementation of two recently developed simultaneous and selective inference techniques: the group Benjamini-Hochberg and hierarchical false discovery rate procedures. Unlike many multiple testing schemes, these methods specifically incorporate existing information about the grouped or hierarchical dependence between hypotheses under consideration while controlling the false discovery rate. Doing so increases statistical power and interpretability. Furthermore, these procedures provide novel approaches to the central problem of encoding complex dependency between hypotheses.
We briefly describe the group Benjamini-Hochberg and hierarchical false discovery rate procedures and then illustrate them using two examples, one a measure of ecological microbial abundances and the other a global temperature time series. For both procedures, we detail the steps associated with the analysis of these particular data sets, including establishing the dependence structures, performing the test, and interpreting the results. These steps are encapsulated by R functions, and we explain their applicability to general data sets.",WOS:000341807500001,JOURNAL OF STATISTICAL SOFTWARE,"['FALSE DISCOVERY RATE', 'RATES']",structSSI: Simultaneous and Selective Inference for Grouped or Hierarchically Structured Data,2014
1040,"This work presents the implementation in R of the alpha-shape of a finite set of points in the three-dimensional space R-3. This geometric structure generalizes the convex hull and allows to recover the shape of non-convex and even non-connected sets in 3D, given a random sample of points taken into it. Besides the computation of the alpha-shape, the R package alphashape3d provides users with tools to facilitate the three-dimensional graphical visualization of the estimated set as well as the computation of important characteristics such as the connected components or the volume, among others.",WOS:000332108600001,JOURNAL OF STATISTICAL SOFTWARE,"['CONVEX-SETS', 'MACROMOLECULES', 'COMPUTATION']",R Implementation of a Polyhedral Approximation to a 3D Set of Points Using the alpha-Shape,2014
1041,"Digital imaging has become omnipresent in the past years with a bulk of applications ranging from medical imaging to photography. When pushing the limits of resolution and sensitivity noise has ever been a major issue. However, commonly used non-adaptive filters can do noise reduction at the cost of a reduced effective spatial resolution only.
Here we present a new package adimpro for R, which implements the propagation-separation approach by (Polzehl and Spokoiny 2006) for smoothing digital images. This method naturally adapts to different structures of different size in the image and thus avoids oversmoothing edges and fine structures. We extend the method for imaging data with spatial correlation. Furthermore we show how the estimation of the dependence between variance and mean value can be included. We illustrate the use of the package through some examples.",WOS:000245822500001,JOURNAL OF STATISTICAL SOFTWARE,,Adaptive smoothing of digital images: the R package adimpro,2007
1042,"The R package equate contains functions for observed-score linking and equating under single-group, equivalent-groups, and nonequivalent-groups with anchor test(s) designs. This paper introduces these designs and provides an overview of observed-score equating with details about each of the supported methods. Examples demonstrate the basic functionality of the equate package.",WOS:000392514200001,JOURNAL OF STATISTICAL SOFTWARE,"['SMALL SAMPLES', 'DISTRIBUTIONS']",equate: An R Package for Observed-Score Linking and Equating,2016
1043,"We propose a general maximum likelihood empirical Bayes (GMLEB) method for the estimation of a mean vector based on observations with i.i.d. normal errors. We prove that under mild moment conditions on the unknown means, the average mean squared error (MSE) of the GMLEB is within an infinitesimal fraction of the minimum average MSE among all separable estimators which use a single deterministic estimating function on individual observations, provided that the risk is of greater order than (logn)(5)/n. We also prove that the GMLEB is uniformly approximately minimax in regular and weak l(P) balls when the order of the length-normalized norm of the unknown means is between (log n)(k1/)n(1)/((p boolean AND 2)) and n/(logn)(k2). Simulation experiments demonstrate that the GMLEB outperforms the James-Stein and several state-of-the-art threshold estimators in a wide range of settings without much down side.",WOS:000268113500002,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'WAVELET SHRINKAGE', 'CONVERGENCE', 'REGRESSION', 'DENSITIES', 'MIXTURES', 'RATES', 'RISK']",GENERAL MAXIMUM LIKELIHOOD EMPIRICAL BAYES ESTIMATION OF NORMAL MEANS,2009
1044,"In this article, we study a partially linear single-index model for longitudinal data under a general framework which includes both the sparse and dense longitudinal data cases. A semiparametric estimation method based on a combination of the local linear smoothing and generalized estimation equations (GEE) is introduced to estimate the two parameter vectors as well as the unknown link function. Under some mild conditions, we derive the asymptotic properties of the proposed parametric and nonparametric estimators in different scenarios, from which we find that the convergence rates and asymptotic variances of the proposed estimators for sparse longitudinal data would be substantially different from those for dense longitudinal data. We also discuss the estimation of the covariance (or weight) matrices involved in the semiparametric GEE method. Furthermore, we provide some numerical studies including Monte Carlo simulation and an empirical application to illustrate our methodology and theory.",WOS:000357441000015,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'AIR-POLLUTION', 'COVARIANCE MATRICES', 'HOSPITAL ADMISSIONS', 'VARIANCE FUNCTION', 'FUNCTIONAL DATA', 'INFERENCES', 'CHILDREN', 'SPARSE', 'HEALTH']",SEMIPARAMETRIC GEE ANALYSIS IN PARTIALLY LINEAR SINGLE-INDEX MODELS FOR LONGITUDINAL DATA,2015
1045,"We investigate function estimation in nonparametric regression models with random design and heteroscedastic correlated noise. Adaptive properties of warped wavelet nonlinear approximations are studied over a wide range of Besov scales, f is an element of B(pi,r)(S), and for a variety of L(P) error measures. We consider error distributions with Long-Range-Dependence parameter alpha, 0 < alpha <= 1; heteroscedasticity is modeled with a design dependent function a. We prescribe a tuning paradigm, under which warped wavelet estimation achieves partial or full adaptivity results with the rates that are shown to be the minimax rates of convergence. For p > 2, it is seen that there are three rate phases, namely the dense, sparse and long range dependence phase, depending on the relative values of s, p, pi and alpha. Furthermore, we show that long range dependence does not come into play for shape estimation f - integral f. The theory is illustrated with some numerical examples.",WOS:000271673500011,ANNALS OF STATISTICS,"['LONG-RANGE DEPENDENCE', 'DENSITY-ESTIMATION', 'TIME-SERIES', 'MEMORY ERRORS', 'SHRINKAGE']",WAVELET REGRESSION IN RANDOM DESIGN WITH HETEROSCEDASTIC DEPENDENT ERRORS,2009
1046,"The package can be used to analyze the performance of step-up and step-down procedures. It can be used to compare powers, calculate the ""false discovery rate"", to study the effects of reduced step procedures, and to calculate P[U <= k], where U is the number of rejected true hypotheses. It can be used to determine the maximum number of steps that can be made and still guarantee (with a given probability) that the number of false rejections will not exceed some specified number. The test statistics are assumed to have a multivariate-t distribution. Examples are included.",WOS:000312288900001,JOURNAL OF STATISTICAL SOFTWARE,,A Package to Study the Performance of Step-Up and Step-Down Test Procedures,2012
1047,"Collaborative recommendation is an information-filtering technique that attempts to present information items that are likely of interest to an Internet user. Traditionally, collaborative systems deal with situations with two types of variables, users and items. In its most common form, the problem is framed as trying to estimate ratings for items that have not yet been consumed by a user. Despite wide-ranging literature, little is known about the statistical properties of recommendation systems. In fact, no clear probabilistic model even exists which would allow us to precisely describe the mathematical forces driving collaborative filtering. To provide an initial contribution to this, we propose to set out a general sequential stochastic model for collaborative recommendation. We offer an in-depth analysis of the so-called cosine-type nearest neighbor collaborative method, which is one of the most widely used algorithms in collaborative filtering, and analyze its asymptotic performance as the number of users grows. We establish consistency of the procedure under mild assumptions on the model. Rates of convergence and examples are also provided.",WOS:000277471000010,ANNALS OF STATISTICS,['SYSTEMS'],STATISTICAL ANALYSIS OF k-NEAREST NEIGHBOR COLLABORATIVE RECOMMENDATION,2010
1048,The computer program DixonText. Critical Values is written in VB.NET to extend the quadrature approach to calculate the critical values with accuracy upto 6 significant digits for Dixon's ratios. Its use in creating the critical values tables in Excel is illustrated.,WOS:000334020100001,JOURNAL OF STATISTICAL SOFTWARE,"['GAUSSIAN QUADRATURES', 'POLYNOMIALS', 'OUTLIERS']",Dixon Test. CriticalValues: A Computer Code to Calculate Critical Values for the Dixon Statistical Data Treatment Approach,2014
1049,"We study kernel estimation of highest-density regions (HDR). Our main contributions are two-fold. First, we derive a uniform-in-bandwidth asymptotic approximation to a risk that is appropriate for HDR estimation. This approximation is then used to derive a bandwidth selection rule for HDR estimation possessing attractive asymptotic properties. We also present the results of numerical studies that illustrate the benefits of our theory and methodology.",WOS:000277471000017,ANNALS OF STATISTICS,"['LEVEL SETS', 'NONPARAMETRIC-ESTIMATION', 'RATES', 'CONTOUR', 'ERROR']",ASYMPTOTICS AND OPTIMAL BANDWIDTH SELECTION FOR HIGHEST DENSITY REGION ESTIMATION,2010
1050,"Quantile-based approaches to the spectral analysis of time series have recently attracted a lot of attention. Several methods for estimation have been proposed in the literature and their statistical properties were analyzed. Yet, so far, neither a systematic method for computation nor a comprehensive software implementation are available to date. This paper contains two main contributions. First, an extensible framework for quantile-based spectral analysis of time series is developed and documented using object-oriented models. A comprehensive, open source reference implementation of this framework is provided in the R package quantspec, which is available from the Comprehensive R Archive Network. The second contribution of the present paper is to provide a detailed tutorial, with worked examples, for this R package. A reader who is already familiar with quantile-based spectral analysis and whose primary interest is not the design of the quantspec package, but how to use it, can read the tutorial and worked examples ( Sections 3 and 4) independently.",WOS:000373922100001,JOURNAL OF STATISTICAL SOFTWARE,"['TIME-SERIES', 'PERIODOGRAM']",Quantile-Based Spectral Analysis in an Object-Oriented Framework and a Reference Implementation in R: The quantspec Package,2016
1051,"Many high-throughput biological data analyses require the calculation of large correlation matrices and/or clustering of a large number of objects. The standard R function for calculating Pearson correlation can handle calculations without missing values efficiently, but is inefficient when applied to data sets with a relatively small number of missing data. We present an implementation of Pearson correlation calculation that can lead to substantial speedup on data with relatively small number of missing entries. Further, we parallelize all calculations and thus achieve further speedup on systems where parallel processing is available. A robust correlation measure, the biweight midcorrelation, is implemented in a similar manner and provides comparable speed. The functions c or and bicor for fast Pearson and biweight midcorrelation, respectively, are part of the updated, freely available R package WGCNA.
The hierarchical clustering algorithm implemented in R function hclust is an order n(3) (n is the number of clustered objects) version of a publicly available clustering algorithm (Murtagh 2012). We present the package flashClust that implements the original algorithm which in practice achieves order approximately n(2), leading to substantial time savings when clustering large data sets.",WOS:000301231300001,JOURNAL OF STATISTICAL SOFTWARE,"['NETWORK ANALYSIS', 'PACKAGE']",Fast R Functions for Robust Correlations and Hierarchical Clustering,2012
1052,"The purpose of this paper is to describe the R package PTAk and how the spatio-temporal context can be taken into account in the analyses. Essentially PTAk() is a multiway multidimensional method to decompose a multi-entries data-array, seen mathematically as a tensor of any order. This PTAk-modes method proposes a way of generalizing SVD (singular value decomposition), as well as some other well known methods included in the R package, such as PARAFAC or CANDECOMP and the PCAn-modes or Tucker-n model. The example datasets cover different domains with various spatio-temporal characteristics and issues: (i) medical imaging in neuropsychology with a functional MRI (magnetic resonance imaging) study, (ii) pharmaceutical research with a pharmacodynamic study with EEG (electro-encephaloegraphic) data for a central nervous system (CNS) drug, and (iii) geographical information system (GIS) with a climatic dataset that characterizes arid and semi-arid variations. All the methods implemented in the R package PTAk also support non-identity metrics, as well as penalizations during the optimization process. As a result of these flexibilities, together with pre-processing facilities, PTAk constitutes a framework for devising extensions of multidimensional methods such as correspondence analysis, discriminant analysis, and multidimensional scaling, also enabling spatio-temporal constraints.",WOS:000281584200001,JOURNAL OF STATISTICAL SOFTWARE,"['SINGULAR-VALUE DECOMPOSITION', 'COMPONENT ANALYSIS', 'SYMMETRIC TENSORS', 'APPROXIMATION', 'RANK', 'ALGORITHMS']",Spatio-Temporal Multiway Decompositions Using Principal Tensor Analysis on k-Modes: The R Package PTAk,2010
1053,"In the sparse linear regression setting, we consider testing the significance of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path. We propose a simple test statistic based on lasso fitted values, called the covariance test statistic, and show that when the true model is linear, this statistic has an Exp(1) asymptotic distribution under the null hypothesis (the null being that all truly active variables are contained in the current lasso model). Our proof of this result for the special case of the first predictor to enter the model (i.e., testing for a single significant predictor variable against the global null) requires only weak assumptions on the predictor matrix X. On the other hand, our proof for a general step in the lasso path places further technical assumptions on X and the generative model, but still allows for the important high-dimensional case p > n, and does not necessarily require that the current lasso model achieves perfect recovery of the truly active variables.
Of course, for testing the significance of an additional variable between two nested linear models, one typically uses the chi-squared test, comparing the drop in residual sum of squares (RSS) to a chi(2)(1) distribution. But when this additional variable is not fixed, and has been chosen adaptively or greedily, this test is no longer appropriate: adaptivity makes the drop in RSS stochastically much larger than chi(2)(1) under the null hypothesis. Our analysis explicitly accounts for adaptivity, as it must, since the lasso builds an adaptive sequence of linear models as the tuning parameter lambda decreases. In this analysis, shrinkage plays a key role: though additional variables are chosen adaptively, the coefficients of lasso active variables are shrunken due to the l(1) penalty. Therefore, the test statistic (which is based on lasso fitted values) is in a sense balanced by these two opposing properties-adaptivity and shrinkage-and its null distribution is tractable and asymptotically Exp(1).",WOS:000336888400001,ANNALS OF STATISTICS,"['DIMENSIONAL LINEAR-MODELS', 'VARIABLE SELECTION', 'REGRESSION', 'RECOVERY', 'REGULARIZATION', 'PARAMETERS', 'SHRINKAGE', 'ALGORITHM', 'FREEDOM']",A SIGNIFICANCE TEST FOR THE LASSO,2014
1054,"In this paper, we study nonparametric models allowing for locally stationary regressors and a regression function that changes smoothly over time. These models are a natural extension of time series models with time-varying coefficients. We introduce a kernel-based method to estimate the time-varying regression function and provide asymptotic theory for our estimates. Moreover, we show that the main conditions of the theory are satisfied for a large class of nonlinear autoregressive processes with a time-varying regression function. Finally, we examine structured models where the regression function splits up into time-varying additive components. As will be seen, estimation in these models does not suffer from the curse of dimensionality.",WOS:000321844300009,ANNALS OF STATISTICS,"['NONLINEAR AUTOREGRESSIVE MODELS', 'UNIFORM-CONVERGENCE RATES', 'VARYING ARCH PROCESSES', 'GEOMETRIC ERGODICITY', 'DEPENDENT DATA', 'NONSTATIONARY', 'INFERENCE']",NONPARAMETRIC REGRESSION FOR LOCALLY STATIONARY TIME SERIES,2012
1055,"The false discovery proportion (FDP) is a convenient way to account for false positives when a large number m of tests are performed simultaneously. Romano and Wolf [Ann. Statist. 35 (2007) 1378-1408] have proposed a general principle that builds FDP controlling procedures from k-family-wise error rate controlling procedures while incorporating dependencies in an appropriate manner; see Korn et al. [J. Statist. Plann. Inference 124 (2004) 379-398]; Romano and Wolf (2007). However, the theoretical validity of the latter is still largely unknown. This paper provides a careful study of this heuristic: first, we extend this approach by using a notion of ""bounding device"" that allows us to cover a wide range of critical values, including those that adapt to m(0), the number of true null hypotheses. Second, the theoretical validity of the latter is investigated both nonasymptotically and asymptotically. Third, we introduce suitable modifications of this heuristic that provide new methods, overcoming the existing procedures with a proven FDP control.",WOS:000355768700008,ANNALS OF STATISTICS,"['FAMILYWISE ERROR RATE', 'CENTRAL-LIMIT-THEOREM', 'FDR CONTROL', 'STEPUP PROCEDURES', 'MOMENT BOUNDS', 'DEPENDENCE', 'INDEPENDENCE', 'INEQUALITIES', 'EXCEEDANCE', 'RATES']",NEW PROCEDURES CONTROLLING THE FALSE DISCOVERY PROPORTION VIA ROMANO-WOLF'S HEURISTIC,2015
1056,"In this paper we elaborate on the potential of the lmer function from the l m e 4 package in R for item response (IRT) modeling. In line with the package, an IRT framework is described based on generalized linear mixed modeling. The aspects of the framework refer to (a) the kind of covariates - their mode (person, item, person-by-item), and their being external vs. internal to responses, and (b) the kind of effects the covariates have - fixed vs. random, and if random, the mode across which the effects are random (persons, items). Based on this framework, three broad categories of models are described: Item covariate models, person covariate models, and person-by-item covariate models, and within each category three types of more specific models are discussed. The models in question are explained and the associated lmer code is given. Examples of models are the linear logistic test model with an error term, differential item functioning models, and local item dependency models. Because the l m e 4 package is for univariate generalized linear mixed models, neither the two-parameter, and three-parameter models, nor the item response models for polytomous response data, can be estimated with the lmer function.",WOS:000288206200001,JOURNAL OF STATISTICAL SOFTWARE,"['RASCH MODEL', 'VARIANCE-COMPONENTS', 'MIXED MODELS', 'IRT MODELS', 'PREDICTORS', 'TESTS']",The Estimation of Item Response Models with the lmer Function from the lme4 Package in R,2011
1057,"This paper is concerned with extensions of the classical Marcenko-Pastur law to time series. Specifically, p-dimensional linear processes are considered which are built from innovation vectors with independent, identically distributed (real- or complex-valued) entries possessing zero mean, unit variance and finite fourth moments. The coefficient matrices of the linear process are assumed to be simultaneously diagonalizable. In this setting, the limiting behavior of the empirical spectral distribution of both sample covariance and symmetrized sample autocovariance matrices is determined in the high-dimensional setting p/n --> c is an element of(0, infinity) for which dimension p and sample size n diverge to infinity at the same rate. The results extend existing contributions available in the literature for the covariance case and are one of the first of their kind for the autocovariance case.",WOS:000352757100008,ANNALS OF STATISTICS,"['DYNAMIC-FACTOR MODEL', 'SAMPLE COVARIANCE MATRICES', 'SPECTRAL DISTRIBUTION', 'EIGENVALUES', 'NUMBER']",ON THE MARCENKO-PASTUR LAW FOR LINEAR TIME SERIES,2015
1058,"Graphical user interfaces (GUIs) are gradually becoming more powerful and more accepted. They are the standard way of interacting with the web and play an increasing role in many software applications. Nevertheless, they have not been generally adopted, and critics point to particular weaknesses and disadvantages. Many of these are due more to flaws in design and implementation than to the basic concepts of GUIs. More attention could be paid to what users want to do and how a GUI might be developed to support these goals. Using a dataset about Oscar nominees and winners, this paper considers what analyses statisticians might carry out and what kind of GUI would be appropriate for these tasks. (It also offers some insights into the Oscars dataset.)",WOS:000305992100001,JOURNAL OF STATISTICAL SOFTWARE,,Oscars and Interfaces,2012
1059,"Graphic processing units (GPUs) are rapidly gaining maturity as powerful general parallel computing devices. A key feature in the development of modern GPUs has been the advancement of the programming model and programming tools. Compute Unified Device Architecture (CUDA) is a software platform for massively parallel high-performance computing on Nvidia many-core GPUs. In functional magnetic resonance imaging (fMRI), the volume of the data to be processed, and the type of statistical analysis to perform call for high-performance computing strategies. In this work, we present the main features of the R-CUDA package cudaBayesreg which implements in CUDA the core of a Bayesian multilevel model for the analysis of brain fMRI data. The statistical model implements a Gibbs sampler for multilevel/hierarchical linear models with a normal prior. The main contribution for the increased performance comes from the use of separate threads for fitting the linear regression model at each voxel in parallel. The R-CUDA implementation of the Bayesian model proposed here has been able to reduce significantly the run-time processing of Markov chain Monte Carlo (MCMC) simulations used in Bayesian fMRI data analyses. Presently, cudaBayesreg is only configured for Linux systems with Nvidia CUDA support.",WOS:000296228600001,JOURNAL OF STATISTICAL SOFTWARE,['COMPUTATION'],cudaBayesreg: Parallel Implementation of a Bayesian Multilevel Model for fMRI Data Analysis,2011
1060,"spatstat is a package for analyzing spatial point pattern data. Its functionality includes exploratory data analysis, model-fitting, and simulation. It is designed to handle realistic datasets, including inhomogeneous point patterns, spatial sampling regions of arbitrary shape, extra covariate data, and `marks' attached to the points of the point pattern.
A unique feature of spatstat is its generic algorithm for fitting point process models to point pattern data. The interface to this algorithm is a function ppm that is strongly analogous to lm and glm.
This paper is a general description of spatstat and an introduction for new users.",WOS:000232806800001,JOURNAL OF STATISTICAL SOFTWARE,"['LIKELIHOOD INFERENCE', 'LOCAL INDICATORS', 'ASSOCIATION', 'POTENTIALS', 'RETINA', 'FOREST', 'CELLS', 'LISA']",spatstat: An R package for analyzing spatial point patterns,2005
1061,"In this paper we present the methodology of multidimensional scaling problems (MDS) solved by means of the majorization algorithm. The objective function to be minimized is known as stress and functions which majorize stress are elaborated. This strategy to solve MDS problems is called SMACOF and it is implemented in an R package of the same name which is presented in this article. We extend the basic SMACOF theory in terms of configuration constraints, three-way data, unfolding models, and projection of the resulting configurations onto spheres and other quadratic surfaces. Various examples are presented to show the possibilities of the SMACOF approach offered by the corresponding package.",WOS:000268700600001,JOURNAL OF STATISTICAL SOFTWARE,"['NONLINEAR DIMENSIONALITY REDUCTION', 'UNKNOWN DISTANCE FUNCTION', 'PROXIMITIES', 'ALGORITHM', 'MODELS', 'SPHERE', 'UNIT']",Multidimensional Scaling Using Majorization: SMACOF in R,2009
1062,"This paper is devoted to the parametric estimation of a shift together with the nonparametric estimation of a regression function in a semiparametric regression model. We implement a very efficient and easy to handle Robbins-Monro procedure. On the one hand, we propose a stochastic algorithm similar to that of Robbins-Monro in order to estimate the shift parameter. A preliminary evaluation of the regression function is not necessary to estimate the shift parameter. On the other hand, we make use of a recursive Nadaraya-Watson estimator for the estimation of the regression function. This kernel estimator takes into account the previous estimation of the shift parameter. We establish the almost sure convergence for both Robbins-Monro and Nadaraya-Watson estimators. The asymptotic normality of our estimates is also provided. Finally, we illustrate our semiparametric estimation procedure on simulated and real data.",WOS:000307608000002,ANNALS OF STATISTICS,"['SHAPE-INVARIANT MODELS', 'NONPARAMETRIC REGRESSION', 'STOCHASTIC-APPROXIMATION', 'NONLINEAR-REGRESSION', 'MAXIMUM-LIKELIHOOD', 'ITERATED LOGARITHM', 'CONVERGENCE', 'ALGORITHMS', 'LAW']",A ROBBINS-MONRO PROCEDURE FOR ESTIMATION IN SEMIPARAMETRIC REGRESSION MODELS,2012
1063,"Graphlet analysis is a useful tool for describing local network topology around individual nodes or edges. A node or an edge can be described by a vector containing the counts of different kinds of graphlets (small induced subgraphs) in which it appears, or the ""roles"" (orbits) it has within these graphlets. We implemented an R package with functions for fast computation of such counts on sparse graphs. Instead of enumerating all induced graphlets, our algorithm is based on the derived relations between the counts, which decreases the time complexity by an order of magnitude in comparison with past approaches.",WOS:000384915900001,JOURNAL OF STATISTICAL SOFTWARE,,Computation of Graphlet Orbits for Nodes and Edges in Sparse Graphs,2016
1064,"GAMLSS is a general framework for fitting regression type models where the distribution of the response variable does not have to belong to the exponential family and includes highly skew and kurtotic continuous and discrete distribution. GAMLSS allows all the parameters of the distribution of the response variable to be modelled as linear/non-linear or smooth functions of the explanatory variables. This paper starts by defining the statistical framework of GAMLSS, then describes the current implementation of GAMLSS in R and finally gives four different data examples to demonstrate how GAMLSS can be used for statistical modelling.",WOS:000252431700001,JOURNAL OF STATISTICAL SOFTWARE,"['PENALIZED LIKELIHOOD', 'CENTILE CURVES', 'INDEX', 'INTERVALS']",Generalized additive models for location scale and shape (GAMLSS) in R,2007
1065,"We consider a problem of recovering a high-dimensional vector mu observed in white noise, where the unknown vector g is assumed to be sparse. The objective of the paper is to develop a Bayesian formalism which gives rise to a family of l(0)-type penalties. The penalties are associated with various choices of the prior distributions pi(n)(center dot) on the number of nonzero entries of mu and, hence, are easy to interpret. The resulting Bayesian estimators lead to a general thresholding rule which accommodates many of the known thresholding and model selection procedures as particular cases corresponding to specific choices of pi(n)(center dot). Furthermore, they achieve optimality in a rather general setting under very mild conditions on the prior. We also specify the class of priors pi(n)(center dot) for which the resulting estimator is adaptively optimal (in the minimax sense) for a wide range of sparse sequences and consider several examples of such priors.",WOS:000251096100017,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'INFLATION CRITERION', 'VARIABLE SELECTION', 'REGRESSION', 'SHRINKAGE', 'MODEL', 'RISK']",On optimality of Bayesian testimation in the normal means problem,2007
1066,"We derive an exact p-value for testing a global null hypothesis in a general adaptive regression setting. Our approach uses the Kac-Rice formula [as described in Random Fields and Geometry (2007) Springer, New York] applied to the problem of maximizing a Gaussian process. The resulting test statistic has a known distribution in finite samples, assuming Gaussian errors. We examine this test statistic in the case of the lasso, group lasso, principal components and matrix completion problems. For the lasso problem, our test relates closely to the recently proposed covariance test of Lockhart et al. [Ann. Statist. (2004) 42 413-468].
In a few specific settings, our proposed tests will be less powerful than other previously known (and well-established) tests. However, it should be noted that the real strength of our proposal here is its generality. We provide a framework for constructing valid tests across a wide class of regularized regression problems, and as far as we can tell, such a unified view was not possible before this work.",WOS:000372594300011,ANNALS OF STATISTICS,"['LASSO', 'SELECTION', 'MAXIMUM']",INFERENCE IN ADAPTIVE REGRESSION VIA THE KAC-RICE FORMULA,2016
1067,"In this paper, we propose a class of high breakdown point estimators for the linear regression model when the response variable contains censored observations. These estimators are robust against high-leverage outliers and they generalize the LMS (least median of squares), S, MM and tau-estimators for linear regression. An important contribution of this paper is that we can define consistent estimators using a bounded loss function (or equivalently, a re-descending score function). Since the calculation of these estimators can be computationally costly, we propose an efficient algorithm to compute them. We illustrate their use on an example and present simulation studies that show that these estimators also have good finite sample properties.",WOS:000253390000005,ANNALS OF STATISTICS,"['LINEAR-REGRESSION', 'SQUARES REGRESSION', 'LARGE SAMPLE', 'COVARIABLES', 'ESTIMATORS']",High breakdown point robust regression with censored data,2008
1068,"Implementation of multivariate and 2D extensions of singular spectrum analysis (SSA) by means of the R package Rssa is considered. The extensions include MSSA for simultaneous analysis and forecasting of several time series and 2D-SSA for analysis of digital images. A new extension of 2D-SSA analysis called shaped 2D-SSA is introduced for analysis of images of arbitrary shape, not necessary rectangular. It is shown that implementation of shaped 2D-SSA can serve as a basis for implementation of MSSA and other generalizations. Efficient implementation of operations with Hankel and Hankel-block-Hankel matrices through the fast Fourier transform is suggested. Examples with code fragments in R, which explain the methodology and demonstrate the proper use of Rssa, are presented.",WOS:000365981600001,JOURNAL OF STATISTICAL SOFTWARE,"['ESPRIT TYPE METHODS', 'DAMPING FACTORS', 'TIME-SERIES', 'IMPLEMENTATION', 'FREQUENCIES', 'PARAMETERS', 'MATRICES', 'SSA']",Multivariate and 2D Extensions of Singular Spectrum Analysis with the Rssa Package,2015
1069,"Problems with truncated data occur in many areas, complicating estimation and inference. Regarding linear regression models, the ordinary least squares estimator is inconsistent and biased for these types of data and is therefore unsuitable for use. Alternative estimators, designed for the estimation of truncated regression models, have been developed. This paper presents the R package truncSP. The package contains functions for the estimation of semi-parametric truncated linear regression models using three different estimators: the symmetrically trimmed least squares, quadratic mode, and left truncated estimators, all of which have been shown to have good asymptotic and finite sample properties. The package also provides functions for the analysis of the estimated models. Data from the environmental sciences are used to illustrate the functions in the package.",WOS:000341021200001,JOURNAL OF STATISTICAL SOFTWARE,"['LIMITS', 'QME']",truncSP: An R Package for Estimation of Semi-Parametric Truncated Linear Regression Models,2014
1070,"The global imbalance (GI) measure is a way for checking balance of baseline covariates that confound efforts to draw valid conclusions about treatment effects on outcomes of interest. In addition, GI is tested by means of a multivariate test. The GI measure and its test overcome some limitations of the common way for assessing the presence of imbalance in observed covariates that were discussed in D'Attoma and Camillo (2011). A user written SAS macro called %GI, to simultaneously measure and test global imbalance of baseline covariates is described. Furthermore, %GI also assesses global imbalance by subgroups obtained through several matching or classification methods (e. g., cluster analysis, propensity score subclassification, Rosenbaum and Rubin 1984), no matter how many groups are examined. %GI works with mixed categorical, ordinal and continuous covariates. Continuous baseline covariates need to be split into categories. It also works in the multi-treatment case. The use of the %GI macro will be illustrated using two artificial examples.",WOS:000326872300001,JOURNAL OF STATISTICAL SOFTWARE,"['PROPENSITY SCORE', 'CAUSAL INFERENCE', 'MATCHING METHODS', 'BIAS']",%GI: A SAS Macro for Measuring and Testing Global Imbalance of Covariates within Subgroups,2012
1071,"Quantile regression has been advocated in survival analysis to assess evolving covariate effects. However, challenges arise when the censoring time is not always observed and may he covariate-dependent, particularly in the presence of continuously-distributed covariates. In spite of several recent advances, existing methods either involve algorithmic complications or impose a probability grid. The former leads to difficulties in the implementation and asymptotics, whereas the latter introduces undesirable grid dependence. To resolve these issues, we develop fundamental and general pantile calculus on cumulative probability scale in this article, upon recognizing that probability and time scales do not always have a one-to-one mapping given a survival distribution. These results give rise to a novel estimation procedure for censored pantile regression, based on estimating integral equations. A numerically reliable and efficient Progressive Localized Minimization (PLMIN) algorithm is proposed for the computation. This procedure reduces exactly to the Kaplan-Meier method in the k-sample problem, and to standard uncensored guanine regression in the absence of censoring. Under regularity conditions, the proposed pantile coefficient estimator is uniformly consistent and converges weakly to a Gaussian process. Simulations show good statistical and algorithmic performance. The proposal is illustrated in the application to a clinical study.",WOS:000277471000012,ANNALS OF STATISTICS,"['SURVIVAL ANALYSIS', 'MODELS']",QUANTILE CALCULUS AND CENSORED REGRESSION,2010
1072,"We extend the empirical likelihood of Owen [Ann. Statist. 18 (1990) 90-120] by partitioning its domain into the collection of its contours and mapping the contours through a continuous sequence of similarity transformations onto the full parameter space. The resulting extended empirical likelihood is a natural generalization of the original empirical likelihood to the full parameter space; it has the same asymptotic properties and identically shaped contours as the original empirical likelihood. It can also attain the second order accuracy of the Bartlett corrected empirical likelihood of DiCiccio, Hall and Romano [Ann. Statist. 19 (1991) 1053-10611. A simple first order extended empirical likelihood is found to be substantially more accurate than the original empirical likelihood. It is also more accurate than available second order empirical likelihood methods in most small sample situations and competitive in accuracy in large sample situations. Importantly, in many one-dimensional applications this first order extended empirical likelihood is accurate for sample sizes as small as ten, making it a practical and reliable choice for small sample empirical likelihood inference.",WOS:000326991200017,ANNALS OF STATISTICS,['RATIO CONFIDENCE-REGIONS'],EMPIRICAL LIKELIHOOD ON THE FULL PARAMETER SPACE,2013
1073,"This paper studies an approximation method for the log-likelihood function of a nonlinear diffusion process using the bridge of the diffusion. The main result (Theorem 1) shows that this approximation converges uniformly to the unknown likelihood function and can therefore be used efficiently with any algorithm for sampling from the law of the bridge. We also introduce an expected maximum likelihood (EML) algorithm for inferring the parameters of discretely observed diffusion processes. The approach is applicable to a subclass of nonlinear SDEs with constant volatility and drift that is linear in the model parameters. In this setting, globally optimal parameters are obtained in a single step by solving a linear system. Simulation Studies to test the EML algorithm show that it performs well when compared with algorithms based on the exact maximum likelihood as well its closed-form likelihood expansions.",WOS:000273800100007,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'STOCHASTIC DIFFERENTIAL-EQUATIONS', 'CLOSED-FORM APPROXIMATION', 'SIMULATED LIKELIHOOD', 'NUMERICAL TECHNIQUES', 'MODELS', 'TIME', 'INFERENCE', 'VOLATILITY', 'ALGORITHM']",GLOBALLY OPTIMAL PARAMETER ESTIMATES FOR NONLINEAR DIFFUSIONS,2010
1074,"The subject of this paper is the problem of nonparametric estimation of a continuous distribution function from observations with measurement errors. We study minimax complexity of this problem when unknown distribution has a density belonging to the Sobolev class, and the error density is ordinary smooth. We develop rate optimal estimators based on direct inversion of empirical characteristic function. We also derive minimax affine estimators of the distribution function which are given by an explicit convex optimization problem. Adaptive versions of these estimators are proposed, and some numerical results demonstrating good practical behavior of the developed procedures are presented.",WOS:000299186500011,ANNALS OF STATISTICS,"['LINEAR FUNCTIONALS', 'NONPARAMETRIC-ESTIMATION', 'DENSITY DECONVOLUTION', 'ADAPTIVE ESTIMATION', 'GEOMETRIZING RATES', 'SHARP OPTIMALITY', 'CONVERGENCE', 'CONVOLUTION']",ON DECONVOLUTION OF DISTRIBUTION FUNCTIONS,2011
1075,"Envelope models and methods represent new constructions that can lead to substantial increases in estimation efficiency in multivariate analyses. The envlp toolbox implements a variety of envelope estimators under the framework of multivariate linear regression, including the envelope model, partial envelope model, heteroscedastic envelope model, inner envelope model, scaled envelope model, and envelope model in the predictor space. The toolbox also implements the envelope model for estimating a multivariate mean. The capabilities of this toolbox include estimation of the model parameters, as well as performing standard multivariate inference in the context of envelope models; for example, prediction and prediction errors, F test for two nested models, the standard errors for contrasts or linear combinations of coefficients, and more. Examples and datasets are contained in the toolbox to illustrate the use of each model. All functions and datasets are documented.",WOS:000349843700001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR-REGRESSION', 'MODELS']",envlp: A MATLAB Toolbox for Computing Envelope Estimators in Multivariate Analysis,2014
1076,"Model-free variable selection has been implemented under the sufficient dimension reduction framework since the seminal paper of Cook [Ann. Statist. 32 (2004) 1062-1092]. In this paper, we extend the marginal coordinate test for sliced inverse regression (SIR) in Cook (2004) and propose a novel marginal SIR utility for the purpose of ultrahigh dimensional feature selection. Two distinct procedures, Dantzig selector and sparse precision matrix estimation, are incorporated to get two versions of sample level marginal SIR utilities. Both procedures lead to model-free variable selection consistency with predictor dimensionality p diverging at an exponential rate of the sample size n. As a special case of marginal SIR, we ignore the correlation among the predictors and propose marginal independence SIR. Marginal independence SIR is closely related to many existing independence screening procedures in the literature, and achieves model-free screening consistency in the ultrahigh dimensional setting. The finite sample performances of the proposed procedures are studied through synthetic examples and an application to the small round blue cell tumors data.",WOS:000389620800013,ANNALS OF STATISTICS,"['FREE VARIABLE SELECTION', 'DISCRIMINANT-ANALYSIS', 'KOLMOGOROV FILTER', 'ORACLE PROPERTIES', 'ADAPTIVE LASSO', 'REDUCTION', 'SHRINKAGE', 'CLASSIFICATION', 'PURSUIT']",ON MARGINAL SLICED INVERSE REGRESSION FOR ULTRAHIGH DIMENSIONAL MODEL-FREE FEATURE SELECTION,2016
1077,"In model selection literature, two classes of criteria perform well asymptotically in different situations: Bayesian information criterion (BIC) (as a representative) is consistent in selection when the true model is finite dimensional (parametric scenario); Akaike's information criterion (AIC) performs well in an asymptotic efficiency when the true model is infinite dimensional (nonparametric scenario). But there is little work that addresses if it is possible and how to detect the situation that a specific model selection problem is in. In this work, we differentiate the two scenarios theoretically under some conditions. We develop a measure, parametricness index (PI), to assess whether a model selected by a potentially consistent procedure can be practically treated as the true model, which also hints on AIC or BIC is better suited for the data for the goal of estimating the regression function. A consequence is that by switching between AIC and BIC based on the PI, the resulting regression estimator is simultaneously asymptotically efficient for both parametric and nonparametric scenarios. In addition, we systematically investigate the behaviors of PI in simulation and real data and show its usefulness.",WOS:000296995500009,ANNALS OF STATISTICS,"['VARIABLE SELECTION', 'LINEAR-REGRESSION', 'MULTIPLE-REGRESSION', 'DENSITY-ESTIMATION', 'ORACLE PROPERTIES', 'ASYMPTOTIC THEORY', 'LEAST-SQUARES', 'INFERENCE', 'LASSO', 'CRITERIA']",PARAMETRIC OR NONPARAMETRIC? A PARAMETRICNESS INDEX FOR MODEL SELECTION,2011
1078,"We study a marginal empirical likelihood approach in scenarios when the number of variables grows exponentially with the sample size. The marginal empirical likelihood ratios as functions of the parameters of interest are systematically examined, and we find that the marginal empirical likelihood ratio evaluated at zero can be used to differentiate whether an explanatory variable is contributing to a response variable or not. Based on this finding, we propose a unified feature screening procedure for linear models and the generalized linear models. Different from most existing feature screening approaches that rely on the magnitudes of some marginal estimators to identify true signals, the proposed screening approach is capable of further incorporating the level of uncertainties of such estimators. Such a merit inherits the self-studentization property of the empirical likelihood approach, and extends the insights of existing feature screening methods. Moreover, we show that our screening approach is less restrictive to distributional assumptions, and can be conveniently adapted to be applied in a broad range of scenarios such as models specified using general moment conditions. Our theoretical results and extensive numerical examples by simulations and data analysis demonstrate the merits of the marginal empirical likelihood approach.",WOS:000326991200015,ANNALS OF STATISTICS,"['GENERALIZED LINEAR-MODELS', 'DIMENSIONAL FEATURE SPACE', 'ESTIMATING EQUATIONS', 'VARIABLE SELECTION', 'ESTIMATORS', 'REGRESSION', 'NETWORK']",MARGINAL EMPIRICAL LIKELIHOOD AND SURE INDEPENDENCE FEATURE SCREENING,2013
1079,"In this paper, we derive valid Edgeworth expansions for studentized versions of a large class of statistics when the data are generated by a strongly mixing process. Under dependence, the asymptotic variance of such a statistic is given by an infinite series of lag-covariances, and therefore, studentizing factors (i.e., estimators of the asymptotic standard error) typically involve an increasing number, say, l of lag-covariance estimators, which are themselves quadratic functions of the observations. The unboundedness of the dimension e of these quadratic functions makes the derivation and the form of the expansions nonstandard. It is shown that in contrast to the case of the studentized means under independence, the derived Edgeworth expansion is a superposition of three distinct series, respectively, given by one in powers of n(-1/2), one in powers of [n/l](-1/2) (resulting from the standard error of the studentizing factor) and one in powers of the bias of the studentizing factor, where n denotes the sample size.",WOS:000273800100012,ANNALS OF STATISTICS,"['ASYMPTOTIC EXPANSIONS', 'STATIONARY OBSERVATIONS', 'RANDOM-VARIABLES', 'BLOCK BOOTSTRAP', 'RANDOM VECTORS', 'SUMS']",EDGEWORTH EXPANSIONS FOR STUDENTIZED STATISTICS UNDER WEAK DEPENDENCE,2010
1080,"The use of copula-based models in EDAs (estimation of distribution algorithms) is currently an active area of research. In this context, the copulaedas package for R provides a platform where EDAs based on copulas can be implemented and studied. The package offers complete implementations of various EDAs based on copulas and vines, a group of well-known optimization problems, and utility functions to study the performance of the algorithms. Newly developed EDAs can be easily integrated into the package by extending an S4 class with generic functions for their main components. This paper presents copulaedas by providing an overview of EDAs based on copulas, a description of the implementation of the package, and an illustration of its use through examples. The examples include running the EDAs defined in the package, implementing new algorithms, and performing an empirical study to compare the behavior of different algorithms on benchmark functions and a real-world problem.",WOS:000341642300001,JOURNAL OF STATISTICAL SOFTWARE,"['META-ELLIPTIC DISTRIBUTIONS', 'ARCHIMEDEAN COPULAS', 'DEPENDENCE', 'OPTIMIZATION', 'MODELS', 'INDEPENDENCE', 'EFFICIENCY', 'TESTS']",copulaedas: An R Package for Estimation of Distribution Algorithms Based on Copulas,2014
1081,"Variable selection, also known as feature selection in machine learning, plays an important role in modeling high dimensional data and is key to data-driven scientific discoveries. We consider here the problem of detecting influential variables under the general index model, in which the response is dependent of predictors through an unknown function of one or more linear combinations of them. Instead of building a predictive model of the response given combinations of predictors, we model the conditional distribution of predictors given the response. This inverse modeling perspective motivates us to propose a stepwise procedure based on likelihood-ratio tests, which is effective and computationally efficient in identifying important variables without specifying a parametric relationship between predictors and the response. For example, the proposed procedure is able to detect variables with pairwise, three-way or even higher-order interactions among p predictors with a computational time of O(p) instead of O(p(k)) (with k being the highest order of interactions). Its excellent empirical performance in comparison with existing methods is demonstrated through simulation studies as well as real data examples. Consistency of the variable selection procedure when both the number of predictors and the sample size go to infinity is established.",WOS:000344632400003,ANNALS OF STATISTICS,"['SUFFICIENT DIMENSION REDUCTION', 'EMBRYONIC STEM-CELLS', 'ORACLE PROPERTIES', 'EXPRESSION', 'LASSO', 'LIKELIHOOD', 'DISCOVERY', 'CANCER']",VARIABLE SELECTION FOR GENERAL INDEX MODELS VIA SLICED INVERSE REGRESSION,2014
1082,"Spatial cluster detection is a classical question in epidemiology: Are cases located near other cases? In order to classify a study area into zones of different risks and determine their boundaries, we have developed a spatial partitioning method based on oblique de- cision trees, which is called spatial oblique decision tree (SpODT). This non-parametric method is based on the classification and regression tree (CART) approach introduced by Leo Breiman. Applied to epidemiological spatial data, the algorithm recursively searches among the coordinates for a threshold or a boundary between zones, so that the risks estimated in these zones are as different as possible. While the CART algorithm leads to rectangular zones, providing perpendicular splits of longitudes and latitudes, the SpODT algorithm provides oblique splitting of the study area, which is more appropriate and accu- rate for spatial epidemiology. Oblique decision trees can be considered as non-parametric regression models. Beyond the basic function, we have developed a set of functions that enable extended analyses of spatial data, providing: inference, graphical representations, spatio-temporal analysis, adjustments on covariates, spatial weighted partition, and the gathering of similar adjacent final classes. In this paper, we propose a new R package, SPODT, which provides an extensible set of functions for partitioning spatial and spatio- temporal data. The implementation and extensions of the algorithm are described. Func- tion usage examples are proposed, looking for clustering malaria episodes in Bandiagara, Mali, and samples showing three different cluster shapes.",WOS:000349847100001,JOURNAL OF STATISTICAL SOFTWARE,"['MALARIA', 'RISK', 'CLUSTERS', 'TREES', 'MALI']",SPODT: An R Package to Perform Spatial Partitioning,2015
1083,"As expression microarrays are typically designed relative to a reference genome, any individual genetic variant that overlaps a probe's genomic position can possibly cause a reduction in hybridization due to the probe no longer being a perfect match to a given sample's mRNA at that locus. If the samples or groups used in a microarray study differ in terms of genetic variants, the results of the microarray experiment can be negatively impacted. The oligoMask package is an R/SQLite framework which can utilize publicly available genetic variants and works in conjunction with the oligo package to read in the expression data and remove microarray probes which are likely to impact a given microarray experiment prior to analysis. Tools are provided for creating an SQLite database containing the probe and variant annotations and for performing the commonly used RMA preprocessing procedure for Affymetrix microarrays. The oligoMask package is freely available at https://github.com/dbottomly/oligoMask.",WOS:000343788100017,R JOURNAL,"['DIFFERENTIAL EXPRESSION', 'POLYMORPHISMS', 'POPULATION', 'GENECHIP', 'SNPS']",oligoMask: A Framework for Assessing and Removing the Effect of Genetic Variants on Microarray Probes,2014
1084,"Diffusion weighted imaging has become and will certainly continue to be all important tool in medical research and diagnostics. Data obtained with diffusion weighted imaging are characterized by a high noise level. Thus, estimation of quantities like anisotropy indices or the main diffusion direction may be significantly compromised by noise in clinical or neuroscience applications.
Here, we present a new package dti for R, which provides functions for the analysis of diffusion weighted data within the diffusion tensor model. This includes smoothing by a recently proposed structural adaptive smoothing procedure based oil the propagation-separation approach in the context of the widely used diffusion tensor model. We extend the procedure and show, how a correction for Rician bias can be incorporated We use a heteroscedastic nonlinear regression model to estimate the diffusion tensor. The smoothing procedure naturally adapts to different structures of different size and thus avoids oversmoothing edges and fine structures.
We illustrate the usage and capabilities of the package through some examples.",WOS:000269819700001,JOURNAL OF STATISTICAL SOFTWARE,"['LOG-EUCLIDEAN METRICS', 'FUNCTIONAL MRI', 'B-MATRIX', 'NOISE', 'FMRI', 'REGISTRATION', 'SPECTROSCOPY', 'FRAMEWORK', 'GRADIENT', 'ECHOES']",Structural Adaptive Smoothing in Diffusion Tensor Imaging: The R Package dti,2009
1085,"We study the problem of nonparametric estimation of a multivariate function g: R(d) -> R that can be represented as a composition of two unknown smooth functions f : R -> R and G :R(d) -> R. We suppose that f and G belong to known smoothness classes of functions, with smoothness gamma and beta, respectively. We obtain the full description of minimax rates of estimation of g in terms of gamma and beta, and propose rate-optimal estimators for the sup-norm loss. For the construction of such estimators, we first prove an approximation result for composite functions that may have an independent interest, and then a result on adaptation to the local structure. Interestingly, the construction of rate-optimal estimators for composite functions (with given, fixed smoothness) needs adaptation, but not in the traditional sense: it is now adaptation to the local structure. We prove that composition models generate only two types of local structures: the local single-index model and the local model with roughness isolated to a single dimension (i.e., a model containing elements of both additive and single-index structure). We also find the zones of (gamma,beta) where no local structure is generated, as well as the zones where the composition modeling leads to faster rates, as compared to the classical nonparametric rates that depend only to the overall smoothness of g.",WOS:000265619700010,ANNALS OF STATISTICS,"['MINIMAX ESTIMATION', 'SUP-NORM', 'REGRESSION', 'MODELS']",NONPARAMETRIC ESTIMATION OF COMPOSITE FUNCTIONS,2009
1086,"We consider the problem of constructing optimal designs for model discrimination between competing regression models. Various new properties of optimal designs with respect to the popular T-optimality criterion are derived, which in many circumstances allow an explicit determination of T-optimal designs. It is also demonstrated, that in nested linear models the number of support points of T-optimal designs is usually too small to estimate all parameters in the extended model. In many cases T-optimal designs are usually not unique, and in this situation we give a characterization of all T-optimal designs. Finally, T-optimal designs are compared with optimal discriminating designs with respect to alternative criteria by means of a small simulation study.",WOS:000268113500014,ANNALS OF STATISTICS,"['EXPONENTIAL REGRESSION-MODELS', '2 RIVAL MODELS', 'POLYNOMIAL REGRESSION', 'EQUIVALENCE', 'CRITERIA']",OPTIMAL DISCRIMINATION DESIGNS,2009
1087,"MatchIt implements the suggestions of Ho, Imai, King, and Stuart (2007) for improving parametric statistical models by preprocessing data with nonparametric matching methods. MatchIt implements a wide range of sophisticated matching methods, making it possible to greatly reduce the dependence of causal inferences on hard-to-justify, but commonly made, statistical modeling assumptions. The software also easily fits into existing research practices since, after preprocessing data with MatchIt, researchers can use whatever parametric model they would have used without MatchIt, but produce inferences with substantially more robustness and less sensitivity to modeling assumptions. MatchIt is an R program, and also works seamlessly with Zelig.",WOS:000292097500001,JOURNAL OF STATISTICAL SOFTWARE,['TRAINING-PROGRAMS'],MatchIt: Nonparametric Preprocessing for Parametric Causal Inference,2011
1088,"This paper presents the R package CAvariants (Lombardo and Beh, 2017). The package performs six variants of correspondence analysis on a two-way contingency table. The main function that shares the same name as the package - CAvariants - allows the user to choose (via a series of input parameters) from six different correspondence analysis procedures. These include the classical approach to (symmetrical) correspondence analysis, singly ordered correspondence analysis, doubly ordered correspondence analysis, non symmetrical correspondence analysis, singly ordered non symmetrical correspondence analysis and doubly ordered non symmetrical correspondence analysis. The code provides the flexibility for constructing either a classical correspondence plot or a biplot graphical display. It also allows the user to consider other important features that allow to assess the reliability of the graphical representations, such as the inclusion of algebraically derived elliptical confidence regions. This paper provides R functions that elaborates more fully on the code presented in Beh and Lombardo (2014).",WOS:000395669800011,R JOURNAL,"['2-WAY CONTINGENCY-TABLES', 'ORTHOGONAL POLYNOMIALS', 'MULTIVARIATE-ANALYSIS', 'R-PACKAGE', 'FACTOMINER']",Variants of Simple Correspondence Analysis,2016
1089,"This paper considers the maximum likelihood estimation of panel data models with interactive effects. Motivated by applications in economics and other social sciences, a notable feature of the model is that the explanatory variables are correlated with the unobserved effects. The usual within-group estimator is inconsistent. Existing methods for consistent estimation are either designed for panel data with short time periods or are less efficient. The maximum likelihood estimator has desirable properties and is easy to implement, as illustrated by the Monte Carlo simulations. This paper develops the inferential theory for the maximum likelihood estimator, including consistency, rate of convergence and the limiting distributions. We further extend the model to include time-invariant regressors and common regressors (cross-section invariant). The regression coefficients for the time-invariant regressors are time-varying, and the coefficients for the common regressors are cross-sectionally varying.",WOS:000334256100006,ANNALS OF STATISTICS,"['DYNAMIC FACTOR MODELS', 'ESTIMATORS', 'INFERENCE', 'NUMBER']",THEORY AND METHODS OF PANEL DATA MODELS WITH INTERACTIVE EFFECTS,2014
1090,"A multivariate generalization of the emulator technique described by Hankin (2005) is presented in which random multivariate functions may be assessed. In the standard univariate case (Oakley 1999), a Gaussian process, a finite number of observations is made; here, observations of different types are considered. The technique has the property that marginal analysis (that is, considering only a single observation type) reduces exactly to the univariate theory. The associated software is used to analyze datasets from the field of climate change.",WOS:000301071900001,JOURNAL OF STATISTICAL SOFTWARE,"['DETERMINISTIC FUNCTIONS', 'COVARIANCE FUNCTIONS', 'T-DISTRIBUTION', 'MODEL', 'OUTPUT', 'CALIBRATION']",Introducing multivator: A Multivariate Emulator,2012
1091,"Phase variation in functional data obscures the true amplitude variation when a typical cross-sectional analysis of these responses would be performed. Time warping or curve registration aims at eliminating the phase variation, typically by applying transformations, the warping functions tau(n), to the function arguments. We propose a warping method that jointly estimates a decomposition of the warping function in warping components, and amplitude components. For the estimation routine, adaptive MCMC calculations are performed and implemented in C rather than R to increase computational speed. The R-C interface makes the program user-friendly, in that no knowledge of C is required and all input and output will be handled through R. The R package MRwarping contains all needed files.",WOS:000325947500001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'SHIFTED CURVES', 'SHAPE', 'ALIGNMENT', 'SAMPLE', 'MODELS', 'MCMC']",Warping Functional Datain Rand C via a Bayesian Multi resolution Approach,2013
1092,We study generalized density-based clustering in which sharply defined clusters such as clusters on lower-dimensional manifolds are allowed. We show that accurate clustering is possible even in high dimensions. We propose two data-based methods for choosing the bandwidth and we study the stability properties of density clusters. We show that a simple graph-based algorithm successfully approximates the high density clusters.,WOS:000282402800004,ANNALS OF STATISTICS,"['LEVEL SETS', 'NONPARAMETRIC-ESTIMATION', 'RATES', 'CLASSIFICATION', 'CONVERGENCE', 'CONSISTENCY', 'CLASSIFIERS', 'ESTIMATORS', 'STABILITY', 'SUPPORT']",GENERALIZED DENSITY CLUSTERING,2010
1093,"This paper presents two complementary statistical computing frameworks that address challenges in parallel processing and the analysis of massive data. First, the for e a c h package allows users of the R programming environment to de fine parallel loops that may be run sequentially on a single machine, in parallel on a symmetric multiprocessing (SMP) machine, or in cluster environments without platform-specific code. Second, the bigmemory package implements memory-and file-mapped data structures that provide (a) access to arbitrarily large data while retaining a look and feel that is familiar to R users and (b) data structures that are shared across processor cores in order to support efficient parallel computing techniques. Although these packages may be used independently, this paper shows how they can be used in combination to address challenges that have effectively been beyond the reach of researchers who lack specialized software development skills or expensive hardware.",WOS:000328131700001,JOURNAL OF STATISTICAL SOFTWARE,,Scalable Strategies for Computing with Massive Data,2013
1094,"In multivariate regression, a K-dimensional response vector is regressed upon a common set of p covariates, with a matrix B* is an element of R-pxK of regression coefficients. We study the behavior of the multivariate group Lasso, in which block regularization based on the l(1)/l(2) norm is used for support union recovery, or recovery of the set of s rows for which B* is nonzero. Under high-dimensional scaling, we show that the multivariate group Lasso exhibits a threshold for the recovery of the exact row pattern with high probability over the random design and noise that is specified by the sample complexity parameter theta(n, p, s): = n/[2 psi(B*) log(p - s)]. Here n is the sample size, and psi(B*) is a sparsity-overlap function measuring a combination of the sparsities and overlaps of the K-regression coefficient vectors that constitute the model. We prove that the multivariate group Lasso succeeds for problem sequences (n, p, s) such that theta(n, p, s) exceeds a critical level theta(u), and fails for sequences such that theta(n, p, s) lies below a critical level theta(l). For the special case of the standard Gaussian ensemble, we show that theta(l) = theta(u) so that the characterization is sharp. The sparsity-overlap function psi(B*) reveals that, if the design is uncorrelated on the active rows, l(1)/l(2) regularization for multivariate regression never harms performance relative to an ordinary Lasso approach and can yield substantial improvements in sample complexity (up to a factor of K) when the coefficient vectors are suitably orthogonal. For more general designs, it is possible for the ordinary Lasso to outperform the multivariate group Lasso. We complement our analysis with simulations that demonstrate the sharpness of our theoretical results, even for relatively small problems.",WOS:000288183800001,ANNALS OF STATISTICS,"['VARIABLE SELECTION', 'ATOMIC DECOMPOSITION', 'SPARSITY RECOVERY', 'MODEL SELECTION', 'GROUP LASSO', 'CONSISTENCY', 'ESTIMATORS']",SUPPORT UNION RECOVERY IN HIGH-DIMENSIONAL MULTIVARIATE REGRESSION,2011
1095,"We present a greedy method for simultaneously performing local bandwidth selection and variable selection in nonparametric regression. The method starts with a local linear estimator with large bandwidths, and incrementally decreases the bandwidth of variables for which the gradient of the estimator with respect to bandwidth is large. The method-called rodeo (regularization of derivative expectation operator)-conducts a sequence of hypothesis tests to threshold derivatives, and is easy to implement. Under certain assumptions on the regression function and sampling density, it is shown that the rodeo applied to local linear smoothing avoids the curse of dimensionality, achieving near optimal minimax rates of convergence in the number of relevant variables, as if these variables were isolated in advance.",WOS:000253390000002,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'VARIABLE SELECTION', 'APPROXIMATION', 'ADAPTATION', 'SPLINES', 'LASSO']","Rodeo: Sparse, greedy nonparametric regression",2008
1096,"Studies that validate statistical methods for functional magnetic resonance imaging (fMRI) data often use simulated data to ensure that the ground truth is known. However, simulated fMRI data are almost always generated using in-house procedures because a well-accepted simulation method is lacking. In this article we describe the R package neuRosim, which is a collection of data generation functions for neuroimaging data. We will demonstrate the possibilities to generate data from simple time series to complete 4D images and the possibilities for the user to create her own data generation method.",WOS:000296719000001,JOURNAL OF STATISTICAL SOFTWARE,"['EVENT-RELATED FMRI', 'MR IMAGE-ANALYSIS', 'FUNCTIONAL MRI', 'PHYSIOLOGICAL NOISE', 'STATISTICAL-METHODS', 'TIME-SERIES', 'ACTIVATION', 'SIMULATION', 'RESOLUTION', 'ARTIFACTS']",neuRosim: An R Package for Generating fMRI Data,2011
1097,"The two-phase design has recently received attention in the statistical literature as an extension to the traditional case-control study for settings where a predictor of interest is rare or subject to missclassification. Despite a thorough methodological treatment and the potential for substantial efficiency gains, the two-phase design has not been widely adopted. This may be due, in part, to a lack of general-purpose, readily-available soft-ware. The osDesign package for R provides a suite of functions for analyzing data from a two-phase and/or case-control design, as well as evaluating operating characteristics, including bias, efficiency and power. The evaluation is simulation-based, permitting flexible application of the package to a broad range of scientific settings. Using lung cancer mortality data from Ohio, the package is illustrated with a detailed case-study in which two statistical goals are considered: (i) the evaluation of small-sample operating characteristics for two-phase and case-control designs and (ii) the planning and design of a future two-phase study.",WOS:000294232100001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD', 'REGRESSION-MODELS', 'SAMPLE-SIZE', '2-STAGE', 'DISEASE', 'EXPOSURE']","osDesign: An R Package for the Analysis, Evaluation, and Design of Two-Phase and Case-Control Studies",2011
1098,"The contributed R package Newdistns written by the authors is introduced. This package computes the probability density function, cumulative distribution function, quantile function, random numbers and some measures of inference for nineteen families of distributions. Each family is flexible enough to encompass a large number of structures. The use of the package is illustrated using a real data set. Also robustness of random number generation is checked by simulation.",WOS:000373918600001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED RAYLEIGH DISTRIBUTION', 'BIRNBAUM-SAUNDERS DISTRIBUTION', 'BURR XII DISTRIBUTION', 'WEIBULL DISTRIBUTION', 'LIFETIME DATA', 'GEOMETRIC DISTRIBUTION', 'GUMBEL DISTRIBUTION', 'PARETO DISTRIBUTION', 'EXPONENTIAL-DISTRIBUTION', 'LAPLACE DISTRIBUTION']",Newdistns: An R Package for New Families of Distributions,2016
1099,The amei package for R is a tool that provides a flexible statistical framework for generating optimal epidemiological interventions that are designed to minimize the total expected cost of an emerging epidemic. Uncertainty regarding the underlying disease parameters is propagated through to the decision process via Bayesian posterior inference. The strategies produced through this framework are adaptive: vaccination schedules are iteratively adjusted to reflect the anticipated trajectory of the epidemic given the current population state and updated parameter estimates. This document briefly covers the background and methodology underpinning the implementation provided by the package and contains extensive examples showing the functions and methods in action.,WOS:000281593500001,JOURNAL OF STATISTICAL SOFTWARE,"['STOCHASTIC EPIDEMICS', 'BAYESIAN-INFERENCE', 'VACCINATION STRATEGIES', 'MODELS', 'TRANSMISSION', 'POPULATIONS', 'SMALLPOX', 'DISEASES']",An R Package for the Adaptive Management of Epidemiological Interventions,2010
1100,"The efficiency of two Bayesian order estimators is studied. By using nonparametric techniques, we prove new underestimation and overestimation bounds. The results apply to various models, including mixture models. In this case, the errors are shown to be O(e(-an)) and O((log n)(b) / root n) (a, b > 0), respectively.",WOS:000254502700016,ANNALS OF STATISTICS,"['POSTERIOR DISTRIBUTIONS', 'CONVERGENCE-RATES', 'MODEL']",Bounds for Bayesian order identification with application to mixtures,2008
1101,"We study a variable length Markov chain model associated with a group of stationary processes that share the same context tree but each process has potentially different conditional probabilities. We propose a new model selection and estimation method which is computationally efficient. We develop oracle and adaptivity inequalities, as well as model selection properties, that hold under continuity of the transition probabilities and polynomial (ss)-mixing. In particular, model misspecification is allowed. These results are applied to interesting families of processes. For Markov processes, we obtain uniform rate of convergence for the estimation error of transition probabilities as well as perfect model selection results. For chains of infinite order with complete connections, we obtain explicit uniform rates of convergence on the estimation of conditional probabilities, which have an explicit dependence on the processes' continuity rates. Similar guarantees are also derived for renewal processes. Our results are shown to be applicable to discrete stochastic dynamic programming problems and to dynamic discrete choice models. We also apply our estimator to a linguistic study, based on recent work by Galves et al. [Ann. Appl. Stat. 6 (2012) 186-209], of the rhythmic differences between Brazilian and European Portuguese.",WOS:000396804900011,ANNALS OF STATISTICS,"['LENGTH MARKOV-CHAINS', 'MODEL-SELECTION', 'WEIGHTING METHOD', 'HETEROGENEITY', 'REGRESSION', 'REDUNDANCY', 'RENEWAL']",APPROXIMATE GROUP CONTEXT TREE,2017
1102,"A number of settings arise in which it is of interest to predict Principal Component (PC) scores for new observations using data from an initial sample. In this paper, we demonstrate that naive approaches to PC score prediction can be substantially biased toward 0 in the analysis of large matrices. This phenomenon is largely related to known inconsistency results for sample eigenvalues and eigenvectors as both dimensions of the matrix increase. For the spiked eigenvalue model for random matrices, we expand the generality of these results, and propose bias-adjusted PC score prediction. In addition, we compute the asymptotic correlation coefficient between PC scores from sample and population eigenvectors. Simulation and real data examples from the genetics literature show the improved bias and numerical properties of our estimators.",WOS:000290231500008,ANNALS OF STATISTICS,"['SAMPLE COVARIANCE MATRICES', 'WHOLE-GENOME ASSOCIATION', 'GEOMETRIC REPRESENTATION', 'LARGEST EIGENVALUE', 'CONSISTENCY', 'SURVIVAL', 'MODELS']",CONVERGENCE AND PREDICTION OF PRINCIPAL COMPONENT SCORES IN HIGH-DIMENSIONAL SETTINGS,2010
1103,"This article studies local and global inference for smoothing spline estimation in a unified asymptotic framework. We first introduce a new technical tool called functional Bahadur representation, which significantly generalizes the traditional Bahadur representation in parametric models, that is, Bahadur [Ann. Inst. Statist. Math. 37 (1966) 577-580]. Equipped with this tool, we develop four interconnected procedures for inference: (i) pointwise confidence interval; (ii) local likelihood ratio testing; (iii) simultaneous confidence band; (iv) global likelihood ratio testing. In particular, our confidence intervals are proved to be asymptotically valid at any point in the support, and they are shorter on average than the Bayesian confidence intervals proposed by Wahba [J. R. Stat. Soc. Ser. B Stat. Methodol. 45 (1983) 133-150] and Nychka [J. Amer. Statist. Assoc. 83 (1988) 1134-1143]. We also discuss a version of the Wilks phenomenon arising from local/global likelihood ratio testing. It is also worth noting that our simultaneous confidence bands are the first ones applicable to general quasi-likelihood models. Furthermore, issues relating to optimality and efficiency are carefully addressed. As a by-product, we discover a surprising relationship between periodic and nonperiodic smoothing splines in terms of inference.",WOS:000327746100011,ANNALS OF STATISTICS,"['SIMULTANEOUS CONFIDENCE BANDS', 'VARYING-COEFFICIENT MODELS', 'GENERALIZED LINEAR-MODELS', 'NONPARAMETRIC REGRESSION', 'LIKELIHOOD', 'HYPOTHESIS', 'ESTIMATORS', 'INTERVALS', 'DENSITY', 'DERIVATIVES']",LOCAL AND GLOBAL ASYMPTOTIC INFERENCE IN SMOOTHING SPLINE MODELS,2013
1104,"R2MLwiN is a new package designed to run the multilevel modeling software program MLwiN from within the R environment. It allows for a large range of models to be specified which take account of a multilevel structure, including continuous, binary, proportion, count, ordinal and nominal responses for data structures which are nested, cross-classified and/or exhibit multiple membership. Estimation is available via iterative generalized least squares (IGLS), which yields maximum likelihood estimates, and also via Markov chain Monte Carlo (MCMC) estimation for Bayesian inference. As well as employing MLwiN's own MCMC engine, users can request that MLwiN write BUGS model, data and initial values statements for use with WinBUGS or OpenBUGS (which R2MLwiN automatically calls via rbugs), employing IGLS starting values from MLwiN. Users can also take advantage of MLwiN's graphical user interface: for example to specify models and inspect plots via its interactive equations and graphics windows. R2MLwiN is supported by a large number of examples, reproducing all the analyses conducted in MLwiN's IGLS and MCMC manuals.",WOS:000389073500001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR MIXED MODELS', 'FITTING MULTILEVEL MODELS', 'WINBUGS']",R2MLwiN: A Package to Run MLwiN from within R,2016
1105,"Spatial statistics is a growing discipline providing important analytical techniques in a wide range of disciplines in the natural and social sciences. In the R package GWmodel, we present techniques from a particular branch of spatial statistics, termed geographically weighted (GW) models. GW models suit situations when data are not described well by some global model, but where there are spatial regions where a suitably localized calibration provides a better description. The approach uses a moving window weighting technique, where localized models are found at target locations. Outputs are mapped to provide a useful exploratory tool into the nature of the data spatial heterogeneity. Currently, GWmodel includes functions for: GW summary statistics, GW principal components analysis, GW regression, and GW discriminant analysis; some of which are provided in basic and robust forms.",WOS:000349847400001,JOURNAL OF STATISTICAL SOFTWARE,"['VARYING COEFFICIENT MODELS', 'DISCRIMINANT-ANALYSIS', 'PARAMETER SELECTION', 'REGRESSION-MODELS', 'NONSTATIONARITY', 'COLLINEARITY', 'PRICES', 'TOOLS', 'TESTS', 'LASSO']",GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models,2015
1106,"Longitudinal studies are often conducted to explore the cohort and age effects in many scientific areas. The within cluster correlation structure plays a very important role in longitudinal data analysis. This is because not only can an estimator be improved by incorporating the within cluster correlation structure into the estimation procedure, but also the within cluster correlation structure can sometimes provide valuable insights in practical problems. For example, it can reveal the correlation strengths among the impacts of various factors. Motivated by data typified by a set from Bangladesh pertinent to the use of contraceptives, we propose a random effect varying-coefficient model, and an estimation procedure for the within cluster correlation structure of the proposed model. The estimation procedure is optimization-free and the proposed estimators enjoy asymptotic normality under mild conditions. Simulations suggest that the proposed estimation is practicable for finite samples and resistent against mild forms of model m is specification. Finally, we analyze the data mentioned above with the new random effect varying-coefficient model together with the proposed estimation procedure, which reveals some interesting sociological dynamics.",WOS:000253077800023,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'MIXED-EFFECTS MODELS', 'LINEAR-MODELS', 'CLUSTERED DATA', 'REGRESSION']",Estimation of the covariance matrix of random effects in longitudinal studies,2007
1107,"We analyze the convergence rate of a simplified version of a popular Gibbs sampling method used for statistical discovery of gene regulatory binding motifs in DNA sequences. This sampler satisfies a very strong form of ergodicity (uniform). However, we show that, due to multimodality of the posterior distribution, the rate of convergence often decreases exponentially as a function of the length of the DNA sequence. Specifically, we show that this occurs whenever there is more than one true repeating pattern in the data. In practice there are typically multiple such patterns in biological data, the goal being to detect the most well-conserved and frequently-occurring of these. Our findings match empirical results, in which the motif-discovery Gibbs sampler has exhibited such poor convergence that it is used only for finding modes of the posterior distribution (candidate motifs) rather than for obtaining samples from that distribution. Ours are some of the first meaningful bounds on the convergence rate of a Markov chain method for sampling from a multimodal posterior distribution, as a function of statistical quantities like the number of observations.",WOS:000317451200004,ANNALS OF STATISTICS,"['MONTE-CARLO', 'GIBBS SAMPLER', 'GEOMETRIC ERGODICITY', 'MODEL', 'DISTRIBUTIONS', 'ESTIMATORS', 'ALGORITHMS', 'ALIGNMENT', 'PARALLEL']",CONVERGENCE RATE OF MARKOV CHAIN METHODS FOR GENOMIC MOTIF DISCOVERY,2013
1108,"Spatial microsimulation is a methodology aiming to simulate entities such as households, individuals or businesses in the finest possible scale. This process requires the use of individual based microdatasets. The package presented in this work facilitates the production of small area population microdata by combining various datasets such as census data and individual based datasets. This package includes a parallel implementation of random selection with optimization to select a group of individual records that match a macro description. This methodological approach has been used in a number of topics ranging from measuring inequalities in educational attainment (Kavroudakis, Ballas, and Birkin 2012) to estimating poverty at small area levels (Tanton, McNamara, Harding, and Morrison 2007). The development of the method over recent years is driving computational complexity to the edge as it uses modern computational approaches for the combination of data. The R package sms presented in this work uses parallel processing approaches for the efficient production of small area population microdata, which can be subsequently used for geographical analysis. Finally, a complete case study of fitting geographical data with the R package is presented and discussed.",WOS:000366013900001,JOURNAL OF STATISTICAL SOFTWARE,"['DATA-ABSTRACTION', 'INFORMATION', 'POLICIES', 'LESSONS', 'FUTURE', 'MODELS']",sms: Microdata for Geographical Analysis in R,2015
1109,"Gaussian process (GP) models are commonly used statistical metamodels for emulating expensive computer simulators. Fitting a GP model can be numerically unstable if any pair of design points in the input space are close together. Hanjan, Haynes, and Karsten (2011) proposed a computationally stable approach for fitting GP models to deterministic computer simulators. They used a genetic algorithm based approach that is robust but computationally intensive for maximizing the likelihood. This paper implements a slightly modified version of the model proposed by Ranjan el al. (2011) in the R package GPfit. A novel parameterization of the spatial correlation function and a clustering based multistart gradient based optimization algorithm yield robust optimization that is typically faster than the genetic algorithm based approach. We present two examples with R codes to illustrate the usage of the main functions in GPfit. Several test functions are used for performance comparison with the popular R package mlegp. We also use GPfit for a real application, i.e., for emulating the tidal kinetic energy model for the Bay of Fundy, Nova Scotia, Canada. GPfit is free software and distributed under the General Public License and available from the Comprehensive R Archive Network.",WOS:000352916200001,JOURNAL OF STATISTICAL SOFTWARE,"['OPTIMIZATION', 'REGRESSION']",GPfit: An R Package for Fitting a Gaussian Process Model to Deterministic Simulator Outputs,2015
1110,"We derive sharp performance bounds for least squares regression With L-1 regularization front parameter estimation accuracy and feature selection quality perspectives. The main result proved for L-1 regularization extends it similar result in [Ann. Statist. 35 (2007) 2313-2351] for the Dantzig selector. It gives an affirmative answer to an open question in [Ann. Statist. 35 (2007) 2358-2364]. Moreover, the result leads to an extended view of feature selection that allows less restrictive conditions than some recent work. Based on the theoretical insights, a novel two-stage L-1-regularization procedure with selective penalization is analyzed. It is shown that if the target parameter vector can be decomposed as the sum of a sparse parameter vector with large coefficients and another less sparse vector with relatively small coefficients, then the two-stage procedure can lead to improved performance.",WOS:000268604900002,ANNALS OF STATISTICS,"['DANTZIG SELECTOR', 'STATISTICAL ESTIMATION', 'LASSO', 'RECOVERY', 'LARGER']",SOME SHARP PERFORMANCE BOUNDS FOR LEAST SQUARES REGRESSION WITH L-1 REGULARIZATION,2009
1111,"Let (X(i))(i)=1,..., n be a possibly nonstationary sequence such that L(X(i)) = P(n), if i <= n theta and L(X(i)) = Q(n), if i > n theta, where 0 < theta < 1 is the location of the change-point to be estimated. We construct a class of estimators based on the empirical measures and a seminorm on the space of measures defined through a family of functions F. We prove the consistency of the estimator and give rates of convergence under very general conditions. In particular, the 1/n rate is achieved for a wide class of processes including long-range dependent sequences and even nonstationary ones. The approach unifies, generalizes and improves on the existing results for both parametric and nonparametric change-point estimation, applied to independent, Short-range dependent and as well long-range dependent sequences.",WOS:000249568000018,ANNALS OF STATISTICS,,Optimal rate of convergence for nonparametric change-point estimators for nonstationary sequences,2007
1112,"BARD is the first (and at time of writing, only) open source software package for general redistricting and redistricting analysis. BARD provides methods to create, display, compare, edit, automatically refine, evaluate, and profile political districting plans. BARD aims to provide a framework for scientific analysis of redistricting plans and to facilitate wider public participation in the creation of new plans.
BARD facilitates map creation and refinement through command-line, graphical user interface, and automatic methods. Since redistricting is a computationally complex partitioning problem not amenable to an exact optimization solution, BARD implements a variety of selectable metaheuristics that can be used to refine existing or randomly-generated redistricting plans based on user-determined criteria.
Furthermore, BARD supports automated generation of redistricting plans and profiling of plans by assigning different weights to various criteria, such as district compactness or equality of population. This functionality permits exploration of trade-offs among criteria. The intent of a redistricting authority may be explored by examining these trade-offs and inferring which reasonably observable plans were not adopted.
Redistricting is a computationally-intensive problem for even modest-sized states. Performance is thus an important consideration in BARD's design and implementation. The program implements performance enhancements such as evaluation caching, explicit memory management, and distributed computing across snow clusters.",WOS:000292096900001,JOURNAL OF STATISTICAL SOFTWARE,"['OPTIMIZATION', 'EVOLUTION']",BARD: Better Automated Redistricting,2011
1113,"In many situations, data are recorded over a period of time and may be regarded as realizations of a stochastic process. In this paper, robust estimators for the principal components are considered by adapting the projection pursuit approach to the functional data setting. Our approach combines robust projection-pursuit with different smoothing methods. Consistency of the estimators are shown under mild assumptions. The performance of the classical and robust procedures are compared in a simulation study under different contamination schemes.",WOS:000300383200003,ANNALS OF STATISTICS,"['DISTRIBUTIONS', 'ESTIMATORS', 'MATRICES', 'SCALE']",ROBUST FUNCTIONAL PRINCIPAL COMPONENTS: A PROJECTION-PURSUIT APPROACH,2011
1114,"In many contexts, confidentiality constraints severely restrict access to unique and valuable microdata. Synthetic data which mimic the original observed data and preserve the relationships between variables but do not contain any disclosive records are one possible solution to this problem. The synthpop package for R, introduced in this paper, provides routines to generate synthetic versions of original data sets. We describe the methodology and its consequences for the data characteristics. We illustrate the package features using a survey data example.",WOS:000392514800001,JOURNAL OF STATISTICAL SOFTWARE,"['MICRODATA', 'IMPUTATION']",synthpop: Bespoke Creation of Synthetic Data in R,2016
1115,"The R package multgee implements the local odds ratios generalized estimating equations (GEE) approach proposed by Touloumis et al. (2013), a GEE approach for correlated multinomial responses that circumvents theoretical and practical limitations of the GEE method. A main strength of multgee is that it provides GEE routines for both ordinal (ordLORgee) and nominal (nomLORgee) responses, while relevant softwares in R and SAS are restricted to ordinal responses under a marginal cumulative link model specification. In addition, multgee offers a marginal adjacent categories logit model for ordinal responses and a marginal baseline category logit model for nominal. Further, utility functions are available to ease the local odds ratios structure selection (intrinsic.pars) and to perform a Wald type goodness-of-fit test between two nested GEE models (waldts). We demonstrate the application of multgee through a clinical trial with clustered ordinal multinomial responses.",WOS:000352914800001,JOURNAL OF STATISTICAL SOFTWARE,"['CONTINGENCY-TABLES', 'ASSOCIATION MODELS', 'REGRESSION-MODELS', 'LEAST-SQUARES', 'ORDINAL DATA']",R Package multgee: A Generalized Estimating Equations Solver for Multinomial Response,2015
1116,"The research of developing a general methodology for the construction of good nonregular designs has been very active in the last decade. Recent research by Xu and Wong [Statist. Sinica 17 (2007) 1191-1213] suggested a new class of nonregular designs Constructed from quaternary codes. This paper explores the properties and uses Of quaternary codes toward the construction of quarter-fraction nonregular designs. Some theoretical results are obtained regarding the aliasing structure Of Such designs. Optimal designs are constructed under the maximum resolution, minimum aberration and maximum projectivity criteria. These designs often have larger generalized resolution and larger projectivity than regular designs of the same size. It is further shown that some of these designs have generalized minimum aberration and maximum projectivity among all possible designs.",WOS:000268604900018,ANNALS OF STATISTICS,"['GENERALIZED MINIMUM ABERRATION', 'ORTHOGONAL ARRAYS', 'NONREGULAR DESIGNS', 'MOMENT ABERRATION', 'G(2)-ABERRATION', 'CLASSIFICATION', 'PROJECTION', 'CRITERIA']",QUARTER-FRACTION FACTORIAL DESIGNS CONSTRUCTED VIA QUATERNARY CODES,2009
1117,"This article studies the estimation of the causal effect of a time-varying treatment on time-to-an-event or on some other continuously distributed outcome. The paper applies to the situation where treatment is repeatedly adapted to time-dependent patient characteristics. The treatment effect cannot be estimated by simply conditioning on these time-dependent patient characteristics, as they may themselves be indications of the treatment effect. This time-dependent confounding is common in observational studies. Robins [(1992) Biometrika 79 321-334, (1998b) Encyclopedia of Biostatistics 6 4372-4389] has proposed the so-called structural nested models to estimate treatment effects in the presence of time-dependent confounding. In this article we provide a conceptual framework and formalization for structural nested models in continuous time. We show that the resulting estimators are consistent and asymptotically normal. Moreover, as conjectured in Robins [(1998b) Encyclopedia of Biostatistics 6 4372-4389], a test for whether treatment affects the outcome of interest can be performed without specifying a model for treatment effect. We illustrate the ideas in this article with an example.",WOS:000256504400017,ANNALS OF STATISTICS,"['CONFOUNDING FACTORS', 'INFERENCE', 'SURVIVAL']",Statistical modeling of causal effects in continuous time,2008
1118,"This paper is concerned with the selection and estimation of fixed and random effects in linear mixed effects models. We propose a class of nonconcave penalized profile likelihood methods for selecting and estimating important fixed effects. To overcome the difficulty of unknown covariance matrix of random effects, we propose to use a proxy matrix in the penalized profile likelihood. We establish conditions on the choice of the proxy matrix and show that the proposed procedure enjoys the model selection consistency where the number of fixed effects is allowed to grow exponentially with the sample size. We further propose a group variable selection strategy to simultaneously select and estimate important random effects, where the unknown covariance matrix of random effects is replaced with a proxy matrix. We prove that, with the proxy matrix appropriately chosen, the proposed procedure can identify all true random effects with asymptotic probability one, where the dimension of random effects vector is allowed to increase exponentially with the sample size. Monte Carlo simulation studies are conducted to examine the finite-sample performance of the proposed procedures. We further illustrate the proposed procedures via a real data example.",WOS:000312899000010,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'GENERALIZED INFORMATION CRITERION', 'VARYING-COEFFICIENT MODELS', 'ORACLE PROPERTIES', 'LONGITUDINAL DATA', 'DIVERGING NUMBER', 'ELASTIC-NET', 'REGRESSION', 'REGULARIZATION', 'CLASSIFICATION']",VARIABLE SELECTION IN LINEAR MIXED EFFECTS MODELS,2012
1119,"We consider the problem of detecting whether or not, in a given sensor network, there is a cluster of sensors which exhibit an ""unusual behavior."" Formally, suppose we are given a set of nodes and attach a random variable to each node. We observe a realization of this process and want to decide between the following two hypotheses: under the null, the variables are i.i.d. standard normal; under the alternative, there is a cluster of variables that are i.i.d. normal with positive mean and unit variance, while the rest are i.i.d. standard normal. We also address surveillance settings where each sensor in the network collects information over time. The resulting model is similar, now with a time series attached to each node. We again observe the process over time and want to decide between the null, where all the variables are i.i.d. standard normal, and the alternative, where there is an emerging cluster of i.i.d. normal variables with positive mean and unit variance. The growth models used to represent the emerging cluster are quite general and, in particular, include cellular automata used in modeling epidemics. In both settings, we consider classes of clusters that are quite general, for which we obtain a lower bound on their respective minimax detection rate and show that some form of scan statistic, by far the most popular method in practice, achieves that same rate to within a logarithmic factor. Our results are not limited to the normal location model, but generalize to any one-parameter exponential family when the anomalous clusters are large enough.",WOS:000288183800009,ANNALS OF STATISTICS,"['SENSOR NETWORKS', 'DEFORMABLE TEMPLATE', 'DISEASE SURVEILLANCE', 'HIGHER CRITICISM', 'SCAN STATISTICS', 'RANDOM GROWTH', 'TRACKING', 'CLASSIFICATION', 'SIGNALS', 'MAXIMA']",DETECTION OF AN ANOMALOUS CLUSTER IN A NETWORK,2011
1120,"We study estimation of a multivariate function f : R-d -> R when the observations are available from the function Af. where A is a known linear operator. Both the Gaussian white noise model and density estimation are studied. We define an L-2-empirical risk functional which is used to define a delta-net minimizer and a dense empirical risk minimizer. Upper bounds for the mean integrated squared error of the estimators are given. The upper bounds show how the difficulty of the estimation depends on the operator through the norm of the adjoint of the inverse of the operator and on the underlying function class through the entropy of the class. Corresponding lower bounds are also derived. As examples, we consider convolution operators and the Radon transform. In these examples, the estimators achieve the optimal rates of convergence. Furthermore, a new type of oracle inequality is given for inverse problems in additive models.",WOS:000273800100014,ANNALS OF STATISTICS,"['CONVERGENCE', 'RATES', 'POINTWISE']",EMPIRICAL RISK MINIMIZATION IN INVERSE PROBLEMS,2010
1121,"Extracting useful information from high-dimensional data is an important focus of today's statistical research and practice. Penalized loss function minimization has been shown to be effective for this task both theoretically and empirically. With the virtues of both regularization and sparsity, the L-1-penalized squared error minimization method Lasso has been popular in regression models and beyond.
In this paper, we combine different norms including L-1 to form an intelligent penalty in order to add side information to the fitting of a regression or classification model to obtain reasonable estimates. Specifically, we introduce the Composite Absolute Penalties (CAP) family, which allows given grouping and hierarchical relationships between the predictors to be expressed. CAP penalties are built by defining groups and combining the properties of norm penalties at the across-group and within-group levels. Grouped selection occurs for nonoverlapping groups. Hierarchical variable selection is reached by defining groups with particular overlapping patterns. We propose using the BLASSO and cross-validation to compute CAP estimates in general. For a subfamily of CAP estimates involving only the L-1 and L-proportional to norms, we introduce the iCAP algorithm to trace the entire regularization path for the grouped selection problem. Within this subfamily, unbiased estimates of the degrees of freedom (df) are derived so that the regularization parameter is selected without cross-validation. CAP is shown to improve on the predictive performance of the LASSO in a series of simulated experiments, including cases with p >> n and possibly mis-specified groupings. When the complexity of a model is properly calculated, iCAP is seen to be parsimonious in the experiments.",WOS:000271673500013,ANNALS OF STATISTICS,"['REGRESSION', 'LASSO', 'PREDICTION', 'SHRINKAGE', 'MODEL']",THE COMPOSITE ABSOLUTE PENALTIES FAMILY FOR GROUPED AND HIERARCHICAL VARIABLE SELECTION,2009
1122,"We extend the approach in [Ann. Statist. 38 (2010) 2499-2524] for identifying locally optimal designs for nonlinear models. Conceptually the extension is relatively simple, but the consequences in terms of applications are profound. As we will demonstrate, we can obtain results for locally optimal designs under many optimality criteria and for a larger class of models than has been done hitherto. In many cases the results lead to optimal designs with the minimal number of support points.",WOS:000310650900014,ANNALS OF STATISTICS,['LA GARZA PHENOMENON'],IDENTIFYING LOCALLY OPTIMAL DESIGNS FOR NONLINEAR MODELS: A SIMPLE EXTENSION WITH PROFOUND CONSEQUENCES,2012
1123,"This article discusses and describes SSMMATLAB, a set of programs written by the author in MATLAB for the statistical analysis of state space models. The state space model considered is very general. It may have univariate or multivariate observations, time-varying system matrices, exogenous inputs, regression effects, incompletely specified initial conditions, such as those that arise with cointegrated VARMA models, and missing values. There are functions to put frequently used models, such as multiplicative VARMA models, VARMAX models in echelon form, cointegrated VARMA models, and univariate structural or ARIMA model-based unobserved components models, into state space form. There are also functions to implement the Hillmer-Tiao canonical decomposition and the smooth trend and cycle estimation proposed by Gomez (2001). Once the model is in state space form, other functions can be used for likelihood evaluation, model estimation, forecasting and smoothing. A set of examples is presented in the SSMMATLAB manual to illustrate the use of these functions.",WOS:000365978800001,JOURNAL OF STATISTICAL SOFTWARE,"['TIME-SERIES', 'SOFTWARE']",SSMMATLAB: A Set of MATLAB Programs for the Statistical Analysis of State Space Models,2015
1124,"Orthogonal fractional factorial designs (OFFDs) are frequently used in many fields of application, including medicine, engineering and agriculture. In this paper we present a software tool for the generation of minimum size OFFDs. The software, that has been implemented in SAS/IML, puts neither a restriction on the number of levels of each factor nor on the orthogonality constraints. The algorithm is based on the joint use of polynomial counting function and complex coding of levels and follows the approach presented in Fontana (2013).",WOS:000320041000001,JOURNAL OF STATISTICAL SOFTWARE,,Minimum-Size Mixed-Level Orthogonal Fractional Factorial Designs Generation: A SAS-Based Algorithm,2013
1125,"Jacquez's Q is a set of statistics for detecting the presence and location of space-time clusters of disease exposure. Until now, the only implementation was available in the proprietary SpaceStat software which is not suitable for a pipeline Linux environment. We have developed an open source implementation of Jacquez's Q statistics in Python using an object-oriented approach. The most recent source code for the implementation is available at https://github.com/sjirjies/pyJacqQ under the GPL-3. It has a command line interface and a Python application programming interface.",WOS:000392513700001,JOURNAL OF STATISTICAL SOFTWARE,,pyJacqQ: Python Implementation of Jacquez's Q-Statistics for Space-Time Clustering of Disease Exposure in Case-Control Studies,2016
1126,"Rchoice is a package in R for estimating models with individual heterogeneity for both cross-sectional and panel (longitudinal) data. In particular, the package allows binary, ordinal and count response, as well as continuous and discrete covariates. Individual heterogeneity is modeled by allowing the parameter associated with each observed variable (e.g., its coefficient) to vary randomly across individuals according to some pre-specified distribution. Simulated maximum likelihood method is implemented for the estimation of the moments of the distributions. In addition, functions for plotting the conditional individual-specific coefficients and their confidence interval are provided. This article is a general description of Rchoice and all functionalities are illustrated using real databases.",WOS:000392514600001,JOURNAL OF STATISTICAL SOFTWARE,"['SIMULATED LIKELIHOOD ESTIMATION', 'LOGIT MODEL', 'HETEROGENEITY']",Discrete Choice Models with Random Parameters in R: The Rchoice Package,2016
1127,"Package distrMod provides an object oriented (more specifically S4-style) implementation of probability models. Moreover, it contains functions and methods to compute minimum criterion estimators - in particular, maximum likelihood and minimum distance estimators.",WOS:000281588200001,JOURNAL OF STATISTICAL SOFTWARE,,R Package distrMod: S4 Classes and Methods for Probability Models,2010
1128,"We propose a new algorithm for computing extreme probabilities of Kolmogorov's goodness-of-fit measure, D-n. This algorithm is an improved version of the method originally proposed by Wang, Tsang, and Marsaglia (2003) based on a result from Durbin (1973). The new algorithm keeps the same numerical precision of the Wang et al. (2003) method, but is more efficient: it features linear instead of quadratic space complexity and has better time complexity for a common range of input parameters of practical importance. The proposed method is implemented in the R package kolmim, which also includes an improved routine to perform one-sample two-sided exact Kolmogorv-Smirnov tests.",WOS:000365975800001,JOURNAL OF STATISTICAL SOFTWARE,,An Improved Evaluation of Kolmogorov's Distribution,2015
1129,"The WeDiBaDis package provides a user friendly environment to perform discriminant analysis (supervised classification). WeDiBaDis is an easy to use package addressed to the biological and medical communities, and in general, to researchers interested in applied studies. It can be suitable when the user is interested in the problem of constructing a discriminant rule on the basis of distances between a relatively small number of instances or units of known unbalanced-class membership measured on many (possibly thousands) features of any type. This is a current situation when analyzing genetic biomedical data. This discriminant rule can then be used both, as a means of explaining differences among classes, but also in the important task of assigning the class membership for new unlabeled units. Our package implements two discriminant analysis procedures in an R environment: the well-known distance-based discriminant analysis (DB-discriminant) and a weighted-distance-based discriminant (WDB-discriminant), a novel classifier rule that we introduce. This new procedure is based on an improvement of the DB rule taking into account the statistical depth of the units. This article presents both classifying procedures and describes the implementation of each in detail. We illustrate the use of the package using an ecological and a genetic experimental example. Finally, we illustrate the effectiveness of the new proposed procedure (WDB), as compared with DB. This comparison is carried out using thirty-eight, high-dimensional, class-unbalanced, cancer data sets, three of which include clinical features.",WOS:000395669800029,R JOURNAL,"['DATA DEPTH', 'GENE-EXPRESSION', 'DROSOPHILA-SUBOBSCURA', 'CLASSIFICATION', 'POPULATIONS', 'COEFFICIENT', 'CLASSIFIERS', 'PREDICTION', 'AGREEMENT', 'CANCER']",Weighted Distance Based Discriminant Analysis: The R Package WeDiBaDis,2016
1130,"The ability to implement statistical models in the BUGS language facilitates Bayesian inference by automating MCMC algorithms. Software packages that interpret the BUGS language include OpenBUGS, WinBUGS, and JAGS. R packages that link BUGS software to the R environment, including rjags and R2WinBUGS, are widely used in Bayesian analysis. Indeed, many packages in the Bayesian task view on CRAN (http://cran.rproject.org/web/views/Bayesian.html) depend on this integration. However, the R and BUGS languages use different representations of common probability density functions, creating a potential for errors to occur in the implementation or interpretation of analyses that use both languages. Here we review different parameterizations used by the R and BUGS languages, describe how to translate between the languages, and provide an R function, r2bugs.distributions, that transforms parameterizations from R to BUGS and back again.",WOS:000321944400021,R JOURNAL,,Translating Probability Density Functions: From R to BUGS and Back Again,2013
1131,"Genotyping platforms such as Affymetrix can be used to assess genotype-phenotype as well as copy number-phenotype associations at millions of markers. While genotyping algorithms are largely concordant when assessed on HapMap samples, tools to assess copy number changes are more variable and often discordant. One explanation for the discordance is that copy number estimates are susceptible to systematic differences between groups of samples that were processed at different times or by different labs. Analysis algorithms that do not adjust for batch effects are prone to spurious measures of association. The R package crlmm implements a multilevel model that adjusts for batch effects and provides allele-specific estimates of copy number. This paper illustrates a workfow for the estimation of allele-specific copy number and integration of the marker-level estimates with complimentary Bioconductor software for inferring regions of copy number gain or loss. All analyses are performed in the statistical environment R",WOS:000290086900001,JOURNAL OF STATISTICAL SOFTWARE,"['ARRAY CGH DATA', 'CIRCULAR BINARY SEGMENTATION', 'HIDDEN MARKOV-MODELS', 'SNP ARRAYS', 'GENOME', 'ALGORITHM', 'NORMALIZATION', 'EXPLORATION', 'SOFTWARE', 'IMPACT']",Using the R Package crlmm for Genotyping and Copy Number Estimation,2011
1132,"We study the maximum smoothed likelihood estimator (MSLE) for interval censoring, case 2, in the so-called separated case. Characterizations in terms of convex duality conditions are given and strong consistency is proved. Moreover, we show that, under smoothness conditions on the underlying distributions and using the usual bandwidth choice in density estimation, the local convergence rate is n(-2/5) and the limit distribution is normal, in contrast with the rate n(-1/3) of the ordinary maximum likelihood estimator.",WOS:000344632400014,ANNALS OF STATISTICS,"['ASYMPTOTICALLY OPTIMAL ESTIMATION', 'FUNCTIONALS']",MAXIMUM SMOOTHED LIKELIHOOD ESTIMATORS FOR THE INTERVAL CENSORING MODEL,2014
1133,"Units sampled from finite populations typically come with different inclusion probabilities. Together with additional preprocessing steps of the raw data, this yields unequal sampling weights of the observations. Whenever indicators are estimated from such complex samples, the corresponding sampling weights have to be taken into account. In addition, many indicators suffer from a strong influence of outliers, which are a common problem in real-world data. The R package laeken is an object-oriented toolkit for the estimation of indicators from complex survey samples via standard or robust methods. In particular the most widely used social exclusion and poverty indicators are implemented in the package. A general calibrated bootstrap method to estimate the variance of indicators for common survey designs is included as well. Furthermore, the package contains synthetically generated close-to-reality data for the European Union Statistics on Income and Living Conditions and the Structure of Earnings Survey, which are used in the code examples throughout the paper. Even though the paper is focused on showing the functionality of package laeken, it also provides a brief mathematical description of the implemented indicator methodology.",WOS:000324372900001,JOURNAL OF STATISTICAL SOFTWARE,"['TAIL INDEX', 'REGRESSION']",Estimation of Social Exclusion Indicators from Complex Surveys: The R Package laeken,2013
1134,The density function of the limiting spectral distribution of general sample covariance matrices is usually unknown. We propose to use kernel estimators which are proved to be consistent. A simulation study is also conducted to show the performance of the estimators.,WOS:000290231500012,ANNALS OF STATISTICS,"['DIMENSIONAL RANDOM MATRICES', 'CONVERGENCE', 'PROBABILITY', 'EIGENVALUES', 'LAW']",NONPARAMETRIC ESTIMATE OF SPECTRAL DENSITY FUNCTIONS OF SAMPLE COVARIANCE MATRICES: A FIRST STEP,2010
1135,"Interest in social network analysis has exploded in the past few years, partly thanks to the advancements in statistical methods and computing for network analysis. A wide range of the methods for network analysis is already covered by existent R packages. However, no comprehensive packages are available to calculate group centrality scores and to identify key players (i.e., those players who constitute the most central group) in a network. These functionalities are important because, for example, many social and health interventions rely on key players to facilitate the intervention. Identifying key players is challenging because players who are individually the most central are not necessarily the most central as a group due to redundancy in their connections. In this paper we develop methods and tools for computing group centrality scores and for identifying key players in social networks. We illustrate the methods using both simulated and empirical examples. The package keyplayer providing the presented methods is available from Comprehensive R Archive Network (CRAN).",WOS:000385276100019,R JOURNAL,"['OPINION LEADERS', 'INTERVENTION']",keyplayer: An R Package for Locating Key Players in Social Networks,2016
1136,"Efron [Biometrika 58 (1971) 403-417] developed a restricted randomization procedure to promote balance between two treatment groups in a sequential clinical trial. He called this the biased coin design. He also introduced the concept of accidental bias, and investigated properties of the procedure with respect to both accidental and selection bias, balance, and randomization-based inference using the steady-state properties of the induced Markov chain. In this paper we revisit this procedure, and derive closed-form expressions for the exact properties of the measures derived asymptotically in Efron's paper. In particular, we derive the exact distribution of the treatment imbalance and the variance-covariance matrix of the treatment assignments. These results have application in the design and analysis of clinical trials, by providing exact formulas to determine the role of the coin's bias probability in the context of selection and accidental bias, balancing properties and randomization-based inference.",WOS:000277471000009,ANNALS OF STATISTICS,"['CLINICAL-TRIALS', 'DESIGN', 'ALLOCATION']",EXACT PROPERTIES OF EFRON'S BIASED COIN RANDOMIZATION PROCEDURE,2010
1137,"Numerical optimization is often an essential aspect of mathematical analysis in science, technology and other areas. The function optim() provides basic optimization capabilities and is among the most widely used functions in R. Additionally, there are various packages and functions for solving various types of optimization problem (the optimization task view on Comprehensive R Archive Network provides a comprehensive list of available options for solving optimization problems in R). In this special volume, four papers are presented which discuss some of the areas in numerical optimization where significant developments have been made recently to enhance the capabilities in R. This introduction provides a brief overview of the volume.",WOS:000345288200001,JOURNAL OF STATISTICAL SOFTWARE,['SYSTEM'],Numerical Optimization in R: Beyond optim,2014
1138,"This paper introduces the R package threg, which implements the estimation procedure of a threshold regression model, which is based on the first-hitting-time of a boundary by the sample path of a Wiener diffusion process. The threshold regression methodology is well suited to applications involving survival and time-to-event data, and serves as an important alternative to the Cox proportional hazards model.
This new package includes four functions: threg, and the methods hr, predict and plot for `threg' objects returned by threg. The threg function is the model-fitting function which is used to calculate regression coefficient estimates, asymptotic standard errors and p values. The hr method for `threg' objects is the hazard-ratio calculation function which provides the estimates of hazard ratios at selected time points for specified scenarios (based on given categories or value settings of covariates). The predict method for `threg objects is used for prediction. And the plot method for `threg' objects provides plots for curves of estimated hazard functions, survival functions and probability density functions of the first-hitting-time; function curves corresponding to different scenarios can be overlaid in the same plot for comparison to give additional research insights.",WOS:000365978600001,JOURNAL OF STATISTICAL SOFTWARE,,The R Package threg to Implement Threshold Regression Models,2015
1139,"In many medical studies, patients can experience several events. The times between consecutive events (gap times) are often of interest and lead to problems that have received much attention recently. In this work we consider the estimation of the bivariate distribution function for censored gap times, using survivalBIV a software application for R. Some related problems such as the estimation of the marginal distribution of the second gap time is also discussed. It describes the capabilities of the program for estimating these quantities using four different approaches, all using the Kaplan-Meier estimator of survival. One of these estimators is based on Bayes' theorem and Kaplan-Meier survival function. Two estimators were recently proposed using the Kaplan-Meier estimator pertaining to the distribution of the total time to weight the bivariate data (de Una-Alvarez and Meira-Machado 2008 and de Una-Alvarez and Amorim 2011). The software can also be used to implement the estimator proposed in Lin, Sun, and Ying (1999), which is based on inverse probability of censoring weighted. The software is illustrated using data from a bladder cancer study.",WOS:000301231800001,JOURNAL OF STATISTICAL SOFTWARE,"['NONPARAMETRIC-ESTIMATION', 'MODELS']",survivalBIV: Estimation of the Bivariate Distribution Function for Sequentially Ordered Events Under Univariate Censoring,2012
1140,"Propensity score analysis is a technique for adjusting for selection bias in observational data. Estimated propensity scores (probability of treatment given observed covariates) are used for stratification of observations. Within strata covariates should be more balanced between the two treatments than without the stratification. PSA graphics is an R package that provides flexible graphical tools to assess within strata balance between treatment groups, as well as how covariate distributions differ across strata. Additional graphical tools facilitate estimation of treatment effects having adjusted for covariate differences. Several new and conventional numerical measures of balance are also provided.",WOS:000263825000001,JOURNAL OF STATISTICAL SOFTWARE,,PSAgraphics: An R Package to Support Propensity Score Analysis,2009
1141,,WOS:000262731400001,ANNALS OF STATISTICS,,RANDOM MATRIX THEORY: A PROGRAM OF THE STATISTICS AND APPLIED MATHEMATICAL SCIENCES INSTITUTE (SAMSI),2008
1142,"Consider M-estimation in a semiparametric model that is characterized by a Euclidean parameter of interest and an infinite-dimensional nuisance parameter. As a general purpose approach to statistical inferences, the bootstrap has found wide applications in semiparametric M-estimation and, because of its simplicity, provides an attractive alternative to the inference approach based on the asymptotic distribution theory. The purpose of this paper is to provide theoretical justifications for the use of bootstrap as a semiparametric inferential tool. We show that, under general conditions, the bootstrap is asymptotically consistent in estimating the distribution of the M-estimate of Euclidean parameter; that is, the bootstrap distribution asymptotically imitates the distribution of the M-estimate. We also show that the bootstrap confidence set has the asymptotically correct coverage probability. These general conclusions hold, in particular, when the nuisance parameter is not estimable at root-n rate, and apply to a broad class of bootstrap methods with exchangeable bootstrap weights. This paper provides a first general theoretical study of the bootstrap in semiparametric models.",WOS:000282402800010,ANNALS OF STATISTICS,"['REGRESSION-MODELS', 'LIKELIHOOD-ESTIMATION', 'BAYESIAN BOOTSTRAP', 'PROFILE SAMPLER', 'CENSORED-DATA', 'INFERENCE', 'EQUATIONS']",BOOTSTRAP CONSISTENCY FOR GENERAL SEMIPARAMETRIC M-ESTIMATION,2010
1143,"We consider the optimal design problem for a comparison of two regression curves, which is used to establish the similarity between the dose response relationships of two groups. An optimal pair of designs minimizes the width of the confidence band for the difference between the two regression functions. Optimal design theory (equivalence theorems, efficiency bounds) is developed for this non-standard design problem and for some commonly used dose response models optimal designs are found explicitly. The results are illustrated in several examples modeling dose response relationships. It is demonstrated that the optimal pair of designs for the comparison of the regression curves is not the pair of the optimal designs for the individual models. In particular, it is shown that the use of the optimal designs proposed in this paper instead of commonly used ""non-optimal"" designs yields a reduction of the width of the confidence band by more than 50%.",WOS:000375175200008,ANNALS OF STATISTICS,"['WEIGHTED POLYNOMIAL REGRESSION', 'LA GARZA PHENOMENON', 'MODELS', 'ALGORITHM', 'CONVERGENCE', 'EQUIVALENCE', 'EFFICIENCY']",OPTIMAL DESIGNS FOR COMPARING CURVES,2016
1144,"Estimating covariance matrices is a problem of fundamental importance in multivariate statistics. In practice it is increasingly frequent to work with data matrices X of dimension if x p, where p and n are both large. Results from random matrix theory show very clearly that in this setting, standard estimators like the sample covariance matrix perform in general very poorly.
In this ""large n, large p"" setting, it is sometimes the case that practitioners are willing to assume that many elements of the population covariance matrix are equal to 0, and hence this matrix is sparse. We develop an estimator to handle this situation. The estimator is shown to be consistent in operator norm, when, for instance, we have p asymptotic to n as n -> infinity. In other words the largest singular value of the difference between the estimator and the population covariance matrix goes to zero. This implies consistency of all the eigenvalues and consistency of eigenspaces associated to isolated eigenvalues.
We also propose a notion of sparsity for matrices, that is, ""compatible"" with spectral analysis and is independent of the ordering of the variables.",WOS:000262731400006,ANNALS OF STATISTICS,"['LIMIT', 'EIGENVALUES', 'SELECTION', 'CLT']",OPERATOR NORM CONSISTENT ESTIMATION OF LARGE-DIMENSIONAL SPARSE COVARIANCE MATRICES,2008
1145,"We perform a finite sample analysis of the detection levels for sparse principal components of a high-dimensional covariance matrix. Our minimax optimal test is based on a sparse eigenvalue statistic. Alas, computing this test is known to be NP-complete in general, and we describe a computationally efficient alternative test using convex relaxations. Our relaxation is also proved to detect sparse principal components at near optimal detection levels, and it performs well on simulated datasets. Moreover, using polynomial time reductions from theoretical computer science, we bring significant evidence that our results cannot be improved, thus revealing an inherent trade off between statistical and computational performance.",WOS:000326991200004,ANNALS OF STATISTICS,"['SAMPLE COVARIANCE MATRICES', 'LARGE HIDDEN CLIQUE', 'LARGEST EIGENVALUE', 'HIGHER CRITICISM', 'GRAPH', 'NORM', 'SIZE', 'PCA', 'INDEPENDENCE', 'SUBMATRICES']",OPTIMAL DETECTION OF SPARSE PRINCIPAL COMPONENTS IN HIGH DIMENSION,2013
1146,"This paper provides closed-form expansions for the log-likelihood function of multivariate diffusions sampled at discrete time intervals. The coefficients of the expansion are calculated explicitly by exploiting the special structure afforded by the diffusion model. Examples of interest in financial statistics and Monte Carlo evidence are included, along with the convergence of the expansion to the true likelihood function.",WOS:000254502700015,ANNALS OF STATISTICS,"['DISCRETELY SAMPLED DIFFUSIONS', 'APPROXIMATION APPROACH', 'CONTINGENT CLAIMS', 'TERM STRUCTURE', 'MODELS']",Closed-form likelihood expansions for multivariate diffusions,2008
1147,"Quantiles play a fundamental role in statistics. The quantile function defines the distribution of a random variable and, thus, provides a way to describe the data that is specular but equivalent to that given by the corresponding cumulative distribution function. There are many advantages in working with quantiles, starting from their properties. The renewed interest in their usage seen in the last years is due to the theoretical, methodological, and software contributions that have broadened their applicability. This paper presents the R package Qtools, a collection of utilities for unconditional and conditional quantiles.",WOS:000395669800009,R JOURNAL,"['REGRESSION QUANTILES', 'ASYMPTOTIC PROPERTIES', 'SAMPLE QUANTILES', 'OF-FIT', 'TRANSFORMATION', 'PROBABILITY', 'KURTOSIS', 'DISTRIBUTIONS', 'SKEWNESS', 'CURVES']",Qtools: A Collection of Models and Tools for Quantile Inference,2016
1148,"The interference model has been widely used and studied in block experiments where the treatment for a particular plot has effects on its neighbor plots. In this paper, we study optimal circular designs for the proportional interference model, in which the neighbor effects of a treatment are proportional to its direct effect. Kiefer's equivalence theorems for estimating both the direct and total treatment effects are developed with respect to the criteria of A, D, E and T. Parallel studies are carried out for the undirectional model, where the neighbor effects do not depend on whether they are from the left or right. Moreover, the connection between optimal designs for the directional and undiretional models is built. Importantly, one can easily develop a computer program for finding optimal designs based on these theorems.",WOS:000357441000012,ANNALS OF STATISTICS,"['NEIGHBOR-BALANCED DESIGNS', 'OPTIMAL CROSSOVER DESIGNS', 'UNIVERSALLY OPTIMAL DESIGNS', 'CORRELATED OBSERVATIONS', 'COMPETITION', 'ADJUSTMENT', 'TRIALS']",OPTIMAL DESIGNS FOR THE PROPORTIONAL INTERFERENCE MODEL,2015
1149,"We introduce an R package, bspmma, which implements a Dirichlet-based random effects model specific to meta-analysis. In meta-analysis, when combining effect estimates from several heterogeneous studies, it is common to use a random-effects model. The usual frequentist or Bayesian models specify a normal distribution for the true effects. However, in many situations, the effect distribution is not normal, e. g., it can have thick tails, be skewed, or be multi-modal. A Bayesian nonparametric model based on mixtures of Dirichlet process priors has been proposed in the literature, for the purpose of accommodating the non-normality. We review this model and then describe a competitor, a semiparametric version which has the feature that it allows for a well-defined centrality parameter convenient for determining whether the overall effect is significant. This second Bayesian model is based on a different version of the Dirichlet process prior, and we call it the ""conditional Dirichlet model."" The package contains functions to carry out analyses based on either the ordinary or the conditional Dirichlet model, functions for calculating certain Bayes factors that provide a check on the appropriateness of the conditional Dirichlet model, and functions that enable an empirical Bayes selection of the precision parameter of the Dirichlet process. We illustrate the use of the package on two examples, and give an interpretation of the results in these two different scenarios.",WOS:000306914300001,JOURNAL OF STATISTICAL SOFTWARE,"['NONSTEROIDAL ANTIINFLAMMATORY DRUGS', 'BREAST-CANCER', 'NONPARAMETRIC PROBLEMS', 'PRIORS', 'INFERENCE', 'MIXTURES', 'ASPIRIN', 'TRIALS', 'RISK']",bspmma: An R Package for Bayesian Semiparametric Models for Meta-Analysis,2012
1150,"We consider full Bayesian inference in the multivariate normal mean model in the situation that the mean vector is sparse. The prior distribution on the vector of means is constructed hierarchically by first choosing a collection of nonzero means and next a prior on the nonzero values. We consider the posterior distribution in the frequentist set-up that the observations are generated according to a fixed mean vector, and are interested in the posterior distribution of the number of nonzero components and the contraction of the posterior distribution to the true mean vector. We find various combinations of priors on the number of nonzero coefficients and on these coefficients that give desirable performance. We also find priors that give suboptimal convergence, for instance, Gaussian priors on the nonzero coefficients. We illustrate the results by simulations.",WOS:000312899000011,ANNALS OF STATISTICS,"['BAYES VARIABLE SELECTION', 'EMPIRICAL-BAYES', 'DANTZIG SELECTOR', 'ADAPTIVE LASSO', 'REGRESSION', 'MODELS']",NEEDLES AND STRAW IN A HAYSTACK: POSTERIOR CONCENTRATION FOR POSSIBLY SPARSE SEQUENCES,2012
1151,"This paper focuses on comparing the survival outcomes of two treatments while adjusting for multiple demographic and clinical factors. We wish to answer the question: ""At what times do the survival outcomes of two treatments differ?"". Previous work by Zhang and Klein (2001) addressed this issue by considering a stratified Cox model and by constructing the confidence band for the differences between survival functions of treatments, for a given subject. In this study, we utilize direct adjusted survivals of treatments, so that treatment comparison can be performed on a general basis. The confidence band for the differences between direct adjusted survivals was constructed using the simulation method. We developed two SAS macros that compute the band. The first macro was developed for data sets consisting of fixed covariates only. The second macro was developed based on the piecewise proportional hazards Cox model and allows for time-dependent variables. We illustrate both macros by analyzing the survival data of a cohort of lymphoma patients.",WOS:000373921000001,JOURNAL OF STATISTICAL SOFTWARE,"['COX REGRESSION-MODEL', 'PROPORTIONAL HAZARDS MODEL']",Confidence Band for the Differences between Two Direct Adjusted Survival Curves,2016
1152,The ability to automatically identify areas of homogeneous texture present within a greyscale image is an important feature of image processing algorithms. This article describes the R package LS2Wstat which employs a recent wavelet-based test of stationarity for locally stationary random fields to assess such spatial homogeneity. By embedding this test within a quadtree image segmentation procedure we are also able to identify texture regions within an image.,WOS:000343788100003,R JOURNAL,"['DEFECT DETECTION', 'SEGMENTATION']",A Multiscale Test of Spatial Stationarity for Textured Images in R,2014
1153,"The population attributable fraction (PAF) is a useful measure for quantifying the impact of exposure to certain risk factors on a particular outcome at the population level. Recently, new model-based methods for the estimation of PAF and its confidence interval for different types of outcomes in a cohort study design have been proposed. In this paper, we introduce SAS macros implementing these methods and illustrate their application with a data example on the impact of different risk factors on type 2 diabetes incidence.",WOS:000293391100001,JOURNAL OF STATISTICAL SOFTWARE,"['TIME-TO-EVENT', 'INTERVAL ESTIMATION', 'HAZARDS MODEL', 'SURVIVAL-DATA', 'RISK', 'SOFTWARE', 'SCALE', 'POINT']",SAS Macros for Calculation of Population Attributable Fraction in a Cohort Study Design,2011
1154,"Improved score tests are modifications of the score test such that the null distribution of the modified test statistic is better approximated by the chi-squared distribution. The literature includes theoretical and empirical evidence favoring the improved test over its unmodified version. However, the developed methodology seems to have been overlooked by data analysts in practice, possibly because of the difficulties associated with the computation of the modified test. In this article, we describe the mdscore package to compute improved score tests in generalized linear models, given a fitted model by the glm () function in R. The package is suitable for applied statistics and simulation experiments. Examples based on real and simulated data are discussed.",WOS:000349842500001,JOURNAL OF STATISTICAL SOFTWARE,,mdscore : An R Package to Compute Improved Score Tests in Generalized Linear Models,2014
1155,"Degradation models are widely used to assess the lifetime information for highly reliable products with quality characteristics whose degradation over time can be related to reliability. The performance of a degradation model largely depends on an appropriate model description of the product's degradation path. The cross-platform package iDEMO (integrated degradation models) is developed in R and the interface is built using the Tcl/Tk bindings provided by the tcltk and tcltk2 packages included with R. It is a tool to build a linear degradation model which can simultaneously consider the unit-to-unit variation, time-dependent structure and measurement error in the degradation paths. The package iDEMO provides the maximum likelihood estimates of the unknown parameters, mean-time-to-failure and q-th quantile, and their corresponding confidence intervals based on the different information matrices. In addition, degradation model selection and goodness-of-fit tests are provided to determine and diagnose the degradation model for the user's current data by the commonly used criteria. By only enabling user interface elements when necessary, input errors are minimized.",WOS:000305989500001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'TO-FAILURE DISTRIBUTION', 'DESIGN']",Integrated Degradation Models in R Using iDEMO,2012
1156,"We investigate high-dimensional nonconvex penalized regression, where the number of covariates may grow at an exponential rate. Although recent asymptotic theory established that there exists a local minimum possessing the oracle property under general conditions, it is still largely an open problem how to identify the oracle estimator among potentially multiple local minima. There are two main obstacles: (1) due to the presence of multiple minima, the solution path is nonunique and is not guaranteed to contain the oracle estimator; (2) even if a solution path is known to contain the oracle estimator, the optimal tuning parameter depends on many unknown factors and is hard to estimate. To address these two challenging issues, we first prove that an easy-to-calculate calibrated CCCP algorithm produces a consistent solution path which contains the oracle estimator with probability approaching one. Furthermore, we propose a high-dimensional BIC criterion and show that it can be applied to the solution path to select the optimal tuning parameter which asymptotically identifies the oracle estimator. The theory for a general class of nonconvex penalties in the ultra-high dimensional setup is established when the random errors follow the sub-Gaussian distribution. Monte Carlo studies confirm that the calibrated CCCP algorithm combined with the proposed high-dimensional BIC has desirable performance in identifying the underlying sparsity pattern for high-dimensional data analysis.",WOS:000327746100008,ANNALS OF STATISTICS,"['CLIPPED ABSOLUTE DEVIATION', 'VARIABLE SELECTION', 'MODEL SELECTION', 'ORACLE PROPERTIES', 'DIVERGING NUMBER', 'ADAPTIVE LASSO', 'LIKELIHOOD', 'REGULARIZATION', 'PARAMETERS', 'ESTIMATORS']",CALIBRATING NONCONVEX PENALIZED REGRESSION IN ULTRA-HIGH DIMENSION,2013
1157,"This paper shows that large nonparametric classes of conditional multivariate densities can be approximated in the Kullback-Leibler distance by different specifications of finite mixtures of normal regressions in which normal means and variances and mixing probabilities can depend on variables in the conditioning set (covariates). These models are a special case of models known as ""mixtures of experts"" in statistics and computer science literature. Flexible specifications include models in which only mixing probabilities, modeled by multinomial logit, depend on the covariates and, in the univariate case, models in which only means of the mixed normals depend flexibly on the covariates. Modeling the variance of the mixed normals by flexible functions of the covariates can weaken restrictions on the class of the approximable densities. Obtained results can be generalized to mixtures of general location scale densities. Rates of convergence and easy to interpret bounds are also obtained for different model specifications. These approximation results can be useful for proving consistency of Bayesian and maximum likelihood density estimators based on these models. The results also have interesting implications for applied researchers.",WOS:000277471000016,ANNALS OF STATISTICS,"['HIERARCHICAL MIXTURES', 'MAXIMUM-LIKELIHOOD', 'EM ALGORITHM', 'OF-EXPERTS', 'MODELS', 'BOUNDS', 'DISTRIBUTIONS']",APPROXIMATION OF CONDITIONAL DENSITIES BY SMOOTH MIXTURES OF REGRESSIONS,2010
1158,"A domain S subset of R-d is said to fulfill the Poincare cone property if any point in the boundary of S is the vertex of a (finite) cone which does not otherwise intersects the closure (S) over bar. For more than a century, this condition has played a relevant role in the theory of partial differential equations, as a shape assumption aimed to ensure the existence of a solution for the classical Dirichlet problem on S. In a completely different setting, this paper is devoted to analyze some statistical applications of the Poincare cone property (when defined in a slightly stronger version). First, we show that this condition can be seen as a sort of generalized convexity: while it is considerably less restrictive than convexity, it still retains some ""convex flavour."" In particular, when imposed to a probability support S, this property allows the estimation of S from a random sample of points, using the ""hull principle"" much in the same way as a convex support is estimated using the convex hull of the sample points. The statistical properties of such hull estimator (consistency, convergence rates, boundary estimation) are considered in detail. Second, it is shown that the class of sets fulfilling the Poincare property is a P-Glivenko-Cantelli class for any absolutely continuous distribution P on R-d. This has some independent interest in the theory of empirical processes, since it extends the classical analogous result, established for convex sets, to a much larger class. Third, an algorithm to approximate the cone-convex hull of a finite sample of points is proposed and some practical illustrations are given.",WOS:000334256100011,ANNALS OF STATISTICS,"['CONVEX-HULL', 'SETS', 'CONVERGENCE', 'RANGE']",ON POINCARE CONE PROPERTY,2014
1159,"In this paper, we investigate the (in)-consistency of different bootstrap methods for constructing confidence intervals in the class of estimators that converge at rate n(1/3). The Grenander estimator, the nonparametric maximum likelihood estimator of an unknown nonincreasing density function f on [0, infinity), is a prototypical example. We focus on this example and explore different approaches to constructing bootstrap confidence intervals for f(t(0)), where t(o) is an element of (0, infinity) is an interior point. We find that the bootstrap estimate, when generating bootstrap samples from the empirical distribution function F(n) or its least concave majorant (F) over tilde (n), does not have any weak limit in probability. We provide a set of sufficient conditions for the consistency of any bootstrap method in this example and show that bootstrapping from a smoothed version of (F) over tilde (n) leads to strongly consistent estimators. The m out of n bootstrap method is also shown to be consistent while generating samples from F(n) and (F) over tilde (n).",WOS:000280359400002,ANNALS OF STATISTICS,['CUBE ROOT ASYMPTOTICS'],INCONSISTENCY OF BOOTSTRAP: THE GRENANDER ESTIMATOR,2010
1160,"Let {X-k, k is an element of Z} be an autoregressive process of order q. Various estimators for the order q and the parameters Theta(q) = (theta(1), ... , theta(q))(T) are known; the order is usually determined with Akaike's criterion or related modifications, whereas Yule-Walker, Burger or maximum likelihood estimators are used for the parameters Theta(q). In this paper, we establish simultaneous confidence bands for the Yule-Walker estimators (theta) over cap (i); more precisely, it is shown that the limiting distribution of maxi (1 <= i <= dn)vertical bar(theta) over cap (i) - theta(i)vertical bar is the Gumbel-type distribution e(-e-z) where q is an element of {0, ... , d(n)} and d(n) = O(n(delta)), delta > 0. This allows to modify some of the currently used criteria (AIC, BIC, HQC, SIC), but also yields a new class of consistent estimators for the order q. These estimators seem to have some potential, since they outperform most of the previously mentioned criteria in a small simulation study. In particular, if some of the parameters {theta(i)}(1 <= i <= dn) are zero or close to zero, a significant improvement can be observed. As a byproduct, it is shown that BIC, HQC and SIC are consistent for q is an element of (0, ... , d(n)} where d(n) = O(n(delta)).",WOS:000304684900019,ANNALS OF STATISTICS,"['SUBSET AUTOREGRESSION', 'TIME-SERIES', 'ASYMPTOTIC THEORY', 'MODEL SELECTION', 'PREDICTION', 'CRITERION', 'MATRICES']",SIMULTANEOUS CONFIDENCE BANDS FOR YULE-WALKER ESTIMATORS AND ORDER SELECTION,2012
1161,,WOS:000312899000005,ANNALS OF STATISTICS,"['COVARIANCE ESTIMATION', 'LASSO']",DISCUSSION: LATENT VARIABLE GRAPHICAL MODEL SELECTION VIA CONVEX OPTIMIZATION,2012
1162,"Determining the optimal number of clusters appears to be a persistant and controversial issue in cluster analysis. Most existing R packages targeting clustering require the user to specify the number of clusters in advance. However, if this subjectively chosen number is far from optimal, clustering may produce seriously misleading results. In order to address this vexing problem, we develop the R package clues to automate and evaluate the selection of an optimal number of clusers, which is widely applicable in the field of clustering analysis. Package clues uses two main procedures, shrinking and partitioning, to estimate an optimal number of clusters by maximizing an index function, either the CH index or the Silhouette index, rather than relying on guessing a pre-specified number. Five agreement indices (Rand index, Hubert and Arabie's adjusted Rand index, Morey and Agresti's adjusted Rand index, Fowlkes and Mallows index and Jaccard index), which measure the degree of agreement between any two partitions, are also provided in clues. In addition to numerical evidence, clues also supplies a deeper insight into the partitioning process with trajectory plots.",WOS:000275203500001,JOURNAL OF STATISTICAL SOFTWARE,['MODEL'],clues: An R Package for Nonparametric Clustering Based on Local Shrinking,2010
1163,"Medical researchers endeavor to identify potentially useful biomarkers to develop marker based screening assays for disease diagnosis and prevention. Useful summary measures which properly evaluate the discriminative ability of diagnostic markers are critical for this purpose. Literature and existing software, for example, R packages nicely cover summary measures for diagnostic markers used for the binary case (e.g., healthy vs. diseased). An intermediate population at an early disease stage usually exists between the healthy and the fully diseased population in many disease processes. Supporting utilities for three-group diagnostic tests are highly desired and important for identifying patients at the early disease stage for timely treatments. However, application packages which provide summary measures for three ordinal groups are currently lacking. This paper focuses on two summary measures of diagnostic accuracy-volume under the receiver operating characteristic surface and the extended Youden index, with three diagnostic groups. We provide the R package DiagTest3Grp to estimate, under both parametric and nonparametric assumptions, the two summary measures and the associated variances, as well as the optimal cut-points for disease diagnosis. An omnibus test for multiple markers and a Wald test for two markers, on independent or paired samples, are incorporated to compare diagnostic accuracy across biomarkers. Sample size calculation under the normality assumption can be performed in the R package to design future diagnostic studies. A real world application evaluating the diagnostic accuracy of neuropsychological markers for Alzheimer's disease is used to guide readers through step-by-step implementation of DiagTest3Grp to demonstrate its utility",WOS:000310774200001,JOURNAL OF STATISTICAL SOFTWARE,"['OPERATING CHARACTERISTIC CURVES', 'ROC CURVE', 'ACCURACY', 'AREAS']",DiagTest3Grp: An R Package for Analyzing Diagnostic Tests with Three Ordinal Groups,2012
1164,"This paper considers the noisy sparse phase retrieval problem: recovering a sparse signal x is an element of R-P from noisy quadratic measurements y(j) = (a(j)'x)(2) + epsilon(j), j = 1,... m, with independent sub-exponential noise epsilon(j). The goals are to understand the effect of the sparsity of x on the estimation precision and to construct a computationally feasible estimator to achieve the optimal rates adaptively. Inspired by the Wirtinger Flow [IEEE Trans. Inform. Theory 61 (2015) 1985-2007] proposed for non-sparse and noiseless phase retrieval, a novel thresholded gradient descent algorithm is proposed and it is shown to adaptively achieve the minimax optimal rates of convergence over a wide range of sparsity levels when the a(j)'s are independent standard Gaussian random vectors, provided that the sample size is sufficiently large compared to the sparsity of x.",WOS:000384397200018,ANNALS OF STATISTICS,"['MATRIX COMPLETION', 'SIGNAL RECOVERY', 'ALGORITHM', 'MAGNITUDE', 'IMAGE']",OPTIMAL RATES OF CONVERGENCE FOR NOISY SPARSE PHASE RETRIEVAL VIA THRESHOLDED WIRTINGER FLOW,2016
1165,"We study the problem of estimating a mean pattern from a set of similar curves in the setting where the variability in the data is due to random geometric deformations and additive noise. We propose an estimator based on the notion of Frechet mean that is a generalization of the standard notion of averaging to non-Euclidean spaces. We derive a minimax rate for this estimation problem, and we show that our estimator achieves this optimal rate under the asymptotics where both the number of curves and the number of sampling points go to infinity.",WOS:000320488200019,ANNALS OF STATISTICS,"['SHIFTED CURVES', 'SHAPE', 'DECONVOLUTION', 'MANIFOLDS', 'CONVERGENCE', 'CONSISTENCY', 'MODELS']",MINIMAX PROPERTIES OF FRECHET MEANS OF DISCRETELY SAMPLED CURVES,2013
1166,"We describe an R package which implements grammatical evolution (GE) for automatic program generation. By performing an unconstrained optimization over a population of R expressions generated via a user-defined grammar, programs which achieve a desired goal can be discovered. The package facilitates the coding and execution of GE programs, and supports parallel execution. In addition, three applications of GE in statistics and machine learning, including hyper-parameter optimization, classification and feature generation are studied.",WOS:000384914000001,JOURNAL OF STATISTICAL SOFTWARE,,gramEvol: Grammatical Evolution in R,2016
1167,"We suggest a robust nearest-neighbor approach to classifying high-dimensional data. The method enhances sensitivity by employing a threshold and truncates to a sequence of zeros and ones in order to reduce the deleterious impact of heavy-tailed data. Empirical rules are suggested for choosing the threshold. They require the bare minimum of data only one data vector is needed from each population. Theoretical and numerical aspects of performance are explored, paying particular attention to the impacts of correlation and heterogeneity among data components. On the theoretical side, it is shown that our truncated, thresholded, nearest-neighbor classifier enjoys the same classification boundary as more conventional, nonrobust approaches, which require finite moments in order to achieve good performance. In particular, the greater robustness of our approach does not come at the price of reduced effectiveness. Moreover, when both training sample sizes equal 1, our new method can have performance equal to that of optimal classifiers that require independent and identically distributed data with known marginal distributions; yet, our classifier does not itself need conditions of this type.",WOS:000271673500004,ANNALS OF STATISTICS,"['PATTERN-CLASSIFICATION', 'CONVERGENCE', 'PERFORMANCE']",ROBUST NEAREST-NEIGHBOR METHODS FOR CLASSIFYING HIGH-DIMENSIONAL DATA,2009
1168,"In this article we introduce the R package LogConcDEAD (Log-concave density estimation in arbitrary dimensions). Its main function is to compute the nonparametric maximum likelihood estimator of a log-concave density. Functions for plotting, sampling from the density estimate and evaluating the density estimate are provided. All of the functions available in the package are illustrated using simple, reproducible examples with simulated data.",WOS:000263105100001,JOURNAL OF STATISTICAL SOFTWARE,['ALGORITHM'],LogConcDEAD: An R Package for Maximum Likelihood Estimation of a Multivariate Log-Concave Density,2009
1169,"This article presents GrapheR, a Graphical User Interface allowing the user to draw customizable and high-quality graphs without knowing any R commands. Six kinds of graph are available: histograms, box-and-whisker plots, bar plots, pie charts, curves and scatter plots. The complete process is described with the examples of a bar plot and a scatter plot illustrating the legendary puzzle of African and European swallows' migrations.",WOS:000208590200009,R JOURNAL,,GrapheR: a Multiplatform GUI for Drawing Customizable Graphs in R,2011
1170,"As a type of search design, a detecting array can be used to generate test suites to identify and detect faults caused by interactions of factors in a component-based system. Recently, the construction and optimality of detecting arrays have been investigated in depth in the case where all the factors are assumed to have the same number of levels. However, for real world applications, it is more desirable to use detecting arrays in which the various factors may have different numbers of levels. This paper gives a general criterion to measure the optimality of a mixed level detecting array in terms of its size. Based on this optimality criterion, the combinatorial characteristics of mixed level detecting arrays of optimum size are investigated. This enables us to construct optimum mixed level detecting arrays with a heuristic optimization algorithm and combinatorial methods. As a result, some existence results for optimum mixed level detecting arrays achieving a lower bound are provided for practical use.",WOS:000342481700011,ANNALS OF STATISTICS,"['COVERING ARRAYS', 'SEARCH DESIGNS']",OPTIMUM MIXED LEVEL DETECTING ARRAYS,2014
1171,"The informR package greatly simplifies the analysis of complex event histories in R by providing user friendly tools to build sufficient statistics for the relevent package. Historically, building sufficient statistics to model event sequences (of the form a -> b) using the egocentric generalization of Butts' (2008) relational event framework for modeling social action has been cumbersome. The informR package simplifies the construction of the complex list of arrays needed by the rem() model fitting for a variety of cases involving egocentric event data, multiple event types, and/or support constraints. This paper introduces these tools using examples from real data extracted from the American Time Use Survey.",WOS:000352911400001,JOURNAL OF STATISTICAL SOFTWARE,['NOCTURIA'],Constructing and Modifying Sequence Statistics for relevent Using informR in R,2015
1172,"We study maximum likelihood estimation for the statistical model for undirected random graphs, known as the beta-model, in which the degree sequences are minimal sufficient statistics. We derive necessary and sufficient conditions, based on the polytope of degree sequences, for the existence of the maximum likelihood estimator (MLE) of the model parameters. We characterize in a combinatorial fashion sample points leading to a nonexistent MLE, and nonestimability of the probability parameters under a nonexistent MLE. We formulate conditions that guarantee that the MLE exists with probability tending to one as the number of nodes increases.",WOS:000321847600002,ANNALS OF STATISTICS,"['PROBABILITY-DISTRIBUTIONS', 'COMPLEX NETWORKS', 'DIRECTED-GRAPHS', 'EXPONENTIAL-FAMILIES', 'EXPOTENTIAL FAMILY', 'CONVEX POLYTOPES', 'DEGREE SEQUENCE', 'TABLES']",MAXIMUM LILKELIHOOD ESTIMATION IN THE beta-MODEL,2013
1173,"This paper will describe my experiences in moving on from Lisp-Stat to Java to R. I was introduced to Lisp-Stat in 1989 and used it actively for teaching and research over the next 10 years. My use of Lisp-Stat culminated in a joint project with Hani Doss and I on Bayesian Sensitivity Analysis and it remains the largest piece of software I wrote using Lisp-Stat. At the time the project was completed, the only open statistical system that could deliver the goods was Lisp-Stat. In this article, I will describe how the power of Lisp, underlying statistical components and dynamic graphics were exploited in the project. When development on Lisp-Stat slowed down, Java was coming into its own as an important language and R became an open source collaborative project. Of course, I have moved on and I use R for most of my work today. I will touch upon with my experience with Java and R briefly.",WOS:000232831500001,JOURNAL OF STATISTICAL SOFTWARE,,Lisp-Stat to Java to R,2005
1174,"Modal regression estimates the local modes of the distribution of Y given X = x, instead of the mean, as in the usual regression sense, and can hence reveal important structure missed by usual regression methods. We study a simple nonparametric method for modal regression, based on a kernel density estimate (KDE) of the joint distribution of Y and X. We derive asymptotic error bounds for this method, and propose techniques for constructing confidence sets and prediction sets. The latter is used to select the smoothing bandwidth of the underlying KDE. The idea behind modal regression is connected to many others, such as mixture regression and density ridge estimation, and we discuss these ties as well.",WOS:000372594300002,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'MEAN-SHIFT', 'EM ALGORITHM', 'MIXTURE', 'MODELS', 'DENSITY', 'APPROXIMATION', 'CONSISTENCY', 'ESTIMATORS', 'UNIFORM']",NONPARAMETRIC MODAL REGRESSION,2016
1175,"The dclone R package contains low level functions for implementing maximum likelihood estimating procedures for complex models using data cloning and Bayesian Markov Chain Monte Carlo methods with support for JAGS, WinBUGS and OpenBUGS.",WOS:000208590000005,R JOURNAL,,dclone: Data Cloning in R,2010
1176,"We formulate and investigate a statistical inverse problem of a random tomographic nature, where a probability density function on R(3) is to be recovered from observation of finitely many of its two-dimensional projections in random and unobservable directions. Such a problem is distinct from the classic problem of tomography where both the projections and the unit vectors normal to the projection plane are observable. The problem arises in single particle electron microscopy, a powerful method that biophysicists employ to learn the structure of biological macromolecules. Strictly speaking, the problem is unidentifiable and an appropriate reformulation is suggested hinging on ideas from Kendall's theory of shape. Within this setup, we demonstrate that a consistent Solution to the problem may be derived, without attempting to estimate the unknown angles, if the density is assumed to admit a mixture representation.",WOS:000271673500007,ANNALS OF STATISTICS,"['POSITRON-EMISSION-TOMOGRAPHY', 'ELECTRON CRYSTALLOGRAPHY', 'MAXIMUM-LIKELIHOOD', 'LEAST-SQUARES', 'RADON SHAPE', 'CRYOMICROSCOPY', 'REGRESSION', 'DIFFUSION', 'DISTANCES']",ON RANDOM TOMOGRAPHY WITH UNOBSERVABLE PROJECTION ANGLES,2009
1177,"A graphical scene that has been produced using the grid graphics package consists of grobs (graphical objects) and viewports. This article describes functions that allow the exploration and inspection of the grobs and viewports in a grid scene, including several functions that are available in a new package called gridDebug. The ability to explore the grobs and viewports in a grid scene is useful for adding more drawing to a scene that was produced using grid and for understanding and debugging the grid code that produced a scene.",WOS:000313198000004,R JOURNAL,,Debugging grid Graphics,2012
1178,"We consider a Bayesian analysis using WinBUGS to estimate the distribution of usual intake for episodically consumed foods and energy (calories). The model uses measures of nutrition and energy intakes via a food frequency questionnaire (FFQ) along with repeated 24 hour recalls and adjusting covariates. In order to estimate the usual intake of the food, we phrase usual intake in terms of person-specific random effects, along with day-to-day variability in food and energy consumption. Three levels are incorporated in the model. The first level incorporates information about whether an individual in fact reported consumption of a particular food item. The second level incorporates the amount of intake from those individuals who reported consumption of the food, and the third level incorporates the energy intake. Estimates of posterior means of parameters and distributions of usual intakes are obtained by using Markov chain Monte Carlo calculations. This R function reports to users point estimates and credible intervals for parameters in the model, samples from their posterior distribution, samples from the distribution of usual intake and usual energy intake, trace plots of parameters and summary statistics of usual intake, usual energy intake and energy adjusted usual intake.",WOS:000326870900001,JOURNAL OF STATISTICAL SOFTWARE,,Intake_epis_food (): An R Function for Fitting a Bivariate Nonlinear Measurement Error Model to Estimate Usual and Energy Intake for Episodically Consumed Foods,2012
1179,"Since zero-coupon rates are rarely directly observable, they have to be estimated from market data. In this paper we review several widely-used parametric term structure estimation methods. We propose a weighted constrained optimization procedure with analytical gradients and a globally optimal start parameter search algorithm. Moreover, we introduce the R package termstrc, which offers a wide range of functions for term structure estimation based on static and dynamic coupon bond and yield data sets. It provides extensive summary statistics and plots to compare the results of the different estimation methods. We illustrate the application of the package through practical examples using market data from European government bonds and yields.",WOS:000281593000001,JOURNAL OF STATISTICAL SOFTWARE,"['INTEREST-RATES', 'CONSISTENT']",Zero-Coupon Yield Curve Estimation with the Package terms,2010
1180,"Invariant coordinate selection (ICS) has recently been introduced as a method for exploring multivariate data. It includes as a special case a method for recovering the unmixing matrix in independent components analysis (ICA). It also serves as a basis for classes of multivariate nonparametric tests, and as a tool in cluster analysis or blind discrimination. The aim of this paper is to briefly explain the (ICS) method and to illustrate how various applications can be implemented using the R package ICS. Several examples are used to show how the ICS method and ICS package can be used in analyzing a multivariate data set.",WOS:000260799800001,JOURNAL OF STATISTICAL SOFTWARE,"['TRANSFORMATION-RETRANSFORMATION', 'SCATTER', 'LOCATION', 'ESTIMATOR']",Tools for Exploring Multivariate Data: The Package ICS,2008
1181,"Conditional autoregressive models are commonly used to represent spatial autocorrelation in data relating to a set of non-overlapping areal units, which arise in a wide variety of applications including agriculture, education, epidemiology and image analysis. Such models are typically specified in a hierarchical Bayesian framework, with inference based on Markov chain Monte Carlo (MCMC) simulation. The most widely used software to fit such models is WinBUGS or OpenBUGS, but in this paper we introduce the R package CARBayes. The main advantage of CARBayes compared with the BUGS software is its ease of use, because: (1) the spatial adjacency information is easy to specify as a binary neighbourhood matrix; and (2) given the neighbourhood matrix the models can be implemented by a single function call in R. This paper outlines the general class of Bayesian hierarchical models that can be implemented in the CARBayes software, describes their implementation via MCMC simulation techniques, and illustrates their use with two worked examples in the fields of house price analysis and disease mapping.",WOS:000328131500001,JOURNAL OF STATISTICAL SOFTWARE,"['SPACE-TIME VARIATION', 'DISEASE RISK', 'FIT']",CARBayes: An R Package for Bayesian Spatial Modeling with Conditional Autoregressive Priors,2013
1182,"The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed to obtain asymptotically efficient estimation of each entry of a precision matrix under a sparseness condition relative to the sample size. When the precision matrix is not sufficiently sparse, or equivalently the sample size is not sufficiently large, a lower bound is established to show that it is no longer possible to achieve the parametric rate in the estimation of each entry. This lower bound result, which provides an answer to the delicate sample size question, is established with a novel construction of a subset of sparse precision matrices in an application of Le Cam's lemma. Moreover, the proposed estimator is proven to have optimal convergence rate when the parametric rate cannot be achieved, under a minimal sample requirement.
The proposed estimator is applied to test the presence of an edge in the Gaussian graphical model or to recover the support of the entire model, to obtain adaptive rate-optimal estimation of the entire precision matrix as measured by the matrix l(q) operator norm and to make inference in latent variables in the graphical model. All of this is achieved under a sparsity condition on the precision matrix and a side condition on the range of its spectrum. This significantly relaxes the commonly imposed uniform signal strength condition on the precision matrix, irrepresentability condition on the Hessian tensor operator of the covariance matrix or the l(1) constraint on the precision matrix. Numerical results confirm our theoretical findings. The ROC curve of the proposed algorithm, Asymptotic Normal Thresholding (ANT), for support recovery significantly outperforms that of the popular GLasso algorithm.",WOS:000355768700003,ANNALS OF STATISTICS,"['COVARIANCE-MATRIX ESTIMATION', 'MIXTURE REGRESSION-MODELS', 'DIMENSIONAL LINEAR-MODELS', 'DANTZIG SELECTOR', 'CONVEX-OPTIMIZATION', 'OPTIMAL RATES', 'LASSO', 'CONVERGENCE', 'L(1)-PENALIZATION', 'REGULARIZATION']",ASYMPTOTIC NORMALITY AND OPTIMALITIES IN ESTIMATION OF LARGE GAUSSIAN GRAPHICAL MODELS,2015
1183,"Data from complex survey designs require special consideration with regard to estimation of finite population parameters and corresponding variance estimation procedures, as a consequence of significant departures from the simple random sampling assumption. In the past decade a number of statistical software packages have been developed to facilitate the analysis of complex survey data. All these statistical software packages are able to treat samples selected from one sampling frame containing all population units. Dual frame surveys are very useful when it is not possible to guarantee a complete coverage of the target population and may result in considerable cost savings over a single frame design with comparable precision. There are several estimators available in the statistical literature but no existing software covers dual frame estimation procedures. This gap is now filled by package Frames2. In this paper we highlight the main features of the package. The package includes the main estimators in dual frame surveys and also provides interval confidence estimation.",WOS:000357431900006,R JOURNAL,['INFERENCE'],Frames2: A Package for Estimation in Dual Frame Surveys,2015
1184,"The performance of spectral clustering can be considerably improved via regularization, as demonstrated empirically in Amini et al. [Ann. Statist. 41 (2013) 2097-2122]. Here, we provide an attempt at quantifying this improvement through theoretical analysis. Under the stochastic block model (SBM), and its extensions, previous results on spectral clustering relied on the minimum degree of the graph being sufficiently large for its good performance. By examining the scenario where the regularization parameter tau is large, we show that the minimum degree assumption can potentially be removed. As a special case, for an SBM with two blocks, the results require the maximum degree to be large (grow faster than logn) as opposed to the minimum degree. More importantly, we show the usefulness of regularization in situations where not all nodes belong to well-defined clusters. Our results rely on a 'bias-variance'-like trade-off that arises from understanding the concentration of the sample Laplacian and the eigengap as a function of the regularization parameter. As a byproduct of our bounds, we propose a data-driven technique DKest (standing for estimated Davis-Kahan bounds) for choosing the regularization parameter. This technique is shown to work well through simulations and on a real data set.",WOS:000379972900014,ANNALS OF STATISTICS,"['STOCHASTIC BLOCKMODEL', 'GRAPHS']",IMPACT OF REGULARIZATION ON SPECTRAL CLUSTERING,2016
1185,"We propose a notion of conditional vector quantile function and a vector quantile regression. A conditional vector quantile function (CVQF) of a random vector Y, taking values in R-d given covariates Z = z, taking values in R-k, is a map u bar right arrow Q(Y vertical bar Z) (u, z), which is monotone, in the sense of being a gradient of a convex function, and such that given that vector U follows a reference non-atomic distribution F-U, for instance uniform distribution on a unit cube in Rd, the random vector Q(Y vertical bar Z) (U, z) has the distribution of Y conditional on Z = z. Moreover, we have a strong representation, Y = Q(Y vertical bar Z) (U, Z) almost surely, for some version of U. The vector quantile regression (VQR) is a linear model for CVQF of Y given Z. Under correct specification, the notion produces strong representation, Y = beta(U)(inverted perpendicular) f (Z), for f (Z) denoting a known set of transformations of Z, where u bar right arrow beta(u)(inverted perpendicular) f (Z) is a monotone map, the gradient of a convex function and the quantile regression coefficients u bar right arrow beta(u) have the interpretations analogous to that of the standard scalar quantile regression. As f (Z) becomes a richer class of transformations of Z, the model becomes nonparametric, as in series modelling. A key property of VQR is the embedding of the classical Monge Kantorovich's optimal transportation problem at its core as a special case. In the classical case, where Y is scalar, VQR reduces to a version of the classical QR, and CVQF reduces to the scalar conditional quantile function. An application to multiple Engel curve estimation is considered.",WOS:000375175200010,ANNALS OF STATISTICS,['MULTIVARIATE QUANTILES'],VECTOR QUANTILE REGRESSION: AN OPTIMAL TRANSPORT APPROACH,2016
1186,"Mokken (1971) developed a scaling procedure for both dichotomous and polytomous items that was later coined Mokken scale analysis (MSA). MSA has been developed ever since, and the developments until 2000 have been implemented in the software package MSP (Molenaar and Sijtsma 2000) and the R package mokken (Van der Ark 2007). This paper describes the new developments in MSA since 2000 that have been implemented in mokken since its first release in 2007. These new developments pertain to invariant item ordering, a new automated item selection procedure based on a genetic algorithm, inclusion of reliability coefficients, and the computation of standard errors for the scalability coefficients. We demonstrate all new applications using data obtained with a transitive reasoning test and a personality test.",WOS:000305117600001,JOURNAL OF STATISTICAL SOFTWARE,"['POLYTOMOUS IRT MODELS', 'MONOTONE LIKELIHOOD RATIO', 'ITEM RESPONSE THEORY', 'LATENT TRAIT', 'ORDERED RESPONSE', 'SUM SCORE', 'RELIABILITY']",New Developments in Mokken Scale Analysis in R,2012
1187,"The predictive capability of a modification of Rissanen's accumulated prediction error (APE) criterion, APES,, is investigated in infinite-order autoregressive (AR(infinity)) models. Instead of accumulating squares of sequential prediction errors from the beginning, APES, is obtained by summing these squared errors from stage n delta(n), where n is the sample size and 1/n <= delta(n) <= 1 - (1/1n) may depend on n. Under certain regularity conditions, an asymptotic expression is derived for the mean-squared prediction error (MSPE) of an AR predictor with order determined by APES,. This expression shows that the prediction performance of APE delta(n), can vary dramatically depending on the choice of delta(n). Another interesting finding is that when delta(n) approaches 1 at a certain rate, APE delta(n), can achieve asymptotic efficiency in most practical situations. An asymptotic equivalence between APES, and an information criterion with a suitable penalty term is also established from the MSPE point of view. This offers new perspectives for understanding the information and prediction-based model selection criteria. Finally, we provide the first asymptotic efficiency result for the case when the underlying AR(infinity) model is allowed to degenerate to a finite autoregression.",WOS:000248692700013,ANNALS OF STATISTICS,"['ASYMPTOTICALLY EFFICIENT SELECTION', 'STOCHASTIC REGRESSION-MODELS', 'LEAST-SQUARES PREDICTORS', 'ORDER SELECTION', 'IDENTIFICATION', 'INFERENCE', 'BOUNDS']","Accumulated prediction errors, information criteria and optimal forecasting for autoregressive time series",2007
1188,"Random forests are a learning algorithm proposed by Breiman [Mach. Leant. 45 (2001) 5-32] that combines several randomized decision trees and aggregates their predictions by averaging. Despite its wide usage and outstanding practical performance, little is known about the mathematical properties of the procedure. This disparity between theory and practice originates in the difficulty to simultaneously analyze both the randomization process and the highly data-dependent tree structure. In the present paper, we take a step forward in forest exploration by proving a consistency result for Breiman's [Mach. Learn. 45 (2001) 5-32] original algorithm in the context of additive regression models. Our analysis also sheds an interesting light on how random forests can nicely adapt to sparsity.",WOS:000357441000016,ANNALS OF STATISTICS,"['RANDOM SURVIVAL FORESTS', 'REGRESSION', 'CLASSIFICATION']",CONSISTENCY OF RANDOM FORESTS,2015
1189,"A variance reduction technique in nonparametric smoothing is proposed: at each point of estimation, form a linear combination of a preliminary estimator evaluated at nearby points with the coefficients specified so that the asymptotic bias remains unchanged. The nearby points are chosen to maximize the variance reduction. We study in detail the case of univariate local linear regression. While the new estimator retains many advantages of the local linear estimator, it has appealing asymptotic relative efficiencies. Bandwidth selection rules are available by a simple constant factor adjustment of those for local linear estimation. A simulation study indicates that the finite sample relative efficiency often matches the asymptotic relative efficiency for moderate sample sizes. This technique is very general and has a wide range of applications.",WOS:000248987600003,ANNALS OF STATISTICS,"['LOCAL LINEAR-REGRESSION', 'NONPARAMETRIC REGRESSION', 'DENSITY-ESTIMATION', 'BIAS REDUCTION', 'BANDWIDTH', 'ESTIMATORS']",Reducing variance in univariate smoothing,2007
1190,"We present a link that allows R, S-PLUS and Excel to call the functions in the 1p_solve system. 1p_solve is free software (licensed under the GNU Lesser GPL) that solves linear and mixed integer linear programs of moderate size (on the order of 10,000 variables and 50,000 constraints). R does not include this ability (though two add-on packages off er linear programs without integer variables), while S-PLUS users need to pay extra for the NuOPT library in order to solve these problems. Our link manages the interface between these statistical packages and 1p_solve.
Excel has a built-in add-in named Solver that is capable of solving mixed integer programs, but only with fewer than 200 variables. This link allows Excel users to handle substantially larger problems at no extra cost. While our primary concern has been the Windows operating system, the package has been tested on some Unix-type systems as well.",WOS:000232890700001,JOURNAL OF STATISTICAL SOFTWARE,,"Calling the lp_solve linear program software from R,S-PLUS and Excel",2005
1191,,WOS:000253077800007,ANNALS OF STATISTICS,"['ATOMIC DECOMPOSITION', 'BASIS PURSUIT', 'REGRESSION', 'LASSO']",Discussion: The Dantzig selector: Statistical estimation when p is much larger than n,2007
1192,"Undirected graphs can be used to describe matrix variate distributions. In this paper, we develop new methods for estimating the graphical structures and underlying parameters, namely, the row and column covariance and inverse covariance matrices from the matrix variate data. Under sparsity conditions, we show that one is able to recover the graphs and covariance matrices with a single random matrix from the matrix variate normal distribution. Our method extends, with suitable adaptation, to the general setting where replicates are available. We establish consistency and obtain the rates of convergence in the operator and the Frobenius norm. We show that having replicates will allow one to estimate more complicated graphical structures and achieve faster rates of convergence. We provide simulation evidence showing that we can recover graphical structures as well as estimating the precision matrices, as predicted by theory.",WOS:000336888400009,ANNALS OF STATISTICS,"['DIMENSIONAL COVARIANCE ESTIMATION', 'NONCONCAVE PENALIZED LIKELIHOOD', 'SELECTION', 'MODELS', 'LASSO', 'CONVERGENCE']",GEMINI: GRAPH ESTIMATION WITH MATRIX VARIATE NORMAL INSTANCES,2014
1193,"This paper brings a contribution to the Bayesian theory of nonparametric and semiparametric estimation. We are interested in the asymptotic normality of the posterior distribution in Gaussian linear regression models when the number of regressors increases with the sample size. Two kinds of Bernstein-von Mises theorems are obtained in this framework: nonparametric theorems for the parameter itself, and semiparametric theorems for functionals of the parameter. We apply them to the Gaussian sequence model and to the regression of functions in Sobolev and C(alpha) classes, in which we get the minimax convergence rates. Adaptivity is reached for the Bayesian estimators of functionals in our applications.",WOS:000299186500014,ANNALS OF STATISTICS,"['POSTERIOR DISTRIBUTIONS', 'ASYMPTOTIC NORMALITY', 'EXPONENTIAL-FAMILIES', 'CONVERGENCE-RATES', 'PARAMETERS', 'MODEL']",BERNSTEIN-VON MISES THEOREMS FOR GAUSSIAN REGRESSION WITH INCREASING NUMBER OF REGRESSORS,2011
1194,,WOS:000208589800001,R JOURNAL,,Untitled,2009
1195,"The R package plink has been developed to facilitate the linking of mixed-format tests for multiple groups under a common item design using unidimensional and multidimensional IRT-based methods. This paper presents the capabilities of the package in the context of the unidimensional methods. The package supports nine unidimensional item response models (the Rasch model, 1PL, 2PL, 3PL, graded response model, partial credit and generalized partial credit model, nominal response model, and multiple-choice model) and four separate calibration linking methods (mean/sigma, mean/mean, Haebara, and Stocking-Lord). It also includes functions for importing item and/or ability parameters from common IRT software, conducting IRT true-score and observed-score equating, and plotting item response curves and parameter comparison plots.",WOS:000281588400001,JOURNAL OF STATISTICAL SOFTWARE,"['ITEM RESPONSE THEORY', 'EQUATING TESTS', 'RASCH MODEL', 'ABILITY', 'CHOICE']",plink: An R Package for Linking Mixed-Format Tests Using IRT-Based Methods,2010
1196,,WOS:000389620800005,ANNALS OF STATISTICS,,"DISCUSSION OF ""INFLUENTIAL FEATURES PCA FOR HIGH DIMENSIONAL CLUSTERING""",2016
1197,We propose a two-step estimating procedure for generalized additive partially linear models with clustered data using estimating equations. Our proposed method applies to the case that the number of observations per cluster is allowed to increase with the number of independent subjects. We establish oracle properties for the two-step estimator of each function component such that it performs as well as the univariate function estimator by assuming that the parametric vector and all other function components are known. Asymptotic distributions and consistency properties of the estimators are obtained. Finite-sample experiments with both simulated continuous and binary response variables confirm the asymptotic results. We illustrate the methods with an application to a U.S. unemployment data set.,WOS:000321845400007,ANNALS OF STATISTICS,"['LONGITUDINAL/CLUSTERED DATA', 'NONPARAMETRIC REGRESSION', 'POLYNOMIAL SPLINE', 'EFFICIENT ESTIMATION', 'LOCAL ASYMPTOTICS']",TWO-STEP SPLINE ESTIMATING EQUATIONS FOR GENERALIZED ADDITIVE PARTIALLY LINEAR MODELS WITH LARGE CLUSTER SIZES,2012
1198,"We propose a new approach for identifying the support points of a locally optimal design when the model is a nonlinear model. In contrast to the commonly used geometric approach, we use an approach based on algebraic tools. Considerations are restricted to models with two parameters, and the general results are applied to often used special cases, including logistic, probit, double exponential and double reciprocal models for binary data, a loglinear Poisson regression model for count data, and the Michaelis-Men ten model. The approach, which is also of value for multi-stage experiments, works both with constrained and unconstrained design regions and is relatively easy to implement.",WOS:000263129000019,ANNALS OF STATISTICS,"['BINARY RESPONSE EXPERIMENTS', 'GENERALIZED LINEAR-MODELS', 'MICHAELIS-MENTEN MODEL', 'OPTICAL-DENSITY DATA', 'LOGISTIC-REGRESSION', 'EFFICIENT DESIGNS', 'FIELLER', 'ROBUST']",SUPPORT POINTS OF LOCALLY OPTIMAL DESIGNS FOR NONLINEAR MODELS WITH TWO PARAMETERS,2009
1199,For many ecological analyses powerful statistical tools are required for a profound analysis of spatial and time based data sets. In order to avoid many common errors of analysis and data acquisition a graphical user interface can help to focus on the task of the analysis and minimize the time to fulfill certain tasks in a programming language like R. In this paper we present a graphical user interface for R embedded in the ecological modeling software Bio7 which is based on an Eclipse rich client platform. We demonstrate that within the Bio7 platform R can not only be effectively combined with Java but also with the powerful components of Eclipse. Additionally we present some custom Bio7 components which interact with R and make use of some useful advanced concepts and libraries of this connection. Our overview on the Bio7 R interface also emphasizes a broad applicability for disciplines beyond ecological modelling.,WOS:000305990100001,JOURNAL OF STATISTICAL SOFTWARE,['PACKAGE'],A Graphical User Interface for R in a Rich Client Platform for Ecological Modeling,2012
1200,"We suggest a new method, called Functional Additive Regression, or FAR, for efficiently performing high-dimensional functional regression. FAR extends the usual linear regression model involving a functional predictor, X(t), and a scalar response, Y, in two key respects. First, FAR uses a penalized least squares optimization approach to efficiently deal with high-dimensional problems involving a large number of functional predictors. Second, FAR extends beyond the standard linear regression setting to fit general nonlinear additive models. We demonstrate that FAR can be implemented with a wide range of penalty functions using a highly efficient coordinate descent algorithm. Theoretical results are developed which provide motivation for the FAR optimization criterion. Finally, we show through simulations and two real data sets that FAR can significantly outperform competing methods.",WOS:000362697700015,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'DIMENSION REDUCTION', 'VARIABLE SELECTION', 'MODEL ESTIMATION', 'LINEAR-MODELS', 'SINGLE', 'DISCRIMINATION', 'PREDICTORS', 'ESTIMATORS']",FUNCTIONAL ADDITIVE REGRESSION,2015
1201,"The package VIM (Templ, Alfons, Kowarik, and Prantner 2016) is developed to explore and analyze the structure of missing values in data using visualization methods, to impute these missing values with the built-in imputation methods and to verify the imputation process using visualization tools, as well as to produce high-quality graphics for publications.
This article focuses on the different imputation techniques available in the package. Four different imputation methods are currently implemented in VIM, namely hot-deck imputation, k-nearest neighbor imputation, regression imputation and iterative robust model-based imputation (Templ, Kowarik, and Filzmoser 2011). All of these methods are implemented in a flexible manner with many options for customization. Furthermore in this article practical examples are provided to highlight the use of the implemented methods on real-world applications.
In addition, the graphical user interface of VIM has been re-implemented from scratch resulting in the package VIMGUI (Schopfhauser, Templ, Alfons, Kowarik, and Prantner 2016) to enable users without extensive R skills to access these imputation and visualization methods.",WOS:000392513900001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIPLE IMPUTATION', 'MISSING DATA']",Imputation with the R Package VIM,2016
1202,"This paper presents the R package alphahull which implements the alpha-convex hull and the alpha-shape of a finite set of points in the plane. These geometric structures provide an informative overview of the shape and properties of the point set. Unlike the convex hull, the alpha-convex hull and the alpha-shape are able to reconstruct non-convex sets. This flexibility make them specially useful in set estimation. Since the implementation is based on the intimate relation of theses constructs with Delaunay triangulations, the R package alphahull also includes functions to compute Voronoi and Delaunay tesselations. The usefulness of the package is illustrated with two small simulation studies on boundary length estimation.",WOS:000276954400001,JOURNAL OF STATISTICAL SOFTWARE,"['DELAUNAY TRIANGULATION', 'SUPPORT', 'ALGORITHM', 'PLANE', 'SETS']",Generalizing the Convex Hull of a Sample: The R Package alphahull,2010
1203,"Reuse of controls from nested case-control designs can increase efficiency in many situations, for instance with competing risks or in other multiple endpoints situations. The matching between cases and controls must be broken when controls are to be used for other endpoints. A weighted analysis can then be performed to take care of the biased sampling from the cohort. We present the R package multipleNCC for reuse of controls in nested case-control studies by inverse probability weighting of the partial likelihood. The package handles right-censored, left-truncated and additionally matched data, and varying numbers of sampled controls and the whole analysis is carried out using one simple command. Four weight estimators are presented and variance estimation is explained. The package is illustrated by analyzing health survey data from three counties in Norway for two causes of death: cardiovascular disease and death from alcohol abuse, liver disease, and accidents and violence. The data set is included in the package.",WOS:000395669800002,R JOURNAL,"['ELECTROMAGNETIC-FIELDS', 'RISK-FACTORS', 'CANCER', 'COHORT', 'EXPOSURE', 'DESIGNS', 'ESTIMATORS', 'OUTCOMES']",multipleNCC: Inverse Probability Weighting of Nested Case-Control Data,2016
1204,"plotKML is an R package that provides methods for writing the most common R spatial classes into KML files. It builds up on the existing XML parsing functionality (X M L package), and provides similar plotting functionality as the lattice package. Its main objective is to provide a simple interface to generate KML files with a small number of arguments, and allows users to visually explore spatio-temporal data available in R : points, polygons, gridded maps, trajectory-type data, vertical pro files, ground photographs, time series vector objects or raster images, along with the results of spatial analysis such as geostatistical mapping, spatial simulations of vector and gridded objects, optimized sampling designs, species distribution models and similar. A generic plotKML ( ) function automatically determines the parsing order and visualizes data directly from R; lower level functions can be combined to allow for new user-created visualization templates. In comparison to other packages writing KML, plotKML seems to be more object oriented, it links more closely to the existing R classes for spatio-temporal data (sp, spacetime and raster packages) than the alternatives, and provides users with the possibility to create their own templates.",WOS:000349845200001,JOURNAL OF STATISTICAL SOFTWARE,"['GOOGLE-EARTH', 'PACKAGE']",plotKML: Scientific Visualization of Spatio-Temporal Data,2015
1205,"We illustrate how to fit multilevel models in the MLwiN package seamlessly from within Stata using the Stata program runmlwin. We argue that using MLwiN and Stata in combination allows researchers to capitalize on the best features of both packages. We provide examples of how to use runmlwin to fit continuous, binary, ordinal, nominal and mixed response multilevel models by both maximum likelihood and Markov chain Monte Carlo estimation.",WOS:000316316800001,JOURNAL OF STATISTICAL SOFTWARE,,runmlwin: A Program to Run the MLwiN Multilevel Modeling Software from within Stata,2013
1206,"This paper considers the problem of adaptive estimation of a mean pattern in a randomly shifted curve model. We show that this problem can be transformed into a linear inverse problem, where the density of the random shifts plays the role of a convolution operator. An adaptive estimator of the mean pattern, based on wavelet thresholding is proposed. We study its consistency for the quadratic risk as the number of observed curves tends to infinity, and this estimator is shown to achieve a near-minimax rate of convergence over a large class of Besov balls. This rate depends both on the smoothness of the common shape of the curves and on the decay of the Fourier coefficients of the density of the random shifts. Hence, this paper makes a connection between mean pattern estimation and the statistical analysis of linear inverse problems, which is a new point of view on curve registration and image warping problems. We also provide a new method to estimate the unknown random shifts between curves. Some numerical experiments are given to illustrate the performances of our approach and to compare them with another algorithm existing in the literature.",WOS:000280359400016,ANNALS OF STATISTICS,"['LINEAR INVERSE PROBLEMS', 'EXTRINSIC SAMPLE MEANS', 'WAVELET DECONVOLUTION', 'TEMPLATE ESTIMATION', 'CONSISTENCY', 'TRANSFORM', 'MANIFOLDS']",A DECONVOLUTION APPROACH TO ESTIMATION OF A COMMON SHAPE IN A SHIFTED CURVES MODEL,2010
1207,"Flexible multivariate distributions are needed in many areas. The popular multivariate Gaussian distribution is however very restrictive and cannot account for features like asymmetry and heavy tails. Therefore dependence modeling using copulas is nowadays very common to account for such patterns. The use of copulas is however challenging in higher dimensions, where standard multivariate copulas suffer from rather inflexible structures. Vine copulas overcome such limitations and are able to model complex dependency patterns by benefiting from the rich variety of bivariate copulas as building blocks. This article presents the R package CDVine which provides functions and tools for statistical inference of canonical vine (C-vine) and D-vine copulas. It contains tools for bivariate exploratory data analysis and for bivariate copula selection as well as for selection of pair-copula families in a vine. Models can be estimated either sequentially or by joint maximum likelihood estimation. Sampling algorithms and graphical methods are also included.",WOS:000315018900001,JOURNAL OF STATISTICAL SOFTWARE,"['ARCHIMEDEAN COPULAS', 'FINANCIAL RETURNS', 'RANDOM-VARIABLES', 'CONSTRUCTIONS', 'SELECTION', 'DECOMPOSITION', 'INFERENCE']",Modeling Dependence with C- and D-Vine Copulas: The R Package CDVine,2013
1208,"Item response theory (IRT) is widely used in assessment and evaluation research to explain how participants respond to item level stimuli. Several R packages can be used to estimate the parameters in various IRT models, the most flexible being the ltm (Rizopoulos 2006), eRm (Mair and Hatzinger 2007), and MCMCpack (Martin, Quinn, and Park 2011) packages. However these packages have limitations in that ltm and eRm can only analyze unidimensional IRT models effectively and the exploratory multidimensional extensions available in MCMCpack requires prior understanding of Bayesian estimation convergence diagnostics and are computationally intensive. Most importantly, multidimensional confirmatory item factor analysis methods have not been implemented in any R package. The mirt package was created for estimating multidimensional item response theory parameters for exploratory and confirmatory models by using maximum-likelihood methods. The Gauss-Hermite quadrature method used in traditional EM estimation (e.g., Bock and Aitkin 1981) is presented for exploratory item response models as well as for confirmatory bifactor models (Gibbons and Hedeker 1992). Exploratory and confirmatory models are estimated by a stochastic algorithm described by Cai (2010a,b). Various program comparisons are presented and future directions for the package are discussed.",WOS:000305117700001,JOURNAL OF STATISTICAL SOFTWARE,"['CHAIN MONTE-CARLO', 'MAXIMUM-LIKELIHOOD', 'BIFACTOR ANALYSIS', 'EM ALGORITHM', 'INFORMATION', 'MODELS']",mirt: A Multidimensional Item Response Theory Package for the R Environment,2012
1209,"Angoff's delta plot is a straightforward and not computationally intensive method to identify differential item functioning (DIF) among dichotomously scored items. This approach was recently improved by proposing an optimal threshold selection and by considering several item purification processes. Moreover, to support practical DIF analyses with the delta plot and these improvements, the R package deltaPlotR was also developed. The purpose of this paper is twofold: to outline the delta plot by describing the original method and its recent improvements in a user-oriented way, and to illustrate the structure and performances of the deltaPlotR package. A real data set about language skill assessment is being analyzed as an illustrative example.",WOS:000341808100001,JOURNAL OF STATISTICAL SOFTWARE,"['I ERROR INFLATION', 'RESPONSE THEORY', 'DIF DETECTION', 'DOWN-SYNDROME', 'BIAS']",deltaPlotR: An R Package for Differential Item Functioning Analysis with Angoff's Delta Plot,2014
1210,"Sufficient dimension reduction [J. Amer Statist. Assoc. 86 (1991) 316-342] has long been a prominent issue in multivariate nonparametric regression analysis. To uncover the central dimension reduction space, we propose in this paper an adaptive composite quantile approach. Compared to existing methods, (1) it requires minimal assumptions and is capable of revealing all dimension reduction directions; (2) it is robust against outliers and (3) it is structure-adaptive, thus more efficient. Asymptotic results are proved and numerical examples are provided, including a real data analysis.",WOS:000342481700015,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'CONDITIONAL QUANTILE', 'CENTRAL SUBSPACE', 'U-PROCESSES', 'KERNEL', 'ESTIMATORS', 'MODEL', 'ASYMPTOTICS', 'DIRECTIONS', 'VARIANCE']",AN ADAPTIVE COMPOSITE QUANTILE APPROACH TO DIMENSION REDUCTION,2014
1211,"We investigate proper scoring rules for continuous distributions on the real line. It is known that the log score is the only such rule that depends on the quoted density only through its value at the outcome that materializes. Here we allow further dependence on a finite number in of derivatives of the density at the outcome, and describe a large class of such m-local proper scoring rules: these exist for all even m but no odd m. We further show that for m >= 2 all such m-local rules can be computed without knowledge of the normalizing constant of the distribution.",WOS:000304684900021,ANNALS OF STATISTICS,['INFERENCE'],PROPER LOCAL SCORING RULES,2012
1212,"In functional linear regression, the slope ""parameter"" is a function. Therefore, in a nonparametric context, it is determined by an infinite number of unknowns. Its estimation involves solving an ill-posed problem and has points of contact with a range of methodologies, including statistical smoothing and deconvolution. The standard approach to estimating the slope function is based explicitly on functional principal components analysis and, consequently, on spectral decomposition in terms of eigenvalues and eigenfunctions. We discuss this approach in detail and show that in certain circumstances, optimal convergence rates are achieved by the PCA technique. An alternative approach based on quadratic regularisation is suggested and shown to have advantages from some points of view.",WOS:000247498100004,ANNALS OF STATISTICS,"['INVERSE PROBLEMS', 'BLIND DECONVOLUTION', 'MODELS', 'SAMPLE']",Methodology and convergence rates for functional linear regression,2007
1213,"Motivated by normalizing DNA microarray data and by predicting the interest rates, we explore nonparametric estimation of additive models with highly correlated covariates. We introduce two novel approaches for estimating the additive components, integration estimation and pooled backfitting estimation. The former is designed for highly correlated covariates, and the latter is useful for nonhighly correlated covariates. Asymptotic normalities of the proposed estimators are established. Simulations are conducted to demonstrate finite sample behaviors of the proposed estimators, and real data examples are given to illustrate the value of the methodology.",WOS:000277471000005,ANNALS OF STATISTICS,"['LOCAL POLYNOMIAL REGRESSION', '2-WAY SEMILINEAR MODEL', 'CDNA MICROARRAY DATA', 'ASYMPTOTIC PROPERTIES', 'NORMALIZATION']",ESTIMATION IN ADDITIVE MODELS WITH HIGHLY OR NONHIGHLY CORRELATED COVARIATES,2010
1214,"We consider the on-line predictive version of the standard problem of linear regression; the goal is to predict each consecutive response given the corresponding explanatory variables and all the previous observations. The standard treatment of prediction in linear regression analysis has two drawbacks: (1) the classical prediction intervals guarantee that the probability of error is equal to the nominal significance level epsilon, but this property per se does not imply that the long-run frequency of error is close to epsilon; (2) it is not suitable for prediction of complex systems as it assumes that the number of observations exceeds the number of parameters. We state a general result showing that in the on-line protocol the frequency of error for the classical prediction intervals does equal the nominal significance level, up to statistical fluctuations. We also describe alternative regression models in which informative prediction intervals can be found before the number of observations exceeds the number of parameters. One of these models, which only assumes that the observations are independent and identically distributed, is popular in machine learning but greatly underused in the statistical theory of regression.",WOS:000265619700017,ANNALS OF STATISTICS,,ON-LINE PREDICTIVE LINEAR REGRESSION,2009
1215,"This paper uses the invariance principle to solve the incidental parameter problem of [Econometrica 16 (1948) 1-32]. We seek group actions that preserve the structural parameter and yield a maximal invariant in the parameter space with fixed dimension. M-estimation from the likelihood of the maximal invariant statistic yields the maximum invariant likelihood estimator (MILE). Consistency of MILE for cases in which the likelihood of the maximal invariant is the product of marginal likelihoods is straightforward. We illustrate this result with a stationary autoregressive model with fixed effects and an agent-specific monotonic transformation model.
Asymptotic properties of MILE, when the likelihood of the maximal invariant does not factorize, remain an open question. We are able to provide consistent, asymptotically normal and efficient results of MILE when invariance yields Wishart distributions. Two examples are an instrumental variable (IV) model and a dynamic panel data model with fixed effects.",WOS:000271673500021,ANNALS OF STATISTICS,"['NUISANCE PARAMETERS', 'PANEL-DATA', 'ESTIMATORS', 'INFERENCE', 'DISTRIBUTIONS', 'REGRESSION', 'MODEL']",A MAXIMUM LIKELIHOOD METHOD FOR THE INCIDENTAL PARAMETER PROBLEM,2009
1216,"Data from many scientific areas often come with measurement error. Density or distribution function estimation from contaminated data and nonparametric regression with errors-in-variables are two important topics in measurement error models. In this paper, we present a new software package decon for R, which contains a collection of functions that use the deconvolution kernel methods to deal with the measurement error problems. The functions allow the errors to be either homoscedastic or heteroscedastic. To make the deconvolution estimators computationally more efficient in R, we adapt the fast Fourier transform algorithm for density estimation with error-free data to the deconvolution kernel estimation. We discuss the practical selection of the smoothing parameter in deconvolution methods and illustrate the use of the package through both simulated and real examples.",WOS:000288205900001,JOURNAL OF STATISTICAL SOFTWARE,"['KERNEL DENSITY-ESTIMATION', 'IN-VARIABLES PROBLEM', 'NONPARAMETRIC REGRESSION', 'BANDWIDTH SELECTION', 'OPTIMAL RATES', 'DISTRIBUTIONS', 'CONVERGENCE', 'PREDICTION', 'QUANTILES', 'CHOICE']",Deconvolution Estimation in Measurement Error Models: The R Package decon,2011
1217,"A major breakthrough in the visualization of dissimilarities between pairs of objects was the formulation of the least-squares multidimensional scaling (MDS) model as defined by the Stress function. This function is quite flexible in that it allows possibly nonlinear transformations of the dissimilarities to be represented by distances between points in a low dimensional space. To obtain the visualization, the Stress function should be minimized over the coordinates of the points and the over the transformation. In a series of papers, Jan de Leeuw has made a significant contribution to majorization methods for the minimization of Stress in least-squares MDS. In this paper, we present a review of the majorization algorithm for MDS as implemented in the smacof package and related approaches. We present several illustrative examples and special cases.",WOS:000389127600001,JOURNAL OF STATISTICAL SOFTWARE,"['GLOBAL OPTIMIZATION', 'DECOMPOSITION', 'VISUALIZATION', 'CONVERGENCE', 'SERIATION']",Multidimensional Scaling by Majorization: A Review,2016
1218,"Exploratory factor analysis is a widely used statistical technique in the social sciences. It attempts to identify underlying factors that explain the pattern of correlations with in a set of observed variables. A statistical software package is needed to perform the calculations. However, there are some limitations with popular statistical software packages,like SPSS. The R programming language is a free software package for statistical and graphical computing. It offers many packages written by contributors from all over the world and programming resources that allow it to overcome the dialog limitations of SPSS. This paper offers an SPSS dialog written in the R programming language with the help of some packages,so that researchers with little or no knowledge in programming, or those who are accustomed to making their calculations based on statistical dialogs, have more options when applying factor analysis to their data and hence can adopt a better approach when dealing with ordinal, Likert-type data.",WOS:000301070300001,JOURNAL OF STATISTICAL SOFTWARE,"['EXPLORATORY FACTOR-ANALYSIS', 'PARALLEL ANALYSIS', 'CORRELATION-MATRICES', 'NUMBER', 'ROTATION', 'COMPONENTS', 'STATISTICS', 'VARIABLES', 'ACCURACY', 'PATTERN']",An SPSS R-Menu for Ordinal Factor Analysis,2012
1219,"Balancing treatment allocation for influential covariates is critical in clinical trials. This has become increasingly important as more and more biomarkers are found to be associated with different diseases in translational research (genomics, proteomics and metabolomics). Stratified permuted block randomization and minimization methods [Pocock and Simon Biometrics 31 (1975) 103-115, etc.] are the two most popular approaches in practice. However, stratified permuted block randomization fails to achieve good overall balance when the number of strata is large, whereas traditional minimization methods also suffer from the potential drawback of large within-stratum imbalances. Moreover, the theoretical bases of minimization methods remain largely elusive. In this paper, we propose a new covariate-adaptive design that is able to control various types of imbalances. We show that the joint process of within-stratum imbalances is a positive recurrent Markov chain under certain conditions. Therefore, this new procedure yields more balanced allocation. The advantages of the proposed procedure are also demonstrated by extensive simulation studies. Our work provides a theoretical tool for future research in this area.",WOS:000310650900019,ANNALS OF STATISTICS,"['BIASED COIN DESIGNS', 'SEQUENTIAL CLINICAL-TRIALS', 'TREATMENT ALLOCATION', 'PROGNOSTIC-FACTORS', 'URN MODELS', 'CANCER', 'STRATIFICATION', 'MINIMIZATION', 'THEOREMS', 'THERAPY']",ASYMPTOTIC PROPERTIES OF COVARIATE-ADAPTIVE RANDOMIZATION,2012
1220,"Canonical correlations analysis (CCA) is an exploratory statistical method to highlight correlations between two datasets acquired on the same experimental units. The cancor () function in R (R Development Core Team 2007) performs the core of computations but further work was required to provide the user with additional tools to facilitate the interpretation of the results. We implemented an R package, CCA, freely available from the Comprehensive R Archive Network (CRAN, http://CRAN.R-project.org/), to develop numerical and graphical outputs and to enable the user to handle missing values. The CCA package also includes a regularized version of CCA to deal with datasets with more variables than units. Illustrations are given through the analysis of a dataset coming from a nutrigenomic study in the mouse.",WOS:000252620500001,JOURNAL OF STATISTICAL SOFTWARE,['DISCRIMINANT-ANALYSIS'],CCA: An R package to extend canonical correlation analysis,2008
1221,"asympTest is an R package implementing large sample tests and confidence intervals. One and two sample mean and variance tests (differences and ratios) are considered. The test statistics are all expressed in the same form as the Student t-test, which facilitates their presentation in the classroom. This contribution also fills the gap of a robust (to non-normality) alternative to the chi-square single variance test for large samples, since no such procedure is implemented in standard statistical software.",WOS:000208589800005,R JOURNAL,,asympTest: A Simple R Package for Classical Parametric Statistical Tests and Confidence Intervals in Large Samples,2009
1222,"We present two recently released R packages, DiceKriging and DiceOptim, for the approximation and the optimization of expensive-to-evaluate deterministic functions. Following a self-contained mini tutorial on Kriging-based approximation and optimization, the functionalities of both packages are detailed and demonstrated in two distinct sections. In particular, the versatility of DiceKriging with respect to trend and noise specifications, covariance parameter estimation, as well as conditional and unconditional simulations are illustrated on the basis of several reproducible numerical experiments. We then put to the fore the implementation of sequential and parallel optimization strategies relying on the expected improvement criterion on the occasion of DiceOptim's presentation. An appendix is dedicated to complementary mathematical and computational details.",WOS:000310773900001,JOURNAL OF STATISTICAL SOFTWARE,"['GLOBAL OPTIMIZATION', 'MODELS', 'LIKELIHOOD', 'CODE']","DiceKriging, DiceOptim: Two R Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization",2012
1223,"In the context of correlated Multiple tests, we aim to nonasymptotically control the family-wise error rate (FWER) using resampling-type procedures. We observe repeated realizations of a Gaussian random vector in possibly high dimension and with an unknown covariance matrix, and consider the one- and two-sided multiple testing problem for the mean values of its coordinates. We address this problem by using the confidence regions developed in the companion paper [Ann. Statist. (2009), to appear], which lead directly to single-step procedures, these can then be improved using step-down algorithms, following an established general methodology laid down by Romano and Wolf [J. Amer Statist. Assoc. 100 (2005) 94-108]. This gives rise to several different procedures, whose performances are compared Using simulated data.",WOS:000273800100003,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'RANDOMIZATION TESTS', 'BOOTSTRAP']","SOME NONASYMPTOTIC RESULTS ON RESAMPLING IN HIGH DIMENSION, II: MULTIPLE TESTS",2010
1224,"Taking advantage of the S4 class system of the programming environment R, which facilitates the creation and maintenance of reusable and modular components, an object-oriented framework for robust multivariate analysis was developed. The framework resides in the packages robust base and rrcov and includes an almost complete set of algorithms for computing robust multivariate location and scatter, various robust methods for principal component analysis as well as robust linear and quadratic discriminant analysis. The design of these methods follows common patterns which we call statistical design patterns in analogy to the design patterns widely used in software engineering. The application of the framework to data analysis as well as possible extensions by the development of new methods is demonstrated on examples which themselves are part of the package rrcov",WOS:000270821700001,JOURNAL OF STATISTICAL SOFTWARE,"['PRINCIPAL COMPONENT ANALYSIS', 'COVARIANCE DETERMINANT ESTIMATOR', 'LINEAR DISCRIMINANT-ANALYSIS', 'PROJECTION-PURSUIT APPROACH', 'DISPERSION MATRICES', 'S-ESTIMATORS', 'HIGH DIMENSION', 'MONTE-CARLO', 'LOCATION', 'REGRESSION']",An Object-Oriented Framework for Robust Multivariate Analysis,2009
1225,"Comparing text strings in terms of distance functions is a common and fundamental task in many statistical text-processing applications. Thus far, string distance functionality has been somewhat scattered around R and its extension packages, leaving users with inconistent interfaces and encoding handling. The stringdist package was designed to offer a low-level interface to several popular string distance algorithms which have been re-implemented in C for this purpose. The package offers distances based on counting q-grams, edit-based distances, and some lesser known heuristic distance functions. Based on this functionality, the package also offers inexact matching equivalents of R's native exact matching functions match and % in%.",WOS:000343788100012,R JOURNAL,,The stringdist Package for Approximate String Matching,2014
1226,"Quantifying non-linear dependence structures between two random variables is a challenging task. There exist several bona-fide dependence measures able to capture the strength of the non-linear association, but they typically give little information about how the variables are associated. This problem has been recognized by several authors and has given rise to the concept of local measures of dependence. A local measure of dependence is able to capture the ""local"" dependence structure in a particular region. The idea is that the global dependence structure is better described by a portfolio of local measures of dependence computed in different regions than a one-number measure of dependence. This paper introduces the R package localgauss which estimates and visualizes a measure of local dependence called local Gaussian correlation. The package provides a function for estimation, a function for local independence testing and corresponding functions for visualization purposes, which are all demonstrated with examples.",WOS:000332112600001,JOURNAL OF STATISTICAL SOFTWARE,"['DEPENDENCE FUNCTION', 'INDEPENDENCE']","Introducing localgauss, an R Package for Estimating and Visualizing Local Gaussian Correlation",2014
1227,"This paper discusses the problem of determining optimal designs for regression models, when the observations are dependent and taken on an interval. A complete solution of this challenging optimal design problem is given for a broad class of regression models and covariance kernels. We propose a class of estimators which are only slightly more complicated than the ordinary least-squares estimators. We then demonstrate that we can design the experiments, such that asymptotically the new estimators achieve the same precision as the best linear unbiased estimator computed for the whole trajectory of the process. As a by-product, we derive explicit expressions for the BLUE in the continuous time model and analytic expressions for the optimal designs in a wide class of regression models. We also demonstrate that for a finite number of observations the precision of the proposed procedure, which includes the estimator and design, is very close to the best achievable. The results are illustrated on a few numerical examples.",WOS:000368022000005,ANNALS OF STATISTICS,"['LINEAR-MODELS', 'EQUIDISTANT', 'PARAMETERS']",OPTIMAL DESIGNS IN REGRESSION WITH CORRELATED ERRORS,2016
1228,"We consider the common nonlinear regression model where the variance, as well as the mean, is a parametric function of the explanatory variables. The c-optimal design problem is investigated in the case when the parameters of both the mean and the variance function are of interest. A geometric characterization of c-optimal designs in this context is presented, which generalizes the classical result of Elfving [Ann. Math. Statist. 23 (1952) 255-262] for c-optimal designs. As in Elfving's famous characterization, c-optimal designs can be described as representations of boundary points of a convex set. However, in the case where there appear parameters of interest in the variance, the structure of the Elfving set is different. Roughly speaking, the Elfving set corresponding to a heteroscedastic regression model is the convex hull of a set of ellipsoids induced by the underlying model and indexed by the design space. The c-optimal designs are characterized as representations of the points where the line in direction of the vector c intersects the boundary of the new Elfving set. The theory is illustrated in several examples including pharmacokinetic models with random effects.",WOS:000271673700013,ANNALS OF STATISTICS,"['OPTIMAL MINIMAX DESIGNS', 'LINEAR-MODELS', 'CONSTRUCTION', 'CRITERION', 'RESPECT', 'THEOREM']",A GEOMETRIC CHARACTERIZATION OF c-OPTIMAL DESIGNS FOR HETEROSCEDASTIC REGRESSION,2009
1229,"Many common diseases, such as the flu and cardiovascular disease, increase markedly in winter and dip in summer. These seasonal patterns have been part of life for millennia and were first noted in ancient Greece by both Hippocrates and Herodotus. Recent interest has focused on climate change, and the concern that seasons will become more extreme with harsher winter and summer weather. We describe a set of R functions designed to model seasonal patterns in disease. We illustrate some simple descriptive and graphical methods, a more complex method that is able to model non-stationary patterns, and the case-crossover to control for seasonal confounding.",WOS:000313197700002,R JOURNAL,,Analysing Seasonal Data,2012
1230,"For high dimensional statistical models, researchers have begun to focus on situations which can be described as having relatively few moderately large coefficients. Such situations lead to some very subtle statistical problems. In particular, Ingster and Donoho and Jin have considered a sparse normal means testing problem, in which they described the precise demarcation or detection boundary. Meinshausen and Rice have shown that it is even possible to estimate consistently the fraction of nonzero coordinates on a subset of the detectable region, but leave unanswered the question of exactly in which parts of the detectable region consistent estimation is possible.
In the present paper we develop a new approach for estimating the fraction of nonzero means for problems where the nonzero means are moderately large. We show that the detection region described by Ingster and Donoho and Jin turns out to be the region where it is possible to consistently estimate the expected fraction of nonzero coordinates. This theory is developed further and minimax rates of convergence are derived. A procedure is constructed which attains the optimal rate of convergence in this setting. Furthermore, the procedure also provides an honest lower bound for confidence intervals while minimizing the expected length of such an interval. Simulations are used to enable comparison with the work of Meinshausen and Rice, where a procedure is given but where rates of convergence have not been discussed. Extensions to more general Gaussian mixture models are also given.",WOS:000253077800010,ANNALS OF STATISTICS,,Estimation and confidence sets for sparse normal mixtures,2007
1231,"We show that scale-adjusted versions of the centroid-based classifier enjoys optimal properties when used to discriminate between two very high-dimensional populations where the principal differences are in location. The scale adjustment removes the tendency of scale differences to confound differences in means. Certain other distance-based methods, for example, those founded on nearest-neighbor distance, do not have optimal performance in the sense that we propose. Our results permit varying degrees of sparsity and signal strength to be treated, and require only mild conditions on dependence of vector components. Additionally, we permit the marginal distributions of vector components to vary extensively. In addition to providing theory we explore numerical properties of a centroid-based classifier, and show that these features reflect theoretical accounts of performance.",WOS:000275510800017,ANNALS OF STATISTICS,"['RANGE CORRELATIONS', 'GENE-EXPRESSION', 'CLASSIFICATION']",OPTIMAL PROPERTIES OF CENTROID-BASED CLASSIFIERS FOR VERY HIGH-DIMENSIONAL DATA,2010
1232,"Due to recent advances in methods and software for model-based clustering, and to the interpretability of the results, clustering procedures based on probability models are increasingly preferred over heuristic methods. The clustering process estimates a model for the data that allows for overlapping clusters, producing a probabilistic clustering that quantifies the uncertainty of observations belonging to components of the mixture. The resulting clustering model can also be used for some other important problems in multivariate analysis, including density estimation and discriminant analysis. Examples of the use of model-based clustering and classification techniques in chemometric studies include multivariate image analysis, magnetic resonance imaging, microarray image segmentation, statistical process control, and food authenticity. We review model-based clustering and related methods for density estimation and discriminant analysis, and show how the R package mclust can be applied in each instance.",WOS:000244067800001,JOURNAL OF STATISTICAL SOFTWARE,"['CLUSTER-ANALYSIS', 'SEGMENTATION', 'SELECTION']",Model-based methods of classification: Using the mclust software in chemometrics,2007
1233,"In this paper, we derive tail approximations of integrals of exponential functions or Gaussian random fields with varying mean functions and approximations of the associated point processes. This study is motivated naturally by multiple applications such as hypothesis testing for spatial models and financial applications.",WOS:000304684900011,ANNALS OF STATISTICS,"['VALUE-AT-RISK', 'TIME-SERIES', 'TAIL PROBABILITIES', 'REGRESSION-MODEL', 'BROWNIAN-MOTION', 'MONTE-CARLO', 'MAXIMUM', 'SPACE', 'COUNTS', 'STATISTICS']",SOME ASYMPTOTIC RESULTS OF GAUSSIAN RANDOM FIELDS WITH VARYING MEAN FUNCTIONS AND THE ASSOCIATED PROCESSES,2012
1234,"This article describes the extreme value analysis (EVA) R package extRemes version 2.0, which is completely redesigned from previous versions. The functions primarily provide utilities for implementing univariate EVA, with a focus on weather and climate applications, including the incorporation of covariates, as well as some functionality for assessing bivariate tail dependence.",WOS:000389073100001,JOURNAL OF STATISTICAL SOFTWARE,"['ARCTIC OSCILLATION', 'MAXIMUM', 'TEMPERATURES', 'STATISTICS', 'ESTIMATORS', 'INFERENCE', 'ARIZONA', 'PHOENIX', 'PARETO', 'TRENDS']",extRemes 2.0: An Extreme Value Analysis Package in R,2016
1235,,WOS:000208589900010,R JOURNAL,,Conference Review: The 2nd Chinese R Conference,2010
1236,"A SAS macro for fitting an extension of the Dale (1986) regression model to bivariate ordinal data is provided. The macro is described in detail and examples from Dale (1986) and McMillan, Hanson, Bedrick, and Lapham (2005) are discussed.",WOS:000232890400001,JOURNAL OF STATISTICAL SOFTWARE,['ORDERED RESPONSES'],SAS macro BDM for fitting the Dale regression model to bivariate ordinal response data,2005
1237,"In this paper, we deal with the problem of marginalization over and conditioning on two disjoint subsets of the node set of chain graphs (CGs) with the LWF Markov property. For this purpose, we define the class of chain mixed graphs (CMGs) with three types of edges and, for this class, provide a separation criterion under which the class of CMGs is stable under marginalization and conditioning and contains the class of LWF CGs as its subclass. We provide a method for generating such graphs after marginalization and conditioning for a given CMG or a given LWF CG. We then define and study the class of anterial graphs, which is also stable under marginalization and conditioning and contains LWF CGs, but has a simpler structure than CMGs.",WOS:000379972900015,ANNALS OF STATISTICS,"['MARKOV PROPERTIES', 'MIXED GRAPHS', 'MODELS', 'SYSTEMS']",MARGINALIZATION AND CONDITIONING FOR LWF CHAIN GRAPHS,2016
1238,"The Cauchy distribution is usually presented as a mathematical curiosity, an exception to the Law of Large Numbers, or even as an ""Evil"" distribution in some introductory courses. It therefore surprised us when Drton and Xiao [Bernoulli 22 (2016) 38-59] proved the following result for m = 2 and conjectured it for m >= 3. Let X = (X-1,...,X-m) and Y = (Y-1 ,...,Y-m) be i.i.d. N(0, Sigma), where Sigma = {sigma(ij)} >= 0 is an m x m and arbitrary covariance matrix with sigma(jj) > 0 for all 1 <= j <= m. Then
Z = Sigma(m)(j=1) w(j) X-j/Y-j similar to Cauchy (0, 1),
as long as (w) over right arrow = (w(1),...,w(m)) is independent of (X, Y), w(j) >= 0, j = 1,..., m, and Sigma(m)(j=1) w(j) = 1. In this note, we present an elementary proof of this conjecture for any m >= 2 by linking Z to a geometric characterization of Cauchy(0, 1) given in Willams [Ann. Math. Stat. 40 (1969) 1083-1085]. This general result is essential to the large sample behavior of Wald tests in many
applications such as factor models and contingency tables. It also leads to other unexpected results such as
Sigma(m)(i=1) Sigma(m)(j=1) w(i)w(j)sigma(ij)/XiXj similar to Levy (0,1)
This generalizes the ""super Cauchy phenomenon"" that the average of m i.i.d. standard Levy variables (i.e., inverse chi-squared variables with one degree of freedom) has the same distribution as that of a single standard Levy variable multiplied by m (which is obtained by taking w(j) = 1/m and Sigma to be the identity matrix).",WOS:000384397200013,ANNALS OF STATISTICS,"['DISTRIBUTED FUNCTIONS', 'RANDOM-VARIABLES', 'HYPOTHESES']",AN UNEXPECTED ENCOUNTER WITH CAUCHY AND LEVY,2016
1239,"Uniformly most powerful tests are statistical hypothesis tests that provide the greatest power against a fixed null hypothesis among all tests of a given size. In this article, the notion of uniformly most powerful tests is extended to the Bayesian setting by defining uniformly most powerful Bayesian tests to be tests that maximize the probability that the Bayes factor, in favor of the alternative hypothesis, exceeds a specified threshold. Like their classical counterpart, uniformly most powerful Bayesian tests are most easily defined in one-parameter exponential family models, although extensions outside of this class are possible. The connection between uniformly most powerful tests and uniformly most powerful Bayesian tests can be used to provide an approximate calibration between p-values and Bayes factors. Finally, issues regarding the strong dependence of resulting Bayes factors and p-values on sample size are discussed.",WOS:000326991200002,ANNALS OF STATISTICS,"['STATISTICAL-INFERENCE', 'MODEL SELECTION', 'HYPOTHESIS']",UNIFORMLY MOST POWERFUL BAYESIAN TESTS,2013
1240,"Logistic regression provides a flexible framework for detecting various types of differential item functioning (DIF). Previous efforts extended the framework by using item response theory (IRT) based trait scores, and by employing an iterative process using group-specific item parameters to account for DIF in the trait scores, analogous to purification approaches used in other DIF detection frameworks. The current investigation advances the technique by developing a computational platform integrating both statistical and IRT procedures into a single program. Furthermore, a Monte Carlo simulation approach was incorporated to derive empirical criteria for various DIF statistics and effect size measures. For purposes of illustration, the procedure was applied to data from a questionnaire of anxiety symptoms for detecting DIF associated with age from the Patient-Reported Outcomes Measurement Information System.",WOS:000288205000001,JOURNAL OF STATISTICAL SOFTWARE,"['CONFIRMATORY FACTOR-ANALYSIS', 'MANTEL-HAENSZEL', 'MEASUREMENT INVARIANCE', 'EXPLAINED VARIATION', 'DIF DETECTION', 'IRT MODELS', 'FIT INDEX', 'BIAS', 'IDENTIFICATION', 'QUESTIONNAIRE']",lordif: An R Package fo rDetecting Differential Item Functioning Using Iterative Hybrid Ordinal Logistic Regression/Item Response Theory and Monte Carlo Simulations,2011
1241,"In this paper, we study the ordinary backfitting and smooth backfitting as methods of fitting additive quantile models. We show that these backfitting quantile estimators are asymptotically equivalent to the corresponding backfitting estimators of the additive components in a specially-designed additive mean regression model. This implies that the theoretical properties of the backfitting quantile estimators are not unlike those of backfitting mean regression estimators. We also assess the finite sample properties of the two backfitting quantile estimators.",WOS:000282402800009,ANNALS OF STATISTICS,"['NONPARAMETRIC-ESTIMATION', 'ASYMPTOTIC PROPERTIES', 'REGRESSION QUANTILES']",BACKFITTING AND SMOOTH BACKFITTING FOR ADDITIVE QUANTILE MODELS,2010
1242,"We derive a Gaussian approximation result for the maximum of a sum of high-dimensional random vectors. Specifically, we establish conditions under which the distribution of the maximum is approximated by that of the maximum of a sum of the Gaussian random vectors with the same covariance matrices as the original vectors. This result applies when the dimension of random vectors (p) is large compared to the sample size (n); in fact, p can be much larger than n, without restricting correlations of the coordinates of these vectors. We also show that the distribution of the maximum of a sum of the random vectors with unknown covariance matrices can be consistently estimated by the distribution of the maximum of a sum of the conditional Gaussian random vectors obtained by multiplying the original vectors with i.i.d. Gaussian multipliers. This is the Gaussian multiplier (or wild) bootstrap procedure. Here too, p can be large or even much larger than n. These distributional approximations, either Gaussian or conditional Gaussian, yield a high-quality approximation to the distribution of the original maximum, often with approximation error decreasing polynomially in the sample size, and hence are of interest in many applications. We demonstrate how our Gaussian approximations and the multiplier bootstrap can be used for modern high-dimensional estimation, multiple hypothesis testing, and adaptive specification testing. All these results contain nonasymptotic bounds on approximation errors.",WOS:000330204900004,ANNALS OF STATISTICS,"['DANTZIG SELECTOR', 'DENSITY-ESTIMATION', 'MODELS', 'LASSO', 'TESTS']",GAUSSIAN APPROXIMATIONS AND MULTIPLIER BOOTSTRAP FOR MAXIMA OF SUMS OF HIGH-DIMENSIONAL RANDOM VECTORS,2013
1243,"We consider the statistical experiment given by a sample y(l),...,y(n) of a stationary Gaussian process with an unknown smooth spectral density f. Asymptotic equivalence, in the sense of Le Cam's deficiency Delta-distance, to two Gaussian experiments with simpler structure is established. The first one is given by independent zero mean Gaussians with variance approximately f(omega(i)), where omega(i) is a uniform grid of points in (-pi, pi) (nonparametric Gaussian scale regression). This approximation is closely related to well-known asymptotic independence results for the periodogram and corresponding inference methods. The second asymptotic equivalence is to a Gaussian white noise model where the drift function is the log-spectral density. This represents the step from a Gaussian scale model to a location model, and also has a counterpart in established inference methods, that is, log-periodogram regression. The problem of simple explicit equivalence maps (Markov kernels), allowing to directly carry over inference, appears in this context but is not solved here.",WOS:000273800100006,ANNALS OF STATISTICS,"['TIME-SERIES ANALYSIS', 'NONPARAMETRIC REGRESSION', 'ADDITIONAL OBSERVATIONS']",ASYMPTOTIC EQUIVALENCE OF SPECTRAL DENSITY ESTIMATION AND GAUSSIAN WHITE NOISE,2010
1244,"The Shenington-Kirkpatrick model of spin glasses, the Hopfield model of neural networks and the Ising spin glass are all models of binary data belonging to the one-parameter exponential family with quadratic sufficient statistic. Under bare minimal conditions, we establish the root N-consistency of the maximum pseudolikelihood estimate of the natural parameter in this family, even at critical temperatures. Since very little is known about the low and critical temperature regimes of these extremely difficult models, the proof requires several new ideas. The author's version of Stein's method is a particularly useful tool. We aim to introduce these techniques into the realm of mathematical statistics through an example and present some open questions.",WOS:000251096100004,ANNALS OF STATISTICS,"['SHERRINGTON-KIRKPATRICK MODEL', 'RANDOM MATRICES', 'SYSTEMS', 'LATTICE']",Estimation in spin glasses: A first step,2007
1245,"Community detection, which aims to cluster N nodes in a given graph into r distinct groups based on the observed undirected edges, is an important problem in network data analysis. In this paper, the popular stochastic block model (SBM) is extended to the generalized stochastic block model (GSBM) that allows for adversarial outlier nodes, which are connected with the other nodes in the graph in an arbitrary way. Under this model, we introduce a procedure using convex optimization followed by k-means algorithm with k = r.
Both theoretical and numerical properties of the method are analyzed. A theoretical guarantee is given for the procedure to accurately detect the communities with small misclassification rate under the setting where the number of clusters can grow with N. This theoretical result admits to the best-known result in the literature of computationally feasible community detection in SBM without outliers. Numerical results show that our method is both computationally fast and robust to different kinds of outliers, while some popular computationally fast community detection algorithms, such as spectral clustering applied to adjacency matrices or graph Laplacians, may fail to retrieve the major clusters due to a small portion of outliers. We apply a slight modification of our method to a political blogs data set, showing that our method is competent in practice and comparable to existing computationally feasible methods in the literature. To the best of the authors' knowledge, our result is the first in the literature in terms of clustering communities with fast growing numbers under the GSBM where a portion of arbitrary outlier nodes exist.",WOS:000355768700004,ANNALS OF STATISTICS,"['STOCHASTIC BLOCK-MODELS', 'CONVEX-OPTIMIZATION', 'MAXIMUM-LIKELIHOOD', 'SPARSE NETWORKS', 'SIGNAL RECOVERY', 'RANDOM GRAPHS', 'BLOCKMODELS', 'CONSISTENCY', 'PREDICTION', 'PARTITIONS']",ROBUST AND COMPUTATIONALLY FEASIBLE COMMUNITY DETECTION IN THE PRESENCE OF ARBITRARY OUTLIER NODES,2015
1246,"We investigate the problem of finding confidence sets for split points in decision trees (CART). Our main results establish the asymptotic distribution of the least squares estimators and some associated residual sum of squares statistics in a binary decision tree approximation to a smooth regression curve. Cube-root asymptotics with nonnormal limit distributions are involved. We study various confidence sets for the split point, one calibrated using the subsampling bootstrap, and others calibrated using plug-in estimates of some nuisance parameters. The performance of the confidence sets is assessed in a simulation study. A motivation for developing such confidence sets comes from the problem of phosphorus pollution in the Everglades. Ecologists have suggested that split points provide a phosphorus threshold at which biological imbalance occurs, and the lower endpoint of the confidence set may be interpreted as a level that is protective of the ecosystem. This is illustrated using data from a Duke University Wetlands Center phosphorus dosing study in the Everglades.",WOS:000248987600004,ANNALS OF STATISTICS,"['CUBE ROOT ASYMPTOTICS', 'REGRESSION']",Confidence sets for split points in decision trees,2007
1247,"Given an n-sample of random vectors (X-i, Y-i)(1 <= i <= n) whose joint law is unknown, the long-standing problem of supervised classification aims to optimally predict the label Y of a given new observation X. In this context, the k-nearest neighbor rule is a popular flexible and intuitive method in non parametric situations. Even if this algorithm is commonly used in the machine learning and statistics communities, less is known about its prediction ability in general finite dimensional spaces, especially when the support of the density of the observations is R-d. This paper is devoted to the study of the statistical properties of the k-nearest neighbor rule in various situations. In particular, attention is paid to the marginal law of X, as well as the smoothness and margin properties of the regression function eta(X) = E[Y vertical bar X]. We identify two necessary and sufficient conditions to obtain uniform consistency rates of classification and derive sharp estimates in the case of the k-nearest neighbor rule. Some numerical experiments are proposed at the end of the paper to help illustrate the discussion.",WOS:000375175200004,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'DENSITY-ESTIMATION', 'CLASSIFIERS', 'CONVERGENCE', 'CONSISTENCY', 'RATES']",CLASSIFICATION IN GENERAL FINITE DIMENSIONAL SPACES WITH THE k-NEAREST NEIGHBOR RULE,2016
1248,basicspace is an R package that conducts Aldrich-McKelvey and Blackbox scaling to recover estimates of the underlying latent dimensions of issue scale data. We illustrate several applications of the package to survey data commonly used in the social sciences. Monte Carlo tests demonstrate that the procedure can recover latent dimensions and reproduce the matrix of responses at moderate levels of error and missing data.,WOS:000373917800001,JOURNAL OF STATISTICAL SOFTWARE,['INDIVIDUAL-DIFFERENCES'],Recovering a Basic Space from Issue Scales in R,2016
1249,"The BayesLCA package for R provides tools for performing latent class analysis within a Bayesian setting. Three methods for fitting the model are provided, incorporating an expectation-maximization algorithm, Gibbs sampling and a variational Bayes approximation. The article briefly outlines the methodology behind each of these techniques and discusses some of the technical difficulties associated with them. Methods to remedy these problems are also described. Visualization methods for each of these techniques are included, as well as criteria to aid model selection.",WOS:000349842200001,JOURNAL OF STATISTICAL SOFTWARE,"['MIXTURE-MODELS', 'MAXIMUM-LIKELIHOOD', 'EM ALGORITHM', 'DISTRIBUTIONS']",BayesLCA: An R Package for Bayesian Latent Class Analysis,2014
1250,"An algorithm is presented for calculating concordance-discordance totals in a time of order N log N, where N is the number of observations, using a balanced binary search tree. These totals can be used to calculate jackknife estimates and confidence limits in the same time order for a very wide range of rank statistics, including Kendall's tau, Somers' D, Harrell's c, the area under the receiver operating characteristic (ROC) curve, the Gini coefficient, and the parameters underlying the sign and rank-sum tests. A Stata package is introduced for calculating confidence intervals for these rank statistics using this algorithm, which has been implemented in the Mata compilable matrix programming language supplied with Stata.",WOS:000235180400001,JOURNAL OF STATISTICAL SOFTWARE,,Efficient calculation of jackknife confidence intervals for rank statistics,2006
1251,"As larger sets of high-throughput data in genomics and proteomics become more readily available, there is a growing need for fast algorithms designed to compute exact p values of distribution-free statistical tests. We present a program for computing the exact distribution of the two-sample Cramer-von Mises test statistic under the null hypothesis that the two samples are drawn from the same continuous distribution. The program makes it possible to handle substantially larger sample sizes than earlier proposed computational tools. The C++ source code for the program is published with this paper, and an R package is under development.",WOS:000243591000001,JOURNAL OF STATISTICAL SOFTWARE,"['DISTRIBUTIONS', 'STATISTICS', 'CRITERION']",A C++ program for the Cramer-von Mises two-sample test,2007
1252,Exponential-family random graph models (ERGMs) represent a powerful and flexible class of models for the statistical analysis of networks. statnet is a suite of software packages that implement these models. This paper details how the capabilities for ERGM modeling can be expanded and customized by programming additional network statistics that may be included in ERGMs. We describe a template R package called ergm.userterms that can be modified for this purpose. It is designed to make this process as straight forward as possible. We also explain some of the internal workings of statnet that will help users develop their own network analysis capabilities.,WOS:000315018800001,JOURNAL OF STATISTICAL SOFTWARE,,ergm.userterms: A Template Package for Extending statnet,2013
1253,,WOS:000277471000023,ANNALS OF STATISTICS,,"REPLY TO ""ON SOME PROBLEMS IN THE ARTICLE EFFICIENT LIKELIHOOD ESTIMATION IN STATE SPACE MODELS"" (vol 38, pg 1282, 2010)",2010
1254,,WOS:000313197700001,R JOURNAL,,Untitled,2012
1255,"We propose a test for model specification of a parametric diffusion process based on a kernel estimation of the transitional density of the process. The empirical likelihood is used to formulate a statistic, for each kernel smoothing bandwidth, which is effectively a Studentized L-2-distance between the kernel transitional density estimator and the parametric transitional density implied by the parametric process. To reduce the sensitivity of the test on smoothing bandwidth choice, the final test statistic is constructed by combining the empirical likelihood statistics over a set of smoothing bandwidths. TO better capture the finite sample distribution of the test statistic and data dependence, the critical value of the test is obtained by a parametric bootstrap procedure. Properties of the test are evaluated asymptotically and numerically by simulation and by a real data example.",WOS:000253390000007,ANNALS OF STATISTICS,"['OF-FIT TESTS', 'MAXIMUM-LIKELIHOOD-ESTIMATION', 'CONTINUOUS-TIME MODELS', 'EMPIRICAL LIKELIHOOD', 'TERM STRUCTURE', 'NONPARAMETRIC-ESTIMATION', 'CONDITIONAL DENSITIES', 'REGRESSION-MODELS', 'INTEREST-RATES', 'SERIES']",A test for model specification of diffusion processes,2008
1256,"Analysis of longitudinal count data has, for long, been done using a generalized linear mixed model (GLMM), in its Poisson-normal version, to account for correlation by specifying normal random effects. Univariate counts are often handled with the negativebinomial (NEGBIN) model taking into account overdispersion by use of gamma random effects. Inherently though, longitudinal count data commonly exhibit both features of correlation and overdispersion simultaneously, necessitating analysis methodology that can account for both. The introduction of the combined model (CM) by Molenberghs, Verbeke, and Demetrio (2007) and Molenberghs, Verbeke, Demetrio, and Vieira (2010) serves this purpose, not only for count data but for the general exponential family of distributions. Here, a Poisson model is specified as the parent distribution of the data with a normally distributed random effect at the subject or cluster level and/or a gamma distribution at observation level. The GLMM and NEGBIN model are special cases. Data can be simulated from (1) the general CM, with random effects, or, (2) its marginal version directly. This paper discusses an implementation of (1) in SAS software (SAS Inc. 2011). One needs to reflect on the mean of both the combined (hierarchical) and marginal models in order to generate correlated and/or overdispersed counts. A pre-specification of the desired marginal mean (in terms of covariates and marginal parameters), a marginal variance-covariance structure and the hierarchical mean (in terms of covariates and regression parameters) is required. The implied hierarchical parameters, the variance-covariance matrix of the random effects, and the variance-covariance matrix of the overdispersion part are then derived from which correlated Poisson data are generated. Sample calls of the SAS macro are presented as well as output.",WOS:000373920300001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR MIXED MODELS', 'POISSON-DISTRIBUTION', 'DISTRIBUTIONS', 'ALGORITHM', 'REGRESSION', 'COPULAS', 'PACKAGE']",Generating Correlated and/or Overdispersed Count Data: A SAS Implementation,2016
1257,,WOS:000348651700001,R JOURNAL,,Untitled,2014
1258,"We study maximum likelihood estimation in log-linear models under conditional Poisson sampling schemes. We derive necessary and sufficient conditions for existence of the maximum likelihood estimator (MLE) of the model parameters and investigate estimability of the natural and mean-value parameters under a nonexistent MLE. Our conditions focus on the role of sampling zeros in the observed table. We situate our results within the framework of extended exponential families, and we exploit the geometric properties of log-linear models. We propose algorithms for extended maximum likelihood estimation that improve and correct the existing algorithms for log-linear model analysis.",WOS:000307608000014,ANNALS OF STATISTICS,"['EXPONENTIAL-FAMILIES', 'CONTINGENCY-TABLES', 'GEOMETRY']",MAXIMUM LIKELIHOOD ESTIMATION IN LOG-LINEAR MODELS,2012
1259,"The purpose of this paper is to discuss empirical risk minimization when the losses are not necessarily bounded and may have a distribution with heavy tails. In such situations, usual empirical averages may fail to provide reliable estimates and empirical risk minimization may provide large excess risk. However, some robust mean estimators proposed in the literature may be used to replace empirical means. In this paper, we investigate empirical risk minimization based on a robust estimate proposed by Catoni. We develop performance bounds based on chaining arguments tailored to Catoni's mean estimator.",WOS:000363437900007,ANNALS OF STATISTICS,"['K-MEANS', 'VECTOR QUANTIZERS', 'HILBERT-SPACES', 'QUANTIZATION', 'CONVERGENCE', 'DISTORTION', 'DESIGN', 'RATES']",EMPIRICAL RISK MINIMIZATION FOR HEAVY-TAILED LOSSES,2015
1260,"A category of item response models is presented with two defining features: they all (i) have a tree representation, and (ii) are members of the family of generalized linear mixed models (GLMM). Because the models are based on trees, they are denoted as IRTree models. The GLMM nature of the models implies that they can all be estimated with the glmer function of the lme4 package in R. The aim of the article is to present four subcategories of models, the first two of which are based on a tree representation for response categories: 1. linear response tree models (e.g., missing response models), 2. nested response tree models (e.g., models for parallel observations regarding item responses such as agreement and certainty), while the last two are based on a tree representation for latent variables: 3. linear latent-variable tree models (e.g., models for change processes), and 4. nested latent-variable tree models (e.g., bi-factor models). The use of the g l m er function is illustrated for all four subcategories. Simulated example data sets and two service functions useful in preparing the data for IRTree modeling with glmer are provided in the form of an R package, irtrees. For all four subcategories also a real data application is discussed.",WOS:000326871200001,JOURNAL OF STATISTICAL SOFTWARE,"['TIME SURVIVAL ANALYSIS', 'ORDERED RESPONSE', 'MISSING-DATA', 'CATEGORIES', 'COGNITION', 'PACKAGE']",IRTrees: Tree-Based Item Response Models of the GLMM Family,2012
1261,"This paper is concerned with the problems of interaction screening and nonlinear classification in a high-dimensional setting. We propose a two-step procedure, IIS-SQDA, where in the first step an innovated interaction screening (ITS) approach based on transforming the original p-dimensional feature vector is proposed, and in the second step a sparse quadratic discriminant analysis (SQDA) is proposed for further selecting important interactions and main effects and simultaneously conducting classification. Our IIS approach screens important interactions by examining only p features instead of all two-way interactions of order O(p(2)). Our theory shows that the proposed method enjoys sure screening property in interaction selection in the high-dimensional setting of p growing exponentially with the sample size. In the selection and classification step, we establish a sparse inequality on the estimated coefficient vector for QDA and prove that the classification error of our procedure can be upper-bounded by the oracle classification error plus some smaller order term. Extensive simulation studies and real data analysis show that our proposal compares favorably with existing methods in interaction selection and high-dimensional classification.",WOS:000355768700010,ANNALS OF STATISTICS,"['LINEAR DISCRIMINANT-ANALYSIS', 'PRECISION MATRIX ESTIMATION', 'VARIABLE SELECTION', 'ENVIRONMENT INTERACTIONS', 'COVARIANCE ESTIMATION', 'LOGISTIC-REGRESSION', 'DANTZIG SELECTOR', 'GENE-EXPRESSION', 'LASSO', 'MODELS']",INNOVATED INTERACTION SCREENING FOR HIGH-DIMENSIONAL NONLINEAR CLASSIFICATION,2015
1262,"When respondents use the ordinal response categories of standard survey questions in different ways, the validity of analyses based on the resulting data can be biased. Anchoring vignettes is a survey design technique intended to correct for some of these problems. The anchors package in R includes methods for evaluating and choosing anchoring vignettes, and for analyzing the resulting data.",WOS:000292096800001,JOURNAL OF STATISTICAL SOFTWARE,,anchors: Software for Anchoring Vignette Data,2011
1263,"Consider a graph with a set of vertices and oriented edges connecting pairs of vertices. Each vertex is associated with a random variable and these are assumed to be independent. In this setting, suppose we wish to solve the following hypothesis testing problem: under the null, the random variables have common distribution N(0, 1) while under the alternative, there is an unknown path along which random variables have distribution N(mu, 1), mu > 0, and distribution N(0, 1) away from it. For which values of the mean shift mu can one reliably detect and for which values is this impossible?
Consider, for example, the usual regular lattice with vertices of the form
{(i, j) : 0 <= i, -i <= j <= i and j has the parity of i}
and oriented edges (i, j) -> (i + 1, j + s), where s = +/- 1. We show that for paths of length m starting at the origin, the hypotheses become distinguishable (in a minimax sense) if mu(m) >> 1/root logm, while they are not if mu(m) << 1/log m. We derive equivalent results in a Bayesian setting where one assumes that all paths are equally likely; there, the asymptotic threshold is mu(m) approximate to m(-14).
We obtain corresponding results for trees (where the threshold is of order I and independent of the size of the tree), for distributions other than the Gaussian and for other graphs. The concept of the predictability profile, first introduced by Benjamini, Pemantle and Peres, plays a crucial role in our analysis.",WOS:000258243000012,ANNALS OF STATISTICS,"['PERCOLATION', 'POLYMERS', 'TREES']",Searching for a trail of evidence in a maze,2008
1264,"In this paper we provide a provably convergent algorithm for the multi-variate Gaussian Maximum Likelihood version of the Behrens-Fisher Problem. Our work builds upon a formulation of the log-likelihood function proposed by Buot and Richards [5]. Instead of focusing on the first order optimality conditions, the algorithm aims directly for the maximization of the log-likelihood function itself to achieve a global solution. Convergence proof and complexity estimates are provided for the algorithm. Computational experiments illustrate the applicability of such methods to high-dimensional data. We also discuss how to extend the proposed methodology to a broader class of problems.
We establish a systematic algebraic relation between the Wald, Likelihood Ratio and Lagrangian Multiplier Test (W >= LR >= LM) in the context of the Behrens-Fisher Problem. Moreover, we use our algorithm to computationally investigate the finite-sample size and power of the Wald, Likelihood Ratio and Lagrange Multiplier Tests, which previously were only available through asymptotic results. The methods developed here are applicable to much higher dimensional settings than the ones available in the literature. This allows us to better capture the role of high dimensionality oil the actual size and power of the tests for finite samples.",WOS:000260554100014,ANNALS OF STATISTICS,"['LINEAR-REGRESSION MODEL', 'CONFLICT', 'CRITERIA', 'HYPOTHESES', 'VARIANCES', 'FREEDOM']","ON THE BEHRENS-FISHER PROBLEM: A GLOBALLY CONVERGENT ALGORITHM AND A FINITE-SAMPLE STUDY OF THE WALD, LR AND LM TESTS",2008
1265,"It is generally accepted that the asset price processes contain jumps. In fact, pure jump models have been widely used to model asset prices and/or stochastic volatilities. The question is: is there any statistical evidence from the high-frequency financial data to support using pure jump models alone? The purpose of this paper is to develop such a statistical test against the necessity of a diffusion component. The test is very simple to use and yet effective. Asymptotic properties of the proposed test statistic will be studied. Simulation studies and some real-life examples are included to illustrate our results.",WOS:000307608000005,ANNALS OF STATISTICS,"['MICROSTRUCTURE NOISE', 'LEVY PROCESSES', 'STOCHASTIC VOLATILITY', 'CURRENCY OPTIONS', 'FUNCTIONALS', 'VALUATION', 'MARKETS', 'DRIVEN']",MODELING HIGH-FREQUENCY FINANCIAL DATA BY PURE JUMP PROCESSES,2012
1266,"We present GeoXp, an R package implementing interactive graphics for exploratory spatial data analysis. We use a data set concerning public schools of the French Midi-Pyrenees region to illustrate the use of these exploratory techniques based on the coupling between a statistical graph and a map. Besides elementary plots like boxplots, histograms or simple scatterplots, GeoXp also couples maps with Moran scatterplots, variogram clouds, Lorenz curves and other graphical tools. In order to make the most of the multidimensionality of the data, GeoXp includes dimension reduction techniques such as principal components analysis and cluster analysis whose results are also linked to the map.",WOS:000303803600001,JOURNAL OF STATISTICAL SOFTWARE,"['DYNAMIC GRAPHICS', 'LOCAL INDICATORS', 'ASSOCIATION', 'STATISTICS']",GeoXp: An R Package for Exploratory Spatial Data Analysis,2012
1267,In this paper we present an R package called bivpois for maximum likelihood estimation of the parameters of bivariate and diagonal inflated bivariate Poisson regression models. An Expectation-Maximization (EM) algorithm is implemented. Inflated models allow for modelling both over-dispersion (or under-dispersion) and negative correlation and thus they are appropriate for a wide range of applications. Extensions of the algorithms for several other models are also discussed. Detailed guidance and implementation on simulated and real data sets using bivpois package is provided.,WOS:000232891800001,JOURNAL OF STATISTICAL SOFTWARE,['EM ALGORITHM'],Bivariate Poisson and diagonal inflated bivariate Poisson regression models in R,2005
1268,"The theory of adaptive estimation and oracle inequalities for the case of Gaussian-shift-finite-interval experiments has made significant progress in recent years. In particular, sharp-minimax adaptive estimators and exact exponential-type oracle inequalities have been suggested for a vast set of functions including analytic and Sobolev with any positive index as well as for Efromovich-Pinsker and Stein blockwise-shrinkage estimators. Is it possible to obtain similar results for a more interesting applied problem of density estimation and/or the dual problem of characteristic function estimation? The answer is ""yes."" In particular, the obtained results include exact exponential-type oracle inequalities which allow to consider, for the first time in the literature, a simultaneous sharp-minimax estimation of Sobolev densities with any positive index (not necessarily larger than 1/2), infinitely differentiable densities (including analytic, entire and stable), as well as of not absolutely integrable characteristic functions. The same adaptive estimator is also rate minimax over a familiar class of distributions with bounded spectrum where the density and the characteristic function can be estimated with the parametric rate.",WOS:000256504400005,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'WAVELET', 'REGRESSION', 'SHRINKAGE', 'EQUIVALENCE', 'SMOOTHNESS']",Adaptive estimation of and oracle inequalities for probability densities and characteristic functions,2008
1269,"We propose a new penalized method for variable selection and estimation that explicitly incorporates the correlation patterns among predictors. This method is based on a combination of the minimax concave penalty and Laplacian quadratic associated with a graph as the penalty function. We call it the sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave penalty for encouraging sparsity and Laplacian quadratic penalty for promoting smoothness among coefficients associated with the correlated predictors. The SLS has a generalized grouping property with respect to the graph represented by the Laplacian quadratic. We show that the SLS possesses an oracle property in the sense that it is selection consistent and equal to the oracle Laplacian shrinkage estimator with high probability. This result holds in sparse, high-dimensional settings with p >> n under reasonable conditions. We derive a coordinate descent algorithm for computing the SLS estimates. Simulation studies are conducted to evaluate the performance of the SLS method and a real data example is used to illustrate its application.",WOS:000296995500007,ANNALS OF STATISTICS,"['COORDINATE DESCENT ALGORITHMS', 'VARIABLE SELECTION', 'PENALIZED REGRESSION', 'MODEL SELECTION', 'ELASTIC-NET', 'LASSO', 'REGULARIZATION', 'NORMALIZATION', 'EXPLORATION', 'PENALTY']",THE SPARSE LAPLACIAN SHRINKAGE ESTIMATOR FOR HIGH-DIMENSIONAL REGRESSION,2011
1270,"This article discusses the design and analysis of mixture experiments with R and illustrates the use of the recent package mixexp. This package provides functions for creating mixture designs composed of extreme vertices and edge and face centroids in constrained mixture regions where components are subject to upper, lower and linear constraints. These designs cannot be created by other R packages. mixexp also provides functions for graphical display of models fit to data from mixture experiments that cannot be created with other R packages.",WOS:000389126400001,JOURNAL OF STATISTICAL SOFTWARE,"['PROCESS VARIABLES', 'EXTREME VERTICES', 'DESIGNS']",Mixture Experiments in R Using mixexp,2016
1271,"Generalized linear and nonlinear mixed models (GLMMs and NLMMs) are commonly used to represent non-Gaussian or nonlinear longitudinal or clustered data. A common assumption is that the random effects are Gaussian. However, this assumption may be unrealistic in some applications, and misspecification of the random effects density may lead to maximum likelihood parameter estimators that are inconsistent, biased, and inefficient. Because testing if the random effects are Gaussian is difficult, previous research has recommended using a flexible random effects density. However, computational limitations have precluded widespread use of flexible random effects densities for GLMMs and NLMMs. We develop a SAS macro, SNP_NLMM, that overcomes the computational challenges to fit GLMMs and NLMMs where the random effects are assumed to follow a smooth density that can be represented by the seminonparametric formulation proposed by Gallant and Nychka (1987). The macro is flexible enough to allow for any density of the response conditional on the random effects and any nonlinear mean trajectory. We demonstrate the SNP_NLMM macro on a GLMM of the disease progression of toenail infection and on a NLMM of intravenous drug concentration over time.",WOS:000331695400001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'MULTIVARIATE-T-DISTRIBUTION', 'HIV VIRAL LOADS', 'LONGITUDINAL DATA', 'MIXING DISTRIBUTION', 'NORMAL/INDEPENDENT DISTRIBUTIONS', 'VARIABLE MODELS', 'EM ALGORITHM', 'EVENT DATA', 'QUADRATURE']",SNP_NLMM: A SAS Macro to Implement a Flexible Random Effects Density for Generalized Linear and Nonlinear Mixed Models,2014
1272,"In semivarying coefficient modeling of longitudinal/clustered data, of primary interest is usually the parametric component which involves unknown constant coefficients. First, we study semiparametric efficiency bound for estimation of the constant coefficients in a general setup. It can be achieved by spline regression using the true within-subject covariance matrices, which are often unavailable in reality. Thus, we propose an estimator when the covariance matrices are unknown and depend only on the index variable. First, we estimate the covariance matrices using residuals obtained from a preliminary estimation based on working independence and both spline and local linear regression. Then, using the covariance matrix estimates, we employ spline regression again to obtain our final estimator. It achieves the semiparametric efficiency bound under normality assumption and has the smallest asymptotic covariance matrix among a class of estimators even when normality is violated. Our theoretical results hold either when the number of within-subject observations diverges or when it is uniformly bounded. In addition, using the local linear estimator of the nonparametric component is superior to using the spline estimator in terms of numerical performance. The proposed method is compared with the working independence estimator and some existing method via simulations and application to a real data example.",WOS:000384397200010,ANNALS OF STATISTICS,"['PARTIALLY LINEAR-MODELS', 'QUADRATIC INFERENCE FUNCTIONS', 'SEMIPARAMETRIC ESTIMATION', 'ESTIMATING EQUATIONS', 'NONPARAMETRIC REGRESSION', 'SELECTION', 'SPLINES']",EFFICIENT ESTIMATION IN SEMIVARYING COEFFICIENT MODELS FOR LONGITUDINAL/CLUSTERED DATA,2016
1273,"We consider the problem of adaptation to the margin and to complexity in binary classification. We suggest an exponential weighting aggregation scheme. We use this aggregation procedure to construct classifiers which adapt automatically to margin and complexity. Two main examples are worked out in which adaptivity is achieved in frameworks proposed by Steinwart and Scovel [Learning Theory. Lecture Notes in Comput. Sci. 3559 (2005) 279-294. Springer, Berlin; Ann. Statist. 35 (2007) 575-607] and Tsybakov [Ann. Statist. 32 (2004) 135-166]. Adaptive schemes, like ERM or penalized ERM, usually involve a minimization step. This is not the case for our procedure.",WOS:000249568000014,ANNALS OF STATISTICS,"['SUPPORT VECTOR MACHINES', 'RISK MINIMIZATION', 'FAST RATES', 'AGGREGATION', 'REGRESSION', 'CLASSIFIERS', 'REGULARIZATION', 'CONSISTENCY', 'PENALTIES', 'BOUNDS']",Simultaneous adaptation to the margin and to complexity in classification,2007
1274,"The R environment provides a natural platform for developing new statistical methods due to the mathematical expressiveness of the language, the large number of existing libraries, and the active developer community. One drawback to R, however, is the learning curve; programming is a deterrent to non-technical users, who typically prefer graphical user interfaces (GUIs) to command line environments. Thus, while statisticians develop new methods in R, practitioners are often behind in terms of the statistical techniques they use as they rely on GUI applications. Meta-analysis is an instructive example; cutting-edge meta-analysis methods are often ignored by the overwhelming majority of practitioners, in part because they have no easy way of applying them. This paper proposes a strategy to close the gap between the statistical state-of-the-science and what is applied in practice. We present open-source meta-analys is software that uses R as the underlying statistical engine, and Python for the GUI. We present a frame work that allows methodologists to implement new methods in R that are then automatically integrated into the GUI for use by end-users, so long as the programmer conforms to our interface. Such an approach allows an intuitive interface for non-technical users while leveraging the latest advanced statistical methods implemented by methodologists",WOS:000305990500001,JOURNAL OF STATISTICAL SOFTWARE,"['CLINICAL-TRIALS', 'METAANALYSIS', 'STATISTICS', 'BINARY', 'MODEL']",Closing the Gap between Methodologists and End-Users: R as a Computational Back-End,2012
1275,"genoud is an R function that combines evolutionary algorithm methods with a derivative-based (quasi-Newton) method to solve difficult optimization problems. genoud may also be used for optimization problems for which derivatives do not exist. genoud solves problems that are nonlinear or perhaps even discontinuous in the parameters of the function to be optimized. When the function to be optimized (for example, a log-likelihood) is nonlinear in the model's parameters, the function will generally not be globally concave and may have irregularities such as saddlepoints or discontinuities. Optimization methods that rely on derivatives of the objective function may be unable to find any optimum at all. Multiple local optima may exist, so that there is no guarantee that a derivative-based method will converge to the global optimum. On the other hand, algorithms that do not use derivative information (such as pure genetic algorithms) are for many problems needlessly poor at local hill climbing. Most statistical problems are regular in a neighborhood of the solution. Therefore, for some portion of the search space, derivative information is useful. The function supports parallel processing on multiple CPUs on a single machine or a cluster of computers.",WOS:000292098000001,JOURNAL OF STATISTICAL SOFTWARE,['NONLINEAR SOLVER'],Genetic Optimization Using Derivatives: The rgenoud Package for R,2011
1276,"We present cna, a package for performing Coincidence Analysis (CNA). CNA is a configurational comparative method for the identification of complex causal dependencies-in particular, causal chains and common cause structures-in configurational data. After a brief introduction to the method's theoretical background and main algorithmic ideas, we demonstrate the use of the package by means of an artificial and a real-life data set. Moreover, we outline planned enhancements of the package that will further increase its applicability.",WOS:000357431900015,R JOURNAL,"['QUALITATIVE COMPARATIVE-ANALYSIS', 'SET RELATIONS', 'FUZZY', 'QCA', 'GENDER']",Identifying Complex Causal Dependencies in Configurational Data with Coincidence Analysis,2015
1277,"Parameters defined via general estimating equations (GEE) can be estimated by maximizing the empirical likelihood (EL). Newey and Smith [Econometrica 72 (2004) 219-255] have recently shown that this EL estimator exhibits desirable higher-order asymptotic properties, namely, that its O(n(-1)) bias is small and that bias-corrected EL is higher-order efficient. Although EL possesses these properties when the model is correctly specified, this paper shows that, in the presence of model misspecification, EL may cease to be root n convergent when the functions defining the moment conditions are unbounded (even when their expectations are bounded). In contrast, the related exponential tilting (ET) estimator avoids this problem. This paper shows that the ET and EL estimators can be naturally combined to yield an estimator called exponentially tilted empirical likelihood (ETEL) exhibiting the same O(n(-1)) bias and the same O(n(-2)) variance as EL, while maintaining root n convergence under model misspecification.",WOS:000248987600007,ANNALS OF STATISTICS,"['RATIO CONFIDENCE-INTERVALS', 'MOMENT CONDITION MODELS', 'GENERALIZED-METHOD', 'ROBUST ESTIMATION', 'SAMPLE PROPERTIES', 'MAXIMUM-ENTROPY', 'GMM ESTIMATORS', 'TESTS', 'INFERENCE', 'MISSPECIFICATION']",Point estimation with exponentially tilted empirical likelihood,2007
1278,"Single-index models are natural extensions of linear models and circumvent the so-called curse of dimensionality. They are becoming increasingly popular in many scientific fields including biostatistics, medicine, economics and financial econometrics. Estimating and testing the model index coefficients beta is one of the most important objectives in the statistical analysis. However, the commonly used assumption on the index coefficients, parallel to beta parallel to = 1, represents a nonregular problem: the true index is on the boundary of the unit ball. In this paper we introduce the EFM approach, a method of estimating functions, to study the single-index model. The procedure is to first relax the equality constraint to one with (d - 1) components of beta lying in an open unit ball, and then to construct the associated (d - 1) estimating functions by projecting the score function to the linear space spanned by the residuals with the unknown link being estimated by kernel estimating functions. The root-n consistency and asymptotic normality for the estimator obtained from solving the resulting estimating equations are achieved, and a Wilks type theorem for testing the index is demonstrated. A noticeable result we obtain is that our estimator for beta has smaller or equal limiting variance than the estimator of Carroll et al. [J. Amer Statist. Assoc. 92 (1997) 447-4891. A fixed-point iterative scheme for computing this estimator is proposed. This algorithm only involves one-dimensional nonparametric smoothers, thereby avoiding the data sparsity problem caused by high model dimensionality. Numerical studies based on simulation and on applications suggest that this new estimating system is quite powerful and easy to implement.",WOS:000293716500012,ANNALS OF STATISTICS,"['GENERALIZED LINEAR-MODELS', 'DIMENSION REDUCTION', 'SEMIPARAMETRIC ESTIMATION', 'REGRESSION', 'PREDICTORS']",THE EFM APPROACH FOR SINGLE-INDEX MODELS,2011
1279,"We show that nonparametric regression is asymptotically equivalent, in Le Cam's sense, to a sequence of Gaussian white noise experiments as the number of observations tends to infinity. We propose a general constructive framework, based on approximation spaces, which allows asymptotic equivalence to be achieved, even in the cases of multivariate and random design.",WOS:000258243000019,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'DENSITY-ESTIMATION']",Asymptotic equivalence for nonparametric regression with multivariate and random design,2008
1280,"Consider the following multi-phase project management problem. Each project is divided into several phases. All projects enter the next phase at the same point chosen by the decision maker based on observations up to that point. Within each phase, one can pursue the projects in any order. When pursuing the project with one unit of resource, the project state changes according to a Markov chain. The probability distribution of the Markov chain is known up to an unknown parameter. When pursued, the project generates a random reward depending on the phase and the state of the project and the unknown parameter. The decision maker faces two problems: (a) how to allocate resources to projects within each phase, and (b) when to enter the next phase, so that the total expected reward is as large as possible. In this paper we formulate the preceding problem as a stochastic scheduling problem and propose asymptotic optimal strategies, which minimize the shortfall from perfect information payoff. Concrete examples are given to illustrate our method.",WOS:000249568000015,ANNALS OF STATISTICS,"['ADAPTIVE ALLOCATION SCHEMES', 'MULTIARMED BANDIT PROBLEM', 'CONTROLLED MARKOV-CHAINS', 'FINITE PARAMETER SPACE', 'ORDER CONSTRAINTS', 'RULES']",Optimal strategies for a class of sequential control problems with precedence relations,2007
1281,"Bayesian inference provides a flexible way of combining data with prior information. However, quantile regression is not equipped with a parametric likelihood, and therefore, Bayesian inference for quantile regression demands careful investigation. This paper considers the Bayesian empirical likelihood approach to quantile regression. Taking the empirical likelihood into a Bayesian framework, we show that the resultant posterior from any fixed prior is asymptotically normal; its mean shrinks toward the true parameter values, and its variance approaches that of the maximum empirical likelihood estimator. A more interesting case can be made for the Bayesian empirical likelihood when informative priors are used to explore commonality across quantiles. Regression quantiles that are computed separately at each percentile level tend to be highly variable in the data sparse areas (e.g., high or low percentile levels). Through empirical likelihood, the proposed method enables us to explore various forms of commonality across quantiles for efficiency gains. By using an MCMC algorithm in the computation, we avoid the daunting task of directly maximizing empirical likelihood. The finite sample performance of the proposed method is investigated empirically, where substantial efficiency gains are demonstrated with informative priors on common features across several percentile levels. A theoretical framework of shrinking priors is used in the paper to better understand the power of the proposed method.",WOS:000307608000018,ANNALS OF STATISTICS,"['RATIO CONFIDENCE-INTERVALS', 'LONGITUDINAL DATA', 'MODELS', 'ESTIMATORS', 'BOOTSTRAP', 'INFERENCE', 'SELECTION']",BAYESIAN EMPIRICAL LIKELIHOOD FOR QUANTILE REGRESSION,2012
1282,"The paper considers linear regression problems where the number of predictor variables is possibly larger than the sample size. The basic motivation of the study is to combine the points of view of model selection and functional regression by using a factor approach: it is assumed that the predictor vector can be decomposed into a sum of two uncorrelated random components reflecting common factors and specific variabilities of the explanatory variables. It is shown that the traditional assumption of a sparse vector of parameters is restrictive in this context. Common factors may possess a significant influence on the response variable which cannot be captured by the specific effects of a small number of individual variables. We therefore propose to include principal components as additional explanatory variables in an augmented regression model. We give finite sample inequalities for estimates of these components. It is then shown that model selection procedures can be used to estimate the parameters of the augmented model, and we derive theoretical properties of the estimators. Finite sample performance is illustrated by a simulation study.",WOS:000299186500009,ANNALS OF STATISTICS,"['FUNCTIONAL LINEAR-REGRESSION', 'DANTZIG SELECTOR', 'PRINCIPAL-COMPONENTS', 'LASSO', 'INEQUALITIES', 'NUMBER']",FACTOR MODELS AND VARIABLE SELECTION IN HIGH-DIMENSIONAL REGRESSION ANALYSIS,2011
1283,"Version 2.10.0 of R includes new code for processing '.Rd' help files. There are some changes to what is allowed, and some new capabilities and opportunities.",WOS:000208589800010,R JOURNAL,,The New R Help System,2009
1284,"The identification of atypical observations and the immunization of data analysis against both outliers and failures of modeling are important aspects of modern statistics. The forward search is a graphics rich approach that leads to the formal detection of outliers and to the detection of model inadequacy combined with suggestions for model enhancement. The key idea is to monitor quantities of interest, such as parameter estimates and test statistics, as the model is fitted to data subsets of increasing size. In this paper we propose some computational improvements of the forward search algorithm and we provide a recursive implementation of the procedure which exploits the information of the previous step. The output is a set of efficient routines for fast updating of the model parameter estimates, which do not require any data sorting, and fast computation of likelihood contributions, which do not require matrix inversion or qr decomposition. It is shown that the new algorithms enable a reduction of the computation time by more than 80%. Furthemore, the running time now increases almost linearly with the sample size. All the routines described in this paper are included in the FSDA toolbox for MATLAB which is freely downloadable from the internet.",WOS:000365986000001,JOURNAL OF STATISTICAL SOFTWARE,['SELECTION'],The Forward Search for Very Large Datasets,2015
1285,"It is common for linear regression models to be plagued with the problem of multicollinearity when two or more regressors are highly correlated. This problem results in unstable estimates of regression coefficients and causes some serious problems in validation and interpretation of the model. Different diagnostic measures are used to detect multicollinearity among regressors. Many statistical software and R packages provide few diagnostic measures for the judgment of multicollinearity. Most widely used diagnostic measures in these software are: coefficient of determination (R-2), variance inflation factor/tolerance limit (VIF/TOL), eigenvalues, condition number (CN) and condition index (CI) etc. In this manuscript, we present an R package, mctest, that computes popular and widely used multicollinearity diagnostic measures. The package also indicates which regressors may be the reason of collinearity among regressors.",WOS:000395669800034,R JOURNAL,['MULTICOLLINEARITY'],mctest: An R Package for Detection of Collinearity among Regressors,2016
1286,"Item response theory (IRT) models are a class of statistical models used to describe the response behaviors of individuals to a set of items having a certain number of options. They are adopted by researchers in social science, particularly in the analysis of performance or attitudinal data, in psychology, education, medicine, marketing and other fields where the aim is to measure latent constructs. Most IRT analyses use parametric models that rely on assumptions that often are not satisfied. In such cases, a nonparametric approach might be preferable; nevertheless, there are not many software implementations allowing to use that.
To address this gap, this paper presents the R package Kern SmoothIRT. It implements kernel smoothing for the estimation of option characteristic curves, and adds several plotting and analytical tools to evaluate the whole test/questionnaire, the items, and the subjects. In order to show the package's capabilities, two real datasets are used, one employing multiple-choice responses, and the other scaled responses.",WOS:000341584500001,JOURNAL OF STATISTICAL SOFTWARE,"['NONPARAMETRIC REGRESSION', 'CHARACTERISTIC CURVE', 'MODELS', 'CONSISTENCY', 'CHOICE', 'SCALE']",KernSmoothIRT: An R Package for Kernel Smoothing in Item Response Theory,2014
1287,"We consider the problem of regularized maximum likelihood estimation for the structure and parameters of a high-dimensional, sparse directed acyclic graphical (DAG) model with Gaussian distribution, or equivalently, of a Gaussian structural equation model. We show that the to-penalized maximum likelihood estimator of a DAG has about the same number of edges as the minimal-edge I-MAP (a DAG with minimal number of edges representing the distribution), and that it converges in Frobenius norm. We allow the number of nodes p to be much larger than sample size n but assume a sparsity condition and that any representation of the true DAG has at least a fixed proportion of its nonzero edge weights above the noise level. Our results do not rely on the faithfulness assumption nor on the restrictive strong faithfulness condition which are required for methods based on conditional independence testing such as the PC-algorithm.",WOS:000320488200006,ANNALS OF STATISTICS,"['MARKOV EQUIVALENCE CLASSES', 'OBSERVATIONAL DATA', 'LASSO']",l(0)-PENALIZED MAXIMUM LIKELIHOOD FOR SPARSE DIRECTED ACYCLIC GRAPHS,2013
1288,"We propose an iterative estimating equations procedure for analysis of longitudinal data. We show that, under very mild conditions, the probability that the procedure converges at an exponential rate tends to one as the sample size increases to infinity. Furthermore, we show that the limiting estimator is consistent and asymptotically efficient, as expected. The method applies to semiparametric regression models with unspecified covariances among the observations. In the special case of linear models, the procedure reduces to iterative reweighted least squares. Finite sample performance of the procedure is studied by simulations, and compared with other methods. A numerical example from a medical study is considered to illustrate the application of the method.",WOS:000251096100016,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD ESTIMATION', 'MODELS']",Iterative estimating equations: Linear convergence and asymptotic properties,2007
1289,"We address the Least Quantile of Squares (LQS) (and in particular the Least Median of Squares) regression problem using modern optimization methods. We propose a Mixed Integer Optimization (MIO) formulation of the LQS problem which allows us to find a provably global optimal solution for the LQS problem. Our MIO framework has the appealing characteristic that if we terminate the algorithm early, we obtain a solution with a guarantee on its sub-optimality. We also propose continuous optimization methods based on first-order subdifferential methods, sequential linear optimization and hybrid combinations of them to obtain near optimal solutions to the LQS problem. The MIO algorithm is found to benefit significantly from high quality solutions delivered by our continuous optimization based methods. We further show that the MIO approach leads to (a) an optimal solution for any dataset, where the data-points (y(i), x(i))'s are not necessarily in general position, (b) a simple proof of the breakdown point of the LQS objective value that holds for any dataset and (c) an extension to situations where there are polyhedral constraints on the regression coefficient vector. We report computational results with both synthetic and real-world datasets showing that the MIO algorithm with warm starts from the continuous optimization methods solve small (n = 100) and medium (n = 500) size problems to provable optimality in under two hours, and outperform all publicly available methods for large-scale (n = 10,000) LQS problems.",WOS:000345884900012,ANNALS OF STATISTICS,"['MULTIPLE LINEAR-REGRESSION', 'SQUARES REGRESSION', 'ROBUST REGRESSION', 'APPROXIMATION ALGORITHM', 'COMPUTER VISION', 'ESTIMATOR', 'SETS']",LEAST QUANTILE REGRESSION VIA MODERN OPTIMIZATION,2014
1290,"We investigate the learning rate of multiple kernel learning (MKL) with l(1) and elastic-net regularizations. The elastic-net regularization is a composition of an l(1)-regularizer for inducing the sparsity and an l(2)-regularizer for controlling the smoothness. We focus on a sparse setting where the total number of kernels is large, but the number of nonzero components of the ground truth is relatively small, and show sharper convergence rates than the learning rates have ever shown for both l(1) and elastic-net regularizations. Our analysis reveals some relations between the choice of a regularization function and the performance. If the ground truth is smooth, we show a faster convergence rate for the elastic-net regularization with less conditions than l(1)-regularization; otherwise, a faster convergence rate for the l(1)-regularization is shown.",WOS:000323271500001,ANNALS OF STATISTICS,"['SUPPORT VECTOR MACHINES', 'GROUP LASSO', 'REGULARIZATION']",FAST LEARNING RATE OF MULTIPLE KERNEL LEARNING: TRADE-OFF BETWEEN SPARSITY AND SMOOTHNESS,2013
1291,"Adaptive confidence intervals for regression functions are constructed under shape constraints of monotonicity and convexity. A natural benchmark is established for the minimum expected length of confidence intervals at a given function in terms of an analytic quantity, the local modulus of continuity. This bound depends not only on the function but also the assumed function class. These benchmarks show that the constructed confidence intervals have near minimum expected length for each individual function, while maintaining a given coverage probability for functions within the class. Such adaptivity is much stronger than adaptive minimaxity over a collection of large parameter spaces.",WOS:000320488200012,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'VARIANCE', 'ADAPTATION']",ADAPTIVE CONFIDENCE INTERVALS FOR REGRESSION FUNCTIONS UNDER SHAPE CONSTRAINTS,2013
1292,"Jan de Leeuw came to University of California, Los Angeles (UCLA) Statistics at a crucial time in its history. We set out some details of what he found when he arrived on UCLA's north campus in 1987, what was there when he left it some 27 years later, and how he fashioned the changes that are now so widely recognized.",WOS:000389127700001,JOURNAL OF STATISTICAL SOFTWARE,,Jan de Leeuw and Statistics at UCLA,2016
1293,The R package classify presents a number of useful functions which can be used to estimate the classification accuracy and consistency of assessments. Classification accuracy refers to the probability that an examinee's achieved grade classification on an assessment refects their true grade. Classification consistency refers to the probability that an examinee will be classified in to the same grade classification under repeated administrations of an assessment. Understanding the classification accuracy and consistency of assessments is important where key decisions are being taken on the basis of grades or classifications. The study of classification accuracy can help to improve the design of assessments and aid public understanding and confidence in those assessments.,WOS:000332110800001,JOURNAL OF STATISTICAL SOFTWARE,"['FIT', 'RELIABILITY', 'INDEXES', 'SCORES']",Classification Accuracy and Consistency under Item Response Theory Models Using the Package classify,2014
1294,"This paper considers the maximum likelihood estimation of factor models of high dimension, where the number of variables (N) is comparable with or even greater than the number of observations (T). An inferential theory is developed. We establish not only consistency but also the rate of convergence and the limiting distributions. Five different sets of identification conditions are considered. We show that the distributions of the MLE estimators depend on the identification restrictions. Unlike the principal components approach, the maximum likelihood estimator explicitly allows heteroskedastic-ities, which re jointly estimated with other parameters. Efficiency of MLE relative to the principal components method is also considered.",WOS:000304684900017,ANNALS OF STATISTICS,"['ARBITRAGE', 'ESTIMATORS', 'NUMBER', 'RETURN']",STATISTICAL ANALYSIS OF FACTOR MODELS OF HIGH DIMENSION,2012
1295,"The study of good nonregular fractional factorial designs has received significant attention over the last two decades. Recent research indicates that designs constructed from quaternary codes (QC) are very promising in this regard. The present paper aims at exploring the fundamental structure and developing a theory to characterize the wordlengths and aliasing indexes for a general (1/4)(p)th-fraction QC design. Then the theory is applied to (1/64)th-fraction QC designs. Examples are given, indicating that there exist some QC designs that have better design properties, and are thus more cost-efficient, than the regular fractional factorial designs of the same size. In addition, a result about the periodic structure of (1/64)th-fraction QC designs regarding resolution is stated.",WOS:000321845400015,ANNALS OF STATISTICS,"['FRACTIONAL FACTORIAL-DESIGNS', 'MINIMUM G(2)-ABERRATION', 'RESOLUTION', 'ONE-8TH']",A CODE ARITHMETIC APPROACH FOR QUATERNARY CODE DESIGNS AND ITS APPLICATION TO (1/64)TH-FRACTIONS,2012
1296,"A multiplier bootstrap procedure for construction of likelihood-based confidence sets is considered for finite samples and a possible model misspecification. Theoretical results justify the bootstrap validity for a small or moderate sample size and allow to control the impact of the parameter dimension p: the bootstrap approximation works if p(3)/n is small. The main result about bootstrap validity continues to apply even if the underlying parametric model is misspecified under the so-called small modelling bias condition. In the case when the true model deviates significantly from the considered parametric family, the bootstrap procedure is still applicable but it becomes a bit conservative: the size of the constructed confidence sets is increased by the modelling bias. We illustrate the results with numerical examples for misspecified linear and logistic regressions.",WOS:000363437900012,ANNALS OF STATISTICS,"['MULTIPLIER BOOTSTRAP', 'WILD BOOTSTRAP', 'JACKKNIFE', 'TESTS']",BOOTSTRAP CONFIDENCE SETS UNDER MODEL MISSPECIFICATION,2015
1297,"The detection and determination of clusters has been of special interest among researchers from different fields for a long time. In particular,assessing whether the clusters are significant is a question that has been asked by a number of experimenters. In Fuentes and Casella (2009), the authors put forth a new methodology for analyzing clusters. It tests the hypothesis II0: kappa - 1 versus II1: kappa - k in a Bayesian setting,where kappa denotes the number of clusters in a population. The bayesclust package implements this approach in R. Here we give an overview of the algorithm and a detailed description of the functions available in the package. The routines in bayesclust allow the user to test for the existence of clusters, and then pick out optimal partitionings of the data. We demonstrate the testing procedure with simulated datasets.",WOS:000305065700001,JOURNAL OF STATISTICAL SOFTWARE,['NUMBER'],bayesclust: An R Package for Testing and Searching for Significant Clusters,2012
1298,"Technological advances in molecular biology over the past decade have given rise to high dimensional and complex datasets offering the possibility to investigate biological associations between a range of genomic features and complex phenotypes. The analysis of this novel type of data generated unprecedented computational challenges which ultimately led to the definition and implementation of computationally efficient statistical models that were able to scale to genome-wide data, including Bayesian variable selection approaches. While extensive methodological work has been carried out in this area, only few methods capable of handling hundreds of thousands of predictors were implemented and distributed. Among these we recently proposed GUESS, a computationally optimised algorithm making use of graphics processing unit capabilities, which can accommodate multiple outcomes. In this paper we propose R2GUESS, an R package wrapping the original C++ source code. In addition to providing a user-friendly interface of the original code automating its parametrisation, and data handling, R2GUESS also incorporates many features to explore the data, to extend statistical inferences from the native algorithm (e.g., effect size estimation, significance assessment), and to visualize outputs from the algorithm. We first detail the model and its parametrisation, and describe in details its optimised implementation. Based on two examples we finally illustrate its statistical performances and flexibility.",WOS:000373916800001,JOURNAL OF STATISTICAL SOFTWARE,"['STOCHASTIC SEARCH', 'STATISTICAL-METHODS', 'MODEL EXPLORATION', 'ASSOCIATION']",R2GUESS: A Graphics Processing Unit-Based R Package for Bayesian Variable Selection Regression of Multivariate Responses,2016
1299,"It is often useful to rerun a command line R script with some slight change in the parameters used to run it - a new set of parameters for a simulation, a different dataset to process, etc. The R package batch provides a means to pass in multiple command line options, including vectors of values in the usual R format, easily into R. The same script can be setup to run things in parallel via different command line arguments. The R package batch also provides a means to simplify this parallel batching by allowing one to use R and an R-like syntax for arguments to spread a script across a cluster or local multicore/multiprocessor computer, with automated syntax for several popular cluster types. Finally it provides a means to aggregate the results together of multiple processes run on a cluster.",WOS:000208805600001,JOURNAL OF STATISTICAL SOFTWARE,,Passing in Command Line Arguments and Parallel Cluster/Multicore Batching in R with batch,2011
1300,In this article we present the Bayesian estimation of spatial probit models in R and provide an implementation in the package spatialprobit. We show that large probit models can be estimated with sparse matrix representations and Gibbs sampling of a truncated multivariate normal distribution with the precision matrix. We present three examples and point to ways to achieve further performance gains through parallelization of the Markov Chain Monte Carlo approach.,WOS:000321944400014,R JOURNAL,['INNOVATION'],Estimating Spatial Probit Models in R,2013
1301,"Two packages, oro.dicom and oro.nifti, are provided for the interaction with and manipulation of medical imaging data that conform to the DICOM standard or ANALYZE/NIfTI formats. DICOM data, from a single file or directory tree, may be uploaded into R using basic data structures: a data frame for the header information and a matrix for the image data. A list structure is used to organize multiple DICOM files. The S4 class framework is used to develop basic ANALYZE and NIfTI classes, where NIfTI extensions may be used to extend the fixed-byte NIfTI header. One example of this, that has been implemented, is an XML-based ""audit trail"" tracking the history of operations applied to a data set. The conversion from DICOM to ANALYZE/NIfTI is straightforward using the capabilities of both packages. The S4 classes have been developed to provide a user-friendly interface to the ANALYZE/NIfTI data formats; allowing easy data input, data output, image processing and visualization.",WOS:000296228900001,JOURNAL OF STATISTICAL SOFTWARE,,Working with the DICOM and NIfTI Data Standards in R,2011
1302,"We introduce a nonparametric test statistic for the permutation test in complete block designs. We find the region in which the statistic exists and consider particularly its properties on the boundary of the region. Further, we prove that saddlepoint approximations for tail probabilities can be obtained inside the interior of this region. Finally, numerical examples are given showing that both accuracy and power of the new statistic improves on these properties of the classical F-statistic under some non-Gaussian models and equals them for the Gaussian case.",WOS:000349738500004,ANNALS OF STATISTICS,['SADDLEPOINT APPROXIMATIONS'],A NEW PERMUTATION TEST STATISTIC FOR COMPLETE BLOCK DESIGNS,2015
1303,"We describe the R package multiPIM, including statistical background, functionality and user options. The package is for variable importance analysis, and is meant primarily for analyzing data from exploratory epidemiological studies, though it could certainly be applied in other areas as well. The approach taken to variable importance comes from the causal inference field, and is different from approaches taken in other R packages. By default, multiPIM uses a double robust targeted maximum likelihood estimator (TMLE) of a parameter akin to the attributable risk. Several regression methods/machine learning algorithms are available for estimating the nuisance parameters of the models, including super learner, a meta-learner which combines several different algorithms into one. We describe a simulation in which the double robust TMLE is compared to the graphical computation estimator. We also provide example analyses using two data sets which are included with the package.",WOS:000340586600001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'WESTERN COLLABORATIVE GROUP', 'CORONARY HEART-DISEASE', 'FOLLOW-UP EXPERIENCE', 'REGRESSION', 'MODELS']",R Package multiPIM: A Causal Inference Approach to Variable Importance Analysis,2014
1304,"In 1998 the UCLA Department of Statistics, which had been one of the major users of Lisp-Stat, and one of the main producers of Lisp-Stat code, decided to switch to S/R. This paper discusses why this decision was made, and what the pros and the cons were.",WOS:000232831800001,JOURNAL OF STATISTICAL SOFTWARE,,On abandoning XLISP-STAT,2005
1305,"This paper studies the estimation of the volatility parameter in a model where the driving process is a Brownian motion or a more general symmetric stable process that is perturbed by another Levy process. We distinguish between a parametric case, where the law of the perturbing process is known, and a semiparametric case, where it is not. In the parametric case, we construct estimators which are asymptotically efficient. In the semiparametric case, we can obtain asymptotically efficient estimators by sampling at a sufficiently high frequency, and these estimators are efficient uniformly in the law of the perturbing process.",WOS:000247498100014,ANNALS OF STATISTICS,"['EMPIRICAL CHARACTERISTIC FUNCTION', 'STABLE DISTRIBUTIONS', 'STATISTICAL-INFERENCE', 'PARAMETERS', 'MODELS', 'JUMPS', 'LAW']",Volatility estimators for discretely sampled Levy processes,2007
1306,,WOS:000258243000002,ANNALS OF STATISTICS,"['VARIABLE SELECTION', 'LOGISTIC-REGRESSION', 'ORACLE PROPERTIES', 'LASSO', 'EXPRESSION']",Discussion: One-step sparse estimates in nonconcave penalized likelihood models,2008
1307,"Let Q = (Qi,...,Qn) be a random vector drawn from the uniform distribution on the set of all n! permutations of {1,2,...,n}. Let Z = (Z1,...,Zn), where Z(j) is the mean zero variance one random variable obtained by centralizing and normalizing Q(j), j = 1,...,n. Assume that X-i, i = 1,...,p are i.i.d. copies of 1 root p Z and X = Xp,n is the p x n random matrix with X-i as its ith row. Then S-n = XX* is called the p x n Spearman's rank correlation matrix which can be regarded as a high dimensional extension of the classical nonparametric statistic Spearman's rank correlation coefficient between two independent random variables. In this paper, we establish a CLT for the linear spectral statistics of this nonparametric random matrix model in the scenario of high dimension, namely, p = p(n) and p/n -> c is an element of (0, infinity) as n -> infinity. We propose a novel evaluation scheme to estimate the core quantity in Anderson and Zeitouni's cumulant method in [Ann. Statist. 36 (2008) 2553-2576] to bypass the so-called joint cumulant summability. In addition, we raise a two-step comparison approach to obtain the explicit formulae for the mean and covariance functions in the CLT. Relying on this CLT, we then construct a distribution-free statistic to test complete independence for components of random vectors. Owing to the nonparametric property, we can use this test on generally distributed random variables including the heavy-tailed ones.",WOS:000363437900010,ANNALS OF STATISTICS,"['SAMPLE COVARIANCE MATRICES', 'LIMIT-THEOREMS', 'DISTRIBUTIONS', 'INDEPENDENCE', 'TESTS', 'RATIO']",SPECTRAL STATISTICS OF LARGE DIMENSIONAL SPEARMAN'S RANK CORRELATION MATRIX AND ITS APPLICATION,2015
1308,"This article describes the benchden package which implements a set of 28 example densities for nonparametric density estimation in R. In addition to the usual functions that evaluate the density, distribution and quantile functions or generate random variates, a function designed to be specifically useful for larger simulation studies has been added. After describing the set of densities and the usage of the package, a small toy example of a simulation study conducted using the benchden package is given.",WOS:000301232000001,JOURNAL OF STATISTICAL SOFTWARE,,The benchden Package: Benchmark Densities for Nonparametric Density Estimation,2012
1309,"This paper considers the problem of multi-sample nonparametric comparison of counting processes with panel count data, which arise naturally when recurrent events are considered. Such data frequently occur in medical follow-up studies and reliability experiments, for example. For the problem considered, we construct two new classes of nonparametric test statistics based on the accumulated weighted differences between the rates of increase of the estimated mean functions of the counting processes over observation times, wherein the nonparametric maximum likelihood approach is used to estimate the mean function instead of the nonparametric maximum pseudo-likelihood. The asymptotic distributions of the proposed statistics are derived and their finite-sample properties are examined through Monte Carlo simulations. The simulation results show that the proposed methods work quite well and are more powerful than the existing test procedures. Two real data sets are analyzed and presented as illustrative examples.",WOS:000265619700002,ANNALS OF STATISTICS,"['INTERVAL-CENSORED DATA', '2-SAMPLE TEST']",NEW MULTI-SAMPLE NONPARAMETRIC TESTS FOR PANEL COUNT DATA,2009
1310,"Social network analysis is extremely well supported by the R community and is routinely used for studying the relationships between people engaged in collaborative activities. While there has been rapid development of new approaches and metrics in this field, the challenging question of validity (how well insights derived from social networks agree with reality) is often difficult to address. We propose the use of several R packages to generate interactive surveys that are specifically well suited for validating social network analyses. Using our web-based survey application, we were able to validate the results of applying community-detection algorithms to infer the organizational structure of software developers contributing to open-source projects.",WOS:000385276100011,R JOURNAL,,An Interactive Survey Application for Validating Social Network Analysis Techniques,2016
1311,"In many semiparametric models that are parameterized by two types of parameters-a Euclidean parameter of interest and an infinite-dimensional nuisance parameter-the two parameters are bundled together, that is, the nuisance parameter is an unknown function that contains the parameter of interest as part of its argument. For example, in a linear regression model for censored survival data, the unspecified error distribution function involves the regression coefficients. Motivated by developing an efficient estimating method for the regression parameters, we propose a general sieve M-theorem for bundled parameters and apply the theorem to deriving the asymptotic theory for the sieve maximum likelihood estimation in the linear regression model for censored survival data. The numerical implementation of the proposed estimating method can be achieved through the conventional gradient-based search algorithms such as the Newton-Raphson algorithm. We show that the proposed estimator is consistent and asymptotically normal and achieves the semiparametric efficiency bound. Simulation studies demonstrate that the proposed method performs well in practical settings and yields more efficient estimates than existing estimating equation based methods. Illustration with a real data example is also provided.",WOS:000300383200009,ANNALS OF STATISTICS,"['FAILURE TIME MODEL', 'CONDITIONAL MOMENT RESTRICTIONS', 'MAXIMUM-LIKELIHOOD-ESTIMATION', 'RANK-TESTS', 'REGRESSION-ANALYSIS', 'LARGE-SAMPLE', 'COX MODEL']","A SIEVE M-THEOREM FOR BUNDLED PARAMETERS IN SEMIPARAMETRIC MODELS, WITH APPLICATION TO THE EFFICIENT ESTIMATION IN A LINEAR MODEL FOR CENSORED DATA",2011
1312,"Public health surveillance aims at lessening disease burden by, e.g., timely recognizing emerging outbreaks in case of infectious diseases. Seen from a statistical perspective, this implies the use of appropriate methods for monitoring time series of aggregated case reports. This paper presents the tools for such automatic aberration detection offered by the R package surveillance. We introduce the functionalities for the visualization, modeling and monitoring of surveillance time series. With respect to modeling we focus on univariate time series modeling based on generalized linear models (GLMs), multivariate GLMs, generalized additive models and generalized additive models for location, shape and scale. Applications of such modeling include illustrating implementational improvements and extensions of the well-known Farrington algorithm, e.g., by spline-modeling or by treating it in a Bayesian context. Furthermore, we look at categorical time series and address overdispersion using beta-binomial or Dirichlet-multinomial modeling. With respect to monitoring we consider detectors based on either a Shewhart-like single timepoint comparison between the observed count and the predictive distribution or by likelihood-ratio based cumulative sum methods. Finally, we illustrate how surveillance can support aberration detection in practice by integrating it into the monitoring workflow of a public health institution. Altogether, the present article shows how well surveillance can support automatic aberration detection in a public health surveillance context.",WOS:000384912500001,JOURNAL OF STATISTICAL SOFTWARE,"['BAYESIAN OUTBREAK DETECTION', 'INFECTIOUS-DISEASE', 'CUSUM', 'CHARTS', 'MODELS', 'REGRESSION', 'ALGORITHM', 'GERMANY']",Monitoring Count Time Series in R: Aberration Detection in Public Health Surveillance,2016
1313,"Generalized additive models have been popular among statisticians and data analysts in multivariate nonparametric regression with non-Gaussian responses including binary and count data. In this paper, a new likelihood approach for fitting generalized additive models is proposed. It aims to maximize a smoothed likelihood. The additive functions are estimated by solving a system of nonlinear integral equations. An iterative algorithm based on smooth backfitting is developed from the Newton-Kantorovich theorem. Asymptotic properties of the estimator and convergence of the algorithm are discussed. It is shown that our proposal based on local linear fit achieves the same bias and variance as the oracle estimator that uses knowledge of the other components. Numerical comparison with the recently proposed two-stage estimator [Ann. Statist. 32 (2004) 2412-2443] is also made.",WOS:000253390000009,ANNALS OF STATISTICS,"['ASYMPTOTIC PROPERTIES', 'REGRESSION-MODELS']",Smooth backfitting in generalized additive models,2008
1314,"We establish minimax optimal rates of convergence for estimation in a high dimensional additive model assuming that it is approximately sparse. Our results reveal a behavior universal to this class of high dimensional problems. In the sparse regime when the components are sufficiently smooth or the dimensionality is sufficiently large, the optimal rates are identical to those for high dimensional linear regression and, therefore, there is no additional cost to entertain a nonparametric model. Otherwise, in the so-called smooth regime, the rates coincide with the optimal rates for estimating a univariate function and, therefore, they are immune to the ""curse of dimensionality.""",WOS:000389620800012,ANNALS OF STATISTICS,"['DANTZIG SELECTOR', 'NONPARAMETRIC REGRESSION', 'COMPONENT SELECTION', 'VARIABLE SELECTION', 'LASSO', 'CONVERGENCE', 'SPARSITY', 'KERNELS']",MINIMAX OPTIMAL RATES OF ESTIMATION IN HIGH DIMENSIONAL ADDITIVE MODELS,2016
1315,"Cloud computing is a relatively new technology that facilitates collaborative creation and modification of documents over the internet in real time. Here we provide an introductory assessment of the available statistical functions in three leading cloud spreadsheets namely Google Spreadsheet, Microsoft Excel Web App, and Zoho Sheet. Our results show that the developers of cloud-based spreadsheets are not performing basic quality control, resulting in statistical computations that are misleading and erroneous. Moreover, the developers do not provide sufficient information regarding the software and the hardware, which can change at any time without notice. Indeed, rerunning the tests after several months we obtained different and sometimes worsened results.",WOS:000315019400001,JOURNAL OF STATISTICAL SOFTWARE,"['MICROSOFT EXCEL 2003', 'STATISTICAL PROCEDURES', 'ACCURACY', 'DISTRIBUTIONS']",Spreadsheets in the Cloud - Not Ready Yet,2013
1316,"This article discusses the problem of estimation of parameters in finite mixtures when the mixture components are assumed to be symmetric and to come from the same location family. We refer to these mixtures as semi-parametric because no additional assumptions other than symmetry are made regarding the parametric form of the component distributions. Because the class of symmetric distributions is so broad, identifiability of parameters is a major issue in these mixtures. We develop a notion of identifiability of finite mixture models, which we call k-identifiability, where k denotes the number of components in the mixture. We give sufficient conditions for k-identifiability of location mixtures of symmetric components when k = 2 or 3. We propose a novel distance-based method for estimating the (location and mixing) parameters from a k-identifiable model and establish the strong consistency and asymptotic normality of the estimator. In the specific case of L-2-distance, we show that our estimator generalizes the Hodges-Lehmann estimator. We discuss the numerical implementation of these procedures, along with an empirical estimate of the component distribution, in the two-component case. In comparisons with maximum likelihood estimation assuming normal components, our method produces somewhat higher standard error estimates in the case where the components are truly normal, but dramatically outperforms the normal method when the components are heavy-tailed.",WOS:000247498100010,ANNALS OF STATISTICS,"['MULTISCALE MAXIMUM-LIKELIHOOD', 'NONPARAMETRIC-ESTIMATION', 'MODELS']",Inference for mixtures of symmetric distributions,2007
1317,"Panel data econometrics is obviously one of the main fields in the profession, but most of the models used are difficult to estimate with R. plm is a package for R which intends to make the estimation of linear panel models straightforward. plm provides functions to estimate a wide variety of models and to make (robust) inference.",WOS:000258204800001,JOURNAL OF STATISTICAL SOFTWARE,"['ERROR COMPONENT MODEL', 'FINITE-SAMPLE PROPERTIES', 'TIME-SERIES', 'CROSS-SECTION', 'ESTIMATORS', 'HETEROSKEDASTICITY', 'SPECIFICATION', 'TESTS', 'DISTURBANCES', 'INFERENCE']",Panel data econometrics in R: The plum package,2008
1318,"Stochastic approximation Monte Carlo (SAMC) has recently been proposed by Liang, Liu and Carroll [J. Amer Statist. Assoc. 102 (2007) 305-320] as a general simulation and optimization algorithm. In this paper, we propose to improve its convergence using smoothing methods and discuss the application of the new algorithm to Bayesian model selection problems. The new algorithm is tested through a change-point identification example. The numerical results indicate that the new algorithm can outperform SAMC and reversible jump MCMC significantly for the model selection problems. The new algorithm represents a general form of the stochastic approximation Markov chain Monte Carlo algorithm. It allows multiple samples to be generated at each iteration, and a bias term to be included in the parameter updating step. A rigorous proof for the convergence of the general algorithm is established under verifiable conditions. This paper also provides a framework oil how to improve efficiency of Monte Carlo simulations by incorporating some nonparametric techniques.",WOS:000268605000002,ANNALS OF STATISTICS,"['CHAIN MONTE-CARLO', 'STOCHASTIC-APPROXIMATION', 'ALGORITHM', 'COMPUTATION', 'OPTIMIZATION', 'CONVERGENCE', 'SIMULATIONS', 'REGRESSION', 'INFERENCE']",IMPROVING SAMC USING SMOOTHING METHODS: THEORY AND APPLICATIONS TO BAYESIAN MODEL SELECTION PROBLEMS,2009
1319,"We consider the problem of uncertainty assessment for low dimensional components in high dimensional models. Specifically, we propose a novel decorrelated score function to handle the impact of high dimensional nuisance parameters. We consider both hypothesis tests and confidence regions for generic penalized M-estimators. Unlike most existing inferential methods which are tailored for individual models, our method provides a general framework for high dimensional inference and is applicable to a wide variety of applications. In particular, we apply this general framework to study five illustrative examples: linear regression, logistic regression, Poisson regression, Gaussian graphical model and additive hazards model. For hypothesis testing, we develop general theorems to characterize the limiting distributions of the decorrelated score test statistic under both null hypothesis and local alternatives. These results provide asymptotic guarantees on the type I errors and local powers. For confidence region construction, we show that the decorrelated score function can be used to construct point estimators that are asymptotically normal and semiparametrically efficient. We further generalize this framework to handle the settings of misspecified models. Thorough numerical results are provided to back up the developed theory.",WOS:000396804900005,ANNALS OF STATISTICS,"['P-REGRESSION PARAMETERS', 'VARIABLE SELECTION', 'ASYMPTOTIC-BEHAVIOR', 'DANTZIG SELECTOR', 'M-ESTIMATORS', 'LASSO', 'CONSISTENCY', 'LIKELIHOOD', 'INTERVALS', 'P2/N']",A GENERAL THEORY OF HYPOTHESIS TESTS AND CONFIDENCE REGIONS FOR SPARSE HIGH DIMENSIONAL MODELS,2017
1320,"The frequentist behavior of nonparametric Bayes estimates, more specifically, rates of contraction of the posterior distributions to shrinking L-r-norm neighborhoods, 1 <= r <= infinity, of the unknown parameter, are studied. A theorem for nonparametric density estimation is proved under general approximation-theoretic assumptions on the prior. The result is applied to a variety of common examples, including Gaussian process, wavelet series, normal mixture and histogram priors. The rates of contraction are minimax-optimal for 1 <= r <= 2, but deteriorate as r increases beyond 2. In the case of Gaussian nonparametric regression a Gaussian prior is devised for which the posterior contracts at the optimal rate in all L-r-norms, 1 <= r <= infinity.",WOS:000300383200004,ANNALS OF STATISTICS,"['CONVERGENCE-RATES', 'CONCENTRATION INEQUALITIES', 'NONPARAMETRIC PROBLEMS', 'DIRICHLET MIXTURES', 'MAXIMUM-LIKELIHOOD', 'DENSITY ESTIMATORS', 'CONSISTENCY', 'SPACES', 'THEOREMS']","RATES OF CONTRACTION FOR POSTERIOR DISTRIBUTIONS IN L-r-METRICS, 1 <= r <= infinity",2011
1321,"The metafor package provides functions for conducting meta-analyses in R. The package includes functions for fitting the meta-analytic fixed- and random-effects models and allows for the inclusion of moderators variables (study-level covariates) in these models. Meta-regression analyses with continuous and categorical moderators can be conducted in this way. Functions for the Mantel-Haenszel and Peto's one-step method for meta-analyses of 2 x 2 table data are also available. Finally, the package provides various plot functions (for example, for forest, funnel, and radial plots) and functions for assessing the model fit, for obtaining case diagnostics, and for tests of publication bias.",WOS:000281593200001,JOURNAL OF STATISTICAL SOFTWARE,"['VARIANCE COMPONENT ESTIMATION', 'DETECT PUBLICATION BIAS', 'CLINICAL-TRIALS', 'CUMULATIVE METAANALYSIS', 'HETEROGENEITY', 'REGRESSION', 'MODELS', 'TESTS', 'PLOTS', 'OPPORTUNITIES']",Conducting Meta-Analyses in R with the metafor Package,2010
1322,"Cluster ensembles have emerged as a powerful meta-learning paradigm that provides improved accuracy and robustness by aggregating several input data clusterings. In particular,link-based similarity method shave recently been introduced with superior performance to the conventional co-association approach. This paper presents a MATLAB package, LinkCluE, that implements the link-based cluster ensemble framework. A variety of functional methods for evaluating clustering results, based on both internal and external criteria,are also provided. Additionally,the underlying algorithms together with the sample uses of the package with interesting real and synthetic datasets are demonstrated herein.",WOS:000281593800001,JOURNAL OF STATISTICAL SOFTWARE,"['GENE-EXPRESSION DATA', 'CLASS DISCOVERY', 'CONSENSUS', 'CLASSIFICATION', 'PARTITIONS', 'ALGORITHM']",LinkCluE: A MATLAB Package for Link-Based Cluster Ensembles,2010
1323,"Consider a Gaussian nonparametric regression problem having both an unknown mean function and unknown variance function. This article presents a class of difference-based kernel estimators for the variance function. Optimal convergence rates that are uniform over broad functional classes and bandwidths are fully characterized, and asymptotic normality is also established. We also show that for suitable asymptotic formulations our estimators achieve the minimax rate.",WOS:000251096100015,ANNALS OF STATISTICS,"['SQUARE SUCCESSIVE DIFFERENCE', 'RESIDUAL VARIANCE', 'GEOMETRIZING RATES', 'CALL CENTER', 'HETEROSCEDASTICITY', 'CONVERGENCE', 'DESIGN', 'MODELS', 'CHOICE', 'RATIO']",Variance estimation in nonparametric regression via the difference sequence method,2007
1324,"The package PIN computes a measure of asymmetric information in financial markets, the so-called probability of informed trading. This is obtained from a sequential trade model and is used to study the determinants of an asset price. Since the probability of informed trading depends on the number of buy- and sell-initiated trades during a trading day, this paper discusses the entire modelling cycle, from data handling to the computation of the probability of informed trading and the estimation of parameters for the underlying theoretical model.",WOS:000321944400009,R JOURNAL,"['LIQUIDITY', 'PRICES']",PIN: Measuring Asymmetric Information in Financial Markets with R,2013
1325,"This article describes a R package Boruta, implementing a novel feature selection algorithm for finding all relevant variables. The algorithm is designed as a wrapper around a Random Forest classification algorithm. It iteratively removes the features which are proved by a statistical test to be less relevant than random probes. The Boruta package provides a convenient interface to the algorithm. The short description of the algorithm and examples of its application are presented.",WOS:000282057200001,JOURNAL OF STATISTICAL SOFTWARE,,Feature Selection with the Boruta Package,2010
1326,"We study the rates of convergence in generalization error achievable by active learning under various types of label noise. Additionally, we study the general problem of model selection for active learning with a nested hierarchy of hypothesis classes and propose an algorithm whose error rate provably converges to the best achievable error among classifiers in the hierarchy at a rate adaptive to both the complexity of the optimal classifier and the noise conditions. In particular, we state sufficient conditions for these rates to be dramatically faster than those achievable by passive learning.",WOS:000288183800011,ANNALS OF STATISTICS,"['EMPIRICAL PROCESSES', 'SAMPLE MODULI', 'INEQUALITIES', 'BOUNDS']",RATES OF CONVERGENCE IN ACTIVE LEARNING,2011
1327,"Biplots simultaneously provide information on both the samples and the variables of a data matrix in two- or three-dimensional representations. The BiplotGUI package provides a graphical user interface for the construction of, interaction with, and manipulation of biplots in R. The samples are represented as points, with coordinates determined either by the choice of biplot, principal coordinate analysis or multidimensional scaling. Various transformations and dissimilarity metrics are available. Information on the original variables is incorporated by linear or non-linear calibrated axes. Goodness-of-fit measures are provided. Additional descriptors can be superimposed, including convex hulls, alpha-bags, point densities and classification regions. Amongst the interactive features are dynamic variable value prediction, zooming and point and axis drag-and-drop. Output can easily be exported to the R workspace for further manipulation. Three-dimensional biplots are incorporated via the rgl package. The user requires almost no knowledge of R syntax.",WOS:000267708500001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIVARIATE-ANALYSIS', 'PRINCIPAL COMPONENT', 'DISTANCE', 'METHODOLOGY', 'COEFFICIENT', 'VARIABLES', 'PLANTS', 'ADE4', 'SETS', 'FIT']",BiplotGUI: Interactive Biplots in R,2009
1328,"Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.",WOS:000365981400001,JOURNAL OF STATISTICAL SOFTWARE,"['LEAST-SQUARES', 'COVARIANCE', 'VARIANCE']",Fitting Linear Mixed-Effects Models Using lme4,2015
1329,"We investigate invariant random fields on the sphere using a new type of spherical wavelets, called needlets. These are compactly supported in frequency and enjoy excellent localization properties in real space, with quasi-exponentially decaying tails. We show that, for random fields on the sphere, the needlet coefficients are asymptotically uncorrelated for any fixed angular distance. This property is used to derive CLT and functional CLT converaence results for polynomial functionals of the needlet coefficients: here the asymptotic theory is considered in the high-frequency sense. Our proposals emerge from strong empirical motivations, especially in connection with the analysis of cosmological data sets.",WOS:000265619700003,ANNALS OF STATISTICS,"['GAUSSIANITY', 'WAVELETS', 'SPHERES']",ASYMPTOTICS FOR SPHERICAL NEEDLETS,2009
1330,"Sequentially randomized designs, more recently known as sequential multiple assignment randomized trial (SMART) designs, are widely used in biomedical research, particlularly in clinically trials, to assess and compare the effects of various treatment sequences. In such designs, patients are initially randomized to one of the first-stage therapies. Then patients meeting some criteria (e.g., no relapse of disease) participate in the second-stage randomized to one of the second-stage therapies. The advantage of such a design is that it allows the investiator to study various treatment sequences where the patients second-stage therapies can be adjusted based on their responses to the first-stage therapies. In the past few years, substantial improvement has been made in the statitical methods for analysing the data from SMARTs. Much of the proposed statitical approaches focus on estimating and comparing the survival outcomes of treatment sequences embedded in the SMART designs. In this article, we introduce the R package DTR, which provides a set of functions that can be used to estimate and compare the effects of different treatment sequences on survival outcomes using the newly proposed statistical approaches. The proposed package is also illustrated using simulated data from SMARTs.",WOS:000365974700001,JOURNAL OF STATISTICAL SOFTWARE,"['2-STAGE RANDOMIZATION DESIGNS', 'ADAPTIVE TREATMENT STRATEGIES', 'HIGH-RISK NEUROBLASTOMA', 'TREATMENT POLICIES', 'CLINICAL-TRIALS', '13-CIS-RETINOIC ACID', 'SAMPLE-SIZE', 'DISTRIBUTIONS', 'INFERENCE']",DTR: An R Package for Estimation and Comparison of Survival Outcomes of Dynamic Treatment Regimes,2015
1331,"Convergence rates of kernel density estimators for stationary time series are well studied. For invertible linear processes, we construct a new density estimator that converges, in the supremum norm, at the better, parametric, rate n(-1/2). Our estimator is a convolution of two different residual-based kernel estimators. We obtain in particular convergence rates for such residual-based kernel estimators; these results are of independent interest.",WOS:000248987600015,ANNALS OF STATISTICS,"['MOVING AVERAGE PROCESSES', 'CENTRAL-LIMIT-THEOREM', 'ASYMPTOTIC NORMALITY', 'FUNCTIONAL ESTIMATION', 'STATIONARY-PROCESSES', 'CONVERGENCE-RATES', 'MARGINAL DENSITY', 'MIXING PROCESSES', 'KERNEL ESTIMATE', 'TIME-SERIES']",Uniformly root-N consistent density estimators for weakly dependent invertible linear processes,2007
1332,"We present estimators for a well studied statistical estimation problem: the estimation for the linear regression model with soft sparsity constraints (lq constraint with 0 <q <= 1) in the high-dimensional setting. We first present a family of estimators, called the projected nearest neighbor estimator and show, by using results from Convex Geometry, that such estimator is within a logarithmic factor of the optimal for any design matrix. Then by utilizing a semi-definite programming relaxation technique developed in [SIAM J. Comput. 36 (2007) 1764-1776], we obtain an approximation algorithm for computing the minimax risk for any such estimation task and also a polynomial time nearly optimal estimator for the important case of l(1) sparsity constraint. Such results were only known before for special cases, despite decades of studies on this problem. We also extend the method to the adaptive case when the parameter radius is unknown.",WOS:000326991200016,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'ORTHOGONAL MATCHING PURSUIT', 'DANTZIG SELECTOR', 'MODEL SELECTION', 'ADAPTIVE ESTIMATION', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'LASSO', 'RISK', 'ALGORITHMS']",NEARLY OPTIMAL MINIMAX ESTIMATOR FOR HIGH-DIMENSIONAL SPARSE LINEAR REGRESSION,2013
1333,"The variance-covariance matrix plays a central role in the inferential theories of high-dimensional factor models in finance and economics. Popular regularization methods of directly exploiting sparsity are not directly applicable to many financial problems. Classical methods of estimating the covariance matrices are based on the strict factor models, assuming independent idiosyncratic components. This assumption, however, is restrictive in practical applications. By assuming sparse error covariance matrix, we allow the presence of the cross-sectional correlation even after taking out common factors, and it enables us to combine the merits of both methods. We estimate the sparse covariance using the adaptive thresholding technique as in Cai and Liu [J. Amer. Statist. Assoc. 106 (2011) 672-684], taking into account the fact that direct observations of the idiosyncratic components are unavailable. The impact of high dimensionality on the covariance matrix estimation based on the factor structure is then studied.",WOS:000311639700007,ANNALS OF STATISTICS,"['SEEMINGLY UNRELATED REGRESSIONS', 'REGULARIZATION', 'NUMBER']",HIGH-DIMENSIONAL COVARIANCE MATRIX ESTIMATION IN APPROXIMATE FACTOR MODELS,2011
1334,"In this paper we examine the implications of the statistical large sample theory for the computational complexity of Bayesian and quasi-Bayesian estimation carried out using Metropolis random walks. Our analysis is motivated by the Laplace-Bernstein-Von Mises central limit theorem, which states that in large samples the posterior or quasi-posterior approaches a. normal density. Using the conditions required for the central limit theorem to hold, we establish polynomial bounds on the computational complexity of general Metropolis random walks methods in large samples. Our analysis covers cases where the underlying log-likelihood or extremum criterion function is possibly nonconcave, discontinuous, and with increasing parameter dimension. However, the central limit theorem restricts the deviations from continuity and log-concavity of the log-likelihood or extremum criterion function in a very specific manner.
Under minimal assumptions required for the central limit theorem to hold under the increasing parameter dimension, we show that the Metropolis algorithm is theoretically efficient even for the canonical Gaussian walk which is studied in detail. Specifically, we show that the running time of the algorithm in large samples is bounded in probability by a polynomial in the parameter dimension d and, in particular, is of stochastic order d(2) in the leading cases after the bum-in period. We then give applications to exponential families, curved exponential families and Z-estimation of increasing dimension.",WOS:000268113500013,ANNALS OF STATISTICS,"['CONDITIONAL MOMENT RESTRICTIONS', 'WALD MEMORIAL LECTURES', 'HIT-AND-RUN', 'EXPONENTIAL-FAMILIES', 'POSTERIOR DISTRIBUTIONS', 'ASYMPTOTIC NORMALITY', 'VOLUME ALGORITHM', 'PARAMETERS TENDS', 'CONVEX-BODIES', 'MONTE-CARLO']",ON THE COMPUTATIONAL COMPLEXITY OF MCMC-BASED ESTIMATORS IN LARGE SAMPLES,2009
1335,"The principles behind the interface to continuous domain spatial models in the R INLA software package for R are described. The integrated nested Laplace approximation (INLA) approach proposed by Rue, Martino, and Chopin (2009) is a computationally effective alternative to MCMC for Bayesian inference. INLA is designed for latent Gaussian models, a very wide and flexible class of models ranging from (generalized) linear mixed to spatial and spatio-temporal models. Combined with the stochastic partial differential equation approach (SPDE, Lindgren, Rue, and Lindstrom 2011), one can accommodate all kinds of geographically referenced data, including areal and geostatistical ones, as well as spatial point process data. The implementation interface covers stationary spatial models, non-stationary spatial models, and also spatio-temporal models, and is applicable in epidemiology, ecology, environmental risk assessment, as well as general geostatistics.",WOS:000349847600001,JOURNAL OF STATISTICAL SOFTWARE,"['GAUSSIAN MODELS', 'DATA SETS', 'STATISTICS', 'EQUATIONS']",Bayesian Spatial Modelling with R-INLA,2015
1336,"This note corrects an error in two related proofs of consistency of community detection: under stochastic block models by Bickel and Chen [Proc. Natl. Acad. ScL USA 106 (2009) 21068-21073] and under degree-corrected stochastic block model by Zhao, Levina and Zhu [Ann. Statist. 40 (2012) 2266-2292].",WOS:000349738500016,ANNALS OF STATISTICS,['MODELS'],CORRECTION TO THE PROOF OF CONSISTENCY OF COMMUNITY DETECTION,2015
1337,,WOS:000247010300001,JOURNAL OF STATISTICAL SOFTWARE,,"An introduction to the special volume on ""psychometrics in R""",2007
1338,"Consider the problem of estimating the entries of a large matrix, when the observed entries are noisy versions of a small random fraction of the original entries. This problem has received widespread attention in recent times, especially after the pioneering works of Emmanuel Candes and collaborators. This paper introduces a simple estimation procedure, called Universal Singular Value Thresholding (USVT), that works for any matrix that has ""a little bit of structure."" Surprisingly, this simple estimator achieves the minimax error rate up to a constant factor. The method is applied to solve problems related to low rank matrix estimation, blockmodels, distance matrix completion, latent space models, positive definite matrix completion, graphon estimation and generalized Bradley Terry models for pairwise comparison.",WOS:000349738500007,ANNALS OF STATISTICS,"['BRADLEY-TERRY MODELS', 'LOW-RANK MATRICES', 'STOCHASTIC BLOCKMODELS', 'PAIRED COMPARISONS', 'RANDOM-VARIABLES', 'NETWORK MODELS', 'RANDOM GRAPHS', 'COMPLETION', 'ALGORITHMS', 'PENALIZATION']",MATRIX ESTIMATION BY UNIVERSAL SINGULAR VALUE THRESHOLDING,2015
1339,"The curve time series framework provides a convenient vehicle to accommodate some nonstationary features into a stationary setup. We propose a new method to identify the dimensionality of curve time series based on the dynamical dependence across different curves. The practical implementation of our method boils down to an eigenanalysis of a finite-dimensional matrix. Furthermore, the determination of the dimensionality is equivalent to the identification of the nonzero eigenvalues of the matrix, which we carry out in terms of some bootstrap tests. Asymptotic properties of the proposed method are investigated. In particular, our estimators for zero-eigenvalues enjoy the fast convergence rate n while the estimators for nonzero eigenvalues converge at the standard root n-rate. The proposed methodology is illustrated with both simulated and real data sets.",WOS:000290231500001,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'FUNCTIONAL DATA', 'INFERENCE', 'MODELS']",IDENTIFYING THE FINITE DIMENSIONALITY OF CURVE TIME SERIES,2010
1340,"P-values have been the focus of considerable criticism based on various considerations. Still, the P-value represents one of the most commonly used statistical tools. When assessing the suitability of a single hypothesized distribution, it is not clear that there is a better choice for a measure of surprise. This paper is concerned with the definition of appropriate model-based P-values for model checking.",WOS:000273800100015,ANNALS OF STATISTICS,,INVARIANT P-VALUES FOR MODEL CHECKING,2010
1341,"The Lexis class in the R package Epi provides tools for creation, manipulation and display of data from multi-state models. Transitions between states are described by rates (intensities); Lexis objects represent this kind of data and provide tools to show states and transitions annotated by relevant summary numbers. Data can be transformed to a form that allows modelling of several transition rates with common parameters.",WOS:000285981700001,JOURNAL OF STATISTICAL SOFTWARE,['COMPETING RISKS'],Using Lexis Objects for Multi-State Models in R,2011
1342,"We introduce an extension of the Polya tree approach for constructing distributions on the space of probability measures. By using optional stopping and optional choice of splitting variables, the construction gives rise to random measures that are absolutely continuous with piecewise smooth densities on partitions that can adapt to fit the data. The resulting ""optional Polya tree"" distribution has large support in total variation topology and yields posterior distributions that are also optional Polya trees with computable parameter values.",WOS:000277471000006,ANNALS OF STATISTICS,"['ASYMPTOTIC-BEHAVIOR', 'DISTRIBUTIONS', 'MODELS']",OPTIONAL POLYA TREE AND BAYESIAN INFERENCE,2010
1343,"Modern construction of uniform confidence bands for nonparametric densities (and other functions) often relies on the classical Smirnov-Bickel-Rosenblatt (SBR) condition; see, for example, Gine and Nickl [Probab. Theory Related Fields 143 (2009) 569-596]. This condition requires the existence of a limit distribution of an extreme value type for the supremum of a studentized empirical process (equivalently, for the supremum of a Gaussian process with the same covariance function as that of the studentized empirical process). The principal contribution of this paper is to remove the need for this classical condition. We show that a considerably weaker sufficient condition is derived from an anti-concentration property of the supremum of the approximating Gaussian process, and we derive an inequality leading to such a property for separable Gaussian processes. We refer to the new condition as a generalized SBR condition. Our new result shows that the supremum does not concentrate too fast around any value.
We then apply this result to derive a Gaussian multiplier bootstrap procedure for constructing honest confidence bands for nonparametric density estimators (this result can be applied in other nonparametric problems as well). An essential advantage of our approach is that it applies generically even in those cases where the limit distribution of the supremum of the studentized empirical process does not exist (or is unknown). This is of particular importance in problems where resolution levels or other tuning parameters have been chosen in a data-driven fashion, which is needed for adaptive constructions of the confidence bands. Finally, of independent interest is our introduction of a new, practical version of Lepski's method, which computes the optimal, nonconservative resolution levels via a Gaussian multiplier bootstrap method.",WOS:000344632400004,ANNALS OF STATISTICS,"['KERNEL DENSITY ESTIMATORS', 'CONCENTRATION INEQUALITIES', 'NONPARAMETRIC REGRESSION', 'EMPIRICAL PROCESSES', 'INTERVALS', 'BOUNDS', 'RATES', 'CONVERGENCE', 'CONSISTENCY', 'INFERENCE']","ANTI-CONCENTRATION AND HONEST, ADAPTIVE CONFIDENCE BANDS",2014
1344,"With modem technology development, functional data are being observed frequently in many scientific fields. A popular method for analyzing such functional data is '' smoothing first, then estimation.'' That is, statistical inference such as estimation and hypothesis testing about functional data is conducted based on the substitution of the underlying individual functions by their reconstructions obtained by one smoothing technique or another. However, little is known about this substitution effect on functional data analysis. In this paper this problem is investigated when the local polynomial kernel (LPK) smoothing technique is used for individual function reconstructions. We find that under some mild conditions, the substitution effect can be ignored asymptotically. Based on this, we construct LPK reconstruction-based estimators for the mean, covariance and noise variance functions of a functional data set and derive their asymptotics. We also propose a GCV rule for selecting good bandwidths for the LPK reconstructions. When the mean function also depends on some time-independent covariates, we consider a functional linear model where the mean function is linearly related to the covariates but the covariate effects are functions of time. The LPK reconstruction-based estimators for the covariate effects and the covariance function are also constructed and their asymptotics are derived. Moreover, we propose a L-2- norm-based global test statistic for a general hypothesis testing problem about the covariate effects and derive its asymptotic random expression. The effect of the bandwidths selected by the proposed GCV rule on the accuracy of the LPK reconstructions and the mean function estimator is investigated via a simulation study. The proposed methodologies are illustrated via an application to a real functional data set collected in climatology.",WOS:000248692700006,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENTS-ANALYSIS', 'NONPARAMETRIC REGRESSION', 'CURVES', 'TOOLS']",Statistical inferences for functional data,2007
1345,"In an early paper, He and Tang [Biometrika 100 (2013) 254-260] introduced and studied a new class of designs, strong orthogonal arrays, for computer experiments, and characterized such arrays through generalized orthogonal arrays. The current paper presents a simple characterization for strong orthogonal arrays of strength three. Besides being simple, this new characterization through a notion of semi-embeddability is more direct and penetrating in terms of revealing the structure of strong orthogonal arrays. Some other results on strong orthogonal arrays of strength three are also obtained along the way, and in particular, two SOA(54,5,27,3)'s are constructed.",WOS:000342481700004,ANNALS OF STATISTICS,"['COMPUTER EXPERIMENTS', 'LATIN HYPERCUBES', '(T,M,S)-NETS', 'DESIGNS']",A CHARACTERIZATION OF STRONG ORTHOGONAL ARRAYS OF STRENGTH THREE,2014
1346,"An important estimation problem that is closely related to large-scale multiple testing is that of estimating the null density and the proportion of nonnull effects. A few estimators have been introduced in the literature; however, several important problems, including the evaluation of the minimax rate of convergence and the construction of rate-optimal estimators, remain open.
In this paper, we consider optimal estimation of the null density and the proportion of nonnull effects. Both minimax lower and upper bounds are derived. The lower bound is established by a two-point testing argument, where at the core is the novel construction of two least favorable marginal densities f(1) and f(2). The density f(1) is heavy tailed both in the spatial and frequency domains and f(2) is a perturbation of f(1) such that the characteristic functions associated with f(1) and f(2) match each other in low frequencies. The minimax upper bound is obtained by constructing estimators which rely on the empirical characteristic function and Fourier analysis. The estimator is shown to be minimax rate optimal.
Compared to existing methods in the literature, the proposed procedure not only provides more precise estimates of the null density and the proportion of the nonnull effects, but also yields more accurate results when used inside some multiple testing procedures which aim at controlling the False Discovery Rate (FDR). The procedure is easy to implement and numerical results are given.",WOS:000273800100004,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'EMPIRICAL BAYES', 'MIXTURES', 'DISTRIBUTIONS', 'MICROARRAYS', 'ORACLE']",OPTIMAL RATES OF CONVERGENCE FOR ESTIMATING THE NULL DENSITY AND PROPORTION OF NONNULL EFFECTS IN LARGE-SCALE MULTIPLE TESTING,2010
1347,"This paper develops empirical likelihood methodology for irregularly spaced spatial data in the frequency domain. Unlike the frequency domain empirical likelihood (FUEL) methodology for time series (on a regular grid), the formulation of the spatial I-DEL needs special care due to lack of the usual orthogonality properties of the discrete Fourier transform for irregularly spaced data and due to presence of nontrivial bias in the periodogram under different spatial asymptotic structures. A spatial FDEL is formulated in the paper taking into account the effects of these factors. The main results of the paper show that Wilks' phenomenon holds for a scaled version of the logarithm of the proposed empirical likelihood ratio statistic in the sense that it is asymptotically distribution-free and has a chi-squared limit. As a result, the proposed spatial FDEL method can be used to build nonparametric, asymptotically correct confidence regions and tests for covariance parameters that are defined through spectral estimating equations, for irregularly spaced spatial data. In comparison to the more common studentization approach, a major advantage of our method is that it does not require explicit estimation of the standard error of an estimator, which is itself a very difficult problem as the asymptotic variances of many common estimators depend on intricate interactions among several population quantities, including the spectral density of the spatial process, the spatial sampling density and the spatial asymptotic structure. Results from a numerical study are also reported to illustrate the methodology and its finite sample properties.",WOS:000352757100003,ANNALS OF STATISTICS,"['LONG-RANGE DEPENDENCE', 'RANDOM-FIELDS', 'ASYMPTOTIC DISTRIBUTIONS', 'CONFIDENCE-REGIONS', 'MIXING CONDITIONS', 'ESTIMATORS', 'RATIO', 'REGRESSION', 'DESIGNS', 'MODELS']",A FREQUENCY DOMAIN EMPIRICAL LIKELIHOOD METHOD FOR IRREGULARLY SPACED SPATIAL DATA,2015
1348,"We describe an approach to creating interactive and animated graphical displays using R's graphics engine and Scalable Vector Graphics, an XML vocabulary for describing two-dimensional graphical displays. We use the svg() graphics device inR and then post-process the resulting XML documents. The post-processing identifies the elements in the SVG that correspond to the different components of the graphical display, e.g., points, axes, labels, lines. One can then annotate these elements to add interactivity and animation effects. One can also use JavaScript to provide dynamic interactive effects to the plot, enabling rich user interactions and compelling visualizations. The resulting SVG documents can be embedded within HTML documents and can involve JavaScript code that integrates the SVG and HTML objects. The functionality is provided via the SVG Annotation package and makes static plots generated via R graphics functions available as stand-alone, interactive and animated plots for the Web and other venues.",WOS:000299641400001,JOURNAL OF STATISTICAL SOFTWARE,,Interactive and Animated Scalable Vector Graphics and R Data Displays,2012
1349,"We present growfunctions for R that offers Bayesian nonparametric estimation models for analysis of dependent, noisy time series data indexed by a collection of domains. This data structure arises from combining periodically published government survey statistics, such as are reported in the Current Population Study (CPS). The CPS publishes monthly, by-state estimates of employment levels, where each state expresses a noisy time series. Published state-level estimates from the CPS are composed from household survey responses in a model-free manner and express high levels of volatility due to insufficient sample sizes. Existing software solutions borrow information over a modeled time-based dependence to extract a de-noised time series for each domain. These solutions, however, ignore the dependence among the domains that may be additionally leveraged to improve estimation efficiency. The growfunctions package offers two fully nonparametric mixture models that simultaneously estimate both a time and domain-indexed dependence structure for a collection of time series: (1) A Gaussian process (GP) construction, which is parameterized through the covariance matrix, estimates a latent function for each domain. The covariance parameters of the latent functions are indexed by domain under a Dirichlet process prior that permits estimation of the dependence among functions across the domains: (2) An intrinsic Gaussian Markov random field prior construction provides an alternative to the GP that expresses different computation and estimation properties. In addition to performing denoised estimation of latent functions from published domain estimates, growfunctions allows estimation of collections of functions for observation units (e. g., households), rather than aggregated domains, by accounting for an informative sampling design under which the probabilities for inclusion of observation units are related to the response variable. growfunctions includes plot functions that allow visual assessments of the fit performance and dependence structure of the estimated functions. Computational efficiency is achieved by performing the sampling for estimation functions using compiled C++.",WOS:000389071800001,JOURNAL OF STATISTICAL SOFTWARE,"['INFERENCE', 'PACKAGE', 'PRIORS']",Bayesian Nonparametric Mixture Estimation for Time-Indexed Functional Data in R,2016
1350,"spectrino is a spectra preparation software utility for the R language and environment for statistical computing. It is an operating-system specific tool, for use under Microsoft Windows, with specialized visualization, organization and preprocessing features for spectra. The software accepts spectral data from analytical instruments and then prepares a data structure to be introduced in R. spectrino has a rich set of features to create data structures and visually manipulate/compare spectra. The application is accessible by a library of functions from within R. These commands allow for the creation and manipulation of data structures in spectrino and the selective extraction of spectral data. Before exporting, the spectra are preprocessed according the requirements of consecutive discriminant analysis. This preprocessing is adjustable by a series of options.",WOS:000244068300001,JOURNAL OF STATISTICAL SOFTWARE,,spectrino software: Spectra visualization and preparation for R,2007
1351,"In recent years, there has been increased interest in methods for gender prediction based on first names that employ various open data sources. These methods have applications from bibliometric studies to customizing commercial offers for web users. Analysis of gender disparities in science based on such methods are published in the most prestigious journals, although they could be improved by choosing the most suited prediction method with optimal parameters and performing validation studies using the best data source for a given purpose. There is also a need to monitor and report how well a given prediction method works in comparison to others. In this paper, the author recommends a set of tools (including one dedicated to gender prediction, the R package called genderizeR), data sources (including the genderize.io API), and metrics that could be fully reproduced and tested in order to choose the optimal approach suitable for different gender analyses.",WOS:000385276100003,R JOURNAL,"['ERROR RATE', 'RULE']",Gender Prediction Methods Based on First Names with genderizeR,2016
1352,"We consider the problem of robustly predicting as well as the best linear combination of d given functions in least squares regression, and variants of this problem including constraints on the parameters of the linear combination. For the ridge estimator and the ordinary least squares estimator, and their variants, we provide new risk bounds of order d/n without logarithmic factor unlike some standard results, where n is the size of the training data. We also provide a new estimator with better deviations in the presence of heavy-tailed noise. It is based on truncating differences of losses in a min-max framework and satisfies a d/n risk bound both in expectation and in deviations. The key common surprising factor of these results is the absence of exponential moment condition on the output distribution while achieving exponential deviations. All risk bounds are obtained through a PAC-Bayesian analysis on truncated differences of losses. Experimental results strongly back up our truncated min-max estimator.",WOS:000299186500022,ANNALS OF STATISTICS,['RATES'],ROBUST LINEAR LEAST SQUARES REGRESSION,2011
1353,"In this paper, we study asymptotic properties of model selection criteria for high-dimensional regression models where the number of covariates is much larger than the sample size. In particular, we consider a class of loss functions calIed the class of quadratically supported risks which is large enough to include the quadratic loss, Huber loss, quantile loss and logistic loss. We provide sufficient conditions for the model selection criteria, which are applicable to the class of quadratically supported risks. Our results extend most previous sufficient conditions for model selection consistency. In addition, sufficient conditions for pathconsistency of the Lasso and nonconvex penalized estimators are presented. Here, pathconsistency means that the probability of the solution path that includes the true model converges to 1. Pathconsistency makes it practically feasible to apply consistent model selection criteria to high-dimensional data. The data-adaptive model selection procedure is proposed which is selection consistent and performs well for finite samples. Results of simulation studies as well as real data analysis are presented to compare the finite sample performances of the proposed data adaptive model selection criterion with other competitors.",WOS:000389620800009,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'TUNING PARAMETER SELECTION', 'ULTRA-HIGH DIMENSION', 'QUANTILE REGRESSION', 'VARIABLE SELECTION', 'DIVERGING NUMBER', 'ESTIMATORS']",CONSISTENT MODEL SELECTION CRITERIA FOR QUADRATICALLY SUPPORTED RISKS,2016
1354,"We consider the problem of detecting the true quantum state among r possible ones, based of measurements performed on n copies of a finite-dimensional quantum system. A special case is the problem of discriminating between r probability measures on a finite sample space, using n i.i.d. observations. In this classical setting, it is known that the averaged error probability decreases exponentially with exponent given by the worst case binary Chernoff bound between any possible pair of the r probability measures. Define analogously the multiple quantum Chernoff bound, considering all possible pairs of states. Recently, it has been shown that this asymptotic error bound is attainable in the case of r pure states, and that it is unimprovable in general. Here we extend the attainability result to a larger class of r-tuples of states which are possibly mixed, but pairwise linearly independent. We also construct a quantum detector which universally attains the multiple quantum Chernoff bound up to a factor 1/3.",WOS:000311639700003,ANNALS OF STATISTICS,"['DISCRIMINATION', 'STATES']",AN ASYMPTOTIC ERROR BOUND FOR TESTING MULTIPLE QUANTUM HYPOTHESES,2011
1355,"This paper is devoted to the introduction of a new class of consistent estimators of the fractal dimension of locally self-similar Gaussian processes. These estimators are based on convex combinations of sample quantiles of discrete variations of a sample path over a discrete grid of the interval [0, 1]. We derive the almost sure convergence and the asymptotic normality for these estimators. The key-ingredient is a Bahadur representation for sample quantiles of nonlinear functions of Gaussian sequences with correlation function decreasing as k(-alpha) L(k) for some alpha > 0 and some slowly varying function L(.).",WOS:000256504400015,ANNALS OF STATISTICS,"['FRACTIONAL BROWNIAN-MOTION', 'BAHADUR REPRESENTATION', 'FRACTAL DIMENSION', 'SEQUENCES', 'VARIABLES']",Hurst exponent estimation of locally self-similar Gaussian processes using sample quantiles,2008
1356,"The PresenceAbsence package for R provides a set of functions useful when evaluating the results of presence-absence analysis, for example, models of species distribution or the analysis of diagnostic tests. The package provides a toolkit for selecting the optimal threshold for translating a probability surface into presence-absence maps specifically tailored to their intended use. The package includes functions for calculating threshold dependent measures such as confusion matrices, percent correctly classified (PCC), sensitivity, specificity, and Kappa, and produces plots of each measure as the threshold is varied. It also includes functions to plot the Receiver Operator Characteristic (ROC) curve and calculates the associated area under the curve (AUC), a threshold independent measure of model quality. Finally, the package computes optimal thresholds by multiple criteria, and plots these optimized thresholds on the graphs.",WOS:000252620100001,JOURNAL OF STATISTICAL SOFTWARE,"['HABITAT MODELS', 'SPECIES DISTRIBUTIONS', 'LOGISTIC-REGRESSION', 'PERFORMANCE', 'ACCURACY', 'CURVE', 'AREA']",PresenceAbsence: An R package for presence absence analysis,2008
1357,"Random ferns is a very simple yet powerful classification method originally introduced for specific computer vision tasks. In this paper, I show that this algorithm may be considered as a constrained decision tree ensemble and use this interpretation to introduce a series of modifications which enable the use of random ferns in general machine learning problems. Moreover, I extend the method with an internal error approximation and an attribute importance measure based on corresponding features of the random forest algorithm. I also present the R package rFerns containing an efficient implementation of this modified version of random ferns.",WOS:000349841700001,JOURNAL OF STATISTICAL SOFTWARE,,rFerns: An Implementation of the Random Ferns Method for General-Purpose Machine Learning,2014
1358,"Most empirical social scientists are surprised that low-level numerical issues in software can have deleterious effects on the estimation process. Statistical analyses that appear to be perfectly successful can be invalidated by concealed numerical problems. We have developed a set of tools, contained in accuracy, a package for R and S-PLUS, to diagnose problems stemming from numerical and measurement error and to improve the accuracy of inferences. The tools included in accuracy include a framework for gauging the computational stability of model results, tools for comparing model results, optimization diagnostics, and tools for collecting entropy for true random numbers generation.",WOS:000249734500001,JOURNAL OF STATISTICAL SOFTWARE,"['GLOBAL MAXIMUM', 'MODEL']",accuracy: Tools for accurate and reliable statistical computing,2007
1359,"Subgroup discovery is a data mining task halfway between descriptive and predictive data mining. Nowadays it is very relevant for researchers due to the fact that the knowledge extracted is simple and interesting. For this task, evolutionary fuzzy systems are well suited algorithms because they can find a good trade-off between multiple objectives in large search spaces. In fact, this paper presents the SDEFSR package, which contains all the evolutionary fuzzy systems for subgroup discovery presented throughout the literature. It is a package without dependencies on other software, providing functions with recommended default parameters. In addition, it brings a graphical user interface to avoid the user having to know all the parameters of the algorithms.",WOS:000395669800020,R JOURNAL,"['ALGORITHM', 'RULES', 'SD']",Subgroup Discovery with Evolutionary Fuzzy Systems in R: The SDEFSR Package,2016
1360,"In recent years, the cost of DNA sequencing has decreased at a rate that has outpaced improvements in memory capacity. It is now common to collect or have access to many gigabytes of biological sequences. This has created an urgent need for approaches that analyze sequences in subsets without requiring all of the sequences to be loaded into memory at one time. It has also opened opportunities to improve the organization and accessibility of information acquired in sequencing projects. The DECIPHER package offers solutions to these problems by assisting in the curation of large sets of biological sequences stored in compressed format inside a database. This approach has many practical advantages over standard bioinformatics workflows, and enables large analyses that would otherwise be prohibitively time consuming.",WOS:000385276100026,R JOURNAL,['GENERATION'],Using DECIPHER v2.0 to Analyze Big Biological Sequence Data in R,2016
1361,,WOS:000312899000003,ANNALS OF STATISTICS,,DISCUSSION: LATENT VARIABLE GRAPHICAL MODEL SELECTION VIA CONVEX OPTIMIZATION,2012
1362,"When considering d possibly dependent random variables, one is often interested in extreme risk regions, with very small probability p. We consider risk regions of the form {z is an element of R(d) : f (z) <= beta}, where f is the joint density and beta a small number. Estimation of such an extreme risk region is difficult since it contains hardly any or no data. Using extreme value theory, we construct a natural estimator of an extreme risk region and prove a refined form of consistency, given a random sample of multivariate regularly varying random vectors. In a detailed simulation and comparison study, the good performance of the procedure is demonstrated. We also apply our estimator to financial data.",WOS:000293716500017,ANNALS OF STATISTICS,"['DENSITY LEVEL SETS', 'NONPARAMETRIC-ESTIMATION', 'INDEX', 'RATES', 'TAIL']",ESTIMATION OF EXTREME RISK REGIONS UNDER MULTIVARIATE REGULAR VARIATION,2011
1363,"One aspect of evaluating the design for an experiment is the discovery of the relationships between subspaces of the data space. Initially we establish the notation and methods for evaluating an experiment with a single randomization. Starting with two structures, or orthogonal decompositions of the data space, we describe how to combine them to form the overall decomposition for a single-randomization experiment that is ""structure balanced."" The relationships between the two structures are characterized using efficiency factors. The decomposition is encapsulated in a decomposition table. Then, for experiments that involve multiple randomizations forming a chain, we take several structures that pairwise are structure balanced and combine them to establish the form of the orthogonal decomposition for the experiment. In particular, it is proven that the properties of the design for Such an experiment are derived in a straightforward manner from those of the individual designs. We show how to formulate an extended decomposition table giving the sources of variation, their relationships and their degrees of freedom, so that competing designs can be evaluated.",WOS:000271673700017,ANNALS OF STATISTICS,"['ORTHOGONAL BLOCK STRUCTURE', 'LINEAR-MODELS', 'VARIANCE', 'DESIGNS', 'COMBINATION', 'INFORMATION', 'BALANCE']",DECOMPOSITION TABLES FOR EXPERIMENTS I. A CHAIN OF RANDOMIZATIONS,2009
1364,"R is a free open-source implementation of the S statistical computing language and programming environment. The current status of R is a command line driven interface with no advanced cross-platform graphical user interface (GUI), but it includes tools for building such. Over the past years, proprietary and non-proprietary GUI solutions have emerged, based on internal or external tool kits, with different scopes and technological concepts. For example, Rgui.exe and Rgui.app have become the de facto GUI on the Microsoft Windows and Mac OS X platforms, respectively, for most users. In this paper we discuss RKWard which aims to be both a comprehensive GUI and an integrated development environment for R. RKWard is based on the KDE software libraries. Statistical procedures and plots are implemented using an extendable plugin architecture based on ECMAScript (JavaScript), R, and XML. RKWard provides an excellent tool to manage different types of data objects; even allowing for seamless editing of certain types. The objective of RKWard is to provide a portable and extensible R interface for both basic and advanced statistical and graphical analysis, while not compromising on flexibility and modularity of the R programming environment itself.",WOS:000305991700001,JOURNAL OF STATISTICAL SOFTWARE,,RKWard: A Comprehensive Graphical User Interface and Integrated Development Environment for Statistical Analysis with R,2012
1365,,WOS:000253077800024,ANNALS OF STATISTICS,,"Strong invariance principles for sequential Bahadur-Kiefer and vervaat error processes of long-range dependent sequences (vol 34, pg 1013, 2006)",2007
1366,"Enumerating nonisomorphic orthogonal arrays is an important, yet very difficult, problem. Although orthogonal arrays with a specified set of parameters have been enumerated in a number of cases, general results are extremely rare. In this paper, we provide a complete solution to enumerating nonisomorphic two-level orthogonal arrays of strength d with d + 2 constraints for any d and any run size n = lambda 2(d). Our results not only give the number of nonisomorphic orthogonal arrays for given d and n, but also provide a systematic way of explicitly constructing these arrays. Our approach to the problem is to make use of the recently developed theory of J-characteristics for fractional factorial designs. Besides the general theoretical results, the paper presents some results from applications of the theory to orthogonal arrays of strength two, three and four.",WOS:000248987600014,ANNALS OF STATISTICS,"['FRACTIONAL FACTORIAL-DESIGNS', 'MINIMUM ABERRATION CRITERIA', 'CATALOG', 'CLASSIFICATION']",Complete enumeration of two-level orthogonal arrays of strength d with d+2 constraints,2007
1367,"One major goal in clinical applications of multi-state models is the estimation of transition probabilities. The usual nonparametric estimator of the transition matrix for non-homogeneous Markov processes is the Aalen-Johansen estimator (Aalen and Johansen 1978). However, two problems may arise from using this estimator: first, its standard error may be large in heavy censored scenarios; second, the estimator may be inconsistent if the process is non-Markovian. The development of the R package TPmsm has been motivated by several recent contributions that account for these estimation problems. Estimation and statistical inference for transition probabilities can be performed using TPmsm. The TPmsm package provides seven different approaches to three-state illness-death modeling. In two of these approaches the transition probabilities are estimated conditionally on current or past covariate measures. Two real data examples are included for illustration of software usage.",WOS:000349843300001,JOURNAL OF STATISTICAL SOFTWARE,"['ILLNESS-DEATH MODEL', 'MULTISTATE MODELS', 'NONPARAMETRIC-ESTIMATION', 'COMPETING RISKS', 'PACKAGE', 'MATRIX']",TPmsm: Estimation of the Transition Probabilities in 3-State Models,2014
1368,"Quasi-least squares (QLS) is an alternative computational approach for estimation of the correlation parameter in the framework of generalized estimating equations (GEE). QLS overcomes some limitations of GEE that were discussed in Crowder (1995). In addition, it allows for easier implementation of some correlation structures that are not available for GEE. We describe a user written SAS macro called %QLS, and demonstrate application of our macro using a clinical trial example for the comparison of two treatments for a common toenail infection. %QLS also computes the lower and upper boundaries of the correlation parameter for analysis of longitudinal binary data that were described by Prentice (1988). Furthermore, it displays a warning message if the Prentice constraints are violated. This warning is not provided in existing GEE software packages and other packages that were recently developed for application of QLS (in Stata, MATLAB, and R). %QLS allows for analysis of continuous, binary, or count data with one of the following working correlation structures: the first-order autoregressive, equicorrelated, Markov, or tri-diagonal structures.",WOS:000281587000001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED ESTIMATING EQUATIONS', 'LONGITUDINAL DATA', 'LINEAR-MODELS', 'FRAMEWORK', 'MATLAB', 'STATA']",%QLS SAS Macro: A SAS Macro for Analysis of Correlated Data Using Quasi-Least Squares,2010
1369,"The crch package provides functions for maximum likelihood estimation of censored or truncated regression models with conditional heteroscedasticity along with suitable standard methods to summarize the fitted models and compute predictions, residuals, etc. The supported distributions include left-or right-censored or truncated Gaussian, logistic, or student-t distributions with potentially different sets of regressors for modeling the conditional location and scale. The models and their R implementation are introduced and illustrated by numerical weather prediction tasks using precipitation data for Innsbruck (Austria).",WOS:000385276100013,R JOURNAL,"['EXTENDED LOGISTIC-REGRESSION', 'LIMITED DEPENDENT-VARIABLES', 'MODEL OUTPUT STATISTICS', 'BETA REGRESSION', 'FORECASTS']",Heteroscedastic Censored and Truncated Regression with crch,2016
1370,"We study two nonlinear methods for statistical linear inverse problems when the operator is not known. The two constructions combine Galerkin regularization and wavelet thresholding. Their performances depend on the underlying structure of the operator, quantified by an index of sparsity. We prove their rate-optimality and adaptivity properties over Besov classes.",WOS:000253390000012,ANNALS OF STATISTICS,"['ADAPTIVE ESTIMATION', 'WAVELET', 'DECOMPOSITION', 'EQUATIONS']",Nonlinear estimation for linear inverse problems with error in the operator,2008
1371,"This paper considers the effect of least squares procedures for nearly unstable linear time series with strongly dependent innovations. Under a general framework and appropriate scaling, it is shown that ordinary least squares procedures converge to functionals of fractional Ornstein-Uhlenbeck processes. We use fractional integrated noise as an example to illustrate the important ideas. In this case, the functionals bear only formal analogy to those in the classical framework with uncorrelated innovations, with Wiener processes being replaced by fractional Brownian motions. It is also shown that limit theorems for the functionals involve nonstandard scaling and nonstandard limiting distributions. Results of this paper shed light on the asymptotic behavior of nearly unstable long-memory processes.",WOS:000251096100007,ANNALS OF STATISTICS,"['FRACTIONAL BROWNIAN-MOTION', 'CENTRAL-LIMIT-THEOREM', 'TIME-SERIES', 'WEAK-CONVERGENCE', 'STOCHASTIC CALCULUS', 'LINEAR-PROCESSES', 'INFERENCE', 'FUNCTIONALS', 'INTEGRATION', 'STATIONARY']",Asymptotic theory of least squares estimators for nearly unstable processes under strong dependence,2007
1372,"The R package MixSim is a new tool that allows simulating mixtures of Gaussian distributions with different levels of overlap between mixture components. Pairwise overlap, defined as a sum of two misclassification probabilities, measures the degree of interaction between components and can be readily employed to control the clustering complexity of datasets simulated from mixtures. These datasets can then be used for systematic performance investigation of clustering and finite mixture modeling algorithms. Among other capabilities of MixSim, there are computing the exact overlap for Gaussian mixtures, simulating Gaussian and non-Gaussian data, simulating outliers and noise variables, calculating various measures of agreement between two partitionings, and constructing parallel distribution plots for the graphical display of finite mixture models. All features of the package are illustrated in great detail. The utility of the package is highlighted through a small comparison study of several popular clustering algorithms.",WOS:000312289800001,JOURNAL OF STATISTICAL SOFTWARE,"['2 HIERARCHICAL CLUSTERINGS', 'SEPARATION']",MixSim: An R Package for Simulating Data to Study Performance of Clustering Algorithms,2012
1373,"This article presents inlinedocs, an R package for generating documentation from comments. The concept of structured, interwoven code and documentation has existed for many years, but existing systems that implement this for the R programming language do not tightly integrate with R code, leading to several drawbacks. This article attempts to address these issues and presents 2 contributions for documentation generation for the R community. First, we propose a new syntax for inline documentation of R code within comments adjacent to the relevant code, which allows for highly readable and maintainable code and documentation. Second, we propose an extensible system for parsing these comments, which allows the syntax to be easily augmented.",WOS:000324371300001,JOURNAL OF STATISTICAL SOFTWARE,,"Sustainable, Extensible Documentation Generation Using inlinedocs",2013
1374,"For functional magnetic resonance imaging (fMRI) studies, researchers can use multi-subject blocked designs to identify active brain regions for a certain stimulus type of interest. Before performing such an experiment, careful planning is necessary to obtain efficient stimulus effect estimators within the available financial resources. The optimal number of subjects and the optimal scanning time for a multi-subject blocked design with fixed experimental costs can be determined using optimal design methods. In this paper, the user-friendly computer program POBE 1.2 (program for optimal design of blocked experiments, version 1.2) is presented. POBE provides a graphical user interface for fMRI researchers to easily and efficiently design their experiments. The computer program POBE calculates the optimal number of subjects and the optimal scanning time for user specified experimental factors and model parameters so that the statistical efficiency is maximised for a given study budget. POBE can also be used to determine the minimum budget for a given power. Furthermore, a maximin design can be determined as efficient design for a possible range of values for the unknown model parameters. In this paper, the computer program is described and illustrated with typical experimental factors for a blocked fMRI experiment.",WOS:000332110300001,JOURNAL OF STATISTICAL SOFTWARE,"['EVENT-RELATED FMRI', 'STATISTICAL-ANALYSIS', 'POWER CALCULATION', 'OPTIMIZATION', 'FSL']",POBE : A Computer Program for Optimal Design of Multi-Subject Blocked fMRI Experiments,2014
1375,"Statistical process control (SPC) descibes a widely-used set of approaches used to detect shifts in processes in, for example, manufacturing. Among these are ""control charts"". Control charts and other SPC techniques have been in use since at least the 1950s, and, because they are comparatively unsophisticated, are often used by management or operations personnel without formal statistical training. These personnel will often have experience with the popular spreadsheet program Excel, but may have less training on a mainstream statistical package. Base Excel does not provide the ability to draw control charts directly, although add-ins for that purpose are available for purchase.
We present a free add-in for Excel that draws the most common sorts of control charts. It follows the development of the textbook of Montgomery (2005), so it may be well-suited for instructional purposes.",WOS:000267708700001,JOURNAL OF STATISTICAL SOFTWARE,,An Excel Add-In for Statistical Process Control Charts,2009
1376,"We introduce the R package npmv that performs nonparametric inference for the comparison of multivariate data samples and provides the results in easy-to-understand, but statistically correct, language. Unlike in classical multivariate analysis of variance, multivariate normality is not required for the data. In fact, the different response variables may even be measured on different scales (binary, ordinal, quantitative). p values are calculated for overall tests (permutation tests and F approximations), and, using multiple testing algorithms which control the familywise error rate, significant subsets of response variables and factor levels are identified. The package may be used for low-or high-dimensional data with small or with large sample sizes and many or few factor levels.",WOS:000392706300001,JOURNAL OF STATISTICAL SOFTWARE,"['FACTORIAL-DESIGNS', 'TESTS']",Nonparametric Inference for Multivariate Data: The R Package npmv,2017
1377,"Let (Xi)(i >= 1) be a stationary mean-zero Gaussian process with covariances rho(k) = E(X(1) X(k+1)) satisfying rho(0) = 1 and rho(k) = k(-D) L(k), where D is in (0, 1), and L is slowly varying at infinity. Consider the U-process {U(n)(r), r is an element of 1} defined as
U(n)(r) = 1/n (n-1) Sigma(1 <= i not equal j <= n) 1{G(X(i), X(j))<= r}
where I is an interval included in R, and G is a symmetric function. In this paper, we provide central and noncentral limit theorems for U(n). They are used to derive, in the long-range dependence setting, new properties of many well-known estimators such as the Hodges-Lehmann estimator, which is a well-known robust location estimator, the Wilcoxon-signed rank statistic, the sample correlation integral and an associated robust scale estimator. These robust estimators are shown to have the same asymptotic distribution as the classical location and scale estimators. The limiting distributions are expressed through multiple Wiener-Ito integrals.",WOS:000293716500003,ANNALS OF STATISTICS,"['STATISTICS', 'ESTIMATORS', 'SEQUENCES', 'LOCATION']",ASYMPTOTIC PROPERTIES OF U-PROCESSES UNDER LONG-RANGE DEPENDENCE,2011
1378,"In medical and epidemiological studies, the odds ratio is a commonly applied measure to approximate the relative risk or risk ratio in cohort studies. It is well known such an approximation is poor and can generate misleading conclusions, if the incidence rate of a study outcome is not rare. However, there are times when the incidence rate is not directly available in the published work. Motivated by real applications, this paper presents methods to convert the odds ratio to the relative risk when published data offers limited information. Specifically, the proposed new methods can convert the odds ratio to the relative risk, if an odds ratio and/or a confidence interval as well as the sample sizes for the treatment and control group are available. In addition, the developed methods can be utilized to approximate the relative risk based on the adjusted odds ratio from logistic regression or other multiple regression models. In this regard, this paper extends a popular method by Zhang and Yu (1998) for converting odds ratios to risk ratios. The objective is novelly mapped into a constrained nonlinear optimization problem, which is solved with both a grid search and nonlinear optimization algorithm. The methods are implemented in R package orsk (Wang 2013) which contains R functions and a Fortran subroutine for efficiency. The proposed methods and software are illustrated with real data applications.",WOS:000325947800001,JOURNAL OF STATISTICAL SOFTWARE,"['CARDIAC-CATHETERIZATION', 'COMMON OUTCOMES', 'RACE', 'SEX']",Converting Odds Ratio to Relative Risk in Cohort Studies with Partial Data Information,2013
1379,"We consider the sampling problem for functional PCA (fPCA), where the simplest example is the case of taking time samples of the underlying functional components. More generally, we model the sampling operation as a continuous linear map from H to R-m, where the functional components to lie in some Hilbert subspace H of L-2, such as a reproducing kernel Hilbert space of smooth functions. This model includes time and frequency sampling as special cases. In contrast to classical approach in fPCA in which access to entire functions is assumed, having a limited number m of functional samples places limitations on the performance of statistical procedures. We study these effects by analyzing the rate of convergence of an M-estimator for the subspace spanned by the leading components in a multi-spiked covariance model. The estimator takes the form of regularized PCA, and hence is computationally attractive. We analyze the behavior of this estimator within a nonasymptotic framework, and provide bounds that hold with high probability as a function of the number of statistical samples n and the number of functional samples m. We also derive lower bounds showing that the rates obtained are minimax optimal.",WOS:000321844300005,ANNALS OF STATISTICS,"['PRINCIPAL-COMPONENTS-ANALYSIS', 'LONGITUDINAL DATA', 'APPROXIMATION']",SAMPLED FORMS OF FUNCTIONAL PCA IN REPRODUCING KERNEL HILBERT SPACES,2012
1380,"New types of designs called nested space-filling designs have been proposed for conducting multiple computer experiments with different levels of accuracy. In this article, we develop several approaches to constructing Such designs. The development of these methods also leads to the introduction of several new discrete mathematics concepts, including nested orthogonal arrays and nested difference matrices.",WOS:000271673500019,ANNALS OF STATISTICS,"['LATIN HYPERCUBE DESIGNS', 'ARRAY SAMPLING DESIGNS', 'CENTRAL-LIMIT-THEOREM', 'ORTHOGONAL ARRAYS', 'COMPUTER EXPERIMENTS', 'STRENGTH 2', 'SIMULATIONS', 'MODELS', 'OUTPUT', 'CALIBRATION']",CONSTRUCTION OF NESTED SPACE-FILLING DESIGNS,2009
1381,"Recent advances in computer recording and storing technology have tremendously increased the presence of functional data, whose graphical representation can be infinite-dimensional curve, image, or shape. When the same functional object is observed over a period of time, such data are known as functional time series. This article makes first attempt to describe several techniques (centered around functional principal component analysis) for modeling and forecasting functional time series from a computational aspect, using a readily-available R addon package. These methods are demonstrated using age-specific Australian fertility rate data from 1921 to 2006, and monthly sea surface temperature data from January 1950 to December 2011.",WOS:000321944400007,R JOURNAL,"['DATA MODELS', 'MORTALITY', 'RAINBOW']",ftsa: An R Package for Analyzing Functional Time Series,2013
1382,"We propose dimension reduction methods for sparse, high-dimensional multivariate response regression models. Both the number of responses and that of the predictors may exceed the sample size. Sometimes viewed as complementary, predictor selection and rank reduction are the most popular strategies for obtaining lower-dimensional approximations of the parameter matrix in such models. We show in this article that important gains in prediction accuracy can be obtained by considering them jointly. We motivate a new class of sparse multivariate regression models, in which the coefficient matrix has low rank and zero rows or can be well approximated by such a matrix. Next, we introduce estimators that are based on penalized least squares, with novel penalties that impose simultaneous row and rank restrictions on the coefficient matrix. We prove that these estimators indeed adapt to the unknown matrix sparsity and have fast rates of convergence. We support our theoretical results with an extensive simulation study and two data analyses.",WOS:000321844300001,ANNALS OF STATISTICS,"['LINEAR-REGRESSION', 'CONVERGENCE', 'ALGORITHM', 'MODELS']",JOINT VARIABLE AND RANK SELECTION FOR PARSIMONIOUS ESTIMATION OF HIGH-DIMENSIONAL MATRICES,2012
1383,"Computational infrastructure for representing persons and citations has been available in R for several years, but has been restructured through enhanced classes ""person"" and ""bibentry"" in recent versions of R. The new features include support for the specification of the roles of package authors (e. g. maintainer, author, contributor, translator, etc.) and more flexible formatting/printing tools among various other improvements. Here, we introduce the new classes and their methods and indicate how this functionality is employed in the management of R packages. Specifically, we show how the authors of R packages can be specified along with their roles in package 'DESCRIPTION' and/or 'CITATION' files and the citations produced from it.",WOS:000313197700010,R JOURNAL,,Who Did What? The Roles of R Package Authors and How to Refer to Them,2012
1384,"This paper studies convergence behavior of latent mixing measures that arise in finite and infinite mixture models, using transportation distances (i.e., Wasserstein metrics). The relationship between Wasserstein distances on the space of mixing measures and f-divergence functionals such as Hellinger and Kullback-Leibler distances on the space of mixture distributions is investigated in detail using various identifiability conditions. Convergence in Wasserstein metrics for discrete measures implies convergence of individual atoms that provide support for the measures, thereby providing a natural interpretation of convergence of clusters in clustering applications where mixture models are typically employed. Convergence rates of posterior distributions for latent mixing measures are established, for both finite mixtures of multivariate distributions and infinite mixtures based on the Dirichlet process.",WOS:000317451200015,ANNALS OF STATISTICS,"['POSTERIOR DISTRIBUTIONS', 'NONPARAMETRIC PROBLEMS', 'DIRICHLET MIXTURES', 'OPTIMAL RATES', 'CONSISTENCY', 'DENSITY', 'DIVERGENCE']",CONVERGENCE OF LATENT MIXING MEASURES IN FINITE AND INFINITE MIXTURE MODELS,2013
1385,"This paper proposes a widely applicable method of approximate maximum-likelihood estimation for multivariate diffusion process from discretely sampled data. A closed-form asymptotic expansion for transition density is proposed and accompanied by an algorithm containing only basic and explicit calculations for delivering any arbitrary order of the expansion. The likelihood function is thus approximated explicitly and employed in statistical estimation. The performance of our method is demonstrated by Monte Carlo simulations from implementing several examples, which represent a wide range of commonly used diffusion models. The convergence related to the expansion and the estimation method are theoretically justified using the theory of Watanabe [Ann. Probab. 15 (1987) 1-39] and Yoshida [J. Japan Statist. Soc. 22 (1992) 139-159] on analysis of the generalized random variables under some standard sufficient conditions.",WOS:000321847600012,ANNALS OF STATISTICS,"['STOCHASTIC DIFFERENTIAL-EQUATIONS', 'DISCRETELY SAMPLED DIFFUSIONS', 'CONTINUOUS-TIME MODELS', 'MALLIAVIN CALCULUS', 'ASYMPTOTIC EXPANSIONS', 'APPROXIMATION APPROACH', 'NONLINEAR DIFFUSIONS', 'TRANSITION DENSITIES', 'SIMULATED LIKELIHOOD', 'WIENER FUNCTIONALS']",MAXIMUM-LIKELIHOOD ESTIMATION FOR DIFFUSION PROCESSES VIA CLOSED-FORM DENSITY EXPANSIONS,2013
1386,"Genetic association studies are commonly conducted to identify genes that explain the variability in a measured trait (e.g., disease status or disease progression). Often, results of these studies are summarized in the form of a p value corresponding to a test of association between each single nucleotide polymorphisms (SNPs) and the trait under study. As genes are comprised of multiple SNPs, post hoc approaches are generally applied to determine gene-level association. For example, if any SNP within a gene is significantly associated with the trait at a genome-wide significance level (p < 5 x 10-8), then the corresponding gene is considered significant. A complementary strategy, termed mixed modeling of meta-analysis p values (MixMAP) was proposed recently to characterize formally the associations between genes (or gene regions) and a trait based on multiple SNP-level p values. Here, the MixMAP package is presented as a means for implementing the MixMAP procedure in R.",WOS:000365980300001,JOURNAL OF STATISTICAL SOFTWARE,,MixMAP: An R Package for Mixed Modeling of Meta-Analysis p Values in Genetic Association Studies,2015
1387,,WOS:000290231500017,ANNALS OF STATISTICS,,"RESIDUAL EMPIRICAL PROCESSES FOR LONG AND SHORT MEMORY TIME SERIES (vol 36, pg 2453, 2008)",2010
1388,"Boosting is an iterative algorithm that combines simple classification rules with 'mediocre' performance in terms of misclassification error rate to produce a highly accurate classification rule. Stochastic gradient boosting provides an enhancement which incorporates a random mechanism at each boosting step showing an improvement in performance and speed in generating the ensemble. ada is an R package that implements three popular variants of boosting, together with a version of stochastic gradient boosting. In addition, useful plots for data analytic purposes are provided along with an extension to the multi-class case. The algorithms are illustrated with synthetic and real data sets.",WOS:000241808000001,JOURNAL OF STATISTICAL SOFTWARE,"['CLASSIFICATION', 'MACHINE']",ada: an R package for stochastic boosting,2006
1389,"We introduce glmulti, an R package for automated model selection and multi-model inference with glm and related functions. From a list of explanatory variables, the provided function glmulti builds all possible unique models involving these variables and, optionally, their pairwise interactions. Restrictions can be specified for candidate models, by excluding specific terms, enforcing marginality, or controlling model complexity. Models are fitted with standard R functions like glm. The n best models and their support (e.g., (Q)AIC, (Q)AICc, or BIC) are returned, allowing model selection and multi-model inference through standard R functions. The package is optimized for large candidate sets by avoiding memory limitation, facilitating parallelization and providing, in addition to exhaustive screening, a compiled genetic algorithm method. This article briefly presents the statistical framework and introduces the package, with applications to simulated and real data.",WOS:000281584500001,JOURNAL OF STATISTICAL SOFTWARE,['FUNCTIONAL MARGINALITY'],glmulti: An R Package for Easy Automated Model Selection with (Generalized) Linear Models,2010
1390,"Generalized linear mixed models provide a flexible framework for modeling a range of data, although with non-Gaussian response variables the likelihood cannot be obtained in closed form. Markov chain Monte Carlo methods solve this problem by sampling from a series of simpler conditional distributions that can be evaluated. The R package M C M C g l m m implements such an algorithm for a range of model fitting problems. More than one response variable can be analyzed simultaneously, and these variables are allowed to follow Gaussian, Poisson, multi(bi) nominal, exponential, zero-inflated and censored distributions. A range of variance structures are permitted for the random effects, including interactions with categorical or continuous variables (i.e., random regression), and more complicated variance structures that arise through shared ancestry, either through a pedigree or through a phylogeny. Missing values are permitted in the response variable(s) and data can be known up to some level of measurement error as in meta-analysis. All simulation is done in C/C++ using the CSparse library for sparse linear systems.",WOS:000275203300001,JOURNAL OF STATISTICAL SOFTWARE,"['PARAMETER EXPANSION', 'HIERARCHICAL-MODELS', 'MULTILEVEL MODELS', 'DATA AUGMENTATION', 'ALGORITHM', 'EM']",MCMC Methods for Multi-Response Generalized Linear Mixed Models: The MCMCglmm R Package,2010
1391,"Precision matrix is of significant importance in a wide range of applications in multivariate analysis. This paper considers adaptive minimax estimation of sparse precision matrices in the high dimensional setting. Optimal rates of convergence are established for a range of matrix norm losses. A fully data driven estimator based on adaptive constrained l(1) minimization is proposed and its rate of convergence is obtained over a collection of parameter spaces. The estimator, called ACLIME, is easy to implement and performs well numerically.
A major step in establishing the minimax rate of convergence is the derivation of a rate-sharp lower bound. A ""two-directional"" lower bound technique is applied to obtain the minimax lower bound. The upper and lower bounds together yield the optimal rates of convergence for sparse precision matrix estimation and show that the ACLIME estimator is adaptively minimax rate optimal for a collection of parameter spaces and a range of matrix norm losses simultaneously.",WOS:000372594300001,ANNALS OF STATISTICS,"['COVARIANCE ESTIMATION', 'GRAPHICAL MODELS', 'SELECTION', 'LASSO']",ESTIMATING SPARSE PRECISION MATRIX: OPTIMAL RATES OF CONVERGENCE AND ADAPTIVE ESTIMATION,2016
1392,"In the last decade, sequential Monte Carlo methods (SMC) emerged as a key tool in computational statistics [see, e.g., Sequential Monte Carlo Methods in Practice (2001) Springer, New York, Monte Carlo Stratergies in Scientific Computing (2001) Springer, New York, Complex Stochastic systems (2001) 109-173]. These algorithms approximate a sequence of distributions by a sequence of weighted empirical measures associated to a weighted population of particles, which are generated recursively.
Despite many theoretical advantages [see, e.g., J. Roy. Statist. Soc. Ser. B 63 (2001) 127-146. Ann. Statist. 33 (2005) 1983-2021, Feymann-Kac Formulae.Genealogical and interacting Particle Systems with Applications (2004) Springer, Ann. Statist. 32 (2004) 2385-2411], the large-sample theory of these approximations remains a question of central interest. In this paper we establish a law of large numbers and a central limit theorem as the number of particles gets large. We introduce the concepts of weighted sample consistency and asymptotic normality, and derive conditions under which the transformations of the weighted sample used in the SMC algorithm preserve these properties. To illustrate our findings, we analyze SMC algorithms to approximate the filtering distribution in state-space models. We show how our techniques allow to relax restrictive technical conditions used in previously reported works and provide grounds to analyze more sophisticated sequential sampling strategies, including branching, resampling at randomly selected times. and so on.",WOS:000260554100013,ANNALS OF STATISTICS,"['ZAKAI EQUATION', 'CONVERGENCE', 'IMPUTATIONS', 'INFERENCE', 'SYSTEMS', 'FILTERS']",LIMIT THEOREMS FOR WEIGHTED SAMPLES WITH APPLICATIONS TO SEQUENTIAL MONTE CARLO METHODS,2008
1393,"The R package FAMT (factor analysis for multiple testing) provides a powerful method for large-scale significance testing under dependence. It is especially designed to select differentially expressed genes in microarray data when the correlation structure among gene expressions is strong. Indeed, this method reduces the negative impact of dependence on the multiple testing procedures by modeling the common information shared by all the variables using a factor analysis structure. New test statistics for general linear contrasts are deduced, taking advantage of the common factor structure to reduce correlation and consequently the variance of error rates. Thus, the FAMT method shows improvements with respect to most of the usual methods regarding the non discovery rate and the control of the false discovery rate (FDR).
The steps of this procedure, each of them corresponding to R functions, are illustrated in this paper by two microarray data analyses. We first present how to import the gene expression data, the covariates and gene annotations. The second step includes the choice of the optimal number of factors, the factor model fitting, and provides a list of selected gene according to a preset FDR control level. Finally, diagnostic plots are provided to help the user interpret the factors using a vailable external information on either genes or arrays.",WOS:000290513100001,JOURNAL OF STATISTICAL SOFTWARE,['FALSE DISCOVERY RATES'],Factor Analysis for Multiple Testing (FAMT): An R Package for Large-Scale Significance Testing under Dependence,2011
1394,"This document describes the new features in version 2.x of the tgp package for R, implementing treed Gaussian process (GP) models. The topics covered include methods for dealing with categorical inputs and excluding inputs from the tree or GP part of the model; fully Bayesian sensitivity analysis for inputs/covariates; sequential optimization of black-box functions; and a new Monte Carlo method for inference in multi-modal posterior distributions that combines simulated tempering and importance sampling. These additions extend the functionality of tgp across all models in the hierarchy: from Bayesian linear models, to classification and regression trees (CART), to treed Gaussian processes with jumps to the limiting linear model. It is assumed that the reader is familiar with the baseline functionality of the package, outlined in the first vignette (Gramacy 2007).",WOS:000275203700001,JOURNAL OF STATISTICAL SOFTWARE,"['CHAIN MONTE-CARLO', 'DISTRIBUTIONS', 'INDEXES', 'SEARCH']","Categorical Inputs, Sensitivity Analysis, Optimization and Importance Tempering with tgp Version 2, an R Package for Treed Gaussian Process Models",2010
1395,"The behavior of maximum likelihood estimates (MLEs) and the likelihood ratio statistic in a family of problems involving pointwise nonparametric estimation of a monotone function is studied. This class of problems differs radically from the usual parametric or semiparametric situations in that the MLE of the monotone function at a point converges to the truth at rate n(1/3) (slower than the usual root n rate) with a non-Gaussian limit distribution. A framework for likelihood based estimation of monotone functions is developed and limit theorems describing the behavior of the MLES and the likelihood ratio statistic are established. In particular, the likelihood ratio statistic is found to be asymptotically pivotal with a limit distribution that is no longer X-2 but can be explicitly characterized in terms of a functional of Brownian motion. Applications of the main results are presented and potential extensions discussed.",WOS:000248692700001,ANNALS OF STATISTICS,"['PUTATIVE SOURCES', 'REGRESSION', 'DENSITY', 'RISK']",Likelihood based inference for monotone response models,2007
1396,"This paper provides ANOVA inference for nonparametric local polynomial regression (LPR) in analogy with ANOVA tools for the classical linear regression model. A surprisingly simple and exact local ANOVA decomposition is established, and a local R-squared quantity is defined to measure the proportion of local variation explained by fitting LPR. A global ANOVA decomposition is obtained by integrating local counterparts, and a global R-squared and a symmetric projection matrix are defined. We show that the proposed projection matrix is asymptotically idempotent and asymptotically orthogonal to its complement, naturally leading to an F-test for testing for no effect. A by-product result is that the asymptotic bias of the ""projected"" response based on local linear regression is of quartic order of the bandwidth. Numerical results illustrate the behaviors of the proposed R-squared and F-test. The ANOVA methodology is also extended to varying coefficient models.",WOS:000260554100003,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'CORRELATION CURVES', 'CHECKING', 'MODELS', 'WEAK']","ANALYSIS OF VARIANCE, COEFFICIENT OF DETERMINATION AND F-TEST FOR LOCAL POLYNOMIAL REGRESSION",2008
1397,"We provide software tools for displaying and publishing interactive 3-dimensional (3D) and 4-dimensional (4D) figures to html webpages, with examples of high-resolution brain imaging. Our framework is based in the R statistical software using the rgl package, a 3D graphics library. We build on this package to allow manipulation of figures including rotation and translation, zooming, coloring of brain substructures, adjusting transparency levels, and addition/or removal of brain structures. The need for better visualization tools of ultra high dimensional data is ever present; we are providing a clean, simple, web-based option. We also provide a package (brainR) for users to readily implement these tools.",WOS:000343788100005,R JOURNAL,['R PACKAGE'],brainR: Interactive 3 and 4D Images of High Resolution Neuroimage Data,2014
1398,"We consider the problem of estimating a sparse linear regression vector beta* under a Gaussian noise model, for the purpose of both prediction and model selection. We assume that prior knowledge is available on the sparsity pattern, namely the set of variables is partitioned into prescribed groups, only few of which are relevant in the estimation process. This group sparsity assumption suggests us to consider the Group Lasso method as a means to estimate beta*. We establish oracle inequalities for the prediction and l(2) estimation errors of this estimator. These bounds hold under a restricted eigenvalue condition on the design matrix. Under a stronger condition, we derive bounds for the estimation error for mixed (2, p)-norms with 1 <= p <= infinity. When p = infinity, this result implies that a thresholded version of the Group Lasso estimator selects the sparsity pattern of beta* with high probability. Next, we prove that the rate of convergence of our upper bounds is optimal in a minimax sense, up to a logarithmic factor, for all estimators over a class of group sparse vectors. Furthermore, we establish lower bounds for the prediction and l(2) estimation errors of the usual Lasso estimator. Using this result, we demonstrate that the Group Lasso can achieve an improvement in the prediction and estimation errors as compared to the Lasso.
An important application of our results is provided by the problem of estimating multiple regression equations simultaneously or multi-task learning. In this case, we obtain refinements of the results in [In Proc. of the 22nd Annual Conference on Learning Theory (COLT) (2009)1, which allow us to establish a quantitative advantage of the Group Lasso over the usual Lasso in the multi-task setting. Finally, within the same setting, we show how our results can be extended to more general noise distributions, of which we only require the fourth moment to be finite. To obtain this extension, we establish a new maximal moment inequality, which may be of independent interest.",WOS:000296995500012,ANNALS OF STATISTICS,"['GROUP LASSO', 'VARIABLE SELECTION', 'DANTZIG SELECTOR', 'LINEAR-MODELS', 'REGRESSION', 'RECOVERY', 'HETEROGENEITY', 'AGGREGATION', 'BIAS']",ORACLE INEQUALITIES AND OPTIMAL INFERENCE UNDER GROUP SPARSITY,2011
1399,"The problem of constructing optimal discriminating designs for a class of regression models is considered. We investigate a version of the optimality criterion criterion as introduced by Atkinson and Fedorov [Biometrika 62 (1975a) 289-303]. The numerical construction of optimal designs is very hard and challenging, if the number of pairwise comparisons is larger than 2. It is demonstrated that optimal designs with respect to this type of criteria can be obtained by solving (nonlinear) vector-valued approximation problems. We use a characterization of the best approximations to develop an efficient algorithm for the determination of the optimal discriminating designs. The new procedure is compared with the currently available methods in several numerical examples, and we demonstrate that the new method can find optimal discriminating designs in situations where the currently available procedures fail.",WOS:000320488200018,ANNALS OF STATISTICS,"['PARAMETER-ESTIMATION', 'DOSE-RESPONSE']",OPTIMAL DISCRIMINATING DESIGNS FOR SEVERAL COMPETING REGRESSION MODELS,2013
1400,"We define a generalized index of jump activity, propose estimators of that index for a discretely sampled process and derive the estimators' properties. These estimators are applicable despite the presence of Brownian volatility in the process, which makes it more challenging to infer the characteristics of the small, infinite activity jumps. When the method is applied to high frequency stock returns, we find evidence of infinitely active jumps in the data and estimate their index of activity.",WOS:000268604900005,ANNALS OF STATISTICS,"['LEVY PROCESSES', 'TAIL']",ESTIMATING THE DEGREE OF ACTIVITY OF JUMPS IN HIGH FREQUENCY DATA,2009
1401,"Clustered binary data with a large number of covariates have become increasingly common in many scientific disciplines. This paper develops an asymptotic theory for generalized estimating equations (GEE) analysis of clustered binary data when the number of covariates grows to infinity with the number of clusters. In this ""large n, diverging p"" framework, we provide appropriate regularity conditions and establish the existence, consistency and asymptotic normality of the GEE estimator. Furthermore, we prove that the sandwich variance formula remains valid. Even when the working correlation matrix is misspecified, the use of the sandwich variance formula leads to an asymptotically valid confidence interval and Wald test for an estimable linear combination of the unknown parameters. The accuracy of the asymptotic approximation is examined via numerical simulations. We also discuss the ""diverging p"" asymptotic theory for general GEE. The results in this paper extend the recent elegant work of Xie and Yang [Ann. Statist. 31 (2003) 310347] and Balan and Schiopu-Kratina [Ann. Statist. 32 (2005) 522-541] in the ""fixed p"" setting.",WOS:000288183800013,ANNALS OF STATISTICS,"['GENERALIZED ESTIMATING EQUATIONS', 'P-REGRESSION PARAMETERS', 'LONGITUDINAL DATA', 'ASYMPTOTIC-BEHAVIOR', 'M-ESTIMATORS', 'SEMIPARAMETRIC REGRESSION', 'ROBUST REGRESSION', 'LINEAR-MODELS', 'LIKELIHOOD', 'CONSISTENCY']",GEE ANALYSIS OF CLUSTERED BINARY DATA WITH DIVERGING NUMBER OF COVARIATES,2011
1402,In a recent paper Yang and Stufken [Ann. Statist. 40 (2012a) 1665-1685] gave sufficient conditions for complete classes of designs for nonlinear regression models. In this note we demonstrate that there is an alternative way to validate this result. Our main argument utilizes the fact that boundary points of moment spaces generated by Chebyshev systems possess unique representations.,WOS:000321847600008,ANNALS OF STATISTICS,"['LOCALLY OPTIMAL DESIGNS', 'GENERALIZED LINEAR-MODELS', 'LA GARZA PHENOMENON']",COMPLETE CLASSES OF DESIGNS FOR NONLINEAR REGRESSION MODELS AND PRINCIPAL REPRESENTATIONS OF MOMENT SPACES,2013
1403,"The RgoogleMaps package provides (1) an R interface to query the Google and the OpenStreetMap servers for Static maps in the form of PNGs, and (2) enables the user to overlay plots on those maps within R. The loa package provides dedicated panel functions to integrate RgoogleMaps within the lattice plotting environment.
In addition to solving the generic task of plotting on a map background in R, we introduce several specific algorithms to detect and visualize spatio-temporal cluster. This task can often be-reduced to detecting over-densities in space relative to a background density. The relative density estimation is framed as a binary classification problem. An integrated hotspot visualizer is presented which allows the efficient identification and visualization of cluster in one environment. Competing clustering methods such as the scan statistic and the density scan offer higher detection power at a much larger computational cost. Such clustering method can then be extended using the lattice trellis framework to provide further insight into the relationship between clusters and potentially influential parameters. While there are other options for such map 'mashups' we believe that the integration of RgoogleMaps and lattice using loa can in certain circumstances be advantageous, e.g., by providing a highly intuitive working environment for multivariate analysis and flexible testbed for the rapid development of novel data visualization .",WOS:000349845100001,JOURNAL OF STATISTICAL SOFTWARE,"['SCAN', 'VISUALIZATION']",RgoogleMaps and loa: Unleashing R Graphics Power on Map Tiles,2014
1404,"We consider the problem of estimating the graph associated with a binary Ising Markov random field. We describe a method based on l(1)-regularized logistic regression, in which the neighborhood of any given node is estimated by performing logistic regression subject to an l(1)-constraint. The method is analyzed under high-dimensional scaling in which both the number of nodes p and maximum neighborhood size d are allowed to grow as a function of the number of observations n. Our main results provide sufficient conditions on the triple (n, p, d) and the model parameters for the method to succeed in consistently estimating the neighborhood of every node in the graph simultaneously. With coherence conditions imposed on the population Fisher information matrix, we prove that consistent neighborhood selection can be obtained for sample sizes n = Omega (d(3) log p) with exponentially decaying error. When these same conditions are imposed directly on the sample matrices, we show that a reduced sample size of n = Omega (d(2) log p) suffices for the method to estimate neighborhoods consistently. Although this paper focuses on the binary graphical models, we indicate how a generalization of the method of the paper would apply to general discrete Markov random fields.",WOS:000277471000001,ANNALS OF STATISTICS,"['MARKOV RANDOM-FIELDS', 'GRAPHS', 'LASSO', 'DISTRIBUTIONS', 'VARIABLES', 'TEXTURE']",HIGH-DIMENSIONAL ISING MODEL SELECTION USING l(1)-REGULARIZED LOGISTIC REGRESSION,2010
1405,"We study trend filtering, a recently proposed tool of Kim et al. [SIAM Rev. 51 (2009) 339-360] for nonparametric regression. The trend filtering estimate is defined as the minimizer of a penalized least squares criterion, in which the penalty term sums the absolute kth order discrete derivatives over the input points. Perhaps not surprisingly, trend filtering estimates appear to have the structure of kth degree spline functions, with adaptively chosen knot points (we say ""appear"" here as trend filtering estimates are not really functions over continuous domains, and are only defined over the discrete set of inputs). This brings to mind comparisons to other nonparametric regression tools that also produce adaptive splines; in particular, we compare trend filtering to smoothing splines, which penalize the sum of squared derivatives across input points, and to locally adaptive regression splines [Ann. Statist. 25 (1997) 387-413], which penalize the total variation of the kth derivative. Empirically, we discover that trend filtering estimates adapt to the local level of smoothness much better than smoothing splines, and further, they exhibit a remarkable similarity to locally adaptive regression splines. We also provide theoretical support for these empirical findings; most notably, we prove that (with the right choice of tuning parameter) the trend filtering estimate converges to the true underlying function at the minimax rate for functions whose kth derivative is of bounded variation. This is done via an asymptotic pairing of trend filtering and locally adaptive regression splines, which have already been shown to converge at the minimax rate [Ann. Statist. 25 (1997) 387-413]. At the core of this argument is a new result tying together the fitted values of two lasso problems that share the same outcome vector, but have different predictor matrices.",WOS:000334256100012,ANNALS OF STATISTICS,"['REGRESSION SPLINES', 'WAVELET SHRINKAGE', 'LASSO', 'SMOOTHNESS']",ADAPTIVE PIECEWISE POLYNOMIAL ESTIMATION VIA TREND FILTERING,2014
1406,"Modern data collection and analysis pipelines often involve a sophisticated mix of applications written in general purpose and specialized programming languages. Many formats commonly used to import and export data between different programs or systems, such as CSV or JSON, are verbose, inefficient, not type-safe, or tied to a specific programming language. Protocol Buffers are a popular method of serializing structured data between applications - while remaining independent of programming languages or operating systems. They off er a unique combination of features, performance, and maturity that seems particularly well suited for data-driven applications and numerical computing. The RProtoBuf package provides a complete interface to Protocol Buffers from the R environment for statistical computing. This paper outlines the general class of data serialization requirements for statistical computing, describes the implementation of the RProtoBuf package, and illustrates its use with example applications in large-scale data collection pipelines and web services.",WOS:000384914100001,JOURNAL OF STATISTICAL SOFTWARE,,RProtoBuf: Efficient Cross-Language Data Serialization in R,2016
1407,,WOS:000208589800012,R JOURNAL,,Conference Review: WZUR(2.0) - The Second Meeting of Polish R Users,2009
1408,"In this work we study an adaptive step-down procedure for testing m hypotheses. It stems from the repeated use of the false discovery rate controlling the linear step-up procedure (sometimes called BH), and makes use of the critical constants iq/[(m + 1 - i (1 - q)], i = 1,..., m. Motivated by its success as a model selection procedure, as well as by its asymptotic optimality, we are interested in its false discovery rate (FDR) controlling properties for a finite number of hypotheses. We prove this step-down procedure controls the FDR at level q for independent test statistics. We then numerically compare it with two other procedures with proven FDR control under independence, both in terms of power under independence and FDR control under positive dependence.",WOS:000265500500003,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'RATES']",AN ADAPTIVE STEP-DOWN PROCEDURE WITH PROVEN FDR CONTROL UNDER INDEPENDENCE,2009
1409,"This article establishes the performance of stochastic blockmodels in addressing the co-clustering problem of partitioning a binary array into subsets, assuming only that the data are generated by a nonparametric process satisfying the condition of separate exchangeability. We provide oracle inequalities with rate of convergence O-P(n(-1/4)) corresponding to profile likelihood maximization and mean-square error minimization, and show that the blockmodel can be interpreted in this setting as an optimal piecewise-constant approximation to the generative nonparametric model. We also show for large sample sizes that the detection of co-clusters in such data indicates with high probability the existence of co-clusters of equal size and asymptotically equivalent connectivity in the underlying generative process.",WOS:000334256100002,ANNALS OF STATISTICS,"['STOCHASTIC BLOCKMODELS', 'CONVERGENT SEQUENCES', 'COMMUNITY DETECTION', 'MODELS']",CO-CLUSTERING SEPARATELY EXCHANGEABLE NETWORK DATA,2014
1410,"The MortalitySmooth package provides a framework for smoothing count data in both one- and two-dimensional settings. Although general in its purposes, the package is specifically tailored to demographers, actuaries, epidemiologists, and geneticists who may be interested in using a practical tool for smoothing mortality data over ages and/or years. The total number of deaths over a specified age- and year-interval is assumed to be Poisson-distributed, and P-splines and generalized linear array models are employed as a suitable regression methodology. Extra-Poisson variation can also be accommodated. Structured in an S 3 object orientation system, MortalitySmooth has two main functions which fit the data and define two classes of objects: Mort1Dsmooth and Mort2Dsmooth. The methods for these classes (print, summary, plot, predict, and residuals) are also included. These features make it easy for users to extract and manipulate the outputs. In addition, a collection of mortality data is provided. This paper provides an overview of the design, aims, and principles of MortalitySmooth, as well as strategies for applying it and extending its use.",WOS:000306914000001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION', 'MODELS']",MortalitySmooth: An R Package for Smoothing Poisson Counts with P-Splines,2012
1411,Dimension reduction is one of the biggest challenges in high-dimensional regression models. We recently introduced a new methodology based on variable clustering as a means to reduce dimensionality. We present here the R package clere that implements some refinements of this methodology. An overview of the package functionalities as well as examples to run an analysis are described. Numerical experiments on real data were performed to illustrate the good predictive performance of our parsimonious method compared to standard dimension reduction approaches.,WOS:000385276100007,R JOURNAL,"['EM ALGORITHM', 'SELECTION', 'MODEL', 'REGULARIZATION', 'PREDICTORS', 'LIKELIHOOD', 'SHRINKAGE']",Variable Clustering in High-Dimensional Linear Regression: The R Package clere,2016
1412,"The bootstrap is a popular and convenient method for quantifying the authority of an empirical ordering of attributes, for example of a ranking of the performance of institutions or of the influence of genes on a response variable. In the first of these examples, the number, p, of quantities being ordered is sometimes only moderate in sire; in the second it can be very large, often much greater than sample sire. However, we show that in both types of problem the conventional bootstrap can produce inconsistency. Moreover, the standard n-out-of-n bootstrap estimator of the distribution of an empirical rank may not converge in the usual sense; the estimator may converge in distribution, but not in probability. Nevertheless, in many cases the bootstrap correctly identifies the support of the asymptotic distribution of ranks. In some contemporary problems, bootstrap prediction intervals for ranks are particularly long, and in this context, we also quantify the accuracy of bootstrap methods, showing that the standard bootstrap gets the order of magnitude of the interval right, but not the constant multiplier of interval length. The m-out-of-n bootstrap can improve performance and produce statistical consistency, but it requires empirical choice of m; we suggest a tuning solution to this problem. We show that in genomic examples, where it might be expected that the standard, ""synchronous"" bootstrap will successfully accommodate non-independence of vector components, that approach can produce misleading results. An ""independent component"" bootstrap can overcome these difficulties, even in cases where components are not strictly independent.",WOS:000271673700008,ANNALS OF STATISTICS,['REGRESSION'],USING THE BOOTSTRAP TO QUANTIFY THE AUTHORITY OF AN EMPIRICAL RANKING,2009
1413,"In quantitative finance, we often model asset prices as semimartingales, with drift, diffusion and jump components. The jump activity index measures the strength of the jumps at high frequencies, and is of interest both in model selection and fitting, and in volatility estimation. In this paper, we give a novel estimate of the jump activity, together with corresponding confidence intervals. Our estimate improves upon previous work, achieving near-optimal rates of convergence, and good finite-sample performance in Monte-Carlo experiments.",WOS:000368022000003,ANNALS OF STATISTICS,"['HIGH-FREQUENCY DATA', 'INTEGRATED VOLATILITY', 'STOCHASTIC-PROCESSES', 'ACTIVITY INDEX', 'MODELS']",NEAR-OPTIMAL ESTIMATION OF JUMP ACTIVITY IN SEMIMARTINGALES,2016
1414,"Many algorithms for inferring causality rely heavily on the faithfulness assumption. The main justification for imposing this assumption is that the set of unfaithful distributions has Lebesgue measure zero, since it can be seen as a collection of hypersurfaces in a hypercube. However, due to sampling error the faithfulness condition alone is not sufficient for statistical estimation, and strong-faithfulness has been proposed and assumed to achieve uniform or high-dimensional consistency. In contrast to the plain faithfulness assumption, the set of distributions that is not strong-faithful has nonzero Lebesgue measure and in fact, can be surprisingly large as we show in this paper. We study the strong-faithfulness condition from a geometric and combinatorial point of view and give upper and lower bounds on the Lebesgue measure of strong-faithful distributions for various classes of directed acyclic graphs. Our results imply fundamental limitations for the PC-algorithm and potentially also for other algorithms based on partial correlation testing in the Gaussian case.",WOS:000320488200002,ANNALS OF STATISTICS,,GEOMETRY OF THE FAITHFULNESS ASSUMPTION IN CAUSAL INFERENCE,2013
1415,"Variance component models are generally accepted for the analysis of hierarchical structured data. A shortcoming is that outcome variables are still treated as measured without an error. Unreliable variables produce biases in the estimates of the other model parameters. The variability of the relationships across groups and the group-effects on individuals' outcomes differ substantially when taking the measurement error in the dependent variable of the model into account. The multilevel model can be extended to handle measurement error using an item response theory (IRT) model, leading to a multilevel IRT model. This extended multilevel model is in particular suitable for the analysis of educational response data where students are nested in schools and schools are nested within cities/countries.",WOS:000247011300001,JOURNAL OF STATISTICAL SOFTWARE,"['SCHOOL EFFECTIVENESS', 'LINEAR-MODEL', 'DISTRIBUTIONS']",Multilevel IRT modeling in practice with the package mlirt,2007
1416,"Let X, X-1,...,X-n be i.i.d. Gaussian random variables in a separable Hilbert space H with zero mean and covariance operator Sigma = E(X circle times X), and let (Sigma) over cap := n(-1) Sigma(n)(j=1) (X-i circle times X-j) be the sample (empirical) covariance operator based on (XI,..,Xn). Denote by P-r the spectral projector of Sigma corresponding to its rth eigenvalue mu(r) and by (P-r) over cap the empirical counterpart of P-r. The main goal of the paper is to obtain tight bounds on
sup(x is an element of R)vertical bar p{parallel to(P-r) over cap - P-r parallel to(2)(2) - E parallel to(P-r) over cap - P-r parallel to(2)(2)/Var(1/2) (parallel to(P-r) over cap - P-r parallel to(2)(2)) <= x} - Phi(x)vertical bar,
where parallel to . parallel to(2) denotes the Hilbert-Schmidt norm and Phi is the standard normal distribution function. Such accuracy of normal approximation of the distribution of squared Hilbert-Schmidt error is characterized in terms of so-called effective rank of Sigma defined as r(Sigma) = tr(Sigma)/parallel to Sigma parallel to(infinity) , where tr(Sigma) is the trace of Sigma and parallel to Sigma parallel to(infinity) is its operator norm, as well as another parameter characterizing the size of Var(parallel to(P-r) over cap - P-r parallel to(2)(2)). Other results include nonasymptotic bounds and asymptotic representations for the mean squared Hilbert-Schmidt norm error E parallel to(P-r) over cap - P-r parallel to(2)(2) and the variance Var(parallel to(P-r) over cap - P-r parallel to(2)(2)), and concentration inequalities for parallel to(P-r) over cap- P-r parallel to(2)(2) around its expectation.",WOS:000396804900004,ANNALS OF STATISTICS,"['CENTRAL-LIMIT-THEOREM', 'PRINCIPAL-COMPONENTS-ANALYSIS', 'SPARSE PCA', 'ASYMPTOTICS', 'INFERENCE', 'BOUNDS', 'FORMS']",NORMAL APPROXIMATION AND CONCENTRATION OF SPECTRAL PROJECTORS OF SAMPLE COVARIANCE,2017
1417,"The US Decennial Census is arguably the most important data set for social science research in the United States. The UScensus2000 suite of packages allows for convenient handling of the 2000 US Census spatial and demographic data. The goal of this article is to showcase the UScensus2000 suite of packages for R, to describe the data contained within these packages, and to demonstrate the helper functions provided for handling this data. The UScensus2000 suite is comprised of spatial and demographic data for the 50 states and Washington DC at four different geographic levels (block, block group, tract, and census designated place). The UScensus2000 suite also contains a number of functions for selecting and aggregating specific geographies or demographic information such as metropolitan statistical areas, counties, etc. These packages rely heavily on the spatial tools developed by Bivand, Pebesma, and Gomez-Rubio (2008), i.e., the sp and maptools packages. This article will provide the necessary background for working with this data set, helper functions, and finish with an applied spatial statistics example.",WOS:000284598400001,JOURNAL OF STATISTICAL SOFTWARE,['SEGREGATION'],US Census Spatial and Demographic Data in R: The UScensus2000 Suite of Packages,2010
1418,,WOS:000311639700012,ANNALS OF STATISTICS,,"ESTIMATION FOR A PARTIAL-LINEAR SINGLE-INDEX MODEL (38, 246, 2010)",2011
1419,"In this article, we present FactoMineR an R package dedicated to multivariate data analysis. The main features of this package is the possibility to take into account different types of variables (quantitative or categorical), different types of structure on the data (a partition on the variables, a hierarchy on the variables, a partition on the individuals) and finally supplementary information (supplementary individuals and variables). Moreover, the dimensions issued from the different exploratory data analyses can be automatically described by quantitative and/or categorical variables. Numerous graphics are also available with various options. Finally, a graphical user interface is implemented within the Rcmdr environment in order to propose an user friendly package.",WOS:000254619400001,JOURNAL OF STATISTICAL SOFTWARE,,FactoMineR: An R package for multivariate analysis,2008
1420,"Group Method of Data Handling (GMDH)-type neural network algorithms are the heuristic self organization method for the modelling of complex systems. GMDH algorithms are utilized for a variety of purposes, examples include identification of physical laws, the extrapolation of physical fields, pattern recognition, clustering, the approximation of multidimensional processes, forecasting without models, etc. In this study, the R package GMDH is presented to make short term forecasting through GMDH-type neural network algorithms. The GMDH package has options to use different transfer functions (sigmoid, radial basis, polynomial, and tangent functions) simultaneously or separately. Data on cancer death rate of Pennsylvania from 1930 to 2000 are used to illustrate the features of the GMDH package. The results based on ARIMA models and exponential smoothing methods are included for comparison.",WOS:000385276100029,R JOURNAL,"['TIME-SERIES', 'FEEDBACK LOOP']",GMDH: An R Package for Short Term Forecasting via GMDH-Type Neural Network Algorithms,2016
1421,"Principal component analysis (PCA) is possibly one of the most widely used statistical tools to recover a low-rank structure of the data. In the high-dimensional settings, the leading eigenvector of the sample covariance can be nearly orthogonal to the true eigenvector. A sparse structure is then commonly assumed along with a low rank structure. Recently, minimax estimation rates of sparse PCA were established under various interesting settings. On the other side, Bayesian methods are becoming more and more popular in high-dimensional estimation, but there is little work to connect frequentist properties and Bayesian methodologies for high-dimensional data analysis. In this paper, we propose a prior for the sparse PCA problem and analyze its theoretical properties. The prior adapts to both sparsity and rank. The posterior distribution is shown to contract to the truth at optimal minimax rates. In addition, a computationally efficient strategy for the rank-one case is discussed.",WOS:000352757100012,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'GAUSSIAN PROCESS PRIORS', 'NONPARAMETRIC PROBLEMS', 'CONVERGENCE-RATES', 'HIGH DIMENSIONS', 'DISTRIBUTIONS', 'CONSISTENCY', 'MATRICES', 'MODELS', 'BOUNDS']",RATE-OPTIMAL POSTERIOR CONTRACTION FOR SPARSE PCA,2015
1422,"SchemaOnRead is a CRAN package that provides an extensible mechanism for importing a wide range of file types into R as well as support for the emerging schema-on-read paradigm in R. The schema-on-read tools within the package include a single function call that recursively reads folders with text, comma separated value, raster image, R data, HDF5, NetCDF, spreadsheet, Weka, Epi Info, Pajek network, R network, HTML, SPSS, Systat, and Stata files. It also recursively reads folders (e.g., schemaOnRead(""folder"")), returning a nested list of the contained elements. The provided tools can be used as-is or easily customized to implement tool chains in R. This paper's contribution is that it introduces and describes the SchemaOnRead package and compares it to related R packages.",WOS:000385276100020,R JOURNAL,,SchemaOnRead: A Package for Schema-on-Read in R,2016
1423,"We present a new MATLAB toolbox under Windows and Linux for nonparametric regression estimation based on the statistical library for least squares support vector machines (StatLSSVM). The StatLSSVM toolbox is written so that only a few lines of code are necessary in order to perform standard nonparametric regression, regression with correlated errors and robust regression. In addition, construction of additive models and pointwise or uniform confidence intervals are also supported. A number of tuning criteria such as classical cross-validation, robust cross-validation and cross-validation for correlated errors are available. Also, minimization of the previous criteria is available without any user interaction.",WOS:000325947200001,JOURNAL OF STATISTICAL SOFTWARE,"['SUPPORT VECTOR MACHINES', 'KERNEL-BASED REGRESSION', 'CROSS-VALIDATION', 'CORRELATED ERRORS', 'DEPENDENT ERRORS', 'MODEL SELECTION', 'SIMPLEX-METHOD', 'ROBUSTNESS', 'MINIMIZATION']",Nonparametric Regression via StatLSSVM,2013
1424,"In this paper, we study the nonparametric maximum likelihood estimator for an event time distribution function at a point in the current status model with observation times supported on a grid of potentially unknown sparsity and with multiple subjects sharing the same observation time. This is of interest since observation time ties occur frequently with current status data. The grid resolution is specified as cn(-gamma) with c > 0 being a scaling constant and gamma > 0 regulating the sparsity of the grid relative to n, the number of subjects. The asymptotic behavior falls into three cases depending on gamma: regular Gaussian-type asymptotics obtain for gamma < 1/3, nonstandard cube-root asymptotics prevail when gamma > 1/3 and gamma = 1/3 serves as a boundary at which the transition happens. The limit distribution at the boundary is different from either of the previous cases and converges weakly to those obtained with gamma is an element of (0, 1/3) and gamma is an element of (1/3, infinity) as c goes to infinity and 0, respectively. This weak convergence allows us to develop an adaptive procedure to construct confidence intervals for the value of the event time distribution at a point of interest without needing to know or estimate gamma, which is of enormous advantage from the perspective of inference. A simulation study of the adaptive procedure is presented.",WOS:000304684900003,ANNALS OF STATISTICS,"['INTERVAL-CENSORED-DATA', 'ESTIMATORS', 'BOOTSTRAP', 'DISCRETE', 'DENSITY', 'TIMES', 'MODEL', 'GMLE']",LIKELIHOOD BASED INFERENCE FOR CURRENT STATUS DATA ON A GRID: A BOUNDARY PHENOMENON AND AN ADAPTIVE INFERENCE PROCEDURE,2012
1425,"Longitudinal studies are essential tools in medical research. In these studies, variables are not restricted to single measurements but can be seen as variable-trajectories, either single or joint. Thus, an important question concerns the identification of homogeneous patient trajectories.
kml and kml3d are R packages providing an implementation of k-means designed to work specifically on trajectories (kml) or on joint trajectories (kml3d). They provide various tools to work on longitudinal data: imputation methods for trajectories (nine classic and one original), methods to de fine starting conditions in k-means (four classic and three original) and quality criteria to choose the best number of clusters (four classic and one original). In addition, they off er graphic facilities to ""visualize"" the trajectories, either in 2D (single trajectory) or 3D (joint-trajectories). The 3D graph representing the mean joint-trajectories of each cluster can be exported through LATEX in a 3D dynamic rotating PDF graph (Figures 1 and 9).",WOS:000365973900001,JOURNAL OF STATISTICAL SOFTWARE,"['PATTERN-MIXTURE MODELS', 'MISSING DATA', 'EDUCATIONAL-ATTAINMENT', 'ALGORITHM', 'TRAJECTORIES', 'CHILDHOOD', 'INATTENTION', 'SELECTION', 'OVULATION', 'NUMBER']",kml and kml3d: R Packages to Cluster Longitudinal Data,2015
1426,"In this article, the problem of semi-parametric inference on the parameters of a multidimensional Levy process L(t) with independent components based on the low-frequency observations of the corresponding time-changed Levy process L(T)(,), where T is a nonnegative, nondecreasing real-valued process independent of L(t), is studied. We show that this problem is closely related to the problem of composite function estimation that has recently gotten much attention in statistical literature. Under suitable identifiability conditions, we propose a consistent estimate for the Levy density of L(t) and derive the uniform as well as the pointwise convergence rates of the estimate proposed. Moreover, we prove that the rates obtained are optimal in a minimax sense over suitable classes of time-changed Levy models. Finally, we present a simulation study showing the performance of our estimation algorithm in the case of time-changed Normal Inverse Gaussian (NIG) Levy processes.",WOS:000296995500013,ANNALS OF STATISTICS,"['NONPARAMETRIC-ESTIMATION', 'MODELS']",STATISTICAL INFERENCE FOR TIME-CHANGED LEVY PROCESSES VIA COMPOSITE CHARACTERISTIC FUNCTION ESTIMATION,2011
1427,"This paper is a mixture of my personal experiences of Jan de Leeuw as a supervisor of my master's and Ph.D. theses, as well as a sketch of how three-way analysis, the subject Jan chose for me, developed over time. The emphasis is on where it is and was applied, and to what extent it stole the hearts of applied researchers in different disciplines. Furthermore, the paper contains some musings about how we should go about promoting the use of the techniques, especially in the social and behavioural sciences. Finally, an overview is provided of available software and attention is paid to how (three-way) software may be designed to encourage its use by the scientific community, as it befits a paper in the Journal of Statistical Software.",WOS:000389126700001,JOURNAL OF STATISTICAL SOFTWARE,"['ALTERNATING LEAST-SQUARES', '3-MODE FACTOR-ANALYSIS', 'TENSOR DECOMPOSITIONS', 'PRINCIPAL COMPONENT', '3-WAY', 'MODELS', 'ALGORITHMS', 'CHEMISTRY', 'MATLAB']",My Multiway Analysis: From Jan de Leeuw to TWPack and Back,2016
1428,"We consider the statistical deconvolution problem where one observes n replications from the model Y = X + epsilon, where X is the unobserved random signal of interest and epsilon is an independent random error with distribution phi. Under weak assumptions on the decay of the Fourier transform of phi, we derive upper bounds for the finite-sample sup-norm risk of wavelet deconvolution density estimators f(n) for the density f of X, where f : R -> R is assumed to be bounded. We then derive lower bounds for the minimax sup-norm risk over Besov balls in this estimation problem and show that wavelet deconvolution density estimators attain these bounds. We further show that linear estimators adapt to the unknown smoothness of f if the Fourier transform of phi decays exponentially and that a corresponding result holds true for the hard thresholding wavelet estimator if phi decays polynomially. We also analyze the case where f is a ""supersmooth""/analytic density. We finally show how our results and recent techniques from Rademacher processes can be applied to construct global confidence bands for the density f.",WOS:000288183800006,ANNALS OF STATISTICS,"['KERNEL DENSITY ESTIMATORS', 'NONPARAMETRIC DECONVOLUTION', 'CONCENTRATION INEQUALITIES', 'CONFIDENCE BANDS', 'SHARP OPTIMALITY', 'LIMIT-THEOREMS', 'OPTIMAL RATES', 'CONVERGENCE', 'MINIMIZATION', 'CONSISTENCY']",GLOBAL UNIFORM RISK BOUNDS FOR WAVELET DECONVOLUTION ESTIMATORS,2011
1429,"An important component of quality control for statistical graphics software is the ability not only to test that code runs without errors, but also to test that code produces the right result. The simple way to test for the correct result in graphical output is to test whether a test file differs from a control file; this is effective in determining whether a difference exists. However, the test can be significantly enhanced by also producing a graphical image of any difference; this makes it much easier to determine how and why two files differ. This article describes the graphicsQC package for R, which provides functions for producing and comparing files of graphical output and for generating are port of the results, including images of any differences.",WOS:000266310100001,JOURNAL OF STATISTICAL SOFTWARE,,Quality Control for Statistical Graphics: The graphicsQC Package for R,2009
1430,"In quantile regression, various quantiles of a response variable Y are modelled as functions of covariates (rather than its mean). An important application is the construction of reference curves/surfaces and conditional prediction intervals for Y. Recently, a nonparametric quantile regression method based on the concept of optimal quantization was proposed. This method competes very well with k-nearest neighbor, kernel, and spline methods. In this paper, we describe an R package, called QuantifQuantile, that allows to perform quantization-based quantile regression. We describe the various functions of the package and provide examples.",WOS:000368551800006,R JOURNAL,['CONDITIONAL QUANTILE'],QuantifQuantile: An R Package for Performing Quantile Regression Through Optimal Quantization,2015
1431,"Tree based methods in S or R are extremely useful and popular. For simple trees and memorable variables it is easy to predict the outcome for a new case using only a standard decision tree diagram. However, for large trees or trees where the variable description is complex the decision tree diagram is often not enough. This article describes pinktoe: an R package containing two tools to assist with the semi-automatic traversal of trees. The PT tool creates a widget for each node to be visited in the tree that is needed to make a decision and permits the user to make decisions using radiobuttons. The pinktoe function generates a suite of HTML and Perl files that permit a CGI-enabled website to issue step-by-step questions to a user wishing to make a prediction using a tree.",WOS:000232890300001,JOURNAL OF STATISTICAL SOFTWARE,['EARLY DAY MOTIONS'],pinktoe: Semi-automatic traversal of trees,2005
1432,"The multiplicative censoring model introduced in Vardi [Biometrika 76 (1989) 751-761] is an incomplete data problem whereby two independent samples from the lifetime distribution G, X-m = (X-1, ... , X-m) and Z(n) = (Z(1), ... , Z(n)), are observed subject to a form of coarsening. Specifically, sample X-m is fully observed while Y-n = (Y-1, ... , Y-n) is observed instead of Z(n), where Y-i = U(i)Z(i) and (U-1, ... , U-n) is an independent sample from the standard uniform distribution. Vardi [Biometrika 76 (1989) 751-761] showed that this model unifies several important statistical problems, such as the deconvolution of an exponential random variable, estimation under a decreasing density constraint and an estimation problem in renewal processes. In this paper, we establish the large-sample properties of kernel density estimators under the multiplicative censoring model. We first construct a strong approximation for the process root k((G) over cap - G), where (G) over cap is a solution of the nonparametric score equation based on (X-n, Y-n), and k = m + n is the total sample size. Using this strong approximation and a result on the global modulus of continuity, we establish conditions for the strong uniform consistency of kernel density estimators. We also make use of this strong approximation to study the weak convergence and integrated squared error properties of these estimators. We conclude by extending our results to the setting of length-biased sampling.",WOS:000304684900007,ANNALS OF STATISTICS,"['EMPIRICAL DISTRIBUTIONS', 'NONPARAMETRIC-ESTIMATION', 'STRONG APPROXIMATIONS', 'RANDOM CENSORSHIP', 'LENGTH BIAS', 'MODELS', 'DERIVATIVES', 'CONSISTENCY', 'STATISTICS', 'SELECTION']",LARGE-SAMPLE STUDY OF THE KERNEL DENSITY ESTIMATORS UNDER MULTIPLICATIVE CENSORING,2012
1433,"We investigate the relationship between the structure of a discrete graphical model and the support of the inverse of a generalized covariance matrix. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph reflects the conditional independence structure of the graph. Our work extends results that have previously been established only in the context of multivariate Gaussian graphical models, thereby addressing an open question about the significance of the inverse covariance matrix of a non-Gaussian distribution. The proof exploits a combination of ideas from the geometry of exponential families, junction tree theory and convex analysis. These population-level results have various consequences for graph selection methods, both known and novel, including a novel method for structure estimation for missing or corrupted observations. We provide nonasymptotic guarantees for such methods and illustrate the sharpness of these predictions via simulations.",WOS:000330204900012,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD', 'SELECTION', 'REGRESSION', 'LASSO', 'RECOVERY', 'SYSTEMS']",STRUCTURE ESTIMATION FOR DISCRETE GRAPHICAL MODELS: GENERALIZED COVARIANCE MATRICES AND THEIR INVERSES,2013
1434,"The absence of user-friendly software has long been a major obstacle to the routine application of Bayesian methods in business and industry. It will only be through widespread application of the Bayesian approach to real problems that issues, such as the use of prior distributions, can be practically resolved in the same way that the choice of significance levels has been in the classical approach; although most Bayesians would hope for a much more satisfactory resolution. It is only relatively recently that any general purpose Bayesian software has been available; by far the most widely used such package is WinBUGS. Although this software has been designed to enable an extremely wide variety of models to be coded relatively easily, it is unlikely that many will bother to learn the language and its nuances unless they are already highly motivated to try Bayesian methods. This paper describes a graphical user interface, programmed by the author, which facilitates the specification of a wide class of generalised linear mixed models for analysis using WinBUGS. The program, BugsXLA (v2.1), is an Excel Add-In that not only allows the user to specify a model as one would in a package such as SAS or S-PLUS, but also aids the specification of priors and control of the MCMC run itself. Inevitably, developing a program such as this forces one to think again about such issues as choice of default priors, parameterisation and assessing convergence. I have tried to adopt currently perceived good practices, but mainly share my approach so that others can apply it and, through constructive criticism, play a small part in the ultimate development of the first Bayesian software package truly useable by the average data analyst.",WOS:000232890800001,JOURNAL OF STATISTICAL SOFTWARE,,BugsXLA: Bayes for the common man,2005
1435,"Consistency is a key property of all statistical procedures analyzing randomly sampled data. Surprisingly, despite decades of work, little is known about consistency of most clustering algorithms. In this paper we investigate consistency of the popular family of spectral clustering algorithms, which clusters the data with the help of eigenvectors of graph Laplacian matrices. We develop new methods to establish that, for increasing sample size, those eigenvectors converge to the eigenvectors of certain limit operators. As a result, we can prove that one of the two major classes of spectral clustering (normalized clustering) converges under very general conditions, while the other (unnormalized clustering) is only consistent under strong additional assumptions, which are not always satisfied in real data. We conclude that our analysis provides strong evidence for the superiority of normalized spectral clustering.",WOS:000254502700003,ANNALS OF STATISTICS,"['INTEGRAL-OPERATORS', 'SPARSE MATRICES', 'ALGORITHM', 'GRAPHS', 'REDUCTION']",Consistency of spectral clustering,2008
1436,"In multiple change-point problems, different data segments often follow different distributions, for which the changes may occur in the mean, scale or the entire distribution from one segment to another. Without the need to know the number of change-points in advance, we propose a nonparametric maximum likelihood approach to detecting multiple change-points. Our method does not impose any parametric assumption on the underlying distributions of the data sequence, which is thus suitable for detection of any changes in the distributions. The number of change-points is determined by the Bayesian information criterion and the locations of the change-points can be estimated via the dynamic programming algorithm and the use of the intrinsic order structure of the likelihood function. Under some mild conditions, we show that the new method provides consistent estimation with an optimal rate. We also suggest a prescreening procedure to exclude most of the irrelevant points prior to the implementation of the nonparametric likelihood method. Simulation studies show that the proposed method has satisfactory performance of identifying multiple change-points in terms of estimation accuracy and computation time.",WOS:000338477800006,ANNALS OF STATISTICS,"['OF-FIT TESTS', 'RANKING ALGORITHM', 'RATIO', 'ESTIMATORS', 'SELECTION', 'REGRESSION', 'ISOCHORES', 'SHRINKAGE', 'SEQUENCE', 'MODELS']",NONPARAMETRIC MAXIMUM LIKELIHOOD APPROACH TO MULTIPLE CHANGE-POINT PROBLEMS,2014
1437,"TMB is an open source R package that enables quick implementation of complex nonlinear random effects (latent variable) models in a manner similar to the established AD Model Builder package (ADMB, http://admb-project.org/; Fournier et al. 2011). In addition, it offers easy access to parallel computations. The user defines the joint likelihood for the data and the random effects as a C++ template function, while all the other operations are done in R; e.g., reading in the data. The package evaluates and maximizes the Laplace approximation of the marginal likelihood where the random effects are automatically integrated out. This approximation, and its derivatives, are obtained using automatic differentiation (up to order three) of the joint likelihood. The computations are designed to be fast for problems with many random effects (approximate to 10(6)) and parameters (approximate to 10(3)). Computation times using ADMB and TMB are compared on a suite of examples ranging from simple models to large spatial models where the random effects are a Gaussian random field. Speedups ranging from 1.5 to about 100 are obtained with increasing gains for large problems.",WOS:000373922600001,JOURNAL OF STATISTICAL SOFTWARE,['STATE-SPACE MODELS'],TMB: Automatic Differentiation and Laplace Approximation,2016
1438,"This program is designed to improve causal inference via a method of matching that is widely applicable in observational data and easy to understand and use (if you understand how to draw a histogram, you will understand this method). The program implements the coarsened exact matching (CEM) algorithm, described below. CEM may be used alone or in combination with any existing matching method. This algorithm, and its statistical properties, are described in Iacus, King, and Porro (2008).",WOS:000267708000001,JOURNAL OF STATISTICAL SOFTWARE,,cem: Software for Coarsened Exact Matching,2009
1439,"Lisp-Stat was originally developed as a framework for experimenting with dynamic graphics in statistics. To support this use, it evolved into a platform for more general statistical computing. The choice of the Lisp language as the basis of the system was in part coincidence and in part a very deliberate decision. This paper describes the background behind the choice of Lisp, as well as the advantages and disadvantages of this choice. The paper then discusses some lessons that can be drawn from experience with Lisp-Stat and with the R language to guide future development of Lisp-Stat, R, and similar systems.",WOS:000232832000001,JOURNAL OF STATISTICAL SOFTWARE,['ENVIRONMENTS'],Some notes on the past and future of Lisp-Stat,2005
1440,"For time-to-event data with finitely many competing risks, the proportional hazards model has been a popular tool for relating the cause-specific outcomes to covariates [Prentice et al. Biometrics 34 (1978) 541-554]. This article studies an extension of this approach to allow a continuum of competing risks, in which the cause of failure is replaced by a continuous mark only observed at the failure time. We develop inference for the proportional hazards model in which the regression parameters depend nonparametrically on the mark and the baseline hazard depends nonparametrically on both time and mark. This work is motivated by the need to assess HIV vaccine efficacy, while taking into account the genetic divergence of infecting HIV viruses in trial participants from the HIV strain that is contained in the vaccine, and adjusting for covariate effects. Mark-specific vaccine efficacy is expressed in terms of one of the regression functions in the mark-specific proportional hazards model. The new approach is evaluated in simulations and applied to the first HIV vaccine efficacy trial.",WOS:000263129000015,ANNALS OF STATISTICS,"['COMPETING RISKS', 'CLINICAL-TRIALS', 'VACCINE', 'TIME', 'COEFFICIENTS']",PROPORTIONAL HAZARDS MODELS WITH CONTINUOUS MARKS,2009
1441,"A notion of local U-statistic process is introduced and central limit theorems in various norms are obtained for it. This involves the development of several inequalities for U-processes that may be useful in other contexts. This local U-statistic process is based on an estimator of the density of a function of several sample variables proposed by Frees [J. Amer Statist. Assoc. 89 (1994) 517-525] and, as a consequence, uniform in bandwidth central limit theorems in the sup and in the L-p norms are obtained for these estimators.",WOS:000248692700008,ANNALS OF STATISTICS,"['LIMIT-THEOREMS', 'EMPIRICAL PROCESSES', 'CONVERGENCE', 'RATES', 'INEQUALITIES', 'CONSISTENCY', 'DISTANCES']",On local U-statistic processes and the estimation of densities of functions of several sample variables,2007
1442,"We study semiparametric varying-coefficient partially linear models when some linear covariates are not observed, but ancillary variables are available. Semiparametric profile least-square based estimation procedures are developed for parametric and nonparametric components after we calibrate the error-prone covariates. Asymptotic properties of the proposed estimators are established. We also propose the profile least-square based ratio test and Wald test to identify significant parametric and nonparametric components. To improve accuracy of the proposed tests for small or moderate sample sizes, a wild bootstrap version is also proposed to calculate the critical values. Intensive simulation experiments are conducted to illustrate the proposed approaches.",WOS:000263129000016,ANNALS OF STATISTICS,"['GENERATED REGRESSORS', 'ESTIMATORS', 'VARIABLES']",STATISTICAL INFERENCE FOR SEMIPARAMETRIC VARYING-COEFFICIENT PARTIALLY LINEAR MODELS WITH ERROR-PRONE LINEAR COVARIATES,2009
1443,"We investigate the frequentist coverage of Bayesian credible sets in a nonparametric setting. We consider a scale of priors of varying regularity and choose the regularity by an empirical Bayes method. Next we consider a central set of prescribed posterior probability in the posterior distribution of the chosen regularity. We show that such an adaptive Bayes credible set gives correct uncertainty quantification of ""polished tail"" parameters, in the sense of high probability of coverage of such parameters. On the negative side, we show by theory and example that adaptation of the prior necessarily leads to gross and haphazard uncertainty quantification for some true parameters that are still within the hyperrectangle regularity scale.",WOS:000357441000001,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'CONFIDENCE BANDS', 'INVERSE PROBLEMS', 'INTERVALS', 'PRIORS', 'INFERENCE', 'MODEL']",FREQUENTIST COVERAGE OF ADAPTIVE NONPARAMETRIC BAYESIAN CREDIBLE SETS,2015
1444,"We consider the hypothesis testing problem of deciding whether an observed high-dimensional vector has independent normal components or, alternatively, if it has a small subset of correlated components. The correlated components may have a certain combinatorial structure known to the statistician. We establish upper and lower bounds for the worst-case (minimax) risk in terms of the size of the correlated subset, the level of correlation, and the structure of the class of possibly correlated sets. We show that some simple tests have near-optimal performance in many cases, while the generalized likelihood ratio test is suboptimal in some important cases.",WOS:000304684900016,ANNALS OF STATISTICS,"['HIGHER CRITICISM', 'RANDOM-FIELDS', 'MIXTURES', 'RATES']",DETECTION OF CORRELATIONS,2012
1445,"In this paper, the maximal nonlinear conditional correlation of two random vectors X and Y given another random vector Z. denoted by rho(1) (X. Y vertical bar Z), is defined as a measure of conditional association, which satisfies certain desirable properties. When Z is continuous, a test for testing the conditional independence of X and Y given Z is constructed based on the estimator of a weighted average of the form Sigma(nZ)(k=1) f(Z)(z(k))rho(2)(1) (X, Y vertical bar Z = z(k)). where f(Z) is the probability density function of Z and the z(k)'s are some points in the range of Z. Under some conditions, it is shown that the test statistic is asymptotically normal under conditional independence, and the test is consistent.",WOS:000280359400005,ANNALS OF STATISTICS,['ASSOCIATION'],TESTING CONDITIONAL INDEPENDENCE USING MAXIMAL NONLINEAR CONDITIONAL CORRELATION,2010
1446,"Let (Y, (X(i))(1 <= i <= p)) be a real zero mean Gaussian vector and V be a subset of {1,..., p}. Suppose we are given n i.i.d. replications of this vector. We propose anew test for testing that Y is independent of (X(i))(i is an element of{1,...,p}\V) conditionally to (X(i))(i is an element of V) against the general alternative that it is not. This procedure does not depend on any prior information on the covariance of X or the variance of Y and applies in a high-dimensional setting. It straightforwardly extends to test the neighborhood of a Gaussian graphical model. The procedure is based on a model of Gaussian regression with random Gaussian covariates. We give nonasymptotic properties of the test and we prove that it is rate optimal [up to a possible log(n) factor] over various classes of alternatives under some additional assumptions. Moreover, it allows us to derive nonasymptotic minimax rates of testing in this random design setting. Finally, we carry out a simulation study in order to evaluate the performance of our procedure.",WOS:000275510800006,ANNALS OF STATISTICS,"['VARIABLE SELECTION', 'GRAPHICAL MODEL', 'LASSO', 'REGRESSION']",GOODNESS-OF-FIT TESTS FOR HIGH-DIMENSIONAL GAUSSIAN LINEAR MODELS,2010
1447,"Our mi package in R has several features that allow the user to get inside the imputation process and evaluate the reasonableness of the resulting models and imputations. These features include: choice of predictors, models, and transformations for chained imputation models; standard and binned residual plots for checking the fit of the conditional distributions used for imputation; and plots for comparing the distributions of observed and imputed data. In addition, we use Bayesian models and weakly informative prior distributions to construct more stable estimates of imputation models. Our goal is to have a demonstration package that (a) avoids many of the practical problems that arise with existing multivariate imputation programs, and (b) demonstrates state-of-the-art diagnostics that can be applied more generally and can be in corporated in to the software of others.",WOS:000298032300001,JOURNAL OF STATISTICAL SOFTWARE,['REGRESSION-MODELS'],Multiple Imputation with Diagnostics (mi) in R: Opening Windows into the Black Box,2011
1448,Asymptotics of the normalizing constant are computed for a class of one parameter exponential families on permutations which include Mallows models with Spearmans's Footrule and Spearman's Rank Correlation Statistic. The MLE and a computable approximation of the MLE are shown to be consistent. The pseudo-likelihood estimator of Besag is shown to be root n-consistent. An iterative algorithm (IPFP) is proved to converge to the limiting normalizing constant. The Mallows model with Kendall's tau is also analyzed to demonstrate the flexibility of the tools of this paper.,WOS:000372594300014,ANNALS OF STATISTICS,"['RANKING MODELS', 'BIVARIATE DISTRIBUTIONS', 'MARGINALS', 'DEPENDENCE', 'COPULAS']",ESTIMATION IN EXPONENTIAL FAMILIES ON PERMUTATIONS,2016
1449,"Any shape that is drawn using the grid graphics package can have a name associated with it. If a name is provided, it is possible to access, query, and modify the shape after it has been drawn. These facilities allow for very detailed customisations of plots and also for very general transformations of plots that are drawn by packages based on grid.",WOS:000313198000002,R JOURNAL,,What's in a Name? The Importance of Naming grid Grobs When Drawing Plots in R,2012
1450,"Because the stationary bootstrap resamples data blocks of random length, this method has been thought to have the largest asymptotic variance among block bootstraps Lahiri [Ann. Statist. 27 (1999) 386-404]. It is shown here that the variance of the stationary bootstrap surprisingly matches that of a block bootstrap based on nonrandom, nonoverlapping blocks. This argument translates the variance expansion into the frequency domain and provides a unified way of determining variances for other block bootstraps. Some previous results on the stationary bootstrap, related to asymptotic relative efficiency and optimal block size, are also updated.",WOS:000263129000013,ANNALS OF STATISTICS,"['BLOCK BOOTSTRAP', 'DEPENDENT DATA']",A NOTE ON THE STATIONARY BOOTSTRAP'S VARIANCE,2009
1451,"Let (V, A) be a weighted graph with a finite vertex set V, with a symmetric matrix of nonnegative weights A and with Laplacian Delta. Let S-* : V x V bar right arrow R be a symmetric kernel defined on the vertex set V. Consider n i.i.d. observations (X-j, X'(j), Y-j), j = 1, ..., n, where X-j, X'(j) are independent random vertices sampled from the uniform distribution in V and Y-j is an element of R is a real valued response variable such that E(Y-j vertical bar X-j, X'(j)) = S-*(X-j,X'(j)), j = 1 , ..., n. The goal is to estimate the kernel S-* based on the data (X-1, X'(1), Y-1), ..., (X-n, X'(n), Y-n) and under the assumption that S-* is low rank and, at the same time, smooth on the graph (the smoothness being characterized by discrete Sobolev norms defined in terms of the graph Laplacian). We obtain several results for such problems including minimax lower bounds on the L-2-error and upper bounds for penalized least squares estimators both with nonconvex and with convex penalties.",WOS:000320488200008,ANNALS OF STATISTICS,['MATRIX COMPLETION'],LOW RANK ESTIMATION OF SMOOTH KERNELS ON GRAPHS,2013
1452,"Hidden Markov models (HMMs) are probabilistic functions of finite Markov chains, or, put in other words, state space models with finite state space. In this paper, we examine subspace estimation methods for HMMs whose output lies a finite set as well. In particular, we study the geometric structure arising from the nonminimality of the linear state space representation of HMMs, and consistency of a subspace algorithm arising from a certain factorization of the singular value decomposition of the estimated linear prediction matrix, For this algorithm, we show that the estimates of the transition and emission probability matrices are consistent up to a similarity transformation, and that the in-step linear predictor Computed from the estimated system matrices is consistent, i.e., converges to the true optimal linear m-step predictor.",WOS:000271673700015,ANNALS OF STATISTICS,"['PROBABILISTIC FUNCTIONS', 'CHAINS']",SUBSPACE ESTIMATION AND PREDICTION METHODS FOR HIDDEN MARKOV MODELS,2009
1453,"Estimation of large covariance matrices has drawn considerable recent attention, and the theoretical focus so far has mainly been on developing a minimax theory over a fixed parameter space. In this paper, we consider adaptive covariance matrix estimation where the goal is to construct a single procedure which is minimax rate optimal simultaneously over each parameter space in a large collection. A fully data-driven block thresholding estimator is proposed. The estimator is constructed by carefully dividing the sample covariance matrix into blocks and then simultaneously estimating the entries in a block by thresholding. The estimator is shown to be optimally rate adaptive over a wide range of bandable covariance matrices. A simulation study is carried out and shows that the block thresholding estimator performs well numerically. Some of the technical tools developed in this paper can also be of independent interest.",WOS:000312899000009,ANNALS OF STATISTICS,"['MODEL SELECTION', 'CONVERGENCE', 'LIKELIHOOD', 'RATES']",ADAPTIVE COVARIANCE MATRIX ESTIMATION THROUGH BLOCK THRESHOLDING,2012
1454,"Motivated by the problem of testing for the existence of a signal of known parametric structure and unknown ""location"" (as explained below) against a noisy background, we obtain for the maximum of a centered, smooth random field an approximation for the tail of the distribution. For the motivating class of problems this gives approximately the significance level of the maximum score test. The method is based on an application of a likelihood-ratio-identity followed by approximations of local fields. Numerical examples illustrate the accuracy of the approximations.",WOS:000256504400014,ANNALS OF STATISTICS,['TAIL PROBABILITIES'],The distribution of maxima of approximately Gaussian random fields,2008
1455,"We study the nonparametric covariance estimation of a stationary Gaussian field X observed on a regular lattice. In the time series setting, some procedures like AIC are proved to achieve optimal model selection among autoregressive models. However, there exists no such equivalent results of adaptivity in a spatial setting. By considering collections of Gaussian Markov random fields (GMRF) as approximation sets for the distribution of X. we introduce a novel model selection procedure for spatial fields. For all neighborhoods m in a given collection M, this procedure first amounts to computing a covariance estimator of X within the GMRFs of neighborhood m. Then it selects a neighborhood (m) over cap by applying a penalization strategy. The so-defined method satisfies a nonasymptotic oracle-type inequality. If X is a GMRF, the procedure is also minimax adaptive to the sparsity of its neighborhood. More generally, the procedure is adaptive to the rate of approximation of the true distribution by GMRFs with growing neighborhoods.",WOS:000277471000004,ANNALS OF STATISTICS,"['MARKOV RANDOM-FIELDS', 'MODEL SELECTION', 'INEQUALITIES', 'ORDER']",ADAPTIVE ESTIMATION OF STATIONARY GAUSSIAN FIELDS,2010
1456,"This paper describes the core features of the R package mmeta, which implements the exact posterior inference of odds ratio, relative risk, and risk difference given either a single 2x2 table or multiple 2x2 tables when the risks within the same study are independent or correlated.",WOS:000332111200001,JOURNAL OF STATISTICAL SOFTWARE,"['BAYESIAN-ANALYSIS', 'BIVARIATE DISTRIBUTIONS', 'COLORECTAL-CANCER', 'SARMANOV FAMILY', '2 PROPORTIONS', 'MIGRAINE', 'CLOMIPRAMINE', 'TRIAL', 'MODEL', 'RISK']",mmeta: An R Package for Multivariate Meta-Analysis,2014
1457,"Missing data is common in longitudinal studies. We present a package for Farewell's Linear Increments Model for Missing Data (the FLIM package), which can be used to fit linear models for observed increments of longitudinal processes and impute missing data. The method is valid for data with regular observation patterns. The end result is a list of fitted models and a hypothetical complete dataset corresponding to the data we might have observed had individuals not been missing. The FLIM package may also be applied to longitudinal studies for causal analysis, by considering counterfactual data as missing data - for instance to compare the effect of different treatments when only data from observational studies are available. The aim of this article is to give an introduction to the FLIM package and to demonstrate how the package can be applied.",WOS:000348651700012,R JOURNAL,"['LONGITUDINAL DATA', 'DROP-OUT']",Farewell's Linear Increments Model for Missing Data: The FLIM Package,2014
1458,"We consider the problem of high-dimensional Ising (graphical) model selection. We propose a simple algorithm for structure estimation based on the thresholding of the empirical conditional variation distances. We introduce a novel criterion for tractable graph families, where this method is efficient, based on the presence of sparse local separators between node pairs in the underlying graph. For such graphs, the proposed algorithm has a sample complexity of n = Omega(J(min)(-2) log p), where p is the number of variables, and J(min) is the minimum (absolute) edge potential in the model. We also establish nonasymptotic necessary and sufficient conditions for structure estimation.",WOS:000310650900003,ANNALS OF STATISTICS,"['MARKOV RANDOM-FIELDS', 'NETWORKS', 'GRAPHS', 'DISTRIBUTIONS', 'COMPLEXITY', 'SELECTION']",HIGH-DIMENSIONAL STRUCTURE ESTIMATION IN ISING MODELS: LOCAL SEPARATION CRITERION,2012
1459,"Sequential Monte Carlo methods are a very general class of Monte Carlo methods for sampling from sequences of distributions. Simple examples of these algorithms are used very widely in the tracking and signal processing literature. Recent developments illustrate that these techniques have much more general applicability, and can be applied very effectively to statistical inference problems. Unfortunately, these methods are often perceived as being computationally expensive and difficult to implement. This article seeks to address both of these problems.
A C++ template class library for the efficient and convenient implementation of very general Sequential Monte Carlo algorithms is presented. Two example applications are provided: a simple particle filter for illustrative purposes and a state-of-the-art algorithm for rare event estimation.",WOS:000266310800001,JOURNAL OF STATISTICAL SOFTWARE,"['PARTICLE FILTER', 'MODELS']",SMCTC: Sequential Monte Carlo in C plus,2009
1460,"Classification using high-dimensional features arises frequently in many contemporary statistical studies such as tumor classification using microarray or other high-throughput data. The impact of dimensionality on classifications is poorly understood. In a seminal paper, Bickel and Levina [Bernoulli 10 (2004) 989-1010] show that the Fisher discriminant performs poorly due to diverging spectra and they propose to use the independence rule to overcome the problem. We first demonstrate that even for the independence classification rule, classification using all the features can be as poor as the random guessing due to noise accumulation in estimating population centroids in high-dimensional feature space. In fact, we demonstrate further that almost all linear discriminants can perform as poorly as the random guessing. Thus, it is important to select a subset of important features for high-dimensional classification, resulting in Features Annealed Independence Rules (FAIR). The conditions under which all the important features can be selected by the two-sample t-statistic are established. The choice of the optimal number of features, or equivalently, the threshold value of the test statistics are proposed based on an upper bound of the classification error. Simulation studies and real data analysis support our theoretical results and demonstrate convincingly the advantage of our new classification procedure.",WOS:000262731400004,ANNALS OF STATISTICS,"['GENE-EXPRESSION DATA', 'DNA MICROARRAY DATA', 'TUMOR CLASSIFICATION', 'CLASS PREDICTION', 'CANCER', 'PERSISTENCE', 'SELECTION']",HIGH-DIMENSIONAL CLASSIFICATION USING FEATURES ANNEALED INDEPENDENCE RULES,2008
1461,"We consider median regression and, more generally, a possibly infinite collection of quantile regressions in high-dimensional sparse models. In these models, the number of regressors p is very large, possibly larger than the sample size n, but only at most s regressors have a nonzero impact on each conditional quantile of the response variable, where s grows more slowly than n. Since ordinary quantile regression is not consistent in this case, we consider l(1)-penalized quantile regression (l(1)-QR), which penalizes the l(1)-norm of regression coefficients, as well as the post-penalized QR estimator (post-l(1)-QR), which applies ordinary QR to the model selected by l(1)-QR. First, we show that under general conditions l(1)-QR is consistent at the near-oracle rate. root s/n root log(p boolean OR n), uniformly in the compact set u subset of (0, 1) of quantile indices. In deriving this result, we propose a partly pivotal, data-driven choice of the penalty level and show that it satisfies the requirements for achieving this rate. Second, we show that under similar conditions post-l(1)-QR is consistent at the near-oracle rate root s/n root log(p boolean OR n), uniformly over u, even if the l(1)-QR-selected models miss some components of the true models, and the rate could be even closer to the oracle rate otherwise. Third, we characterize conditions under which l(1)-QR contains the true model as a submodel, and derive bounds on the dimension of the selected model, uniformly over u; we also provide conditions under which hard-thresholding selects the minimal true model, uniformly over u.",WOS:000288183800003,ANNALS OF STATISTICS,"['DANTZIG SELECTOR', 'LASSO', 'ESTIMATORS', 'AGGREGATION', 'RECOVERY']",l(1)-PENALIZED QUANTILE REGRESSION IN HIGH-DIMENSIONAL SPARSE MODELS,2011
1462,"Survival methods are used for the statistical modelling of time-to-event data. Survival data are characterized by a set of complete records, in which the time of the event is known; and a set of censored records, in which the event was known to have occurred in an interval. When survival data are spatially referenced, the spatial variation in survival times may be of scientific interest. In this article, we introduce a new R package, spatsurv, for inference with spatially referenced survival data. The specific type of model fitted by this package is a parametric proportional hazards model in which the spatially correlated frailties are modelled by a log-Gaussian stochastic process. The package is extensible in that it allows the user to easily create new models for the baseline hazard function and spatial covariance function. The package implements an advanced adaptive Markov chain Monte Carlo algorithm to deliver Bayesian inference with minimal input from the user.
A particular feature of the new package is the ability to handle large datasets via the use of auxiliary frailties on a regular grid and the technique of circulant embedding for fast matrix computations. We demonstrate the new package on a real-life dataset.",WOS:000399023900001,JOURNAL OF STATISTICAL SOFTWARE,['RANDOM-FIELDS'],spatsurv: An R Package for Bayesian Inference with Spatial Survival Models,2017
1463,"Package exams provides a framework for automatic generation of standardized statistical exams which is especially useful for large-scale exams. To employ the tools, users just need to supply a pool of exercises and a master file controlling the layout of the final PDF document. The exercises are specified in separate Sweave files (containing R code for data generation and LATEX code for problem and solution description) and the master file is a LATEX document with some additional control commands. This paper gives an overview of the main design aims and principles as well as strategies for adaptation and extension. Hands-on illustrations-based on example exercises and control files provided in the package-are presented to get new users started easily.",WOS:000263825400001,JOURNAL OF STATISTICAL SOFTWARE,,Automatic Generation of Exams in R,2009
1464,"The optimization of a real-valued objective function f(U), where U is a p x d, p > d, semi-orthogonal matrix such that (UU)-U-inverted perpendicular = I-d, and f is invariant under right orthogonal transformation of U, is often referred to as a Grassmann manifold optimization. Manifold optimization appears in a wide variety of computational problems in the applied sciences. In this article, we present GrassmannOptim, an R package for Grassmann manifold optimization. The implementation uses gradient-based algorithms and embeds a stochastic gradient method for global search. We describe the algorithms, provide some illustrative examples on the relevance of manifold optimization and finally, show some practical usages of the package.",WOS:000306914400001,JOURNAL OF STATISTICAL SOFTWARE,"['SUFFICIENT DIMENSION REDUCTION', 'BLIND SOURCE SEPARATION', 'ORTHOGONALITY CONSTRAINTS', 'MATRICES', 'DIAGONALIZATION', 'ALGORITHMS', 'REGRESSION', 'GEOMETRY', 'MODELS']",GrassmannOptim: An R Package for Grassmann Manifold Optimization,2012
1465,"In this paper, we derive higher order Edgeworth expansions for the finite sample distributions of the subsampling-based t-statistic and the Wald statistic in the Gaussian location model under the so-called fixed-smoothing paradigm. In particular, we show that the error of asymptotic approximation is at the order of the reciprocal of the sample size and obtain explicit forms for the leading error terms in the expansions. The results are used to justify the second-order correctness of a new bootstrap method, the Gaussian dependent bootstrap, in the context of Gaussian location model.",WOS:000321847600011,ANNALS OF STATISTICS,"['CONSISTENT COVARIANCE-MATRIX', 'AUTOCORRELATION ROBUST-TESTS', 'AUTOCOVARIANCE MATRICES', 'EDGEWORTH EXPANSIONS', 'WEAK DEPENDENCE', 'BOOTSTRAP', 'HETEROSKEDASTICITY', 'ESTIMATORS', 'EIGENVALUES', 'REGRESSION']",FIXED-SMOOTHING ASYMPTOTICS FOR TIME SERIES,2013
1466,"We consider the problem of estimating the distribution function, the density and the hazard rate of the (unobservable.) event time in the current status model. A well studied and natural nonparametric estimator for the distribution function in this model is the nonparametric maximum likelihood estimator (MILE). We study two alternative methods for the estimation of the distribution function, assuming some smoothness of the event time distribution. The first estimator is based oil a maximum smoothed likelihood approach. The second method is based on smoothing the (discrete) MLE of the distribution function. These estimators can be used to estimate the density and hazard rate of the event time distribution based on the plug-in principle.",WOS:000273800100011,ANNALS OF STATISTICS,"['BANDWIDTH SELECTION', 'UNIFORM CONSISTENCY', 'DENSITY ESTIMATORS', 'MONOTONE DENSITY']",MAXIMUM SMOOTHED LIKELIHOOD ESTIMATION AND SMOOTHED MAXIMUM LIKELIHOOD ESTIMATION IN THE CURRENT STATUS MODEL,2010
1467,"A unified family of goodness-of-fit tests based on phi-divergences is introduced and studied. The new family of test statistics S-n(s) includes both the supremum version of the Anderson-Darling statistic and the test statistic of Berk and Jones [Z. Wahrsch. Verw. Gebiete 47 (1979) 47-59] as special cases (s = 2 and s = 1, resp.). We also introduce integral versions of the new statistics.
We show that the asymptotic null distribution theory of Berk and Jones [Z. Wahrsch. Verw. Gebiete 47 (1979) 47-59] and Wellner and Koltchinskii [High Dimensional Probability 111 (2003) 321-332. Birkhauser, Basel] for the Berk-Jones statistic applies to the whole family of statistics S, (s) with s c[-1, 2]. On the side of power behavior, we study the test statistics under fixed alternatives and give extensions of the ""Poisson boundary"" phenomena noted by Berk and Jones for their statistic. We also extend the results of Donoho and Jin [Ann. Statist. 32 (2004) 962-994] by showing that all our new tests for s is an element of [-1, 2] have the same ""optimal detection boundary"" for normal shift mixture alternatives as Tukey's ""higher-criticism"" statistic and the Berk-Jones statistic.",WOS:000251096100008,ANNALS OF STATISTICS,"['EMPIRICAL DISTRIBUTION FUNCTION', 'ASYMPTOTIC-DISTRIBUTION', 'KOLMOGOROV-SMIRNOV', 'ORDER-STATISTICS', 'HIGHER CRITICISM', 'DISTRIBUTIONS', 'COMBINATIONS', 'DEVIATIONS', 'THEOREM']",Goodness-of-fit tests via phi-divergences,2007
1468,"We show that the class of conditional distributions satisfying the coarsening at random (CAR) property for discrete data has a simple and robust algorithmic description based oil randomized uniform multicovers: combinatorial objects generalizing the notion of partition of a set. However, the complexity of a given CAR mechanism can be large: the maximal ""height"" of the needed multicovers can be exponential in the number of points, in the sample space. The results stein from a geometric interpretation of the set of CAR distributions as a convex polytope and a characterization of its extreme points. The hierarchy of CAR models defined in this way could be useful in parsimonious statistical modeling of CAR mechanisms, though the results also raise doubts in applied work as to the meaningfulness of the CAR assumption in its full generality.",WOS:000260554100015,ANNALS OF STATISTICS,['IGNORABILITY'],AN ALGORITHMIC AND A GEOMETRIC CHARACTERIZATION OF COARSENING AT RANDOM,2008
1469,"Mixtures of r independent distributions for two discrete random variables can be represented by matrices of nonnegative rank r. Likelihood inference for the model of such joint distributions leads to problems in real algebraic geometry that are addressed here for the first time. We characterize the set of fixed points of the Expectation-Maximization algorithm, and we study the boundary of the space of matrices with nonnegative rank at most 3. Both of these sets correspond to algebraic varieties with many irreducible components.",WOS:000349738500015,ANNALS OF STATISTICS,"['MATRIX FACTORIZATION', 'LIKELIHOOD', 'GEOMETRY', 'MODELS']",FIXED POINTS EM ALGORITHM AND NONNEGATIVE RANK BOUNDARIES,2015
1470,"This article presents the PST R package for categorical sequence analysis with probabilistic suffix trees (PSTs), i.e., structures that store variable-length Markov chains (VLMCs). VLMCs allow to model high-order dependencies in categorical sequences with parsimonious models based on simple estimation procedures. The package is specifically adapted to the field of social sciences, as it allows for VLMC models to be learned from sets of individual sequences possibly containing missing values; in addition, the package is extended to account for case weights. This article describes how a VLMC model is learned from one or more categorical sequences and stored in a PST. The PST can then be used for sequence prediction, i.e., to assign a probability to whole observed or artificial sequences. This feature supports data mining applications such as the extraction of typical patterns and outliers. This article also introduces original visualization tools for both the model and the outcomes of sequence prediction. Other features such as functions for pattern mining and artificial sequence generation are described as well. The PST package also allows for the computation of probabilistic divergence between two models and the fitting of segmented VLMCs, where sub-models fitted to distinct strata of the learning sample are stored in a single PST.",WOS:000389071900001,JOURNAL OF STATISTICAL SOFTWARE,"['HIDDEN MARKOV-MODELS', 'SELF-RATED HEALTH', 'SPEECH RECOGNITION', 'CHAIN MODELS', 'ORDER', 'PREDICTION', 'LENGTH', 'SERIES', 'TIME']",Analyzing State Sequences with Probabilistic Suffix Trees: The PST R Package,2016
1471,,WOS:000208589900001,R JOURNAL,,Untitled,2010
1472,"Accelerometers are a valuable tool for measuring physical activity (PA) in epidemiological studies. However, considerable processing is needed to convert time-series accelerometer data into meaningful variables for statistical analysis. This article describes two recently developed R packages for processing accelerometer data. The package accelerometry contains functions for performing various data processing procedures, such as identifying periods of non-wear time and bouts of activity. The functions are flexible, computationally efficient, and compatible with uniaxial or triaxial data. The package nhanesaccel is specifically for processing data from the National Health and Nutrition Examination Survey (NHANES), years 2003-2006. Its primary function generates measures of PA volume, intensity, frequency, and patterns according to user-specified data processing methods. This function can process the NHANES 2003-2006 dataset in under one minute, which is a drastic improvement over existing software. This article highlights important features of packages accelerometry and nhanesaccel and demonstrates typical usage for PA researchers.",WOS:000348651700006,R JOURNAL,"['PHYSICAL-ACTIVITY', 'METABOLIC SYNDROME', 'TIME']","Flexible R Functions for Processing Accelerometer Data, with Emphasis on NHANES 2003-2006",2014
1473,"We consider the problem of estimating an unknown theta is an element of R-n from noisy observations under the constraint that theta belongs to certain convex polyhedral cones in R-n. Under this setting, we prove bounds for the risk of the least squares estimator (LSE). The obtained risk bound behaves differently depending on the true sequence theta which highlights the adaptive behavior of theta. As special cases of our general result, we derive risk bounds for the LSE in univariate isotonic and convex regression. We study the risk bound in isotonic regression in greater detail: we show that the isotonic LSE converges at a whole range of rates from log n/n (when theta is constant) to n(-2/3) (when is uniformly increasing in a certain sense). We argue that the bound presents a benchmark for the risk of any estimator in isotonic regression by proving nonasymptotic local minimax lower bounds. We prove an analogue of our bound for model misspecification where the true theta is not necessarily nondecreasing.",WOS:000357441000018,ANNALS OF STATISTICS,"['GRENANDER ESTIMATOR', 'ASYMPTOTIC-BEHAVIOR', 'CONVERGENCE']",ON RISK BOUNDS IN ISOTONIC AND OTHER SHAPE RESTRICTED REGRESSION PROBLEMS,2015
1474,"Ultrahigh-dimensional variable selection plays an increasingly important role in contemporary scientific discoveries and statistical research. Among others, Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] propose an independent screening framework by ranking the marginal correlations. They showed that the correlation ranking procedure possesses a sure independence screening property within the context of the linear model with Gaussian covariates and responses. In this paper, we propose a more general version of the independent learning with ranking the maximum marginal likelihood estimates or the maximum marginal likelihood itself in generalized linear models. We show that the proposed methods, with Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] as a very special case, also possess the sure screening property with vanishing false selection rate. The conditions under which the independence learning possesses a sure screening is surprisingly simple. This justifies the applicability of such a simple method in a wide spectrum. We quantify explicitly the extent to which the dimensionality can be reduced by independence screening, which depends on the interactions of the covariance matrix of covariates and true parameters. Simulation studies are used to illustrate the utility of the proposed approaches. In addition, we establish an exponential inequality for the quasi-maximum likelihood estimator which is useful for high-dimensional statistical learning.",WOS:000290231500007,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'REGRESSION-MODELS', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'LASSO', 'TRANSFORMATIONS', 'INEQUALITIES']",SURE INDEPENDENCE SCREENING IN GENERALIZED LINEAR MODELS WITH NP-DIMENSIONALITY,2010
1475,"In many important statistical applications, the number of variables or parameters p is much larger than the number of observations n. Suppose then that we have observations y = X beta + z, where beta epsilon R-p is a parameter vector of interest, X is a data matrix with possibly far fewer rows than columns, n << p, and the z(i)'s are i.i.d. N(0, sigma(2)). Is it possible to estimate beta reliably based on the noisy data y?
To estimate beta, we introduce a new estimator-we call it the Dantzig selector-which is a solution to the l(1)-regularization problem
(min) ((beta) over tilde epsilon Rp) parallel to(beta) over tilde parallel to l(1) subject to parallel to X*r parallel to l(infinity) <= (1 + t(-1)) root 2 log p.sigma,
where r is the residual vector y - X (beta) over tilde and t is a positive scalar. We show that if X obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector beta is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability,
parallel to(beta) over cap-beta parallel to(2)(l2) <= C-2 . 2 log p . (sigma(2) + Sigma(i) min(beta(2)(i), sigma(2))).
Our results are nonasymptotic and we give values for the constant C. Even though n may be much smaller than p, our estimator achieves a loss within a logarithmic factor of the ideal mean squared error one would achieve with an oracle which would supply perfect information about which coordinates are nonzero, and which were above the noise level.
In multivariate regression and from a model selection viewpoint, our result says that it is possible nearly to select the best subset of variables by solving a very simple convex program, which, in fact, can easily be recast as a convenient linear program (LP).",WOS:000253077800001,ANNALS OF STATISTICS,"['UNCERTAINTY PRINCIPLES', 'SIGNAL RECOVERY', 'MODEL SELECTION', 'REGRESSION', 'BASES', 'RECONSTRUCTION', 'SHRINKAGE', 'SYSTEMS']",The Dantzig selector: Statistical estimation when p is much larger than n,2007
1476,"The quotient correlation is defined here as an alternative to Pearson's correlation that is more intuitive and flexible in cases where the tail behavior of data is important. It measures nonlinear dependence where the regular correlation coefficient is generally not applicable. One of its most useful features is a test statistic that has high power when testing nonlinear dependence in cases where the Fisher's Z-transformation test may fail to reach a right conclusion. Unlike most asymptotic test statistics, which are either normal or chi(2), this test statistic has a limiting gamma distribution (henceforth, the gamma test statistic). More than the common usages of correlation, the quotient correlation can easily and intuitively be adjusted to values at tails. This adjustment generates two new concepts-the tail quotient correlation and the tail independence test statistics, which are also gamma statistics. Due to the fact that there is no analogue of the correlation coefficient in extreme value theory, and there does not exist an efficient tail independence test statistic, these two new concepts may open up a new field of study. In addition, an alternative to Spearman's rank correlation, a rank based quotient correlation, is also defined. The advantages of using these new concepts are illustrated with simulated data and a real data analysis of internet traffic.",WOS:000254502700019,ANNALS OF STATISTICS,"['STATIONARY-SEQUENCES', 'ASYMPTOTIC INDEPENDENCE', 'EXTREME VALUES', 'DEPENDENCE', 'MAXIMA']",Quotient correlation: A sample based alternative to Pearson's correlation,2008
1477,"The need for hands-on computer laboratory experience in undergraduate and graduate statistics education has been firmly established in the past decade. As a result a number of attempts have been undertaken to develop novel approaches for problem-driven statistical thinking, data analysis and result interpretation. In this paper we describe an integrated educational web-based framework for: interactive distribution modeling, virtual online probability experimentation, statistical data analysis, visualization and integration. Following years of experience in statistical teaching at all college levels using established licensed statistical software packages, like STATA, S-PLUS, R, SPSS, SAS, Systat, etc., we have attempted to engineer a new statistics education environment, the Statistics Online Computational Resource (SOCR). This resource performs many of the standard types of statistical analysis, much like other classical tools. In addition, it is designed in a plug-in object-oriented architecture and is completely platform independent, web-based, interactive, extensible and secure. Over the past 4 years we have tested, fine-tuned and reanalyzed the SOCR framework in many of our undergraduate and graduate probability and statistics courses and have evidence that SOCR resources build student's intuition and enhance their learning.",WOS:000241208100001,JOURNAL OF STATISTICAL SOFTWARE,"['PERSONAL VIEW', 'EDUCATION', 'BASICS']",SOCR: Statistics Online Computational Resource,2006
1478,"It has been over 40 years since I got to know Jan. This period almost entirely overlaps my career as a psychometrician. During these years, I have had many contacts with him. This paper reviews some of my early interactions, focussing on the following topics: (1) An episode surrounding the inception of the ALSOS project, and (2) Jan's unpublished (and some lost) notes and papers that I cherished and quoted in my work, including (2a) the ELEGANT algorithm for squared distance scaling, (2b) the INDISCAL method for nonmetric multidimensional scaling (MDS), and (2c) notes on DEDICOM.",WOS:000389127500001,JOURNAL OF STATISTICAL SOFTWARE,"['OPTIMAL-SCALING FEATURES', 'LEAST-SQUARES METHOD', 'UNKNOWN DISTANCE FUNCTION', 'INDIVIDUAL-DIFFERENCES', 'COMPONENT ANALYSIS', 'QUALITATIVE DATA', 'MAJORIZATION', 'PROXIMITIES', 'VARIABLES', 'DEDICOM']",My Early Interactions with Jan and Some of His Lost Papers,2016
1479,"Consider a random sample in the max-domain of attraction of a multivariate extreme value distribution such that the dependence structure of the attractor belongs to a parametric model. A new estimator for the unknown parameter is defined as the value that minimizes the distance between a vector of weighted integrals of the tail dependence function and their empirical counterparts. The minimization problem has, with probability tending to one, a unique, global solution. The estimator is consistent and asymptotically normal. The spectral measures of the tail dependence models to which the method applies can be discrete or continuous. Examples demonstrate the applicability and the performance of the method.",WOS:000310650900018,ANNALS OF STATISTICS,"['EXTREMES', 'MODEL', 'DISTRIBUTIONS']",AN M-ESTIMATOR FOR TAIL DEPENDENCE IN ARBITRARY DIMENSIONS,2012
1480,"The general clustering algorithms do not guarantee optimality because of the hardness of the problem. Polynomial-time methods can find the clustering corresponding to the exact optimum only in special cases. For example, the dynamic programming algorithm can solve the one-dimensional clustering problem, i.e., when the items to be clustered can be characterised by only one scalar number. Optimal one-dimensional clustering is provided by package Ckmeans.1d.dp in R. The paper shows a possible generalisation of the method implemented in this package to multidimensional data: the dynamic programming method can be applied to find the optimum clustering of vectors when only subsequent items may form a cluster. Sequential data are common in various fields including telecommunication, bioinformatics, marketing, transportation etc. The proposed algorithm can determine the optima for a range of cluster numbers in order to support the case when the number of clusters is not known in advance.",WOS:000385276100023,R JOURNAL,"['K-MEANS', 'QUANTIZATION', 'NUMBER']",clustering.sc.dp: Optimal Clustering with Sequential Constraint by Using Dynamic Programming,2016
1481,"Social scientists have long hand-labeled texts to create datasets useful for studying topics from congressional policymaking to media reporting. Many social scientists have begun to incorporate machine learning into their toolkits. RTextTools was designed to make machine learning accessible by providing a start-to-finish product in less than 10 steps. After installing RTextTools, the initial step is to generate a document term matrix. Second, a container object is created, which holds all the objects needed for further analysis. Third, users can use up to nine algorithms to train their data. Fourth, the data are classified. Fifth, the classification is summarized. Sixth, functions are available for performance evaluation. Seventh, ensemble agreement is conducted. Eighth, users can cross-validate their data. Finally, users write their data to a spreadsheet, allowing for further manual coding if required.",WOS:000321944400002,R JOURNAL,,RTextTools: A Supervised Learning Package for Text Classification,2013
1482,"We describe an R package for determining the optimal price of an asset which is ""perishable"" in a certain sense, given the intensity of customer arrivals and a time-varying price sensitivity function which specifies the probability that a customer will purchase an asset offered at a given price at a given time. The package deals with the case of customers arriving in groups, with a probability distribution for the group size being specified. The methodology and software allow for both discrete and continuous pricing. The class of possible models for price sensitivity functions is very wide, and includes piecewise linear models. A mechanism for constructing piecewise linear price sensitivity functions is provided.",WOS:000341642600001,JOURNAL OF STATISTICAL SOFTWARE,"['PERISHABLE ASSETS', 'YIELD MANAGEMENT', 'MODEL', 'PRICES']",Optimal Asset Pricing,2014
1483,"This article presents an algorithm that generates a conservative confidence interval of a specified length and coverage probability for the power of a Monte Carlo test (such as a bootstrap or permutation test). It is the first method that achieves this aim for almost any Monte Carlo test. Previous research has focused on obtaining as accurate a result as possible for a fixed computational effort, without providing a guaranteed precision in the above sense. The algorithm we propose does not have a fixed effort and runs until a confidence interval with a user-specified length and coverage probability can be constructed. We show that the expected effort required by the algorithm is finite in most cases of practical interest, including situations where the distribution of the p-value is absolutely continuous or discrete with finite support. The algorithm is implemented in the R-package simctest, available on CRAN.",WOS:000317451200005,ANNALS OF STATISTICS,,AN ALGORITHM TO COMPUTE THE POWER OF MONTE CARLO TESTS WITH GUARANTEED PRECISION,2013
1484,"We consider the estimation of parametric fractional time series models in which not only is the memory parameter unknown, but one may not know whether it lies in the stationary/invertible region or the nonstationary or non-invertible regions. In these circumstances, a proof of consistency (which is a prerequisite for proving asymptotic normality) can be difficult owing to nonuniform convergence of the objective function over a large admissible parameter space. In particular, this is the case for the conditional sum of squares estimate, which can be expected to be asymptotically efficient under Gaussianity. Without the latter assumption, we establish consistency and asymptotic normality for this estimate in case of a quite general univariate model. For a multivariate model, we establish asymptotic normality of a one-step estimate based on an initial root n-consistent estimate.",WOS:000311639700001,ANNALS OF STATISTICS,"['LONG-RANGE DEPENDENCE', 'STATIONARY', 'PARAMETER', 'INTEGRATION', 'INFERENCE']",GAUSSIAN PSEUDO-MAXIMUM LIKELIHOOD ESTIMATION OF FRACTIONAL TIME SERIES MODELS,2011
1485,"Kernel methods for deconvolution have attractive features, and prevail in the literature. However, they have disadvantages, which include the fact that they are usually suitable only for cases where the error distribution is infinitely supported and its characteristic function does not ever vanish. Even in these settings, optimal convergence rates are achieved by kernel estimators only when the kernel is chosen to adapt to the unknown smoothness of the target distribution. In this paper we suggest alternative ridge methods, not involving kernels in any way. We show that ridge methods (a) do not require the assumption that the error-distribution characteristic function is nonvanishing; (b) adapt themselves remarkably well to the smoothness of the target density, with the result that the degree of smoothness does not need to be directly estimated; and (c) give optimal convergence rates in a broad range of settings.",WOS:000249568000008,ANNALS OF STATISTICS,"['KERNEL DENSITY-ESTIMATION', 'MEASUREMENT ERROR', 'BANDWIDTH SELECTION', 'CONTAMINATED SAMPLE', 'OPTIMAL RATES', 'CONVERGENCE', 'DISTRIBUTIONS', 'REGRESSION', 'VARIABLES']",A ridge-parameter approach to deconvolution,2007
1486,"The purpose of the package fmri is the analysis of single subject functional magnetic resonance imaging (fMRI) data. It provides fMRI analysis from time series modeling by a linear model to signal detection and publication quality images. Specifically, it implements structural adaptive smoothing methods with signal detection for adaptive noise reduction which avoids blurring of activation areas.
Within this paper we describe the complete pipeline for fMRI analysis using fmri. We describe data reading from various medical imaging formats and the linear modeling used to create the statistical parametric maps. We review the rationale behind the structural adaptive smoothing algorithms and explain their usage from the package fmri. We demonstrate the results of such analysis using two experimental datasets. Finally, we report on the usage of a graphical user interface for some of the package functions.",WOS:000296719200001,JOURNAL OF STATISTICAL SOFTWARE,"['EXCURSION SETS', 'RESONANCE', 'IMPLEMENTATION', 'MAXIMA', 'FIELDS']",Statistical Parametric Maps for Functional MRI Experiments in R: The Package fmri,2011
1487,"We derive convenient uniform concentration bounds and finite sample multivariate normal approximation results for quadratic forms, then describe some applications involving variance components estimation in linear random-effects models. Random-effects models and variance components estimation are classical topics in statistics, with a corresponding well-established asymptotic theory. However, our finite sample results for quadratic forms provide additional flexibility for easily analyzing random effects models in nonstandard settings, which are becoming more important in modern applications (e.g., genomics). For instance, in addition to deriving novel non-asymptotic bounds for variance components estimators in classical linear random-effects models, we provide a concentration bound for variance components estimators in linear models with correlated random-effects and discuss an application involving sparse random-effects models. Our general concentration bound is a uniform version of the Hanson-Wright inequality. The main normal approximation result in the paper is derived using Reinert and Rollin [Ann. Probab. (2009) 37 2150-2173] embedding technique for Stein's method of exchangeable pairs.",WOS:000396804900012,ANNALS OF STATISTICS,"['GENOME-WIDE ASSOCIATION', 'ASYMPTOTIC-DISTRIBUTION', 'NORMAL APPROXIMATION', 'HERITABILITY', 'MODEL', 'SNPS']",FLEXIBLE RESULTS FOR QUADRATIC FORMS WITH APPLICATIONS TO VARIANCE COMPONENTS ESTIMATION,2017
1488,"Assessing the assumption of multivariate normality is required by many parametric multivariate statistical methods, such as MANOVA, linear discriminant analysis, principal component analysis, canonical correlation, etc. It is important to assess multivariate normality in order to proceed with such statistical methods. There are many analytical methods proposed for checking multivariate normality. However, deciding which method to use is a challenging process, since each method may give different results under certain conditions. Hence, we may say that there is no best method, which is valid under any condition, for normality checking. In addition to numerical results, it is very useful to use graphical methods to decide on multivariate normality. Combining the numerical results from several methods with graphical approaches can be useful and provide more reliable decisions. Here, we present an R package, MVN, to assess multivariate normality. It contains the three most widely used multivariate normality tests, including Mardia's, Henze-Zirkler's and Royston's, and graphical approaches, including chi-square Q-Q, perspective and contour plots. It also includes two multivariate outlier detection methods, which are based on robust Mahalanobis distances. Moreover, this package offers functions to check the univariate normality of marginal distributions through both tests and plots. Furthermore, especially for non-R users, we provide a user-friendly web application of the package. This application is available at http://www.biosoft.hacettepe.edu.tr/MVN/.",WOS:000348651700013,R JOURNAL,"['WILK-W', 'TESTS', 'SKEWNESS', 'KURTOSIS', 'SHAPIRO', 'SAMPLES']",MVN: An R Package for Assessing Multivariate Normality,2014
1489,"We develop general theory for finding locally optimal designs in a class of single-covariate models under any differentiable optimality criterion. Yang and Stuficen [Ann. Statist. 40 (2012) 1665-1681] and Dette and Schorning [Ann. Statist. 41 (2013) 1260-1267] gave complete class results for optimal designs under such models. Based on their results, saturated optimal designs exist; however, how to find such designs has not been addressed. We develop tools to find saturated optimal designs, and also prove their uniqueness under mild conditions.",WOS:000349738500002,ANNALS OF STATISTICS,"['LA GARZA PHENOMENON', 'NONLINEAR MODELS', 'REGRESSION-MODELS', 'POINTS']",SATURATED LOCALLY OPTIMAL DESIGNS UNDER DIFFERENTIABLE OPTIMALITY CRITERIA,2015
1490,"As any real-life data, data modeled by linear mixed-effects models often contain out-liers or other contamination. Even little contamination can drive the classic estimates far away from what they would be without the contamination. At the same time, datasets that require mixed-effects modeling are often complex and large. This makes it difficult to spot contamination. Robust estimation methods aim to solve both problems: to provide estimates where contamination has only little influence and to detect and flag contamination.
We introduce an R package, robustlmm, to robustly fit linear mixed-effects models. The package's functions and methods are designed to closely equal those offered by lme4, the R package that implements classic linear mixed-effects model estimation in R. The robust estimation method in robustlmm is based on the random effects contamination model and the central contamination model. Contamination can be detected at all levels of the data. The estimation method does not make any assumption on the data's grouping structure except that the model parameters are estimable. robustlmm supports hierarchical and non-hierarchical (e.g., crossed) grouping structures. The robustness of the estimates and their asymptotic efficiency is fully controlled through the function interface. Individual parts (e.g., fixed effects and variance components) can be tuned independently.
In this tutorial, we show how to fit robust linear mixed-effects models using robustlmm, how to assess the model fit, how to detect outliers, and how to compare different fits.",WOS:000392704700001,JOURNAL OF STATISTICAL SOFTWARE,['BOOTSTRAP'],robustlmm: An R Package for Robust Estimation of Linear Mixed-Effects Models,2016
1491,"The R package gset calculates equivalence and futility boundaries based on the exact bivariate non-central t test statistics. It is the first R package that targets specifically at the group sequential test of equivalence hypotheses. The exact test approach adopted by gset neither assumes the large-sample normality of the test statistics nor ignores the contribution to the overall Type I error rate from rejecting one out of the two one-sided hypotheses under a null value. The features of gset include: error spending functions, computation of equivalence boundaries and futility boundaries, either binding or nonbinding, depiction of stagewise boundary plots, and operating characteristics of a given group sequential design including empirical Type I error rate, empirical power, expected sample size, and probability of stopping at an interim look due to equivalence or futility.",WOS:000348651700015,R JOURNAL,"['INTERIM ANALYSES', 'CLINICAL-TRIALS', 'DESIGNS', 'BIOEQUIVALENCE']",gset: An R Package for Exact Sequential Test of Equivalence Hypothesis Based on Bivariate Non-Central t-Statistics,2014
1492,"This paper describes the RNetCDF package (version 1.6), an interface for reading and writing files in Unidata NetCDF format, and gives an introduction to the NetCDF file format. NetCDF is a machine independent binary file format which allows storage of different types of array based data, along with short metadata descriptions. The package presented here allows access to the most important functions of the NetCDF C-interface for reading, writing, and modifying NetCDF datasets. In this paper, we present a short overview on the NetCDF file format and show usage examples of the package.",WOS:000330193300004,R JOURNAL,,RNetCDF - A Package for Reading and Writing NetCDF Datasets,2013
1493,"This article describes two R packages for probabilistic weather forecasting, ensembleBMA, which offers ensemble postprocessing via Bayesian model averaging (BMA), and Prob-ForecastGOP, which implements the geostatistical output perturbation (GOP) method. BMA forecasting models use mixture distributions, in which each component corresponds to an ensemble member, and the form of the component distribution depends on the weather parameter (temperature, quantitative precipitation or wind speed). The model parameters are estimated from training data. The GOP technique uses geostatistical methods to produce probabilistic forecasts of entire weather fields for temperature or pressure, based on a single numerical forecast on a spatial grid. Both packages include functions for evaluating predictive performance, in addition to model fitting and forecasting.",WOS:000208590100010,R JOURNAL,,Probabilistic Weather Forecasting in R,2011
1494,"This software paper describes 'Stylometry with R' (stylo), a flexible R package for the high-level analysis of writing style in stylometry. Stylometry (computational stylistics) is concerned with the quantitative study of writing style, e.g. authorship verification, an application which has considerable potential in forensic contexts, as well as historical research. In this paper we introduce the possibilities of stylo for computational text analysis, via a number of dummy case studies from English and French literature. We demonstrate how the package is particularly useful in the exploratory statistical analysis of texts, e.g. with respect to authorial writing style. Because stylo provides an attractive graphical user interface for high-level exploratory analyses, it is especially suited for an audience of novices, without programming skills (e.g. from the Digital Humanities). More experienced users can benefit from our implementation of a series of standard pipelines for text processing, as well as a number of similarity metrics.",WOS:000385276100008,R JOURNAL,"['AUTHORSHIP ATTRIBUTION', 'CATEGORIZATION']",Stylometry with R: A Package for Computational Text Analysis,2016
1495,"This paper explores the following question: what kind of statistical guarantees can be given when doing variable selection in high-dimensional models? In particular, we look at the error rates and power of some multi-stage regression methods. In the first stage we fit a set of candidate models. In the second stage we select one model by cross-validation. In the third stage we use hypothesis testing to eliminate some variables. We refer to the first two stages as ""screening"" and the last stage as ""cleaning."" We consider three screening methods: the lasso, marginal regression, and forward stepwise regression. Our method gives consistent variable selection under certain conditions.",WOS:000268604900004,ANNALS OF STATISTICS,"['LASSO', 'REGRESSION', 'APPROXIMATION', 'CONSISTENCY']",HIGH-DIMENSIONAL VARIABLE SELECTION,2009
1496,"The data functions that are studied in the course of functional data analysis are assembled from discrete data, and the level of smoothing that is used is generally that which is appropriate for accurate approximation of the conceptually smooth functions that were not actually observed. Existing literature shows that this approach is effective, and even optimal, when using functional data methods for prediction or hypothesis testing. However, in the present paper we show that this approach is not effective in classification problems. There a useful rule of thumb is that undersmoothing is often desirable, but there are several surprising qualifications to that approach. First, the effect of smoothing the training data can be more significant than that of smoothing the new data set to be classified; second, undersmoothing is not always the right approach, and in fact in some cases using a relatively large bandwidth can be more effective; and third, these perverse results are the consequence of very unusual properties of error rates, expressed as functions of smoothing parameters. For example, the orders of magnitude of optimal smoothing parameter choices depend on the signs and sizes of terms in an expansion of error rate, and those signs and sizes can vary dramatically from one setting to another, even for the same classifier.",WOS:000330204900002,ANNALS OF STATISTICS,"['PRINCIPAL-COMPONENTS-ANALYSIS', 'REGRESSION']",UNEXPECTED PROPERTIES OF BANDWIDTH CHOICE WHEN SMOOTHING DISCRETE DATA FOR CONSTRUCTING A FUNCTIONAL DATA CLASSIFIER,2013
1497,"Statistical depth measures the centrality of a point with respect to a given distribution or data cloud. It provides a natural center-outward ordering of multivariate data points and yields a systematic nonparametric multivariate analysis scheme. In particular, the half-space depth is shown to have many desirable properties and broad applicability. However, the empirical half-space depth is zero outside the convex hull of the data. This property has rendered the empirical half-space depth useless outside the data cloud, and limited its utility in applications where the extreme outlying probability mass is the focal point, such as in classification problems and control charts with very small false alarm rates. To address this issue, we apply extreme value statistics to refine the empirical half-space depth in ""the tail."" This provides an important linkage between data depth, which is useful for inference on centrality, and extreme value statistics, which is useful for inference on extremity. The refined empirical half-space depth can thus extend all its utilities beyond the data cloud, and hence broaden greatly its applicability. The refined estimator is shown to have substantially improved upon the empirical estimator in theory and simulations. The benefit of this improvement is also demonstrated through the applications in classification and statistical process control.",WOS:000363437900015,ANNALS OF STATISTICS,"['HALF-SPACE DEPTH', 'MULTIVARIATE', 'REGIONS', 'BOOTSTRAP', 'INFERENCE', 'QUANTILES', 'NOTIONS', 'INDEX', 'TESTS']",BRIDGING CENTRALITY AND EXTREMITY: REFINING EMPIRICAL DATA DEPTH USING EXTREME VALUE STATISTICS,2015
1498,"In this paper we present the R package PerMallows, which is a complete toolbox to work with permutations, distances and some of the most popular probability models for permutations: Mallows and the Generalized Mallows models. The Mallows model is an exponential location model, considered as analogous to the Gaussian distribution. It is based on the definition of a distance between permutations. The Generalized Mallows model is its best-known extension. The package includes functions for making inference, sampling and learning such distributions. The distances considered in PerMallows are Kendall's tau, Cayley, Hamming and Ulam.",WOS:000384916300001,JOURNAL OF STATISTICAL SOFTWARE,"['BRADLEY-TERRY MODELS', 'RANKING MODELS', 'RANDOM PERMUTATIONS']",PerMallows: An R Package for Mallows and Generalized Mallows Models,2016
1499,"Quantitative empirical analyses of a population of interest usually aim to estimate the causal effect of one or more independent variables on a dependent variable. However, only in rare instances is the whole population available for analysis. Researchers tend to estimate causal effects on a selected sample and generalize their conclusions to the whole population. The validity of this approach rests on the assumption that the sample is representative of the population on certain key characteristics. A study using a non-representative sample is lacking in external validity by failing to minimize population choice bias. When the sample is large and non-response bias is not an issue, a random selection process is adequate to ensure external validity. If that is not the case, however, researchers could follow a more deterministic approach to ensure representativeness on the selected characteristics, provided these are known, or can be estimated, in the parent population. Although such approaches exist for matched sampling designs, research on representative sampling and the similarity between the sample and the parent population seems to be lacking. In this article we propose a greedy algorithm for obtaining a representative sample and quantifying representativeness in Stata.",WOS:000326873100001,JOURNAL OF STATISTICAL SOFTWARE,['TESTS'],A Greedy Algorithm for Representative Sampling: repsample in Stata,2013
1500,"The FitAR R (R Development Core Team 2008) package that is available on the Comprehensive R Archive Network is described. This package provides a comprehensive approach to fitting autoregressive and subset autoregressive time series. For long time series with complicated autocorrelation behavior, such as the monthly sunspot numbers, subset autoregression may prove more feasible and/or parsimonious than using AR or ARMA models. The two principal functions in this package are SelectModel and FitAR for automatic model selection and model fitting respectively. In addition to the regular autoregressive model and the usual subset autoregressive models (Tong 1977), these functions implement a new family of models. This new family of subset autoregressive models is obtained by using the partial autocorrelations as parameters and then selecting a subset of these parameters. Further properties and results for these models are discussed in McLeod and Zhang (2006). The advantages of this approach are that not only is an efficient algorithm for exact maximum likelihood implemented but that efficient methods are derived for selecting high-order subset models that may occur in massive datasets containing long time series. A new improved extended BIC criterion, UBIC, developed by Chen and Chen (2008) is implemented for subset model selection. A complete suite of model building functions for each of the three types of autoregressive models described above are included in the package. The package includes functions for time series plots, diagnostic testing and plotting, bootstrapping, simulation, forecasting, Box-Cox analysis, spectral density estimation and other useful time series procedures. As well as methods for standard generic functions including print, plot, predict and others, some new generic functions and methods are supplied that make it easier to work with the output from FitAR for bootstrapping, simulation, spectral density estimation and Box-Cox analysis.",WOS:000259946300001,JOURNAL OF STATISTICAL SOFTWARE,"['INFORMATION CRITERIA', 'MODEL SELECTION', 'TIME-SERIES', 'REGRESSION', 'AUTOCORRELATIONS', 'IDENTIFICATION', 'BIAS']",Improved Subset Autoregression: With R Package,2008
1501,"The management of time and holidays can prove crucial in applications that rely on historical data. Atypical example is the aggregation of a data set recorded in different time zones and under different daylight saving time rules. Besides the time zone conversion function, which is well supported by default classes in R, one might need functions to handle special days or holidays. In this respect, the package timeDate enhances default date-time classes in R and brings new functionalities to time zone management and the creation of holiday calendars.",WOS:000208590100004,R JOURNAL,,Rmetrics - timeDate Package,2011
1502,"Identifying the language used will typically be the first step in most natural language processing tasks. Among the wide variety of language identification methods discussed in the literature, the ones employing the Cavnar and Trenkle (1994) approach to text categorization based on character n-gram frequencies have been particularly successful. This paper presents the R extension package textcat for n-gram based text categorization which implements both the Cavnar and Trenkle approach as well as a reduced n-gram approach designed to remove redundancies of the original approach. A multi-lingual corpus obtained from the Wikipedia pages available on a selection of topics is used to illustrate the functionality of the package and the performance of the provided language identification methods.",WOS:000315019300001,JOURNAL OF STATISTICAL SOFTWARE,,The textcat Package for n-Gram Based Text Categorization in R,2013
1503,"A common strategy for the analysis of object-attribute associations is to derive a low-dimensional spatial representation of objects and attributes which involves a compensatory model (e.g., principal components analysis) to explain the strength of object-attribute associations. As an alternative, probabilistic latent feature models assume that objects and attributes can be represented as a set of binary latent features and that the strength of object-attribute associations can be explained as a non-compensatory (e.g., disjunctive or conjunctive) mapping of latent features. In this paper, we describe the R package plfm which comprises functions for conducting both classical and Bayesian probabilistic latent feature analysis with disjunctive or a conjunctive mapping rules. Print and summary functions are included to summarize results on parameter estimation, model selection and the goodness of fit of the models. As an example the functions of plfm are used to analyze product-attribute data on the perception of car models, and situation-behavior associations on the situational determinants of anger-related behavior.",WOS:000324372700001,JOURNAL OF STATISTICAL SOFTWARE,"['MATRIX DECOMPOSITION MODELS', 'SEMANTIC ANALYSIS', 'ASSOCIATIONS']",An R Package for Probabilistic Latent Feature Analysis of Two-Way Two-Mode Frequencies,2013
1504,"We develop the basic building blocks of a frequency domain framework for drawing statistical inferences on the second-order structure of a stationary sequence of functional data. The key element in such a context is the spectral density operator, which generalises the notion of a spectral density matrix to the functional setting, and characterises the second-order dynamics of the process. Our main tool is the functional Discrete Fourier Transform (fDFT). We derive an asymptotic Gaussian representation of the fDFT, thus allowing the transformation of the original collection of dependent random functions into a collection of approximately independent complex-valued Gaussian random functions. Our results are then employed in order to construct estimators of the spectral density operator based on smoothed versions of the periodogram kernel, the functional generalisation of the periodogram matrix. The consistency and asymptotic law of these estimators are studied in detail. As immediate consequences, we obtain central limit theorems for the mean and the long-run covariance operator of a stationary functional time series. Our results do not depend on structural modelling assumptions, but only functional versions of classical cumulant mixing conditions, and are shown to be stable under discrete observation of the individual curves.",WOS:000320488200007,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'KERNEL REGRESSION', 'PREDICTION', 'OPERATORS', 'MODELS']",FOURIER ANALYSIS OF STATIONARY TIME SERIES IN FUNCTION SPACE,2013
1505,"We study a problem of estimation of a Hermitian nonnegatively definite matrix rho of unit trace (e. g., a density matrix of a quantum system) based on n i.i.d. measurements (X-1, Y-1), ... , (X-n, Y-n), where
Y-j = tr(rho X-j)+ xi(j), j = 1, ... , n,
{X-j} being random i.i.d. Hermitian matrices and {xi(j)} being i.i.d. random variables with E(xi(j) vertical bar X-j) = 0. The estimator
(rho) over cap (epsilon) := S is an element of S-argmin [n(-1) Sigma(n)(j=1) (Y-j - tr(SXj))(2) + epsilon tr(S log S)]
is considered, where S is the set of all nonnegatively definite Hermitian m x m matrices of trace 1. The goal is to derive oracle inequalities showing how the estimation error depends on the accuracy of approximation of the unknown state rho by low-rank matrices.",WOS:000300383200006,ANNALS OF STATISTICS,"['EMPIRICAL PROCESSES', 'COMPLETION', 'INEQUALITY', 'RECOVERY']",VON NEUMANN ENTROPY PENALIZATION AND LOW-RANK MATRIX ESTIMATION,2011
1506,"In quantitative finance, we often model asset prices as a noisy Ito semimartingale. As this model is not identifiable, approximating by a time-changed Levy process can be useful for generative modelling. We give a new estimate of the normalised volatility or time change in this model, which obtains minimax convergence rates, and is unaffected by infinite-variation jumps. In the semimartingale model, our estimate remains accurate for the normalised volatility, obtaining convergence rates as good as any previously implied in the literature.",WOS:000344632400012,ANNALS OF STATISTICS,"['HIGH-FREQUENCY DATA', 'NONPARAMETRIC-ESTIMATION', 'INTEGRATED VOLATILITY', 'MICROSTRUCTURE NOISE', 'DIFFUSION-COEFFICIENT', 'EFFICIENT ESTIMATION', 'JUMPS', 'INFERENCE']",ESTIMATING TIME-CHANGES IN NOISY LEVY MODELS,2014
1507,"Through the use of a system-building approach, an approach that includes finding common ground for the various philosophical paradigms within statistics, Erich L. Lehmann is responsible for much of the synthesis of classical statistical knowledge that developed from the Neyman-Pearson-Wald school. A biographical sketch and a brief summary of some of his many contributions are presented here. His complete bibliography is also included and the references present many other sources of information on his life and his work.",WOS:000299186500002,ANNALS OF STATISTICS,,ERICH LEO LEHMANN-A GLIMPSE INTO HIS LIFE AND WORK,2011
1508,"A sparse precision matrix can be directly translated into a sparse Gaussian graphical model under the assumption that the data follow a joint normal distribution. This neat property makes high-dimensional precision matrix estimation very appealing in many applications. However, in practice we often face nonnormal data, and variable transformation is often used to achieve normality. In this paper we consider the nonparanormal model that assumes that the variables follow a joint normal distribution after a set of unknown monotone transformations. The nonparanormal model is much more flexible than the normal model while retaining the good interpretability of the latter in that each zero entry in the sparse precision matrix of the nonparanormal model corresponds to a pair of conditionally independent variables. In this paper we show that the nonparanormal graphical model can be efficiently estimated by using a rank-based estimation scheme which does not require estimating these unknown transformation functions. In particular, we study the rank-based graphical lasso, the rank-based neighborhood Dantzig selector and the rank-based CLIME. We establish their theoretical properties in the setting where the dimension is nearly exponentially large relative to the sample size. It is shown that the proposed rank-based estimators work as well as their oracle counterparts defined with the oracle data. Furthermore, the theory motivates us to consider the adaptive version of the rank-based neighborhood Dantzig selector and the rank-based CLIME that are shown to enjoy graphical model selection consistency without assuming the irrepresentable condition for the oracle and rank-based graphical lasso. Simulated and real data are used to demonstrate the finite performance of the rank-based estimators.",WOS:000321844300007,ANNALS OF STATISTICS,"['GAUSSIAN CONCENTRATION GRAPHS', 'COVARIANCE-MATRIX ESTIMATION', 'DANTZIG SELECTOR', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'LASSO', 'COPULA', 'BIOSYNTHESIS', 'LIKELIHOOD', 'NETWORKS']",REGULARIZED RANK-BASED ESTIMATION OF HIGH-DIMENSIONAL NONPARANORMAL GRAPHICAL MODELS,2012
1509,"The aim of this paper is to provide a new method for the detection of either favored or avoided distances between genomic events along DNA sequences. These events are modeled by a Hawkes process. The biological problem is actually complex enough to need a nonasymptotic penalized model selection approach. We provide a theoretical penalty that satisfies an oracle inequality even for quite complex families of models. The consecutive theoretical estimator is shown to be adaptive minimax for Holderian functions with regularity in (1/2, 1]: those aspects have not yet been studied for the Hawkes' process. Moreover, we introduce an efficient strategy, named Islands, which is not classically used in model selection, but that happens to be particularly relevant to the biological question we want to answer. Since a multiplicative constant in the theoretical penalty is not computable in practice, we provide extensive simulations to find a data-driven calibration of this constant. The results obtained on real genomic data are coherent with biological knowledge and eventually refine them.",WOS:000282402800007,ANNALS OF STATISTICS,"['EXCITING POINT-PROCESSES', 'MODEL SELECTION', 'INTENSITY', 'POISSON', 'INEQUALITIES', 'REGRESSION', 'PENALTIES']",ADAPTIVE ESTIMATION FOR HAWKES PROCESSES; APPLICATION TO GENOME ANALYSIS,2010
1510,"In this paper, we investigate the asymptotic properties of nonparametric Bayesian mixtures of Betas for estimating a smooth density on [0, 1]. We consider a parametrization of Beta distributions in terms of mean and scale parameters and construct a mixture of these Betas in the mean parameter, while putting a prior on this scaling parameter. We prove that such Bayesian nonparametric models have good frequentist asymptotic properties. We determine the posterior rate of concentration around the true density and prove that it is the minimax rate of concentration when the true density belongs to a Holder class with regularity beta, for all positive beta, leading to a minimax adaptive estimating procedure of the density. We also believe that the approximating results obtained on these mixtures of Beta densities can be of interest in a frequentist framework.",WOS:000273800100005,ANNALS OF STATISTICS,"['DIRICHLET MIXTURES', 'INFERENCE', 'PRIORS']",RATES OF CONVERGENCE FOR THE POSTERIOR DISTRIBUTIONS OF MIXTURES OF BETAS AND ADAPTIVE NONPARAMETRIC ESTIMATION OF THE DENSITY,2010
1511,"The analysis of interaction effects involving genetic variants and environmental exposures on the risk of adverse obstetric and early-life outcomes is generally performed using standard logistic regression in the case-mother and control-mother design. However such an analysis is inefficient because it does not take into account the natural family-based constraints present in the parent-child relationship. Recently, a new approach based on semi-parametric maximum likelihood estimation was proposed. The advantage of this approach is that it takes into account the parental relationship between the mother and her child in estimation. But a package implementing this method has not been widely available. In this paper, we present SPmlficmcm, an R package implementing this new method and we propose an extension of the method to handle missing offspring genotype data by maximum likelihood estimation. Our choice to treat missing data of the offspring genotype was motivated by the fact that in genetic association studies where the genetic data of mother and child are available, there are usually more missing data on the genotype of the offspring than that of the mother. The package builds a non-linear system from the data and solves and computes the estimates from the gradient and the Hessian matrix of the log profile semi-parametric likelihood function. Finally, we analyze a simulated dataset to show the usefulness of the package.",WOS:000384910800001,JOURNAL OF STATISTICAL SOFTWARE,,Semi-Parametric Maximum Likelihood Method for Interaction in Case-Mother Control-Mother Designs: Package SPmlficmcm,2015
1512,"The EM algorithm is a widely used tool in maximum-likelihood estimation in incomplete data problems. Existing theoretical work has focused on conditions under which the iterates or likelihood values converge, and the associated rates of convergence. Such guarantees do not distinguish whether the ultimate fixed point is a near global optimum or a bad local optimum of the sample likelihood, nor do they relate the obtained fixed point to the global optima of the idealized population likelihood (obtained in the limit of infinite data). This paper develops a theoretical framework for quantifying when and how quickly EM-type iterates converge to a small neighborhood of a given global optimum of the population likelihood. For correctly specified models, such a characterization yields rigorous guarantees on the performance of certain two-stage estimators in which a suitable initial pilot estimator is refined with iterations of the EM algorithm. Our analysis is divided into two parts: a treatment of the EM and first-order EM algorithms at the population level, followed by results that apply to these algorithms on a finite set of samples. Our conditions allow for a characterization of the region of convergence of EM-type iterates to a given population fixed point, that is, the region of the parameter space over which convergence is guaranteed to a point within a small neighborhood of the specified population fixed point. We verify our conditions and give tight characterizations of the region of convergence for three canonical problems of interest: symmetric mixture of two Gaussians, symmetric mixture of two regressions and linear regression with covariates missing completely at random.",WOS:000396804900003,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'CONVERGENCE PROPERTIES', 'INCOMPLETE-DATA', 'DATA AUGMENTATION', 'ECM ALGORITHM', 'REGRESSION', 'MIXTURES']",STATISTICAL GUARANTEES FOR THE EM ALGORITHM: FROM POPULATION TO SAMPLE-BASED ANALYSIS,2017
1513,"For binary classification we establish learning rates up to the order of n(-1) for support vector machines (SVMs) with hinge loss and Gaussian RBF kernels. These rates are in terms of two assumptions on the considered distributions: Tsybakov's noise assumption to establish a small estimation error, and a new geometric noise condition which is used to bound the approximation error. Unlike previously proposed concepts for bounding the approximation error, the geometric noise assumption does not employ any smoothness assumption.",WOS:000248987600005,ANNALS OF STATISTICS,"['EMPIRICAL PROCESSES', 'CONCENTRATION INEQUALITIES', 'CLASSIFICATION', 'CONSISTENCY', 'RISK', 'DISCRIMINATION', 'CONVERGENCE', 'CLASSIFIERS', 'SPACES', 'BOUNDS']",Fast rates for support vector machines using gaussian kernels',2007
1514,"We propose new nonparametric estimators of the integrated volatility of an Ito semimartingale observed at discrete times on a fixed time interval with mesh of the observation grid shrinking to zero. The proposed estimators achieve the optimal rate and variance of estimating integrated volatility even in the presence of infinite variation jumps when the latter are stochastic integrals with respect to locally ""stable"" Levy processes, that is, processes whose Levy measure around zero behaves like that of a stable process. On a first step, we estimate locally volatility from the empirical characteristic function of the increments of the process over blocks of shrinking length and then we sum these estimates to form initial estimators of the integrated volatility. The estimators contain bias when jumps of infinite variation are present, and on a second step we estimate and remove this bias by using integrated volatility estimators formed from the empirical characteristic function of the high-frequency increments for different values of its argument. The second step debiased estimators achieve efficiency and we derive a feasible central limit theorem for them.",WOS:000338477800008,ANNALS OF STATISTICS,"['LIMIT-THEOREMS', 'LEVY PROCESS', 'FREQUENCY']",EFFICIENT ESTIMATION OF INTEGRATED VOLATILITY IN PRESENCE OF INFINITE VARIATION JUMPS,2014
1515,"Hypergraph partitioning lies at the heart of a number of problems in machine learning and network sciences. Many algorithms for hypergraph partitioning have been proposed that extend standard approaches for graph partitioning to the case of hypergraphs. However, theoretical aspects of such methods have seldom received attention in the literature as compared to the extensive studies on the guarantees of graph partitioning. For instance, consistency results of spectral graph partitioning under the stochastic block model are well known. In this paper, we present a planted partition model for sparse random nonuniform hypergraphs that generalizes the stochastic block model. We derive an error bound for a spectral hypergraph partitioning algorithm under this model using matrix concentration inequalities. To the best of our knowledge, this is the first consistency result related to partitioning nonuniform hypergraphs.",WOS:000396804900009,ANNALS OF STATISTICS,"['STOCHASTIC BLOCKMODELS', 'NETWORKS', 'GRAPHS']",CONSISTENCY OF SPECTRAL HYPERGRAPH PARTITIONING UNDER PLANTED PARTITION MODEL,2017
1516,"In this paper we present a new R package called sgof for multiple hypothesis testing. The principal aim of this package is to implement SGoF-type multiple testing methods, known to be more powerful than the classical false discovery rate (FDR) and family-wise error rate (FWER) based methods in certain situations, particularly when the number of tests is large. This package includes Binomial and Conservative SGoF and the Bayesian and Beta-Binomial SGoF multiple testing procedures, which are adaptations of the original SGoF method to the Bayesian setting and to possibly correlated tests, respectively. The sgof package also implements the Benjamini-Hochberg and Benjamini-Yekutieli FDR controlling procedures. For each method the package provides (among other things) the number of rejected null hypotheses, estimation of the corresponding FDR, and the set of adjusted p values. Some automatic plots of interest are implemented too. Two real data examples are used to illustrate how sgof works.",WOS:000348651700009,R JOURNAL,"['FALSE DISCOVERY RATE', 'P-VALUES']",sgof: An R Package for Multiple Testing Problems,2014
1517,"Numerical deconvolution is a powerful mathematical operation that can be used to extract the impulse response function of a linear, time-invariant system. We have found this method to be useful for preliminary analysis of dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) data, capable of quickly producing voxel-wise parametric maps describing the heterogeneity of contrast agent kinetics over the entire field of view, typically comprising tens of thousands of voxels. The statistical programming language R is well suited for this type of analysis and when combined with LATEX, via Sweave, allows one to perform all calculations and generate a report with a single script. The purpose of this manuscript is to describe the R package DATforDCEMRI, a Deconvolution Analysis Tool for DCE-MRI contrast agent concentration vs. time data, which allows the user to perform kinetic deconvolution analysis and visualize/explore the resulting voxel-wise parametric maps and associated data.",WOS:000296228400001,JOURNAL OF STATISTICAL SOFTWARE,"['CONTRAST-ENHANCED MRI', 'KINETIC-ANALYSIS', 'BEVACIZUMAB', 'TRACER']",DATforDCEMRI: An R Package for Deconvolution Analysis and Visualization of DCE-MRI Data,2011
1518,"Let A and B be independent, central Wishart matrices in p variables with common covariance and having in and n degrees of freedom, respectively. The distribution of the largest eigenvalue of (A + B)(-1) B has numerous applications in multivariate statistics, but is difficult to calculate exactly. Suppose that in and n grow in proportion to p. We show that after centering and scaling, the distribution is approximated to second-order, O(p(-2/3)), by the Tracy-Widom law. The results are obtained for both complex and then real-valued data by using methods of random matrix theory to study the largest eigenvalue of the Jacobi unitary and orthogonal ensembles. Asymptotic approximations of Jacobi polynomials near the largest zero play a central role.",WOS:000262731400005,ANNALS OF STATISTICS,"['RANDOM MATRICES', 'ORTHOGONAL POLYNOMIALS', 'SPACING DISTRIBUTIONS', 'SYMPLECTIC ENSEMBLES', 'CUERNAVACA MEXICO', 'SPECTRUM EDGE', 'ASYMPTOTICS', 'UNIVERSALITY', 'DETERMINANTS', 'UNITARY']","MULTIVARIATE ANALYSIS AND JACOBI ENSEMBLES: LARGEST EIGENVALUE, TRACY-WIDOM LIMITS AND RATES OF CONVERGENCE",2008
1519,"Adaptive and interacting Markov chain Monte Carlo algorithms (MCMC) have been recently introduced in the literature. These novel simulation algorithms are designed to increase the simulation efficiency to sample complex distributions. Motivated by some recently introduced algorithms (such as the adaptive Metropolis algorithm and the interacting tempering algorithm), we develop a general methodological and theoretical framework to establish both the convergence of the marginal distribution and a strong law of large numbers. This framework weakens the conditions introduced in the pioneering paper by Roberts and Rosenthal [J. Appl. Probab. 44 (2007) 458-475]. It also covers the case when the target distribution p is sampled by using Markov transition kernels with a stationary distribution that differs from p.",WOS:000311639700005,ANNALS OF STATISTICS,"['EQUI-ENERGY SAMPLER', 'METROPOLIS ALGORITHMS', 'MCMC ALGORITHMS', 'LIMIT-THEOREMS', 'ERGODICITY', 'HASTINGS', 'KERNELS', 'RATES']",CONVERGENCE OF ADAPTIVE AND INTERACTING MARKOV CHAIN MONTE CARLO ALGORITHMS,2011
1520,"Functional data often arise from measurements on tine time grids and are obtained by separating an almost continuous time record into natural consecutive intervals, or example, days. The functions thus obtained form a functional time series, and the central issue in the analysis of such data consists in taking into account the temporal dependence of these functional observations. Examples include daily curves of financial transaction data and daily patterns of geophysical and environmental data. For scalar and vector valued stochastic processes, a large number of dependence notions have been proposed, mostly involving mixing type distances between sigma-algebras. In time series analysis, measures of dependence based on moments have proven most useful (autocovariances and cumulants). We introduce a moment-based notion of dependence for functional time series which involves in-dependence. We show that it is applicable to linear as well as nonlinear functional time series. Then we investigate the impact of dependence thus quantified on several important statistical procedures for functional data. We study the estimation of the functional principal components, the long-run covariance matrix, change point detection and the functional linear model. We explain when temporal dependence affects the results obtained for i.i.d. functional observations and when these results are robust to weak dependence.",WOS:000277471000019,ANNALS OF STATISTICS,"['CENTRAL-LIMIT-THEOREM', 'PRINCIPAL COMPONENT ANALYSIS', 'LINEAR-REGRESSION', 'AUTOREGRESSIVE PROCESSES', 'STATIONARY PROCESSES', 'LONGITUDINAL DATA', 'RANDOM-VARIABLES', 'MODELS', 'PREDICTION', 'SEQUENCES']",WEAKLY DEPENDENT FUNCTIONAL DATA,2010
1521,"In this paper, we study the model selection and structure specification for the generalised semi-varying coefficient models (GSVCMs), where the number of potential covariates is allowed to be larger than the sample size. We first propose a penalised likelihood method with the LASSO penalty function to obtain the preliminary estimates of the functional coefficients. Then, using the quadratic approximation for the local log-likelihood function and the adaptive group LASSO penalty (or the local linear approximation of the group SCAD penalty) with the help of the preliminary estimation of the functional coefficients, we introduce a novel penalised weighted least squares procedure to select the significant covariates and identify the constant coefficients among the coefficients of the selected covariates, which could thus specify the semiparametric modelling structure. The developed model selection and structure specification approach not only inherits many nice statistical properties from the local maximum likelihood estimation and nonconcave penalised likelihood method, but also computationally attractive thanks to the computational algorithm that is proposed to implement our method. Under some mild conditions, we establish the asymptotic properties for the proposed model selection and estimation procedure such as the sparsity and oracle property. We also conduct simulation studies to examine the finite sample performance of the proposed method, and finally apply the method to analyse a real data set, which leads to some interesting findings.",WOS:000363437900013,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'VARIABLE SELECTION', 'EFFICIENT ESTIMATION', 'NP-DIMENSIONALITY', 'ORACLE PROPERTIES', 'LONGITUDINAL DATA', 'LINEAR-MODELS', 'REGRESSION', 'LASSO', 'SHRINKAGE']",MODEL SELECTION AND STRUCTURE SPECIFICATION IN ULTRA-HIGH DIMENSIONAL GENERALISED SEMI-VARYING COEFFICIENT MODELS,2015
1522,,WOS:000384397200001,ANNALS OF STATISTICS,,Untitled,2016
1523,"The problem of multiple kernel learning based on penalized empirical risk minimization is discussed. The complexity penalty is determined jointly by the empirical L-2 norms and the reproducing kernel Hilbert space (RKHS) norms induced by the kernels with a data-driven choice of regularization parameters. The main focus is on the case when the total number of kernels is large, but only a relatively small number of them is needed to represent the target function, so that the problem is sparse. The goal is to establish oracle inequalities for the excess risk of the resulting prediction rule showing that the method is adaptive both to the unknown design distribution and to the sparsity of the problem.",WOS:000290231500010,ANNALS OF STATISTICS,"['SUPPORT VECTOR MACHINES', 'DANTZIG SELECTOR', 'INEQUALITIES', 'LASSO']",SPARSITY IN MULTIPLE KERNEL LEARNING,2010
1524,"We propose a novel Rayleigh quotient based sparse quadratic dimension reduction method-named QUADRO (Quadratic Dimension Reduction via Rayleigh Optimization)-for analyzing high-dimensional data. Unlike in the linear setting where Rayleigh quotient optimization coincides with classification, these two problems are very different under nonlinear settings. In this paper, we clarify this difference and show that Rayleigh quotient optimization may be of independent scientific interests. One major challenge of Rayleigh quotient optimization is that the variance of quadratic statistics involves all fourth cross-moments of predictors, which are infeasible to compute for high-dimensional applications and may accumulate too many stochastic errors. This issue is resolved by considering a family of elliptical models. Moreover, for heavy-tail distributions, robust estimates of mean vectors and covariance matrices are employed to guarantee uniform convergence in estimating non-polynomially many parameters, even though only the fourth moments are assumed. Methodologically, QUADRO is based on elliptical models which allow us to formulate the Rayleigh quotient maximization as a convex optimization problem. Computationally, we propose an efficient linearized augmented Lagrangian method to solve the constrained optimization problem. Theoretically, we provide explicit rates of convergence in terms of Rayleigh quotient under both Gaussian and general elliptical models. Thorough numerical results on both synthetic and real datasets are also provided to back up our theoretical results.",WOS:000357441000009,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'NONCONCAVE PENALIZED LIKELIHOOD', 'LINEAR DISCRIMINANT-ANALYSIS', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'CLASSIFICATION', 'LASSO', 'MODELS']",QUADRO: A SUPERVISED DIMENSION REDUCTION METHOD VIA RAYLEIGH QUOTIENT OPTIMIZATION,2015
1525,"Principal component analysis (PCA) is one of the most commonly used statistical procedures with a wide range of applications. This paper considers both minimax and adaptive estimation of the principal subspace in the high dimensional setting. Under mild technical conditions, we first establish the optimal rates of convergence for estimating the principal subspace which are sharp with respect to all the parameters, thus providing a complete characterization of the difficulty of the estimation problem in term of the convergence rate. The lower bound is obtained by calculating the local metric entropy and an application of Fano's lemma. The rate optimal estimator is constructed using aggregation, which, however, might not be computationally feasible.
We then introduce an adaptive procedure for estimating the principal subspace which is fully data driven and can be computed efficiently. It is shown that the estimator attains the optimal rates of convergence simultaneously over a large collection of the parameter spaces. A key idea in our construction is a reduction scheme which reduces the sparse PCA problem to a high-dimensional multivariate regression problem. This method is potentially also useful for other related problems.",WOS:000330204900014,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'COVARIANCE-MATRIX ESTIMATION', 'LOW-RANK MATRICES', 'HIGH DIMENSION', 'POWER METHOD', 'CONVERGENCE', 'APPROXIMATION', 'CONSISTENCY', 'PERTURBATION', 'EIGENVALUE']",SPARSE PCA: OPTIMAL RATES AND ADAPTIVE ESTIMATION,2013
1526,"This paper proves fixed domain asymptotic results for estimating a smooth invertible transformation f:R(2) -> R(2) when observint, the deformed oil a dense,rid in a bounded, simply connected random field Z o f on a dense grid in a bounded, simply connected domain Omega, where Z is assumed to be an isotropic Gaussian random field on R(2). The estimate f is constructed on a simply connected domain U, such that (U) over bar subset of Omega and is defined using kernel smoothed quadratic variations, Bergman projections and results from quasi con formal theory. We show, under mild assumptions oil the random field Z and the deformation f, that (f) over cap -> R(theta) f + c uniformly oil compact subsets of U with probability one as the grid spacing goes to zero. where R(theta) is an unidentifiable rotation and c is all unidentifiable translation.",WOS:000268604900009,ANNALS OF STATISTICS,"['LEVY-BAXTER THEOREM', 'ESTIMATING DEFORMATIONS', 'QUADRATIC VARIATIONS', 'SPACE DEFORMATION', 'BROWNIAN SHEET', 'IDENTIFICATION', 'INCREMENTS', 'TEXTURE', 'VERSION', 'SHAPE']",CONSISTENT ESTIMATES OF DEFORMED ISOTROPIC GAUSSIAN RANDOM FIELDS ON THE PLANE,2009
1527,"In this paper we offer a unified approach to the problem of nonparametric regression on the unit interval. It is based on a universal, honest and nonasymptotic confidence region A(n) which is defined by a set of linear in-equalities involving the values of the functions at the design points. Interest will typically center on certain simplest functions in A(n) where simplicity can be defined in terms of shape (number of local extremes, intervals of convexity/concavity) or smoothness (bounds on derivatives) or a combination of both. Once some form of regularization has been decided upon the confidence region can be used to provide honest nonasymptotic confidence bounds which are less informative but conceptually much simpler.",WOS:000268605000001,ANNALS OF STATISTICS,"['MULTIRESOLUTION', 'MAXIMUM', 'CURVES', 'TESTS', 'BALLS', 'BANDS', 'SETS']","NONPARAMETRIC REGRESSION, CONFIDENCE REGIONS AND REGULARIZATION",2009
1528,"Hypothesis error (HE) plots, introduced in Friendly (2007), provide graphical methods to visualize hypothesis tests in multivariate linear models, by displaying hypothesis and error covariation as ellipsoids and providing visual representations of effect size and significance. These methods are implemented in the heplots for R (Fox, Friendly, and Monette 2009a) and SAS (Friendly 2006), and apply generally to designs with fixed-effect factors (MANOVA), quantitative regressors (multivariate multiple regression) and combined cases (MANCOVA).
This paper describes the extension of these methods to repeated measures designs in which the multivariate responses represent the outcomes on one or more ""within-subject"" factors. This extension is illustrated using the heplots for R. Examples describe one-sample profile analysis,designs with multiple between-S and within-S factors,and doubly multivariate designs,with multivariate responses observed on multiple occasion",WOS:000284597800001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIVARIATE LINEAR-MODELS', 'PACKAGE']",HE Plots for Repeated Measures Designs,2010
1529,"We propose a class of locally and asymptotically optimal tests, based on multivariate ranks and signs for the homogeneity of scatter matrices in M. elliptical populations. Contrary to the existing parametric procedures, these tests remain valid without any moment assumptions, and thus are perfectly robust against heavy-tailed distributions (validity robustness). Nevertheless, they reach semiparametric efficiency bounds at correctly specified elliptical densities and maintain high powers under all (efficiency robustness). In particular, their normal-score version outperforms traditional Gaussian likelihood ratio tests and their pseudo-Gaussian robustifications under a very broad range of non-Gaussian densities including, for instance, all multivariate Student and power-exponential distributions.",WOS:000256504400010,ANNALS OF STATISTICS,"['COVARIANCE MATRICES', 'ADAPTIVE ESTIMATION', 'M-FUNCTIONALS', 'MULTIVARIATE', 'SHAPE', 'BOOTSTRAP', 'DISTRIBUTIONS', 'EFFICIENCY', 'VARIANCES', 'INFERENCE']",Optimal rank-based tests for homogeneity of scatter,2008
1530,"In this paper we present PaCAL, a Python package for arithmetical computations on random variables. The package is capable of performing the four arithmetic operations: addition, subtraction, multiplication and division, as well as computing many standard functions of random variables. Summary statistics, random number generation, plots, and histograms of the resulting distributions can easily be obtained and distribution parameter fitting is also available. The operations are performed numerically and their results interpolated allowing for arbitrary arithmetic operations on random variables following practically any probability distribution encountered in practice. The package is easy to use, as operations on random variables are performed just as they are on standard Python variables. Independence of random variables is, by default, assumed on each step but some computations on dependent random variables are also possible. We demonstrate on several examples that the results are very accurate, often close to machine precision. Practical applications include statistics, physical measurements or estimation of error distributions in scientific computations.",WOS:000341020300001,JOURNAL OF STATISTICAL SOFTWARE,"['CLENSHAW-CURTIS', 'QUADRATURE']",PaCAL: A Python Package for Arithmetic Computations with Random Variables,2014
1531,"Fan, Gijbels and King [Ann. Statist. 25 (1997) 1661-1690] considered the estimation of the risk function psi(x) in the proportional hazards model. Their proposed estimator is based on integrating the estimated derivative function obtained through a local version of the partial likelihood. They proved the large sample properties of the derivative function, but the large sample properties of the estimator for the risk function itself were not established. In this paper, we consider direct estimation of the relative risk function Psi(x(2)) - Psi(x(1)) for any location normalization point x(1). The main novelty in our approach is that we select observations in shrinking neighborhoods of both x(1) and x(2) when constructing a local version of the partial likelihood, whereas Fan, Gijbels and King [Ann. Statist. 25 (1997) 1661-1690] only concentrated on a single neighborhood, resulting in the cancellation of the risk function in the local likelihood function. The asymptotic properties of our estimator are rigorously established and the variance of the estimator is easily estimated. The idea behind our approach is extended to estimate the differences between groups. A simulation study is carried out.",WOS:000248987600018,ANNALS OF STATISTICS,"['MODEL', 'EFFICIENCY']",Local partial likelihood estimation in proportional hazards regression,2007
1532,"We consider the spectral properties of a class of regularized estimators of (large) empirical covariance matrices corresponding to stationary (but not necessarily Gaussian) sequences, obtained by banding. We prove a law of large numbers (similar to that proved in the Gaussian case by Bickel and Levina), which implies that the spectrum of a banded empirical covariance matrix is an efficient estimator. Our main result is a central limit theorem in the same regime, which to our knowledge is new, even in the Gaussian setup.",WOS:000262731400002,ANNALS OF STATISTICS,,A CLT FOR REGULARIZED SAMPLE COVARIANCE MATRICES,2008
1533,"Q is a methodology to explore the distinct subjective perspectives that exist within a group. It is used increasingly across disciplines. The methodology is semi-qualitative and the data are analysed using data reduction methods to discern the existing patterns of thought. This package is the first to perform Q analysis in R, and it provides many advantages to the existing software: namely, it is fully cross-platform, the algorithms can be transparently examined, it provides results in a clearly structured and tabulated form ready for further exploration and modelling, it produces a graphical summary of the results, and it generates a more concise report of the distinguishing and consensus statements. This paper introduces the methodology and explains how to use the package, its advantages as well as its limitations. I illustrate the main functions with a dataset on value patterns about democracy.",WOS:000348651700014,R JOURNAL,"['AGRICULTURE', 'DISCOURSES']",qmethod: A Package to Explore Human Perspectives Using Q Methodology,2014
1534,"In this paper, we systematically study the consistency of sliced average variance estimation (SAVE). The findings reveal that when the response is continuous, the asymptotic behavior of SAVE is rather different from that of sliced inverse regression (SIR). SIR can achieve root n, consistency even when each slice contains only two data points. However, SAVE cannot be root n consistent and it even turns out to be not consistent when each slice contains a fixed number of data points that do not depend on n, where n is the sample size. These results theoretically confirm the notion that SAVE is more sensitive to the number of slices than SIR. Taking this into account, a bias correction is recommended in order to allow SAVE to be root n consistent. In contrast, when the response is discrete and takes finite values, root n consistency can be achieved. Therefore, an approximation through discretization, which is commonly used in practice, is studied. A simulation study is carried out for the purposes of illustration.",WOS:000247498100003,ANNALS OF STATISTICS,"['DIMENSION REDUCTION', 'INVERSE REGRESSION', 'CHECKS']",Asymptotics for sliced average variance estimation,2007
1535,We consider nonparametric estimation of mean regression and conditional variance (or volatility) functions in nonlinear stochastic regression models. Simultaneous confidence bands are constructed and the coverage probabilities are shown to be asymptotically correct. The imposed dependence structure allows applications in many linear and nonlinear autoregressive processes. The results are applied to the S&P 500 Index data.,WOS:000258243000016,ANNALS OF STATISTICS,"['VARIANCE', 'PROBABILITIES', 'DEVIATIONS', 'VOLATILITY', 'BOOTSTRAP', 'INFERENCE', 'MODELS']",Confidence bands in nonparametric time series regression,2008
1536,"Let (X-1, Y-1),..., (X-n, Y-n) be an i.i.d sample from a bivariate distribution function that lies in the max-domain of attraction of an extreme value distribution. The asymptotic joint distribution of the standardized component-wise maxima V-i=1(n) X-i and V-i=1(n) Y-i is then characterized by the marginal extreme value indices and the tail copula R. We propose a procedure for constructing asymptotically distribution-free goodness-of-fit tests for the tail copula R. The procedure is based on a transformation of a suitable empirical process derived from a semi-parametric estimator of R. The transformed empirical process converges weakly to a standard Wiener process, paving the way for a multitude of asymptotically distribution-free goodness-of-fit tests. We also extend our results to the m-variate (m > 2) case. In a simulation study we show that the limit theorems provide good approximations for finite samples and that tests based on the transformed empirical process have high power.",WOS:000352757100015,ANNALS OF STATISTICS,"['MODEL CHECKS', 'REGRESSION', 'DEPENDENCE', 'ESTIMATOR', 'EXTREMES']",ASYMPTOTICALLY DISTRIBUTION-FREE GOODNESS-OF-FIT TESTING FOR TAIL COPULAS,2015
1537,,WOS:000208589700002,R JOURNAL,,"Facets of R Special invited paper on ""The Future of R""",2009
1538,"Multi-state models are a very useful tool to answer a wide range of questions in survival analysis that cannot, or only in a more complicated way, be answered by classical models. They are suitable for both biomedical and other applications in which time-to-event variables are analyzed. However, they are still not frequently applied. So far, an important reason for this has been the lack of available software. To overcome this problem, we have developed the mstate package in R for the analysis of multi-state models. The package covers all steps of the analysis of multi-state models, from model building and data preparation to estimation and graphical representation of the results. It can be applied to non-and semi-parametric (Cox) models. The package is also suitable for competing risks models, as they are a special category of multi-state models.
This article offers guidelines for the actual use of the software by means of an elaborate multi-state analysis of data describing post-transplant events of patients with blood cancer. The data have been provided by the EBMT (the European Group for Blood and Marrow Transplantation). Special attention will be paid to the modeling of different covariate effects (the same for all transitions or transition-specific) and different baseline hazard assumptions (different for all transitions or equal for some).",WOS:000285981800001,JOURNAL OF STATISTICAL SOFTWARE,"['LANDMARKING', 'PREDICTION']",mstate: An R Package for the Analysis of Competing Risks and Multi-State Models,2011
1539,"We study the rates of convergence of the posterior distribution for Bayesian density estimation with Dirichlet mixtures of normal distributions as the prior. The true density is assumed to be twice continuously differentiable. The bandwidth is given a sequence of priors which is obtained by scaling a single prior by an appropriate order. In order to handle this problem, we derive a new general rate theorem by considering a countable covering of the parameter space whose prior probabilities satisfy a summability condition together with certain individual bounds on the Hellinger metric entropy. We apply this new general theorem on posterior convergence rates by computing bounds for Hellinger (bracketing) entropy numbers for the involved class of densities, the error in the approximation of a smooth density by normal mixtures and the concentration rate of the prior. The best obtainable rate of convergence of the posterior turns out to be equivalent to the well-known frequentist rate for integrated mean squared error n(-2/5) up to a logarithmic factor.",WOS:000248987600009,ANNALS OF STATISTICS,"['CONSISTENCY', 'LIKELIHOOD', 'SIEVE']",Posterior convergence rates of dirichlet mixtures at smooth densities,2007
1540,"The problem of adaptive multivariate function estimation in the single-index regression model with random design and weak assumptions on the noise is investigated. A novel estimation procedure that adapts simultaneously to the unknown index vector and the smoothness of the link function by selecting from a family of specific kernel estimators is proposed. We establish a pointwise oracle inequality which, in its turn, is used to judge the quality of estimating the entire function (""global"" oracle inequality). Both the results are applied to the problems of pointwise and global adaptive estimation over a collection of Holder and Nikol'skii functional classes, respectively.",WOS:000334256100001,ANNALS OF STATISTICS,"['RANDOM DESIGN', 'NONLINEAR ESTIMATION', 'ADAPTATION', 'MINIMAX']",ADAPTIVE ESTIMATION UNDER SINGLE-INDEX CONSTRAINT IN A REGRESSION MODEL,2014
1541,"When modelling physical systems, analysts will frequently be confronted by differential equations which cannot be solved analytically. In this instance, numerical integration will usually be the only way forward. However, for autonomous systems of ordinary differential equations (ODEs) in one or two dimensions, it is possible to employ an instructive qualitative analysis foregoing this requirement, using so-called phase plane methods. Moreover, this qualitative analysis can even prove to be highly useful for systems that can be solved analytically, or will be solved numerically anyway. The package phaseR allows the user to perform such phase plane analyses: determining the stability of any equilibrium points easily, and producing informative plots.",WOS:000348651700005,R JOURNAL,,phaseR: An R Package for Phase Plane Analysis of Autonomous ODE Systems,2014
1542,"This article describes the R package DEoptim, which implements the differential evolution algorithm for global optimization of a real-valued function of a real-valued parameter vector. The implementation of differential evolution in DEoptim interfaces with C code for efficiency. The utility of the package is illustrated by case studies in fitting a Parratt model for X-ray reflectometry data and a Markov-switching generalized autoregressive conditional heteroskedasticity model for the returns of the Swiss Market Index.",WOS:000289228700001,JOURNAL OF STATISTICAL SOFTWARE,"['RAY', 'REFINEMENT', 'MANAGEMENT']",DEoptim: An R Package for Global Optimization by Differential Evolution,2011
1543,"We consider symmetric hypothesis testing in quantum statistics, where the hypotheses are density operators on a finite-dimensional complex Hilbert space, representing states of a finite quantum system. We prove a lower bound on the asymptotic rate exponents of Bayesian error probabilities. The bound represents a quantum extension of the Chernoff bound, which gives the best asymptotically achievable error exponent in classical discrimination between two probability measures on a finite set. In our framework, the classical result is reproduced if the two hypothetic density operators commute. Recently, it has been shown elsewhere [Phys. Rev. Lett. 98 (2007) 1605041 that the lower bound is achievable also in the generic quantum (noncommutative) case. This implies that our result is one part of the definitive quantum Chernoff bound.",WOS:000265500500018,ANNALS OF STATISTICS,['ASYMPTOTICS'],THE CHERNOFF LOWER BOUND FOR SYMMETRIC QUANTUM HYPOTHESIS TESTING,2009
1544,We study model selection and model averaging in generalized additive partial linear models (GAPLMs). Polynomial spline is used to approximate nonparametric functions. The corresponding estimators of the linear parameters are shown to be asymptotically normal. We then develop a focused information criterion (FIC) and a frequentist model average (FMA) estimator on the basis of the quasi-likelihood principle and examine theoretical properties of the FIC and FMA. The major advantages of the proposed procedures over the existing ones are their computational expediency and theoretical reliability. Simulation experiments have provided evidence of the superiority of the proposed procedures. The approach is further applied to a real-world data example.,WOS:000288183800005,ANNALS OF STATISTICS,"['SEMIPARAMETRIC REGRESSION', 'POLYNOMIAL SPLINES', 'TENSOR-PRODUCTS', 'SELECTION', 'ESTIMATORS', 'INFERENCE']",FOCUSED INFORMATION CRITERION AND MODEL AVERAGING FOR GENERALIZED ADDITIVE PARTIAL LINEAR MODELS,2011
1545,"Let (X) over tilde (MxN) be a rectangular data matrix with independent real-valued entries [(x) over tilde (ij)] satisfying E (x) over tilde (ij) = 0 and E (x) over tilde (2)(ij) = 1/M, N, M -> infinity. These entries have a subexponential decay at the tails. We will be working in the regime N/M = d(N), lim(N ->infinity) d(N) not equal 0, 1, infinity. In this paper we prove the edge universality of correlation matrices X-dagger X, where the rectangular matrix X (called the standardized matrix) is obtained by normalizing each column of the data matrix (X) over tilde by its Euclidean norm. Our main result states that asymptotically the k-point (k >= 1) correlation functions of the extreme eigenvalues (at both edges of the spectrum) of the correlation matrix X-dagger X converge to those of the Gaussian correlation matrix, that is, Tracy-Widom law, and, thus, in particular, the largest and the smallest eigenvalues of X-dagger X after appropriate centering and rescaling converge to the Tracy-Widom distribution. The asymptotic distribution of extreme eigenvalues of the Gaussian correlation matrix has been worked out only recently. As a corollary of the main result in this paper, we also obtain that the extreme eigenvalues of Gaussian correlation matrices are asymptotically distributed according to the Tracy-Widom law. The proof is based on the comparison of Green functions, but the key obstacle to be surmounted is the strong dependence of the entries of the correlation matrix. We achieve this via a novel argument which involves comparing the moments of product of the entries of the standardized data matrix to those of the raw data matrix. Our proof strategy may be extended for proving the edge universality of other random matrix ensembles with dependent entries and hence is of independent interest.",WOS:000310650900017,ANNALS OF STATISTICS,"['WIGNER RANDOM MATRICES', 'COVARIANCE MATRICES', 'LARGEST EIGENVALUES', 'LOCAL STATISTICS', 'SEMICIRCLE LAW', 'DELOCALIZATION', 'ENSEMBLES']",EDGE UNIVERSALITY OF CORRELATION MATRICES,2012
1546,"Estimating the location and scale parameters is common in statistics, using, for instance, the well-known sample mean and standard deviation. However, inference can be contaminated by the presence of outliers if modeling is done with light-tailed distributions such as the normal distribution. In this paper, we study robustness to outliers in location-scale parameter models using both the Bayesian and frequentist approaches. We find sufficient conditions (e.g., on tail behavior of the model) to obtain whole robustness to outliers, in the sense that the impact of the outliers gradually decreases to nothing as the conflict grows infinitely. To this end, we introduce the family of log-Pareto-tailed symmetric distributions that belongs to the larger family of log-regularly varying distributions.",WOS:000357441000011,ANNALS OF STATISTICS,['INFERENCE'],ROBUSTNESS TO OUTLIERS IN LOCATION-SCALE PARAMETER MODEL USING LOG-REGULARLY VARYING DISTRIBUTIONS,2015
1547,"The familiar Sigma(OBS - EXP)(2)/EXP goodness-of-fit measure is commonly used to test whether an observed sequence came from the realization of n independent identically distributed (iid) discrete random variables. It can be quite effective for testing for identical distribution, but is not suited for assessing independence, as it pays no attention to the order in which output values are received.
This note reviews a way to adjust or tamper, that is, monkey-with the classical test to make it test for independence as well as identical distribution - in short, to test for both the i's in iid, using monkey tests similar to those in the Diehard Battery of Tests of Randomness (Marsaglia 1995).",WOS:000232928700001,JOURNAL OF STATISTICAL SOFTWARE,,Monkeying with the goodness-of-fit test,2005
1548,"This paper presents the RMatlab-app2web tool which enables the use of R or MATLAB scripts as CGI programs for generating dynamic web content. RMatlab-app2web is highly adjustable. It can be run on both, Windows and Unix-like systems. CGI scripts written in PHP take information entered on web-based forms on the client browser, pass it to R or MATLAB on the server and display the output on the client browser. Adjustable to the servers requirements, the data transfer procedure can use either the GET or the POST routine. The application allows to call R or MATLAB to run previously written scripts. It does not allow to run completely flexible user code. We run a multivariate OLS regression to demonstrate the use of the RMatlab-app2web tool.",WOS:000323909900001,JOURNAL OF STATISTICAL SOFTWARE,,RMatlab-app2web: Web Deployment of R/MATLAB Applications,2013
1549,"In nonparametric regression problems involving multiple predictors, there is typically interest in estimating an anisotropic multivariate regression surface in the important predictors while discarding the unimportant ones. Our focus is on defining a Bayesian procedure that leads to the minimax optimal rate of posterior contraction (up to a log factor) adapting to the unknown dimension and anisotropic smoothness of the true surface. We propose such an approach based on a Gaussian process prior with dimension-specific scalings, which are assigned carefully-chosen hyperpriors. We additionally show that using a homogenous Gaussian process with a single bandwidth leads to a sub-optimal rate in anisotropic cases.",WOS:000334256100014,ANNALS OF STATISTICS,"['BAYESIAN DENSITY-ESTIMATION', 'POSTERIOR DISTRIBUTIONS', 'MODEL SELECTION', 'PROCESS PRIORS', 'VARIABLE SELECTION', 'CONVERGENCE-RATES', 'REGRESSION', 'MIXTURES', 'INFERENCE', 'BOUNDS']",ANISOTROPIC FUNCTION ESTIMATION USING MULTI-BANDWIDTH GAUSSIAN PROCESSES,2014
1550,We present the hglm package for fitting hierarchical generalized linear models. It can be used for linear mixed models and generalized linear mixed models with random effects for a variety of links and a variety of distributions for both the outcomes and the random effects. Fixed effects can also be fitted in the dispersion part of the model.,WOS:000208590000004,R JOURNAL,,hglm: A Package for Fitting Hierarchical Generalized Linear Models,2010
1551,"We consider parameter estimation, hypothesis testing and variable selection for partially time-varying coefficient models. Our asymptotic theory has the useful feature that it can allow dependent, nonstationary error and covariate processes. With a two-stage method, the parametric component can be estimated with a n(1/2)-convergence rate. A simulation-assisted hypothesis testing procedure is proposed for testing significance and parameter constancy. We further propose an information criterion that can consistently select the true set of significant predictors. Our method is applied to autoregressive models with time-varying coefficients. Simulation results and a real data application are provided.",WOS:000310650900004,ANNALS OF STATISTICS,"['CONSISTENT COVARIANCE-MATRIX', 'COEFFICIENT MODELS', 'LONGITUDINAL DATA', 'SERIES MODELS', 'NONPARAMETRIC REGRESSION', 'AUTOREGRESSIVE PROCESSES', 'LINEAR-MODELS', 'STATIONARY-PROCESSES', 'EFFICIENT ESTIMATION', 'SPECIFICATION TESTS']",INFERENCE OF TIME-VARYING REGRESSION MODELS,2012
1552,"This paper presents the lubridate package for R, which facilitates working with dates and times. Date-times create various technical problems for the data analyst. The paper highlights these problems and offers practical advice on how to solve them using lubridate. The paper also introduces a conceptual framework for arithmetic with date-times in R.",WOS:000289228400001,JOURNAL OF STATISTICAL SOFTWARE,,Dates and Times Made Easy with lubridate,2011
1553,"We consider regression problems where the number of predictors greatly exceeds the number of observations. We propose a method for variable selection that first estimates the regression function, yielding a ""preconditioned"" response variable. The primary method used for this initial regression is supervised principal components. Then we apply a standard procedure such as forward stepwise selection or the LASSO to the preconditioned response variable. In a number of simulated and real data examples, this two-step procedure outperforms forward stepwise selection or the usual LASSO (applied directly to the raw outcome). We also show that under a certain Gaussian latent variable model, application of the LASSO to the preconditioned response variable is consistent as the number of predictors and observations increases. Moreover, when the observational noise is rather large, the suggested procedure can give a more accurate estimate than LASSO. We illustrate our method on some real problems, including survival analysis with microarray data.",WOS:000258243000007,ANNALS OF STATISTICS,"['GENE-EXPRESSION', 'MODEL SELECTION', 'LASSO', 'SURVIVAL']","""Preconditioning"" for feature selection and regression in high-dimensional problems'",2008
1554,"The gridSVG package can be used to generate a grid-based R plot in an SVG format, with the ability to add special effects to the plot. The special effects include animation, interactivity, and advanced graphical features, such as masks and filters. This article provides a basic introduction to important functions in the gridSVG package and discusses the advantages and disadvantages of gridSVG compared to similar R packages.",WOS:000343788100014,R JOURNAL,,The gridSVG Package,2014
1555,"In this paper, we consider moderate deviations for Good's coverage estimator. The moderate deviation principle and the self-normalized moderate deviation principle for Good's coverage estimator are established. The results are also applied to the hypothesis testing problem and the confidence interval for the coverage.",WOS:000320488200009,ANNALS OF STATISTICS,"['RANDOM-VARIABLES', 'PROBABILITY', 'NUMBER']",MODERATE DEVIATIONS FOR A NONPARAMETRIC ESTIMATOR OF SAMPLE COVERAGE,2013
1556,"This paper discusses the numerical precision of five spreadsheets (Calc, Excel, Gnumeric, NeoOffice and Oleo) running on two hardware platforms (i386 and amd64) and on three operating systems (Windows Vista, Ubuntu Intrepid and Mac OS Leopard). The methodology consists of checking the number of correct significant digits returned by each spreadsheet when computing the sample mean, standard deviation, first-order autocorrelation, F statistic in ANOVA tests, linear and nonlinear regression and distribution functions. A discussion about the algorithms for pseudorandom number generation provided by these platforms is also conducted. We conclude that there is no safe choice among the spreadsheets here assessed: they all fail in nonlinear regression and they are not suited for Monte Carlo experiments.",WOS:000276707400001,JOURNAL OF STATISTICAL SOFTWARE,"['MICROSOFT EXCEL 2003', 'STATISTICAL PROCEDURES', 'NUMBER GENERATORS', 'SOFTWARE', 'DISTRIBUTIONS', 'RELIABILITY', 'ERRORS', 'AIR']",On the Numerical Accuracy of Spreadsheets,2010
1557,"Item response theory models (IRT) are increasingly becoming established in social science research, particularly in the analysis of performance or attitudinal data in psychology, education, medicine, marketing and other fields where testing is relevant. We propose the R package eRm ( extended Rasch modeling) for computing Rasch models and several extensions.
A main characteristic of some IRT models, the Rasch model being the most prominent, concerns the separation of two kinds of parameters, one that describes qualities of the subject under investigation, and the other relates to qualities of the situation under which the response of a subject is observed. Using conditional maximum likelihood (CML) estimation both types of parameters may be estimated independently from each other. IRT models are well suited to cope with dichotomous and polytomous responses, where the response categories may be unordered as well as ordered. The incorporation of linear structures allows for modeling the effects of covariates and enables the analysis of repeated categorical measurements.
The eRm package fits the following models: the Rasch model, the rating scale model (RSM), and the partial credit model (PCM) as well as linear reparameterizations through covariate structures like the linear logistic test model (LLTM), the linear rating scale model (LRSM), and the linear partial credit model (LPCM). We use an unitary, efficient CML approach to estimate the item parameters and their standard errors. Graphical and numeric tools for assessing goodness-of-fit are provided.",WOS:000247011900001,JOURNAL OF STATISTICAL SOFTWARE,"['PARTIAL CREDIT MODEL', 'EXTENSION']",Extended Rasch modeling: The eRm package for the application of IRT models in R,2007
1558,"ggmcmc is an R package for analyzing Markov chain Monte Carlo simulations from Bayesian inference. By using a well known example of hierarchical/ multilevel modeling, the article reviews the potential uses and options of the package, ranging from classical convergence tests to caterpillar plots or posterior predictive checks.",WOS:000384912200001,JOURNAL OF STATISTICAL SOFTWARE,,ggmcmc: Analysis of MCMC Samples and Bayesian Inference,2016
1559,"Targeted maximum likelihood estimation (TMLE) is a general approach for constructing an efficient double-robust semi-parametric substitution estimator of a causal effect parameter or statistical association measure. tmle is a recently developed R package that implements TMLE of the effect of a binary treatment at a single point in time on an outcome of interest, controlling for user supplied covariates, including an additive treatment effect, relative risk, odds ratio, and the controlled direct effect of a binary treatment controlling for a binary intermediate variable on the pathway from treatment to the outcome. Estimation of the parameters of a marginal structural model is also available. The package allows outcome data with missingness, and experimental units that contribute repeated records of the point-treatment data structure, thereby allowing the analysis of longitudinal data structures. Relevant factors of the likelihood may be modeled or fit data-adaptively according to user specifications, or passed in from an external estimation procedure. Effect estimates, variances, p values, and 95% confidence intervals are provided by the software.",WOS:000312290100001,JOURNAL OF STATISTICAL SOFTWARE,"['MARGINAL STRUCTURAL MODELS', 'CAUSAL INFERENCE', 'ROBUST ESTIMATION']",tmle: An R Package for Targeted Maximum Likelihood Estimation,2012
1560,"We study sparse principal components analysis in high dimensions, where p (the number of variables) can be much larger than n (the number of observations), and analyze the problem of estimating the subspace spanned by the principal eigenvectors of the population covariance matrix. We introduce two complementary notions of eq subspace sparsity: row sparsity and column sparsity. We prove nonasymptotic lower and upper bounds on the minimax subspace estimation error for 0 <= q <= I. The bounds are optimal for row sparse subspaces and nearly optimal for column sparse subspaces, they apply to general classes of covariance matrices, and they show that l(q) constrained estimates can achieve optimal minimax rates without restrictive spiked covariance conditions. Interestingly, the form of the rates matches known results for sparse regression when the effective noise variance is defined appropriately. Our proof employs a novel variational sine theorem that may be useful in other regularized spectral estimation problems.",WOS:000330204900008,ANNALS OF STATISTICS,"['COMPONENT ANALYSIS', 'VARIABLE SELECTION', 'REGRESSION', 'PCA', 'APPROXIMATION', 'CONSISTENCY', 'LASSO', 'MODEL']",MINIMAX SPARSE PRINCIPAL SUBSPACE ESTIMATION IN HIGH DIMENSIONS,2013
1561,"Test statistics are often strongly dependent in large-scale multiple testing applications. Most corrections for multiplicity are unduly conservative for correlated test statistics, resulting in a loss of power to detect true positives. We show that the Westfall-Young permutation method has asymptotically optimal power for a broad class of testing problems with a block-dependence and sparsity structure among the tests, when the number of tests tends to infinity.",WOS:000311639700009,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'GENOME-WIDE ASSOCIATION', 'HIGHER CRITICISM', 'COMPLEX TRAITS', 'DISEASES', 'GENES']",ASYMPTOTIC OPTIMALITY OF THE WESTFALL-YOUNG PERMUTATION PROCEDURE FOR MULTIPLE TESTING UNDER DEPENDENCE,2011
1562,"Barry and Hartigan (1993) propose a Bayesian analysis for change point problems. We provide a brief summary of selected work on change point problems, both preceding and following Barry and Hartigan. We outline Barry and Hartigan's approach and off er a new R package, pkgbcp (Erdman and Emerson 2007), implementing their analysis. We discuss two frequentist alternatives to the Bayesian analysis, the recursive circular binary segmentation algorithm (Olshen and Venkatraman 2004) and the dynamic programming algorithm of (Bai and Perron 2003). We illustrate the application of bcp with economic and microarray data from the literature.",WOS:000252431200001,JOURNAL OF STATISTICAL SOFTWARE,['REGIME'],bcp: An R package for performing a Bayesian analysis of change point problems,2007
1563,,WOS:000258243000005,ANNALS OF STATISTICS,"['LEAST ANGLE REGRESSION', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'LASSO', 'CONSISTENCY']",Rejoinder: One-step sparse estimates in nonconcave penalized likelihood models,2008
1564,"Analysis of covariance serves two important purposes in a randomized clinical trial. First, there is a reduction of variance for the treatment effect which provides more powerful statistical tests and more precise confidence intervals. Second, it provides estimates of the treatment effect which are adjusted for random imbalances of covariates between the treatment groups. The nonparametric analysis of covariance method of Koch, Tangen, Jung, and Amara (1998) defines a very general methodology using weighted least-squares to generate covariate-adjusted treatment effects with minimal assumptions. This methodology is general in its applicability to a variety of outcomes, whether continuous, binary, ordinal, incidence density or time-to-event. Further, its use has been illustrated in many clinical trial setting, such as multi-center, dose-response and non-inferiority trials.
NParCov3 is a SAS/IML macro written to conduct the nonparametric randomization-based covariance analyses of Koch et al. (1998). The software can analyze a variety of outcomes and can account for stratification. Data from multiple clinical trials will be used for illustration.",WOS:000306914200001,JOURNAL OF STATISTICAL SOFTWARE,"['TO-EVENT DATA', 'CLINICAL-TRIALS']",NParCov3: A SAS/IML Macro for Nonparametric Randomization-Based Analysis of Covariance,2012
1565,"In the period 1991-2015, algorithmic advances in Mixed Integer Optimization (MIO) coupled with hardware improvements have resulted in an astonishing 450 billion factor speedup in solving MIO problems. We present a MIO approach for solving the classical best subset selection problem of choosing k out of p features in linear regression given n observations. We develop a discrete extension of modern first-order continuous optimization methods to find high quality feasible solutions that we use as warm starts to a MIO solver that finds provably optimal solutions. The resulting algorithm ( a) provides a solution with a guarantee on its suboptimality even if we terminate the algorithm early, (b) can accommodate side constraints on the coefficients of the linear regression and (c) extends to finding best subset solutions for the least absolute deviation loss function. Using a wide variety of synthetic and real datasets, we demonstrate that our approach solves problems with n in the 1000s and p in the 100s in minutes to provable optimality, and finds near optimal solutions for n in the 100s and p in the 1000s in minutes. We also establish via numerical experiments that the MIO approach performs better than Lasso and other popularly used sparse learning procedures, in terms of achieving sparse solutions with good predictive power.",WOS:000372594300013,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'DIMENSIONAL LINEAR-REGRESSION', 'VARIABLE SELECTION', 'L(1) MINIMIZATION', 'ATOMIC DECOMPOSITION', 'ORACLE PROPERTIES', 'LASSO', 'SHRINKAGE', 'SPARSITY', 'RECOVERY']",BEST SUBSET SELECTION VIA A MODERN OPTIMIZATION LENS,2016
1566,"The clusters of a distribution are often defined by the connected components of a density level set. However, this definition depends on the user-specified level. We address this issue by proposing a simple, generic algorithm, which uses an almost arbitrary level set estimator to estimate the smallest level at which there are more than one connected components. In the case where this algorithm is fed with histogram-based level set estimates, we provide a finite sample analysis, which is then used to show that the algorithm consistently estimates both the smallest level and the corresponding connected components. We further establish rates of convergence for the two estimation problems, and last but not least, we present a simple, yet adaptive strategy for determining the width-parameter of the involved density estimator in a data-depending way.",WOS:000362697700010,ANNALS OF STATISTICS,"['LEVEL SETS', 'NONPARAMETRIC-ESTIMATION', 'SINGLE LINKAGE', 'RATES', 'CLASSIFICATION', 'CONSISTENCY', 'ESTIMATORS', 'SUPPORT', 'CONTOUR', 'TREE']",FULLY ADAPTIVE DENSITY-BASED CLUSTERING,2015
1567,"The simecol package provides an open structure to implement, simulate and share ecological models. A generalized object-oriented architecture improves readability and potential code re-use of models and makes simecol-models freely extendable and simple to use. The simecol package was implemented in the S4 class system of the programming language R. Reference applications, e. g. predator-prey models or grid models are provided which can be used as a starting point for own developments. Compact example applications and the complete code of an individual-based model of the water flea Daphnia document the efficient usage of simecol for various purposes in ecological modeling, e. g. scenario analysis, stochastic simulations and individual based population dynamics. Ecologists are encouraged to exploit the abilities of simecol to structure their work and to use R and object-oriented programming as a suitable medium for the distribution and share of ecological modeling code.",WOS:000252430300001,JOURNAL OF STATISTICAL SOFTWARE,"['DAPHNIA POPULATION-DYNAMICS', 'LIFE-HISTORY', 'MIDSUMMER DECLINE', 'FISH PREDATION', 'TEMPERATURE', 'SYSTEMS', 'FOOD', 'CONSEQUENCES', 'SIMULATION', 'MORTALITY']",simecol : An object-oriented framework for ecological modeling in R,2007
1568,We establish global rates of convergence for the Maximum Likelihood Estimators (MLEs) of log-concave and s-concave densities on R. The main finding is that the rate of convergence of the MLE in the Hellinger metric is no worse than n(-2/5) when -1 < s < infinity where s = 0 corresponds to the log-concave case. We also show that the MLE does not exist for the classes of s-concave densities with s < -1.,WOS:000375175200003,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'CONVEX-FUNCTIONS', 'INEQUALITIES', 'CONSISTENCY', 'ESTIMATORS', 'REGRESSION']",GLOBAL RATES OF CONVERGENCE OF THE MLES OF LOG-CONCAVE AND s-CONCAVE DENSITIES,2016
1569,"We present multiple factor analysis for contingency tables (MFACT) and its implementation in the FactoMineR package. This method, through an option of the MFA function, allows us to deal with multiple contingency or frequency tables, in addition to the categorical and quantitative multiple tables already considered in previous versions of the package. Thanks to this revised function, either a multiple contingency table or a mixed multiple table integrating quantitative, categorical and frequency data can be tackled.
The FactoMineR package (Le et al., 2008; Husson et al., 2011) offers the most commonly used principal component methods: principal component analysis (PCA), correspondence analysis (CA; Benzecri, 1973), multiple correspondence analysis (MCA; Lebart et al., 2006) and multiple factor analysis (MFA; Escofier and Pages, 2008). Detailed presentations of these methods enriched by numerous examples can be consulted at the website http://factominer.free.fr/.
An extension of the MFA function that considers contingency or frequency tables as proposed by Becue-Bertaut and Pages (2004, 2008) is detailed in this article.
First, an example is presented in order to motivate the approach. Next, the mortality data used to illustrate the method are introduced. Then we briefly describe multiple factor analysis (MFA) and present the principles of its extension to contingency tables. A real example on mortality data illustrates the handling of the MFA function to analyse these multiple tables and, finally, conclusions are presented.",WOS:000321944400004,R JOURNAL,,Multiple Factor Analysis for Contingency Tables in the FactoMineR Package,2013
1570,"We introduce a new criterion, the Rank Selection Criterion (RSC), for selecting the optimal reduced rank estimator of the coefficient matrix in multivariate response regression models. The corresponding RSC estimator minimizes the Frobenius norm of the fit plus a regularization term proportional to the number of parameters in the reduced rank model.
The rank of the RSC estimator provides a consistent estimator of the rank of the coefficient matrix; in general, the rank of our estimator is a consistent estimate of the effective rank, which we define to be the number of singular values of the target matrix that are appropriately large. The consistency results are valid not only in the classic asymptotic regime, when n, the number of responses, and p, the number of predictors, stay bounded, and m, the number of observations, grows, but also when either, or both, n and p grow, possibly much faster than m.
We establish minimax optimal bounds on the mean squared errors of our estimators. Our finite sample performance bounds for the RSC estimator show that it achieves the optimal balance between the approximation error and the penalty term.
Furthermore, our procedure has very low computational complexity, linear in the number of candidate models, making it particularly appealing for large scale problems. We contrast our estimator with the nuclear norm penalized least squares (NNP) estimator, which has an inherently higher computational complexity than RSC, for multivariate regression models. We show that NNP has estimation properties similar to those of RSC, albeit under stronger conditions. However, it is not as parsimonious as RSC.
We offer a simple correction of the NNP estimator which leads to consistent rank estimation. We verify and illustrate our theoretical findings via an extensive simulation study.",WOS:000291183300021,ANNALS OF STATISTICS,['REGRESSION'],OPTIMAL SELECTION OF REDUCED RANK ESTIMATORS OF HIGH-DIMENSIONAL MATRICES,2011
1571,We consider a wavelet thresholding approach to adaptive variance function estimation in heteroscedastic nonparametric regression. A data-driven estimator is constructed by applying wavelet thresholding to the squared first-order differences of the observations. We show that the variance function estimator is nearly optimally adaptive to the smoothness of both the mean and variance functions. The estimator is shown to achieve the optimal adaptive rate of convergence under the pointwise squared error simultaneously over a range of smoothness classes. The estimator is also adaptively within a logarithmic factor of the minimax risk under the global mean integrated squared error over a collection of spatially inhomogeneous function classes. Numerical implementation and simulation results are also discussed.,WOS:000260554100001,ANNALS OF STATISTICS,"['WAVELET SHRINKAGE', 'INEQUALITY']",ADAPTIVE VARIANCE FUNCTION ESTIMATION IN HETEROSCEDASTIC NONPARAMETRIC REGRESSION,2008
1572,"Of the seven elementary catastrophes in catastrophe theory, the ""cusp"" model is the most widely applied. Most applications are however qualitative. Quantitative techniques for catastrophe modeling have been developed, but so far the limited availability of flexible software has hindered quantitative assessment. We present a package that implements and extends the method of Cobb (Cobb and Watson 1980; Cobb, Koppstein, and Chen 1983), and makes it easy to quantitatively fit and compare different cusp catastrophe models in a statistically principled way. After a short introduction to the cusp catastrophe, we demonstrate the package with two instructive examples.",WOS:000272259100001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION', 'SELECTION', 'CRITIQUE', 'MODEL']",Fitting the Cusp Catastrophein R: A cusp Package Primer,2009
1573,"Rating scales,such as Likert scales, are very common in marketing research, customer satisfaction studies, psychometrics, opinion surveys, population studies, and numerous other fields. We recommend diverging stacked bar charts as the primary graphical display technique for Likert and related scales. We also show other applications where diverging stacked bar charts are useful. Many examples of plots of Likert scales are given. We discuss the perceptual and programming issues in constructing these graphs. We present two implementations for diverging stacked bar charts. Most examples in this paper were drawn with the likert function included in the HH package in R. We also have a dashboard in Tableau.",WOS:000334020400001,JOURNAL OF STATISTICAL SOFTWARE,,Design of Diverging Stacked Bar Charts for Likert Scales and Other Applications,2014
1574,"We consider settings where the observations are drawn from a zero-mean multivariate (real or complex) normal distribution with the population covariance matrix having eigenvalues of arbitrary multiplicity. We assume that the eigenvectors of the population covariance matrix are unknown and focus on inferential procedures that are based on the sample eigenvalues alone (i.e., ""eigen-inference"").
Results found in the literature establish the asymptotic normality of the fluctuation in the trace of powers of the sample covariance matrix. We develop concrete algorithms for analytically computing the limiting quantities and the covariance of the fluctuations. We exploit the asymptotic normality of the trace of powers of the sample covariance matrix to develop eigenvalue-based procedures for testing and estimation. Specifically. we formulate a Simple test of hypotheses for the population eigenvalues and a technique for estimating the population eigenvalues in settings where the cumulative distribution function of the (nonrandom) population eigenvalues has a staircase structure.
Monte Carlo simulations are used to demonstrate the superiority of the proposed methodologies over classical techniques and the robustness of the proposed techniques in high-dimensional, (relatively) small sample size settings. The improved performance results from the fact that the proposed inference procedures are ""global"" (in a sense that we describe) and exploit ""global"" information thereby overcoming the inherent biases that cripple classical inference procedures which are ""local"" and rely on ""local"" information.",WOS:000262731400010,ANNALS OF STATISTICS,"['SAMPLE COVARIANCE MATRICES', 'HYPERGEOMETRIC-FUNCTIONS', 'LAPLACE APPROXIMATIONS', '2ND-ORDER FREENESS', 'FLUCTUATIONS', 'EIGENVALUES', 'POPULATION', 'CLT']",STATISTICAL EIGEN-INFERENCE FROM LARGE WISHART MATRICES,2008
1575,"Functional data analysis has become a major branch of nonparametric statistics and is a fast evolving field. Peter Hall has made fundamental contributions to this area and its theoretical underpinnings. He wrote more than 25 papers in functional data analysis between 1998 and 2016 and from 2005 on was a tenured faculty member with a 25% appointment in the Department of Statistics at the University of California, Davis. This article describes aspects of his appointment and academic life in Davis and also some of his major results in functional data analysis, along with a brief history of this area. It concludes with an outlook on new types of functional data and an emerging field of ""random objects"" that subsumes functional data analysis as it deals with more complex data structures.",WOS:000384397200005,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'NONPARAMETRIC REGRESSION-ANALYSIS', 'LINEAR-REGRESSION', 'LONGITUDINAL DATA', 'CONVERGENCE-RATES', 'GROWTH-CURVES', 'DISCRETE-DATA', 'PREDICTION', 'MODELS', 'CLASSIFICATION']","PETER HALL, FUNCTIONAL DATA ANALYSIS AND RANDOM OBJECTS",2016
1576,"We describe an R package cts for fitting a modified form of continuous time autoregressive model, which can be particularly useful with unequally sampled time series. The estimation is based on the application of the Kalman filter. The paper provides the methods and algorithms implemented in the package, including parameter estimation, spectral analysis, forecasting, model checking and Kalman smoothing. The package contains R functions which interface underlying Fortran routines. The package is applied to geophysical and medical data for illustration.",WOS:000318237300001,JOURNAL OF STATISTICAL SOFTWARE,['SERIES'],cts: An R Package for Continuous Time Autoregressive Models via Kalman Filter,2013
1577,"Gene expression data can be associated with various clinical outcomes. In particular, these data can be of importance in discovering survival-associated genes for medical applications. As alternatives to traditional statistical methods, sophisticated methods and software programs have been developed to overcome the high-dimensional difficulty of microarray data. Nevertheless, new algorithms and software programs are needed to include practical functions such as the discovery of multiple sets of survival-associated genes and the incorporation of risk factors, and to use in the R environment which many statisticians are familiar with. For survival modeling with microarray data, we have developed a software program (called rbsurv) which can be used conveniently and interactively in the R environment. This program selects survival-associated genes based on the partial likelihood of the Cox model and separates training and validation sets of samples for robustness. It can discover multiple sets of genes by iterative forward selection rather than one large set of genes. It can also allow adjustment for risk factors in microarray survival modeling. This software package, the rbsurv package, can be used to discover survival-associated genes with microarray data conveniently.",WOS:000263105000001,JOURNAL OF STATISTICAL SOFTWARE,"['GENE-EXPRESSION DATA', 'ENRICHED HYALURONAN-BINDING', 'HUMAN GLIOBLASTOMAS', 'PREDICT SURVIVAL', 'REGRESSION', 'CANCER', 'PROFILES', 'OUTCOMES', 'GLIOMAS', 'MATRIX']",Robust Likelihood-Based Survival Modeling with Microarray Data,2009
1578,"We study the problem of estimating the leading eigenvectors of a high-dimensional population covariance matrix based on independent Gaussian observations. We establish a lower bound on the minimax risk of estimators under the l(2) loss, in the joint limit as dimension and sample size increase to infinity, under various models of sparsity for the population eigenvectors. The lower bound on the risk points to the existence of different regimes of sparsity of the eigenvectors. We also propose a new method for estimating the eigenvectors by a two-stage coordinate selection scheme.",WOS:000321847600001,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'COVARIANCE-MATRIX ESTIMATION', 'APPROXIMATION', 'CONVERGENCE', 'CONSISTENCY', 'RATES', 'MODEL']",MINIMAX BOUNDS FOR SPARSE PCA WITH NOISY HIGH-DIMENSIONAL DATA,2013
1579,"In this paper, the maximum L-q-likelihood estimator (MLqE), a new parameter estimator based on nonextensive entropy [Kibernetika 3 (1967) 30-35] is introduced. The properties of the MLqE are studied via asymptotic analysis and computer simulations. The behavior of the MLqE is characterized by the degree of distortion q applied to the assumed model. When q is properly chosen for small and moderate sample sizes, the MLqE can successfully trade bias for precision, resulting in a substantial reduction of the mean squared error. When the sample size is large and q tends to 1, a necessary and sufficient condition to ensure a proper asymptotic normality and efficiency of MLqE is established.",WOS:000275510800007,ANNALS OF STATISTICS,"['STATISTICAL MECHANICS', 'INFORMATION THEORY', 'MATRIX', 'DIVERGENCE', 'ROBUST', 'MODEL']",MAXIMUM L-q-LIKELIHOOD ESTIMATION,2010
1580,"We describe some functions in the R package ggm to derive from a given Markov model, represented by a directed acyclic graph, different types of graphs induced after marginalizing over and conditioning on some of the variables. The package has a few basic functions that find the essential graph, the induced concentration and covariance graphs, and several types of chain graphs implied by the directed acyclic graph (DAG) after grouping and reordering the variables. These functions can be useful to explore the impact of latent variables or of selection effects on a chosen data generating model.",WOS:000236151300001,JOURNAL OF STATISTICAL SOFTWARE,,Independencies induced from a graphical Markov model after marginalization and conditioning: the R package ggm,2006
1581,"The R package bclust is useful for clustering high-dimensional continuous data. The package uses a parametric spike-and-slab Bayesian model to downweight the effect of noise variables and to quantify the importance of each variable in agglomerative clustering. We take advantage of the existence of closed-form marginal distributions to estimate the model hyper-parameters using empirical Bayes, thereby yielding a fully automatic method. We discuss computational problems arising in implementation of the procedure and illustrate the usefulness of the package through examples.",WOS:000303804200001,JOURNAL OF STATISTICAL SOFTWARE,"['GEOMETRIC REPRESENTATION', 'MICROARRAY EXPERIMENTS', 'EXPRESSION DATA', 'MIXTURE MODEL', 'REGRESSION']",High-Dimensional Bayesian Clustering with Variable Selection: The R Package bclus,2012
1582,"While linear mixed modeling methods are foundational concepts introduced in any statistical education, adequate general methods for interval estimation involving models with more than a few variance components are lacking, especially in the unbalanced setting. Generalized fiducial inference provides a possible framework that accommodates this absence of methodology. Under the fabric of generalized fiducial inference along with sequential Monte Carlo methods, we present an approach for interval estimation for both balanced and unbalanced Gaussian linear mixed models. We compare the proposed method to classical and Bayesian results in the literature in a simulation study of two-fold nested models and two-factor crossed designs with an interaction term. The proposed method is found to be competitive or better when evaluated based on frequentist criteria of empirical coverage and average length of confidence intervals for small sample sizes. A MATLAB implementation of the proposed algorithm is available from the authors.",WOS:000312899000012,ANNALS OF STATISTICS,"['MONTE-CARLO METHODS', 'CONFIDENCE-INTERVALS', 'VARIANCE-COMPONENTS', 'SEQUENTIAL IMPUTATIONS', 'INSTRUMENT RESOLUTION', 'INVERSE PROBABILITY', 'LIMITED RESOLUTION', 'UNCERTAINTY', 'DISTRIBUTIONS', 'PARAMETERS']",GENERALIZED FIDUCIAL INFERENCE FOR NORMAL LINEAR MIXED MODELS,2012
1583,"In the analysis of cluster data, the regression coefficients are frequently assumed to be the same across all clusters. This hampers the ability to Study the varying impacts of factors on each cluster. In this paper, a semiparametric model is introduced to account for varying impacts of factors over clusters by using cluster-level covariates. It achieves the parsimony of parametrization and allows the explorations of nonlinear interactions. The random effect ill the semiparametric model also accounts for within-cluster correlation. Local. linear-based estimation procedure is proposed for estimating functional coefficients, residual variance and within-cluster correlation matrix. The asymptotic properties of the proposed estimators are established, and the method for constructing Simultaneous confidence bands are proposed and studied. In addition, relevant hypothesis testing problems ire addressed. Simulation studies are carried out to demonstrate the methodological power of the proposed methods in the finite sample. The proposed model and methods are used to analyse the second birth interval in Bangladesh, leading to some interesting findings.",WOS:000268604900011,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'LONGITUDINAL DATA', 'NONPARAMETRIC REGRESSION', 'CONFIDENCE BANDS', 'INFERENCE', 'SELECTION', 'SPLINE']",A SEMIPARAMETRIC MODEL FOR CLUSTER DATA,2009
1584,"Visualization of spatial data on a map aids not only in data exploration but also in communication to impart spatial conception or ideas to others. Although recent cartographic functions in R are rapidly becoming richer, proportional symbol mapping, which is one of the common mapping approaches, has not been packaged thus far. Based on the theories of proportional symbol mapping developed in cartography, the authors developed some functions for proportional symbol mapping using R, including mathematical and perceptual scaling. An example of these functions demonstrated the new expressive power and options available in R, particularly for the visualization of conceptual point data.",WOS:000235181200001,JOURNAL OF STATISTICAL SOFTWARE,,Proportional symbol mapping in R,2006
1585,"The ggfortify package provides a unified interface that enables users to use one line of code to visualize statistical results of many R packages using ggplot2 idioms. With the help of ggfortify, statisticians, data scientists, and researchers can avoid the sometimes repetitive work of using the ggplot2 syntax to achieve what they need.",WOS:000395669800032,R JOURNAL,,ggfortify: Unified Interface to Visualize Statistical Results of Popular R Packages,2016
1586,"Recent theoretical advances in nestedness analysis have led to the introduction of several alternative metrics to overcome most of the problems biasing the use of matrix 'temperature' calculated by Atmar's Nestedness Temperature Calculator. However, all of the currently available programs for nestedness analysis lack the user friendly appeal that has made the Nestedness Temperature Calculator one of the most popular community ecology programs. The software package NeD is an intuitive open source application for nestedness analysis that can be used online or locally under different operating systems. NeD is able to automatically handle different matrix formats, has batch functionalities and produces an output that can be easily copied and pasted to a spreadsheet. In addition to numerical results, NeD provides a graphic representation of the matrix under examination and of the corresponding maximally packed matrix. NeD allows users to select among the most used nestedness metrics, and to combine them with different null models. Integrating easiness of use with the recent theoretical advances in the field, NeD provides researchers not directly involved in theoretical debates with a simple yet robust statistical tool for a more conscious performance of nestedness analysis. NeD can be accessed at http//purl.ocic.org/ned.",WOS:000341808500001,JOURNAL OF STATISTICAL SOFTWARE,"['MATRICES', 'NETWORKS', 'SUBSETS']",Nestedness for Dummies (NeD): A User-Friendly Web Interface for Exploratory Nestedness Analysis,2014
1587,"Casella and Robert [Biometrika 83 (1996) 81-94] presented a general Rao-Blackwellization principle for accept-reject and Metropolis-Hastings schemes that leads to significant decreases in the variance of the resulting estimators, but at a high cost in computation and storage. Adopting a completely different perspective, we introduce instead a universal scheme that guarantees variance reductions in all Metropolis-Hastings-based estimators while keeping the computation cost under control. We establish a central limit theorem for the improved estimators and illustrate their performances on toy examples and on a probit model estimation.",WOS:000288183800008,ANNALS OF STATISTICS,"['WEIGHTED SAMPLES', 'MONTE-CARLO']",A VANILLA RAO-BLACKWELLIZATION OF METROPOLIS-HASTINGS ALGORITHMS,2011
1588,"Zero-inflation problem is very common in ecological studies as well as other areas. Nonparametric regression with zero-inflated data may be studied via the zero-inflated generalized additive model (ZIGAM), which assumes that the zero-inflated responses come from a probabilistic mixture of zero and a regular component whose distribution belongs to the 1-parameter exponential family. With the further assumption that the probability of non-zero-inflation is some monotonic function of the mean of the regular component, we propose the constrained zero-inflated generalized additive model (COZIGAM) for analyzing zero-inflated data. When the hypothesized constraint obtains, the new approach provides a unified framework for modeling zero-inflated data, which is more parsimonious and efficient than the unconstrained ZIGAM. We have developed an R package COZIGAM which contains functions that implement an iterative algorithm for fitting ZIGAMs and COZIGAMs to zero-inflated data based on the penalized likelihood approach. Other functions included in the package are useful for model prediction and model selection. We demonstrate the use of the COZIGAM package via some simulation studies and a real application.",WOS:000281588300001,JOURNAL OF STATISTICAL SOFTWARE,"['COUNT DATA', 'REGRESSION-MODELS', 'LIKELIHOOD']",Introducing COZIGAM: An R Package for Unconstrained and Constrained Zero-Inflated Generalized Additive Model Analysis,2010
1589,"NHPoisson is an R package for the modeling of nonhomogeneous Poisson processes in one dimension. It includes functions for data preparation, maximum likelihood estimation, covariate selection and inference based on asymptotic distributions and simulation methods. It also provides specific methods for the estimation of Poisson processes resulting from a peak over threshold approach. In addition, the package supports a wide range of model validation tools and functions for generating nonhomogenous Poisson process trajectories. This paper is a description of the package and aims to help those interested in modeling data using nonhomogeneous Poisson processes.",WOS:000352914400001,JOURNAL OF STATISTICAL SOFTWARE,"['RESIDUAL ANALYSIS', 'POINT-PROCESSES', 'SIMULATION', 'EVENTS', 'MODELS']",NHPoisson: An R Package for Fitting and Validating Nonhomogeneous Poisson Processes,2015
1590,"The increasing availability of cloud computing and scientific super computers brings great potential for making R accessible through public or shared resources. This allows us to efficiently run code requiring lots of cycles and memory, or embed R functionality into, e.g., systems and web services. However some important security concerns need to be addressed before this can be put in production. The prime use case in the design of R has always been a single statistician running R on the local machine through the interactive console. Therefore the execution environment of R is entirely unrestricted, which could result in malicious behavior or excessive use of hardware resources in a shared environment. Properly securing an R process turns out to be a complex problem. We describe various approaches and illustrate potential issues using some of our personal experiences in hosting public web services. Finally we introduce the RAppArmor package: a Linux based reference implementation for dynamic sandboxing in R on the level of the operating system.",WOS:000328129200001,JOURNAL OF STATISTICAL SOFTWARE,,The RAppArmor Package: Enforcing Security Policies in R Using Dynamic Sandboxing on Linux,2013
1591,"We present the R package missMDA which performs principal component methods on incomplete data sets, aiming to obtain scores, loadings and graphical representations despite missing values. Package methods include principal component analysis for continuous variables, multiple correspondence analysis for categorical variables, factorial analysis on mixed data for both continuous and categorical variables, and multiple factor analysis for multi-table data. Furthermore, missMDA can be used to perform single imputation to complete data involving continuous, categorical and mixed variables. A multiple imputation method is also available. In the principal component analysis framework, variability across different imputations is represented by confidence areas around the row and column positions on the graphical outputs. This allows assessment of the credibility of results obtained from incomplete data sets.",WOS:000373921300001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIPLE IMPUTATION', 'INCOMPLETE DATA', 'COMPONENT ANALYSIS', 'CROSS-VALIDATION', 'SOFTWARE', 'MODELS', 'PCA']",missMDA: A Package for Handling Missing Values in Multivariate Data Analysis,2016
1592,"Given a finite family of functions, the goal of model selection aggregation is to construct a procedure that mimics the function from this family that is the closest to an unknown regression function. More precisely, we consider a general regression model with fixed design and measure the distance between functions by the mean squared error at the design points. While procedures based on exponential weights are known to solve the problem of model selection aggregation in expectation, they are, surprisingly, sub-optimal in deviation. We propose a new formulation called Q-aggregation that addresses this limitation; namely, its solution leads to sharp oracle inequalities that are optimal in a minimax sense. Moreover, based on the new formulation, we design greedy Q-aggregation procedures that produce sparse aggregation models achieving the optimal rate. The convergence and performance of these greedy procedures are illustrated and compared with other standard methods on simulated examples.",WOS:000310650900022,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'MODEL SELECTION', 'OPTIMAL RATES', 'APPROXIMATION', 'SPARSITY', 'BOUNDS']",DEVIATION OPTIMAL LEARNING USING GREEDY Q-AGGREGATION,2012
1593,"We consider an independence feature screening technique for identifying explanatory variables that locally contribute to the response variable in high-dimensional regression analysis. Without requiring a specific parametric form of the underlying data model, our approach accommodates a wide spectrum of nonparametric and semiparametric model families. To detect the local contributions of explanatory variables, our approach constructs empirical likelihood locally in conjunction with marginal nonparametric regressions. Since our approach actually requires no estimation, it is advantageous in scenarios such as the single-index models where even specification and identification of a marginal model is an issue. By automatically incorporating the level of variation of the nonparametric regression and directly assessing the strength of data evidence supporting local contribution from each explanatory variable, our approach provides a unique perspective for solving feature screening problems. Theoretical analysis shows that our approach can handle data dimensionality growing exponentially with the sample size. With extensive theoretical illustrations and numerical examples, we show that the local independence screening approach performs promisingly.",WOS:000372594300003,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'DIMENSIONAL FEATURE SPACE', 'CONFIDENCE-INTERVALS', 'LINEAR-MODELS', 'VARIABLE SELECTION', 'LONGITUDINAL DATA', 'ADDITIVE-MODELS', 'REGRESSION']",LOCAL INDEPENDENCE FEATURE SCREENING FOR NONPARAMETRIC AND SEMIPARAMETRIC MODELS BY MARGINAL EMPIRICAL LIKELIHOOD,2016
1594,"The pampe package for R implements the panel data approach method for program evaluation designed to estimate the causal effects of political interventions or treatments. This procedure exploits the dependence among cross-sectional units to construct a counterfactual of the treated unit(s), and it is an appropriate method for research events that occur at an aggregate level like countries or regions and that affect only one or a small number of units. The implementation of the pampe package is illustrated using data from Hong Kong and 24 other units, by examining the economic impact of the political and economic integration of Hong Kong with mainland China in 1997 and 2004 respectively.",WOS:000368551800009,R JOURNAL,,An R Package for the Panel Approach Method for Program Evaluation: pampe,2015
1595,"Fechnerian scaling is a procedure for constructing a metric on a set of objects (e.g., colors, symbols, X-ray films, or even statistical models) to represent dissimilarities among the objects ""from the point of view"" of a system (e.g., person, technical device, or even computational algorithm) ""perceiving"" these objects. This metric, called Fechnerian, is computed from a data matrix of pairwise discrimination probabilities or any other pairwise measure which can be interpreted as the degree with which two objects within the set are discriminated from each other. This paper presents the package fechner for performing Fechnerian scaling of object sets in R. We describe the functions of the package. Fechnerian scaling then is demonstrated on real and artificial data sets accompanying the package.",WOS:000268991900001,JOURNAL OF STATISTICAL SOFTWARE,"['DISSIMILARITY CUMULATION THEORY', 'CONTINUOUS STIMULUS SPACES', 'CONNECTED SPACES', 'PSYCHOPHYSICS', 'SIMILARITY', 'DISTANCE', 'METRICS', 'PHYSICS', 'MODEL']",Fechnerian Scaling in R: The Package fechner,2009
1596,"Owing to its practicality as well as strong inferential properties, multiple imputation has been increasingly popular in the analysis of incomplete data. Methods that are not only computationally elegant but also applicable in wide spectrum of statistical incomplete data problems have also been increasingly implemented in a numerous computing environments. Unfortunately, however, the speed of this development has not been replicated in reaching to ""sophisticated"" users. While the researchers have been quite successful in developing the underlying software, documentation in a style that would be most reachable to the greater scientific society has been lacking. The main goal of this special volume is to close this gap by articles that illustrate these software developments. Here I provide a brief history of multiple imputation and relevant software and highlight the contents of the contributions. Potential directions for the future of the software development is also provided.",WOS:000298032200001,JOURNAL OF STATISTICAL SOFTWARE,['LIKELIHOOD'],State of the Multiple Imputation Software,2011
1597,"For my class of one-sided 1 - alpha confidence intervals with a certain monotonicity ordering on the random confidence limit, the smallest interval, in the sense of the set inclusion for the difference of two proportions of two independent binomial random variables, is constructed based on a direct analysis of coverage probability function. A special ordering on the confidence limit is developed and the corresponding smallest confidence interval is derived. This interval is then applied to identify the minimum effective dose (MED) for binary data in dose-response studies, and a multiple test procedure that controls the familywise error rate at level alpha is obtained. A generalization of constructing the smallest one-sided confidence interval to other discrete sample spaces is discussed in the presence of nuisance parameters.",WOS:000275510800022,ANNALS OF STATISTICS,"['MULTIPLE TEST PROCEDURES', 'DOSE-RESPONSE']",ON CONSTRUCTION OF THE SMALLEST ONE-SIDED CONFIDENCE INTERVAL FOR THE DIFFERENCE OF TWO PROPORTIONS,2010
1598,"The growth in the use of computationally intensive statistical procedures, especially with big data, has necessitated the usage of parallel computation on diverse platforms such as multicore, GPUs, clusters and clouds. However, slowdown due to interprocess communication costs typically limits such methods to ""embarrassingly parallel"" (EP) algorithms, especially on non-shared memory platforms. This paper develops a broadly-applicable method for converting many non-EP algorithms into statistically equivalent EP ones. The method is shown to yield excellent levels of speedup for a variety of statistical computations. It also overcomes certain problems of memory limitations.",WOS:000384914400001,JOURNAL OF STATISTICAL SOFTWARE,,Software Alchemy: Turning Complex Statistical Computations into Embarrassingly-Parallel Ones,2016
1599,"Empirical analysis of statistical algorithms often demands time-consuming experiments. We present two R packages which greatly simplify working in batch computing environments. The package Batch Jobs implements the basic objects and procedures to control any batch cluster from within R. It is structured around cluster versions of the well-known higher order functions Map, Reduce and Filter from functional programming. Computations are performed asynchronously and all job states are persistently stored in a database, which can be queried at any point in time. The second package, BatchExperiments, is tailored for the still very general scenario of analyzing arbitrary algorithms on problem instances. It extends package BatchJobs by letting the user define an array of jobs of the kind ""apply algorithm A to problem instance P and store results"". It is possible to associate statistical designs with parameters of problems and algorithms and therefore to systematically study their influence on the results.
The packages' main features are: (a) Convenient usage : All relevant batch system operations are either handled internally or mapped to simple R functions. (b) Portability : Both packages use a clear and well-de fined interface to the batch system which makes them applicable in most high-performance computing environments. (c) Reproducibility : Every computational part has an associated seed to ensure reproducibility even when the underlying batch system changes. (d) Abstraction an d good software design : The code layers for algorithms, experiment definitions and execution are cleanly separated and enable the writing of readable and maintainable code.",WOS:000352915700001,JOURNAL OF STATISTICAL SOFTWARE,,BatchJobs and BatchExperiments: Abstraction Mechanisms for Using R in Batch Environments,2015
1600,"The mpoly package is a general purpose collection of tools for symbolic computing with multivariate polynomials in R. In addition to basic arithmetic, mpoly can take derivatives of polynomials, compute Grobner bases of collections of polynomials, and convert polynomials into a functional form to be evaluated. Among other things, it is hoped that mpoly will provide an R-based foundation for the computational needs of algebraic statisticians.",WOS:000321944400016,R JOURNAL,,mpoly: Multivariate Polynomials in R,2013
1601,"Computerized adaptive testing is becoming increasingly popular due to advancement of modern computer technology. It differs from the conventional standardized testing in that the selection of test items is tailored to individual examinee's ability level. Arising from this selection strategy is a nonlinear sequential design problem. We study, in this paper, the sequential design problem in the context of the logistic item response theory models. We show that the adaptive design obtained by maximizing the item information leads to I consistent and asymptotically normal ability estimator in the case of the Rasch model. Modifications to the maximum information approach are proposed for the two- and three-parameter logistic models. Similar asymptotic properties are established for the modified designs and the resulting estimator. Examples are also given in the case of the two-parameter logistic model to show that without such modifications, the maximum likelihood estimator of the ability parameter may not be consistent.",WOS:000265619700013,ANNALS OF STATISTICS,"['LATENT TRAIT', 'BLOCKING', 'CAT']",NONLINEAR SEQUENTIAL DESIGNS FOR LOGISTIC ITEM RESPONSE THEORY MODELS WITH APPLICATIONS TO COMPUTERIZED ADAPTIVE TESTS,2009
1602,"This paper provides answers to questions regarding the almost sure limiting behavior of rooted, binary tree-structured rules for regression. Examples show that questions raised by Gordon and Olshen in 1984 have negative answers. For these examples of regression functions and sequences of their associated binary tree-structured approximations, for all regression functions except those in a set of the first category, almost sure consistency fails dramatically on events of full probability. One consequence is that almost sure consistency of binary tree-structured rules such as CART requires conditions beyond requiring that (1) the regression function be in L-1, (2) partitions of a Euclidean feature space be into polytopes with sides parallel to coordinate axes, (3) the mesh of the partitions becomes arbitrarily fine almost surely and (4) the empirical learning sample content of each polytope be ""large enough."" The material in this paper includes the solution to a problem raised by Dudley in discussions. The main results have a corollary regarding the lack of almost sure consistency of certain Bayes-risk consistent rules for classification.",WOS:000247498100001,ANNALS OF STATISTICS,['CONSISTENT NONPARAMETRIC REGRESSION'],Tree-structured regression and the differentiation of integrals,2007
1603,"The Ising model is a useful tool for studying complex interactions within a system. The estimation of such a model, however, is rather challenging, especially in the presence of high-dimensional parameters. In this work, we propose efficient procedures for learning a sparse Ising model based on a penalized composite conditional likelihood with nonconcave penalties. Nonconcave penalized likelihood estimation has received a lot of attention in recent years. However, such an approach is computationally prohibitive under high-dimensional Ising models. To overcome such difficulties, we extend the methodology and theory of nonconcave penalized likelihood to penalized composite conditional likelihood estimation. The proposed method can be efficiently implemented by taking advantage of coordinate-ascent and minorization-maximization principles. Asymptotic oracle properties of the proposed method are established with NP-dimensionality. Optimality of the computed local solution is discussed. We demonstrate its finite sample performance via simulation studies and further illustrate our proposal by studying the Human Immunodeficiency Virus type 1 protease structure based on data from the Stanford HIV drug resistance database. Our statistical learning results match the known biological findings very well, although no prior biological information is used in the data analysis procedure.",WOS:000310650900005,ANNALS OF STATISTICS,"['VIRUS TYPE-1 PROTEASE', 'VARIABLE SELECTION', 'LOGISTIC-REGRESSION', 'COORDINATE DESCENT', 'NP-DIMENSIONALITY', 'MUTATION PATTERNS', 'ORACLE PROPERTIES', 'DRUG-RESISTANCE', 'MM ALGORITHMS', 'LASSO']",NONCONCAVE PENALIZED COMPOSITE CONDITIONAL LIKELIHOOD ESTIMATION OF SPARSE ISING MODELS,2012
1604,"The paper is devoted to the problem of estimation of a univariate component in a heteroscedastic nonparametric multiple regression under the mean integrated squared error (MISE) criteria. The aim is to understand how the scale function should be used for estimation of the univariate component. It is known that the scale function does not affect the rate of the MISE convergence, and as a result sharp constants are explored. The paper begins with developing a sharp-minimax theory for a pivotal model Y = f (X) + sigma(X, Z)epsilon, where epsilon is standard normal and independent of the predictor X and the auxiliary vector-covariate Z. It is shown that if the scale sigma(x, z) depends on the auxiliary variable, then a special estimator, which uses the scale (or its estimate), is asymptotically sharp minimax and adaptive to unknown smoothness of f (x). This is an interesting conclusion because if the scale does not depend on the auxiliary covariate Z, then ignoring the heteroscedasticity can yield a sharp minimax estimation. The pivotal model serves as a natural benchmark for a general additive model Y = f (X) + g(Z) + sigma(X, Z)epsilon, where epsilon may depend on (X, Z) and have only a finite fourth moment. It is shown that for this model a data-driven estimator can perform as well as for the benchmark. Furthermore, the estimator, suggested for continuous responses, can be also used for the case of discrete responses. Bernoulli and Poisson regressions, that are inherently heteroscedastic, are particular considered examples for which sharp minimax lower bounds are obtained as well. A numerical study shows that the asymptotic theory sheds light on small samples.",WOS:000323271500007,ANNALS OF STATISTICS,"['ADDITIVE REGRESSION', 'FOURIER-SERIES', 'MODELS', 'DENSITY', 'DESIGN']",NONPARAMETRIC REGRESSION WITH THE SCALE DEPENDING ON AUXILIARY VARIABLE,2013
1605,"We introduce a new M A T L A B software package that implements several recently proposed likelihood-based methods for sufficient dimension reduction. Current capabilities include estimation of reduced subspaces with a fixed dimension d, as well as estimation of d by use of likelihood-ratio testing, permutation testing and information criteria. The methods are suitable for preprocessing data for both regression and classification. Implementations of related estimators are also available. Although the software is more oriented to command-line operation, a graphical user interface is also provided for prototype computations.",WOS:000287815300001,JOURNAL OF STATISTICAL SOFTWARE,"['SLICED INVERSE REGRESSION', 'DISCRIMINANT-ANALYSIS', 'MODELS', 'VISUALIZATION']",LDR: A Package for Likelihood-Based Sufficient Dimension Reduction,2011
1606,"A computational tool for testing for a dose-related trend and/or a pairwise difference in the incidence of an occult tumor via an age-adjusted bootstrap-based poly-k test and the original poly-k test is presented in this paper. The poly-k test ( Bailer and Portier 1988) is a survival-adjusted Cochran-Armitage test, which achieves robustness to effects of differential mortality across dose groups. The original poly-k test is asymptotically standard normal under the null hypothesis. However, the asymptotic normality is not valid if there is a deviation from the tumor onset distribution that is assumed in this test. Our age-adjusted bootstrap-based poly-k test assesses the significance of assumed asymptotic normal tests and investigates an empirical distribution of the original poly-k test statistic using an age-adjusted bootstrap method. A tumor of interest is an occult tumor for which the time to onset is not directly observable. Since most of the animal carcinogenicity studies are designed with a single terminal sacrifice, the present tool is applicable to rodent tumorigenicity assays that have a single terminal sacrifice. The present tool takes input information simply from a user screen and reports testing results back to the screen through a user-interface. The computational tool is implemented in C/C++ and is applied to analyze a real data set as an example. Our tool enables the FDA and the pharmaceutical industry to implement a statistical analysis of tumorigenicity data from animal bioassays via our age-adjusted bootstrap-based poly-k test and the original poly-k test which has been adopted by the National Toxicology Program as its standard statistical test.",WOS:000240206000001,JOURNAL OF STATISTICAL SOFTWARE,"['CARCINOGENICITY', 'BIOASSAYS', 'DISEASE']",A computational tool for testing dose-related trend using an age-adjusted bootstrap-based poly-k test,2006
1607,"The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice 2.9, which extends the functionality of mice 1.0 in several ways. In mice 2.9, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice 2.9 adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice 2.9 can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.",WOS:000298032500001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIPLY-IMPUTED DATA', 'PERCUTANEOUS CORONARY INTERVENTION', 'FULLY CONDITIONAL SPECIFICATION', 'ACTIVE ANTIRETROVIRAL THERAPY', 'MISSING-DATA', 'PROGNOSTIC MODEL', 'PHYSICAL-ACTIVITY', 'SURVIVAL ANALYSIS', 'BIRTH COHORT', 'RHEUMATOID-ARTHRITIS']",mice: Multivariate Imputation by Chained Equations in R,2011
1608,"A novel block prior is proposed for adaptive Bayesian estimation. The prior does not depend on the smoothness of the function or the sample size. It puts sufficient prior mass near the true signal and automatically concentrates on its effective dimension. A rate-optimal posterior contraction is obtained in a general framework, which includes density estimation, white noise model, Gaussian sequence model, Gaussian regression and spectral density estimation.",WOS:000368022000011,ANNALS OF STATISTICS,"['DIMENSIONAL EXPONENTIAL-FAMILIES', 'POSTERIOR CONCENTRATION RATES', 'GAUSSIAN WHITE-NOISE', 'DENSITY-ESTIMATION', 'ASYMPTOTIC EQUIVALENCE', 'CONVERGENCE-RATES', 'NONPARAMETRIC REGRESSION', 'DIRICHLET MIXTURES', 'DISTRIBUTIONS', 'INFERENCE']",RATE EXACT BAYESIAN ADAPTATION WITH MODIFIED BLOCK PRIORS,2016
1609,"We study in this paper a smoothness regularization method for functional linear regression and provide a unified treatment for both the prediction and estimation problems. By developing a tool on simultaneous diagonalization of two positive definite kernels, we obtain shaper results on the minimax rates of convergence and show that smoothness regularized estimators achieve the optimal rates of convergence for both prediction and estimation under conditions weaker than those for the functional principal components based methods developed in the literature. Despite the generality of the method of regularization, we show that the procedure is easily implementable. Numerical results are obtained to illustrate the merits of the method and to demonstrate the theoretical developments.",WOS:000290231500003,ANNALS OF STATISTICS,"['PENALIZED LIKELIHOOD', 'CORRELATED ERRORS', 'ESTIMATORS', 'MODELS', 'CONVERGENCE', 'DESIGNS', 'RATES']",A REPRODUCING KERNEL HILBERT SPACE APPROACH TO FUNCTIONAL LINEAR REGRESSION,2010
1610,"Let the design of an experiment be represented by an s-dimensional vector w of weights with nonnegative components. Let the quality of w for the estimation of the parameters of the statistical model be measured by the criterion of D-optimality, defined as the mth root of the determinant of the information matrix M(w) = Sigma(s)(i=1) w(i)A(i)A(i)(T), where A(i), i = 1, ... , s are known matrices with in rows.
In this paper, we show that the criterion of D-optimality is second-order cone representable. As a result, the method of second-order cone programming can be used to compute an approximate D-optimal design with any system of linear constraints on the vector of weights. More importantly, the proposed characterization allows us to compute an exact D-optimal design, which is possible thanks to high-quality branch-and-cut solvers specialized to solve mixed integer second-order cone programming problems. Our results extend to the case of the criterion of D-K-optimality, which measures the quality of w for the estimation of a linear parameter subsystem defined by a full-rank coefficient matrix K.
We prove that some other widely used criteria are also second-order cone representable, for instance, the criteria of A-, A(K)-, G- and I-optimality.
We present several numerical examples demonstrating the efficiency and general applicability of the proposed method. We show that in many cases the mixed integer second-order cone programming approach allows us to find a provably optimal exact design, while the standard heuristics systematically miss the optimum.",WOS:000362697700012,ANNALS OF STATISTICS,"['NONLINEAR MODELS', 'CONSTRUCTION', 'ALGORITHM', 'OPTIMIZATION', 'REGRESSION', 'CRITERIA']",COMPUTING EXACT D-OPTIMAL DESIGNS BY MIXED INTEGER SECOND-ORDER CONE PROGRAMMING,2015
1611,"The causal inference literature has provided a clear formal definition of confounding expressed in terms of counterfactual independence. The literature has not, however, come to any consensus on a formal definition of a confounder, as it has given priority to the concept of confounding over that of a confounder. We consider a number of candidate definitions arising from various more informal statements made in the literature. We consider the properties satisfied by each candidate definition, principally focusing on (i) whether under the candidate definition control for all ""confounders"" suffices to control for ""confounding"" and (ii) whether each confounder in some context helps eliminate or reduce confounding bias. Several of the candidate definitions do not have these two properties. Only one candidate definition of those considered satisfies both properties. We propose that a ""confounder"" be defined as a pre-exposure covariate C for which there exists a set of other covariates X such that effect of the exposure on the outcome is unconfounded conditional on (X, C) but such that for no proper subset of (X, C) is the effect of the exposure on the outcome unconfounded given the subset. We also provide a conditional analogue of the above definition; and we propose a variable that helps reduce bias but not eliminate bias be referred to as a ""surrogate confounder."" These definitions are closely related to those given by Robins and Morgenstern [Comput. Math. Appl. 14 (1987) 869-916]. The implications that hold among the various candidate definitions are discussed.",WOS:000317451200008,ANNALS OF STATISTICS,"['CAUSAL DIAGRAMS', 'INFERENCE', 'COLLAPSIBILITY', 'STRATIFICATION', 'EPIDEMIOLOGY', 'KNOWLEDGE', 'SELECTION', 'MODELS', 'SCORE']",ON THE DEFINITION OF A CONFOUNDER,2013
1612,"We add a set of convex constraints to the lasso to produce sparse interaction models that honor the hierarchy restriction that an interaction only be included in a model if one or both variables are marginally important. We give a precise characterization of the effect of this hierarchy constraint, prove that hierarchy holds with probability one and derive an unbiased estimate for the degrees of freedom of our estimator. A bound on this estimate reveals the amount of fitting ""saved"" by the hierarchy constraint.
We distinguish between parameter sparsity-the number of nonzero coefficients-and practical sparsity-the number of raw variables one must measure to make a new prediction. Hierarchy focuses on the latter, which is more closely tied to important data collection concerns such as cost, time and effort. We develop an algorithm, available in the R package hierNet, and perform an empirical study of our method.",WOS:000321847600003,ANNALS OF STATISTICS,"['STRUCTURED VARIABLE SELECTION', 'LEAST ANGLE REGRESSION', 'DESIGNED EXPERIMENTS', 'COORDINATE DESCENT', 'LINEAR-MODELS', 'REGULARIZATION', 'PREDICTORS', 'SHRINKAGE', 'SPARSITY', 'FREEDOM']",A LASSO FOR HIERARCHICAL INTERACTIONS,2013
1613,"We derive a nonparametric estimator of the jump-activity index beta of a ""locally-stable"" pure-jump Ito semimartingale from discrete observations of the process on a fixed time interval with mesh of the observation grid shrinking to zero. The estimator is based on the empirical characteristic function of the increments of the process scaled by local power variations formed from blocks of increments spanning shrinking time intervals preceding the increments to be scaled. The scaling serves two purposes: (1) it controls for the time variation in the jump compensator around zero, and (2) it ensures self-normalization, that is, that the limit of the characteristic function-based estimator converges to a nondegenerate limit which depends only on beta. The proposed estimator leads to nontrivial efficiency gains over existing estimators based on power variations. In the Levy case, the asymptotic variance decreases multiple times for higher values of beta. The limiting asymptotic variance of the proposed estimator, unlike that of the existing power variation based estimators, is constant. This leads to further efficiency gains in the case when the characteristics of the semimartingale are stochastic. Finally, in the limiting case of beta = 2, which corresponds to jump-diffusion, our estimator of beta can achieve a faster rate than existing estimators.",WOS:000357441000020,ANNALS OF STATISTICS,"['HIGH-FREQUENCY DATA', 'STOCHASTIC VOLATILITY MODELS', 'LIMIT-THEOREMS', 'LEVY PROCESSES', 'ACTIVITY INDEX', 'INFERENCE', 'ORDER']",JUMP ACTIVITY ESTIMATION FOR PURE-JUMP SEMIMARTINGALES VIA SELF-NORMALIZED STATISTICS,2015
1614,,WOS:000342481700016,ANNALS OF STATISTICS,['BAYESIAN NETWORKS'],"We provide a correction to the expression for scoring Gaussian directed acyclic graphical models derived in Geiger and Heckerman (vol 30, pg 1414, 2002)",2014
1615,"The class of beta regression models is commonly used by practitioners to model variables that assume values in the standard unit interval (0, 1). It is based on the assumption that the dependent variable is beta-distributed and that its mean is related to a set of regressors through a linear predictor with unknown coefficients and a link function. The model also includes a precision parameter which may be constant or depend on a (potentially different) set of regressors through a link function as well. This approach naturally incorporates features such as heteroskedasticity or skewness which are commonly observed in data taking values in the standard unit interval, such as rates or proportions. This paper describes the betareg package which provides the class of beta regressions in the R system for statistical computing. The underlying theory is briefly outlined, the implementation discussed and illustrated in various replication exercises.",WOS:000276707000001,JOURNAL OF STATISTICAL SOFTWARE,"['HETEROSCEDASTICITY', 'PROPORTIONS', 'MODELS', 'ERRORS', 'TESTS']",Beta Regression in R,2010
1616,,WOS:000208590000001,R JOURNAL,,Untitled,2010
1617,"For classification problems with significant class imbalance, subsampling can reduce computational costs at the price of inflated variance in estimating model parameters. We propose a method for subsampling efficiently for logistic regression by adjusting the class balance locally in feature space via an accept reject scheme. Our method generalizes standard case-control sampling, using a pilot estimate to preferentially select examples, whose, responses are conditionally rare given their features. The biased subsampling is corrected by a post-hoc analytic adjustment to the parameters. The method is simple and requires one parallelizable scan over the full data set.
Standard case-control sampling is inconsistent under model misspecification for the population risk-minimizing coefficients theta*. By contrast, our estimator is consistent for theta* provided that the pilot estimate is. Moreover, under correct specification and with a consistent, independent pilot estimate, our estimator has exactly twice the asymptotic variance of the full-sample MLE-even if the selected subsample comprises a miniscule fraction of the full data set, as happens when the original data are severely imbalanced. The factor of two improves to 1 + 1/c we multiply the baseline acceptance probabilities by c > 1 (and weight points with acceptance probability greater than 1), taking roughly 1+c/2 times as many data points into the subsample. Experiments on simulated and real data show that our method can substantially outperform standard case-control subsampling.",WOS:000344632400001,ANNALS OF STATISTICS,"['LOGISTIC-REGRESSION', 'MODELS', 'DISEASE']",LOCAL CASE-CONTROL SAMPLING: EFFICIENT SUBSAMPLING IN IMBALANCED DATA SETS,2014
1618,"This is a macro which facilitates remote execution of WinBUGS from within SAS. The macro pre-processes data for WinBUGS, writes the WinBUGS batch-script, executes this script and reads in output statistics from the WinBUGS log-file back into SAS native format. The user specifies the input and output file names and directory path as well as the statistics to be monitored in WinBUGS. The code works best for a model that has already been set up and checked for convergence diagnostics within WinBUGS. An obvious extension of the use of this macro is for running simulations where the input and output files all have the same name but all that differs between simulation iterations is the input dataset. The functionality and syntax of the macro call are described in this paper and illustrated using a simple linear regression model.",WOS:000252431900001,JOURNAL OF STATISTICAL SOFTWARE,,WinBUGSio: A SAS macro for the remote execution of WinBUGS,2007
1619,"ConvergenceConcepts is an R package, built upon the tkrplot, tcltk and lattice packages, designed to investigate the convergence of simulated sequences of random variables. Four classical modes of convergence may be studied, namely: almost sure convergence (a.s.), convergence in probability (P), convergence in law (L) and convergence in r-th mean (r). This investigation is performed through accurate graphical representations. This package may be used as a pedagogical tool. It may give students a better understanding of these notions and help them to visualize these difficult theoretical concepts. Moreover, some scholars could gain some insight into the behaviour of some random sequences they are interested in.",WOS:000208589800004,R JOURNAL,,ConvergenceConcepts: An R Package to Investigate Various Modes of Convergence,2009
1620,"Deriving optimal designs for nonlinear models is, in general, challenging. One crucial step is to determine the number of support points needed. Current tools handle this on a case-by-case basis. Each combination of model, optimality criterion and objective requires its own proof. The celebrated de la Garza Phenomenon states that under a (p - 1)th-degree polynomial regression model, any optimal design can be based on at most p design points, the minimum number of support points such that all parameters are estimable. Does this conclusion also hold for nonlinear models? If the answer is yes, it would be relatively easy to derive any optimal design, analytically or numerically. In this paper, a novel approach is developed to address this question. Using this new approach, it can be easily shown that the de la Garza phenomenon exists for many commonly studied nonlinear models, such as the Emax model, exponential model, three- and four-parameter log-linear models, Emax-PK1 model, as well as many classical polynomial regression models. The proposed approach unities and extends many well-known results in the optimal design literature. It has four advantages over current tools: (i) it can be applied to many forms of nonlinear models; to continuous or discrete data; to data with homogeneous or nonhomogeneous errors; (ii) it can be applied to any design region; (iii) it can be applied to multiple-stage optimal design and (iv) it can be easily implemented.",WOS:000280359400018,ANNALS OF STATISTICS,"['D-OPTIMAL DESIGNS', 'LOCALLY OPTIMAL DESIGNS', 'REGRESSION-MODELS']",ON THE DE LA GARZA PHENOMENON,2010
1621,"SNPMClust is an R package for genotype clustering and calling with Illumina microarrays. It was originally developed for studies using the GoldenGate custom genotyping platform but can be used with other Illumina platforms, including Infinium BeadChip. The algorithm first rescales the fluorescent signal intensity data, adds empirically derived pseudo-data to minor allele genotype clusters, then uses the package mclust for bivariate Gaussian model fitting. We compared the accuracy and sensitivity of SNPMClust to that of GenCall, Illumina's proprietary algorithm, on a data set of 94 whole-genome amplified buccal (cheek swab) DNA samples. These samples were genotyped on a custom panel which included 1064 SNPs for which the true genotype was known with high confidence. SNPMClust produced uniformly lower false call rates over a wide range of overall call rates.",WOS:000389073700001,JOURNAL OF STATISTICAL SOFTWARE,"['CONOTRUNCAL HEART-DEFECTS', 'HOMOCYSTEINE', 'ALGORITHM', 'GENES']",SNPMClust: Bivariate Gaussian Genotype Clustering and Calling for Illumina Microarrays,2016
1622,"This paper is concerned with solving nonconvex learning problems with folded concave penalty. Despite that their global solutions entail desirable statistical properties, they lack optimization techniques that guarantee global optimality in a general setting. In this paper, we show that a class of nonconvex learning problems are equivalent to general quadratic programs. This equivalence facilitates us in developing mixed integer linear programming reformulations, which admit finite algorithms that find a provably global optimal solution. We refer to this reformulation-based technique as the mixed integer programming-based global optimization (MIPGO). To our knowledge, this is the first global optimization scheme with a theoretical guarantee for folded concave penalized nonconvex learning with the SCAD penalty [J. Amer. Statist. Assoc. 96 (2001) 1348-1360] and the MCP penalty [Ann. Statist. 38 (2001) 894-942]. Numerical results indicate a significant outperformance of MIPGO over the state-of-the-art solution scheme, local linear approximation and other alternative solution techniques in literature in terms of solution quality.",WOS:000372594300007,ANNALS OF STATISTICS,"['VARIABLE SELECTION', 'ORACLE PROPERTIES', 'ALGORITHMS', 'LIKELIHOOD', 'REGRESSION', 'LASSO']",GLOBAL SOLUTIONS TO FOLDED CONCAVE PENALIZED NONCONVEX LEARNING,2016
1623,"Suppose that we observe entries or, more generally, linear combinations of entries of an unknown m x T-matrix A corrupted by noise. We are particularly interested in the high-dimensional setting where the number mT of unknown entries can be much larger than the sample size N. Motivated by several applications, we consider estimation of matrix A under the assumption that it has small rank. This can be viewed as dimension reduction or sparsity assumption. In order to shrink toward a low-rank representation, we investigate penalized least squares estimators with a Schatten-p quasi-norm penalty term, p <= 1. We study these estimators under two possible assumptions-a modified version of the restricted isometry condition and a uniform bound on the ratio ""empirical norm induced by the sampling operator/Frobenius norm."" The main results are stated as nonasymptotic upper bounds on the prediction risk and on the Schatten-q risk of the estimators, where q is an element of [p, 2]. The rates that we obtain for the prediction risk are of the form rmIN (for m = T), up to logarithmic factors, where r is the rank of A. The particular examples of multi-task learning and matrix completion are worked out in detail. The proofs are based on tools from the theory of empirical processes. As a by-product, we derive bounds for the kth entropy numbers of the quasi-convex Schatten class embeddings, S(p)(M) -> S(2)(M) p < 1, which are of independent interest.",WOS:000291183300008,ANNALS OF STATISTICS,"['NORM MINIMIZATION', 'TRACE-NORM', 'LASSO', 'AGGREGATION', 'CONSISTENCY', 'REGRESSION', 'SELECTION', 'ENTROPY']",ESTIMATION OF HIGH-DIMENSIONAL LOW-RANK MATRICES,2011
1624,"Many scientific and economic problems involve the analysis of high-dimensional time series datasets. However, theoretical studies in high-dimensional statistics to date rely primarily on the assumption of independent and identically distributed (i.i.d.) samples. In this work, we focus on stable Gaussian processes and investigate the theoretical properties of l(1)-regularized estimates in two important statistical problems in the context of high-dimensional time series: (a) stochastic regression with serially correlated errors and (b) transition matrix estimation in vector autoregressive (VAR) models. We derive nonasymptotic upper bounds on the estimation errors of the regularized estimates and establish that consistent estimation under high-dimensional scaling is possible via l(1)-regularization for a large class of stable processes under sparsity constraints. A key technical contribution of the work is to introduce a measure of stability for stationary processes using their spectral properties that provides insight into the effect of dependence on the accuracy of the regularized estimates. With this proposed stability measure, we establish some useful deviation bounds for dependent data, which can be used to study several important regularized estimates in a time series setting.",WOS:000357441000010,ANNALS OF STATISTICS,"['VARIABLE SELECTION', 'LASSO', 'COVARIANCE', 'PENALTY']",REGULARIZED ESTIMATION IN SPARSE HIGH-DIMENSIONAL TIME SERIES MODELS,2015
1625,"The aim of this contribution is to connect two previously separated worlds: robotic application development with the Robot Operating System (ROS) and statistical programming with R. This fruitful combination becomes apparent especially in the analysis and visualization of sensory data. We therefore introduce a new language extension for ROS that allows to implement nodes in pure R. All relevant aspects are described in a step-by-step development of a common sensor data transformation node. This includes the reception of raw sensory data via the ROS network, message interpretation, bag-file analysis, transformation and visualization, as well as the transmission of newly generated messages back into the ROS network.",WOS:000330193300013,R JOURNAL,,The R in Robotics rosR: A New Language Extension for the Robot Operating System,2013
1626,"Graphical user interfaces (GUIs) are growing in popularity as a complement or alternative to the traditional command line interfaces to R. RGtk2 is an R package for creating GUIs in R. The package provides programmatic access to GTK+ 2.0, an open-source GUI tool kit written in C. To construct a GUI, the R programmer calls RGtk2 functions that map to functions in the underlying GTK+ library. This paper introduces the basic concepts underlying GTK+ and explains how to use RGtk2 to construct GUIs from R. The tutorial is based on simple and pratical programming examples. We also provide more complex examples illustrating the advanced features of the package. The design of the RGtk2 API and the low-level interface from R to GTK+ are discussed at length. We compare RGtk2 to alternative GUI tool kits for R.",WOS:000285847300001,JOURNAL OF STATISTICAL SOFTWARE,,RGtk2: A Graphical User Interface Tool kit for R,2010
1627,"Item response theory models are often applied when a number items are used to measure a unidimensional latent variable. Originally proposed and used within educational research, they are also used when focus is on physical functioning or psychological wellbeing. Modern applications often need more general models, typically models for multidimensional latent variables or longitudinal models for repeated measurements. This paper describes a SAS macro that fits two-dimensional polytomous Rasch models using a specification of the model that is sufficiently flexible to accommodate longitudinal Rasch models. The macro estimates item parameters using marginal maximum likelihood estimation. A graphical presentation of item characteristic curves is included.",WOS:000365986100001,JOURNAL OF STATISTICAL SOFTWARE,"['ITEM PARAMETER DRIFT', 'RESPONSE THEORY', 'DEPENDENCE', 'PROGRAM', 'TIME', 'LIFE']",%lrasch_mml: A SAS Macro for Marginal Maximum Likelihood Estimation in Longitudinal Polytomous Rasch Models,2015
1628,"A data-driven block thresholding procedure for wavelet regression is proposed and its theoretical and numerical properties are investigated. The procedure empirically chooses the block size and threshold level at each resolution level by minimizing Stein's unbiased risk estimate. The estimator is sharp adaptive over a class of Besov bodies and achieves simultaneously within a small constant factor of the minimax risk over a wide collection of Besov Bodies including both the ""dense"" and ""sparse"" cases. The procedure is easy to implement. Numerical results show that it has, superior finite sample performance in comparison to the other leading wavelet thresholding estimators.",WOS:000265500500001,ANNALS OF STATISTICS,"['NONNEGATIVE GARROTE', 'MINIMAX RISK', 'SHRINKAGE', 'ADAPTIVITY', 'REGRESSION']",A DATA-DRIVEN BLOCK THRESHOLDING APPROACH TO WAVELET ESTIMATION,2009
1629,"This paper presents R utilities for computing and displaying isosurfaces, or three-dimensional contour surfaces, from a three-dimensional array of function values. A version of the marching cubes algorithm that takes into account face and internal ambiguities is used to compute the isosurfaces. Vectorization is used to ensure adequate performance using only R code. Examples are presented showing contours of theoretical densities, density estimatates,and medical imaging data. Rendering can use the rgl package or standard or grid graphics, and a set of tools for representing and rendering surfaces using standard or grid graphics is presented.",WOS:000259616700001,JOURNAL OF STATISTICAL SOFTWARE,,Computing and displaying isosurfaces in R,2008
1630,"We characterize the convergence of the Gibbs sampler which samples from the joint posterior distribution of parameters and missing data in hierarchical linear models with arbitrary symmetric error distributions. We show that the convergence can be uniform, geometric or subgeometric depending on the relative tail behavior of the error distributions, and on the parametrization chosen. Our theory is applied to characterize the convergence of the Gibbs sampler on latent Gaussian process models. We indicate how the theoretical framework we introduce will be useful in analyzing more complex models.",WOS:000253390000004,ANNALS OF STATISTICS,"['CHAIN MONTE-CARLO', 'GAUSSIAN STATE-SPACE', 'LINEAR MIXED MODELS', 'MARKOV-CHAINS', 'DISTRIBUTIONS', 'AUGMENTATION', 'PARAMETER', 'INFERENCE', 'SCHEMES']",Stability of the Gibbs sampler for Bayesian hierarchical models,2008
1631,Efficient rational arithmetic methods that can exactly evaluate the cumulative sampling distribution of the one-sided one sample Kolmogorov-Smirnov (K-S) test have been developed by Brown and Harvey (2007) for sample sizes n up to fifty thou sand. This paper implements in arbitrary precision the same 13 formulae to evaluate the one-sided one sample K-S cumulative sampling distribution. Computational experience identifies the fastest implementation which is then used to calculate confidence interval bandwidths and p values for sample sizes up to ten million.,WOS:000257322500001,JOURNAL OF STATISTICAL SOFTWARE,"['SMIRNOV TYPE STATISTICS', 'KOLMOGOROV']",Arbitrary precision mathematica functions to evaluate the one-sided one sample K-S cumulative sampling distribution,2008
1632,The distance covariance function is a new measure of dependence between random vectors. We drop the assumption of iid data to introduce distance covariance for time series. The R package dCovTS provides functions that compute and plot distance covariance and correlation functions for both univariate and multivariate time series. Additionally it includes functions for testing serial independence based on distance covariance. This paper describes the theoretical background of distance covariance methodology in time series and discusses in detail the implementation of these methods with the R package dCovTS.,WOS:000395669800021,R JOURNAL,"['DEPENDENT WILD BOOTSTRAP', 'STATISTICS', 'COVARIANCE', 'DEGENERATE']",dCovTS: Distance Covariance/Correlation for Time Series,2016
1633,"We present some ways of using R in Excel and build an example application using the package rpart. Starting with simple interactive use of rpart in Excel, we eventually package the code into an Excel-based application, hiding all details (including R itself) from the end user. In the end, our application implements a service-oriented architecture (SOA) with a clean separation of presentation and computation layer.",WOS:000208590200002,R JOURNAL,,Creating and Deploying an Application with (R)Excel and R,2011
1634,"The runjags package provides a set of interface functions to facilitate running Markov chain Monte Carlo models in JAGS from within R. Automated calculation of appropriate convergence and sample length diagnostics, user-friendly access to commonly used graphical outputs and summary statistics, and parallelized methods of running JAGS are provided. Template model specifications can be generated using a standard lme4-style formula interface to assist users less familiar with the BUGS syntax. Automated simulation study functions are implemented to facilitate model performance assessment, as well as drop-k type cross-validation studies, using high performance computing clusters such as those provided by parallel. A module extension for JAGS is also included within runjags, providing the Pareto family of distributions and a series of minimally-informative priors including the DuMouchel and half-Cauchy priors. This paper outlines the primary functions of this package, and gives an illustration of a simulation study to assess the sensitivity of two equivalent model formulations to different prior distributions.",WOS:000384915700001,JOURNAL OF STATISTICAL SOFTWARE,"['CHAIN MONTE-CARLO', 'HIERARCHICAL-MODELS', 'CONVERGENCE', 'VARIANCE', 'EXAMPLE']","runjags: An R Package Providing Interface Utilities, Model Templates, Parallel Computing Methods and Additional Distributions for MCMC Models in JAGS",2016
1635,"In this paper we study the consistency of different bootstrap procedures for constructing confidence intervals (CIs) for the unique jump discontinuity (change-point) in an otherwise smooth regression function in a stochastic design setting. This problem exhibits nonstandard asymptotics, and we argue that the standard bootstrap procedures in regression fail to provide valid confidence intervals for the change-point. We propose a version of smoothed bootstrap, illustrate its remarkable finite sample performance in our simulation study and prove the consistency of the procedure. The m out of it bootstrap procedure is also considered and shown to be consistent. We also provide sufficient conditions for any bootstrap procedure to be consistent in this scenario.",WOS:000293716500009,ANNALS OF STATISTICS,"['MODELS', 'CONVERGENCE', 'ESTIMATORS', 'COVARIATE', 'THRESHOLD', 'CURVES', 'WEAK']",CHANGE-POINT IN STOCHASTIC DESIGN REGRESSION AND THE BOOTSTRAP,2011
1636,"Multistage sampling is commonly used for household surveys when there exists no sampling frame, or when the population is scattered over a wide area. Multistage sampling usually introduces a complex dependence in the selection of the final units, which makes asymptotic results quite difficult to prove. In this work, we consider multistage sampling with simple random without replacement sampling at the first stage, and with an arbitrary sampling design for further stages. We consider coupling methods to link this sampling design to sampling designs where the primary sampling units are selected independently. We first generalize a method introduced by [Magyar Tud. Akad. Mat. Kutato Int. Kozl. 5 (1960) 361-374] to get a coupling with multistage sampling and Bernoulli sampling at the first stage, which leads to a central limit theorem for the Horvitz-Thompson estimator. We then introduce a new coupling method with multistage sampling and simple random with replacement sampling at the first stage. When the first-stage sampling fraction tends to zero, this method is used to prove consistency of a with-replacement bootstrap for simple random without replacement sampling at the first stage, and consistency of bootstrap variance estimators for smooth functions of totals.",WOS:000363437900006,ANNALS OF STATISTICS,"['ASYMPTOTIC NORMALITY', 'FINITE POPULATION', 'VARYING PROBABILITIES', 'BOOTSTRAP', 'REPLACEMENT', 'INFERENCE', 'DESIGNS', '2-STAGE']",COUPLING METHODS FOR MULTISTAGE SAMPLING,2015
1637,We use the fitted Pareto law to construct an accompanying approximation of the excess distribution function. A selection rule of the location of the excess distribution function is proposed based on a stagewise lack-of-fit testing procedure. Our main result is an oracle type inequality for the Kullback-Leibler loss.,WOS:000258243000008,ANNALS OF STATISTICS,"['REGULAR VARIATION', 'SAMPLE FRACTION', 'PARAMETERS', 'INDEX', 'CONVERGENCE', 'BOOTSTRAP', 'RATES']",Statistics of extremes by oracle estimation,2008
1638,"Distance covariance and distance correlation are scalar coefficients that characterize independence of random vectors in arbitrary dimension. Properties, extensions and applications of distance correlation have been discussed in the recent literature, but the problem of defining the partial distance correlation has remained an open question of considerable interest. The problem of partial distance correlation is more complex than partial correlation partly because the squared distance covariance is not an inner product in the usual linear space. For the definition of partial distance correlation, we introduce a new Hilbert space where the squared distance covariance is the inner product. We define the partial distance correlation statistics with the help of this Hilbert space, and develop and implement a test for zero partial distance correlation. Our intermediate results provide an unbiased estimator of squared distance covariance, and a neat solution to the problem of distance correlation for dissimilarities rather than distances.",WOS:000345884900008,ANNALS OF STATISTICS,"['COVARIANCE', 'INDEPENDENCE', 'STATISTICS', 'DEPENDENCE', 'HETEROSIS', 'TESTS']",PARTIAL DISTANCE CORRELATION WITH METHODS FOR DISSIMILARITIES,2014
1639,"We study the problem of estimating the ridges of a density function. Ridge estimation is an extension of mode finding and is useful for understanding the structure of a density. It can also be used to find hidden structure in point cloud data. We show that, under mild regularity conditions, the ridges of the kernel density estimator consistently estimate the ridges of the true density. When the data are noisy measurements of a manifold, we show that the ridges are close and topologically similar to the hidden manifold. To find the estimated ridges in practice, we adapt the modified mean-shift algorithm proposed by Ozertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical experiments verify that the algorithm is accurate.",WOS:000342481700010,ANNALS OF STATISTICS,"['NONLINEAR DIMENSIONALITY REDUCTION', 'MANIFOLD ESTIMATION', 'MEAN SHIFT', 'DENSITY', 'MODE', 'DECONVOLUTION', 'GRADIENT', 'LINES']",NONPARAMETRIC RIDGE ESTIMATION,2014
1640,"In this paper, we describe the R package rTableICC that provides an interface for random generation of 2 x 2 x K and R x C contingency tables constructed over either intraclass-correlated or uncorrelated individuals. Intraclass correlations arise in studies where sampling units include more than one individual and these individuals are correlated. The package implements random generation of contingency tables over individuals with or without intraclass correlations under various sampling plans. The package include two functions for the generation of K 2 x 2 tables over product-multinomial sampling schemes and that of 2 x 2 x K tables under Poisson or multinomial sampling plans. It also contains two functions that generate R x C tables under product-multinomial, multinomial or Poisson sampling plans with or without intraclass correlations. The package also includes a function for random number generation from a given probability distribution. In addition to the contingency table format, the package also provides raw data required for further estimation purposes.",WOS:000385276100005,R JOURNAL,"['LOG ODDS RATIOS', 'INTRACLASS CORRELATION', 'BAYESIAN-ESTIMATION']",rTableICC: An R Package for Random Generation of 2 x 2 x K and R x C Contingency Tables,2016
1641,"Building on ideas from Castillo and Nickl [Ann. Statist. 41 (2013) 1999-2028], a method is provided to study nonparametric Bayesian posterior convergence rates when ""strong"" measures of distances, such as the sup-norm, are considered. In particular, we show that likelihood methods can achieve optimal minimax sup-norm rates in density estimation on the unit interval. The introduced methodology is used to prove that commonly used families of prior distributions on densities, namely log-density priors and dyadic random density histograms, can indeed achieve optimal sup-norm rates of convergence. New results are also derived in the Gaussian white noise model as a further illustration of the presented techniques.",WOS:000344632400013,ANNALS OF STATISTICS,"['GAUSSIAN PROCESS PRIORS', 'DIMENSIONAL EXPONENTIAL-FAMILIES', 'POSTERIOR DISTRIBUTIONS', 'DENSITY-ESTIMATION', 'CONVERGENCE-RATES', 'CONSISTENCY', 'THEOREMS']",ON BAYESIAN SUPREMUM NORM CONTRACTION RATES,2014
1642,"Image processing researchers commonly assert that ""median filtering is better than linear filtering for removing noise in the presence of edges."" Using a straightforward large-n decision-theory framework, this folk-theorem is seen to be false in general. We show that median filtering and linear filtering have similar asymptotic worst-case mean-squared error (MSE) when the signal-to-noise ratio (SNR) is of order 1, which corresponds to the case of constant per-pixel noise level in a digital signal. To see dramatic benefits of median smoothing in an asymptotic setting, the per-pixel noise level should tend to zero (i.e., SNR should grow very large).
We show that a two-stage median filtering using two very different window widths can dramatically outperform traditional linear and median filtering in settings where the underlying object has edges. In this two-stage procedure, the first pass, at a fine scale, aims at increasing the SNR. The second pass, at a coarser scale, correctly exploits the nonlinearity of the median.
Image processing methods based on nonlinear partial differential equations (PDEs) are often said to improve on linear filtering in the presence of edges. Such methods seem difficult to analyze rigorously in a decision-theoretic framework. A popular example is mean curvature motion (MCM), which is formally a kind of iterated median filtering. Our results on iterated median filtering suggest that some PDE-based methods are candidates to rigorously outperform linear filtering in an asymptotic framework.",WOS:000265619700004,ANNALS OF STATISTICS,"['MINIMAX ESTIMATION', 'WAVELET SHRINKAGE']",DOES MEDIAN FILTERING TRULY PRESERVE EDGES BETTER THAN LINEAR FILTERING?,2009
1643,"Spectral sampling is associated with the group of unitary transformations acting on matrices in much the same way that simple random sampling is associated with the symmetric group acting on vectors. This parallel extends to symmetric functions, k-statistics and polykays. We construct spectral k-statistics as unbiased estimators of cumulants of trace powers of a suitable random matrix. Moreover we define normalized spectral polykays in such a way that when the sampling is from an infinite population they return products of free cumulants.",WOS:000320488200021,ANNALS OF STATISTICS,"['SYMMETRIC GROUP', 'CUMULANTS']",NATURAL STATISTICS FOR SPECTRAL SAMPLES,2013
1644,"Gaussian process (GP) regression models make for powerful predictors in out of sample exercises, but cubic runtimes for dense matrix decompositions severely limit the size of data-training and testing-on which they can be deployed. That means that in computer experiment, spatial/geo-physical, and machine learning contexts, GPs no longer enjoy privileged status as data sets continue to balloon in size. We discuss an implementation of local approximate Gaussian process models, in the laGP package for R, that offers a particular sparse-matrix remedy uniquely positioned to leverage modern parallel computing architectures. The laGP approach can be seen as an update on the spatial statistical method of local kriging neighborhoods. We briefly review the method, and provide extensive illustrations of the features in the package through worked-code examples. The appendix covers custom building options for symmetric multi-processor and graphical processing units, and built-in wrapper routines that automate distribution over a simple network of workstations.",WOS:000389071600001,JOURNAL OF STATISTICAL SOFTWARE,"['COMPUTER EXPERIMENTS', 'SENSITIVITY-ANALYSIS', 'SEQUENTIAL DESIGN', 'BAYESIAN-ANALYSIS', 'DATA SETS', 'OPTIMIZATION', 'PREDICTION', 'CALIBRATION', 'LIKELIHOODS', 'EMULATORS']",laGP: Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in R,2016
1645,"This paper proposes a new statistic to test independence between two high dimensional random vectors X: p(1) x 1 and Y : p(2) x 1. The proposed statistic is based on the sum of regularized sample canonical correlation coefficients of X and Y. The asymptotic distribution of the statistic under the null hypothesis is established as a corollary of general central limit theorems (CLT) for the linear statistics of classical and regularized sample canonical correlation coefficients when p(1) and p(2) are both comparable to the sample size n. As applications of the developed independence test, various types of dependent structures, such as factor models, ARCH models and a general uncorrelated but dependent case, etc., are investigated by simulations. As an empirical application, cross-sectional dependence of daily stock returns of companies between different sections in the New York Stock Exchange (NYSE) is detected by the proposed test.",WOS:000352757100001,ANNALS OF STATISTICS,"['COVARIANCE MATRICES', 'EMPIRICAL DISTRIBUTION', 'CONVERGENCE', 'EIGENVALUES', 'STATISTICS', 'MODELS']",INDEPENDENCE TEST FOR HIGH DIMENSIONAL DATA BASED ON REGULARIZED CANONICAL CORRELATION COEFFICIENTS,2015
1646,"Estimation mainly for two classes of popular models, single-index and partially linear single-index models, is studied in this paper. Such models feature nonstationarity. Orthogonal series expansion is used to approximate the unknown integrable link functions in the models and a profile approach is used to derive the estimators. The findings include the dual rate of convergence of the estimators for the single-index models and a trio of convergence rates for the partially linear single-index models. A new central limit theorem is established for a plug-in estimator of the unknown link function. Meanwhile, a considerable extension to a class of partially nonlinear single-index models is discussed in Section 4. Monte Carlo simulation verifies these theoretical results. An empirical study furnishes an application of the proposed estimation procedures in practice.",WOS:000368022000015,ANNALS OF STATISTICS,"['NONPARAMETRIC COINTEGRATING REGRESSION', 'TIME-SERIES', 'BINARY CHOICE', 'NONSTATIONARITY', 'LIKELIHOOD']",ESTIMATION FOR SINGLE-INDEX AND PARTIALLY LINEAR SINGLE-INDEX INTEGRATED MODELS,2016
1647,"The measurement and reporting of model error is of basic importance when constructing models. Here, a general method and an R package, A 3, are presented to support the assessment and communication of the quality of a model fit along with metrics of variable importance. The presented method is accurate, robust, and adaptable to a wide range of predictive modeling algorithms. The method is described along with case studies and a usage guide. It is shown how the method can be used to obtain more accurate models for prediction and how this may simultaneously lead to altered inferences and conclusions about the impact of potential drivers within a system.",WOS:000365978400001,JOURNAL OF STATISTICAL SOFTWARE,,Consistent and Clear Reporting of Results from Diverse Modeling Techniques: The A3 Method,2015
1648,"We consider dimension reduction for regression or classification in which the predictors are matrix- or array-valued. This type of predictor arises when measurements are obtained for each combination of two or more underlying variables-for example, the voltage measured at different channels and times in electroencephalography data. For these applications, it is desirable to preserve the array structure of the reduced predictor (e.g., time versus channel), but this cannot be achieved within the conventional dimension reduction formulation. In this paper, we introduce a dimension reduction method, to be called dimension folding, for matrix- and array-valued predictors that preserves the array structure. In an application of dimension folding to an electroencephalography data set, we correctly classify 97 out of 122 subjects as alcoholic or nonalcoholic based on their electroencephalography in a cross-validation sample.",WOS:000275510800018,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'CANONICAL CORRELATION', 'REDUCTION METHODS', 'BOOTSTRAP', 'GRAPHICS']",ON DIMENSION FOLDING OF MATRIX- OR ARRAY-VALUED STATISTICAL OBJECTS,2010
1649,"The statistical analysis of data which is measured over a spatial region is well established as a scientific tool which makes considerable contributions to a wide variety of application areas. Further development of these tools also remains a central part of the research scene in statistics. However, understanding of the concepts involved often bene fits from an intuitive and experimental approach, as well as a formal description of models and methods. This paper describes software which is intended to assist in this understanding. The role of simulation is advocated, in order to explain the meaning of spatial correlation and to interpret the parameters involved in standard models. Realistic scenarios where decisions on the locations of sampling points in a spatial setting are required are also described. Students are provided with a variety of sampling strategies and invited to select the most appropriate one in two different settings. One involves water sampling in the lagoon of the Mururoa Atoll while the other involves sea bed sampling in a Scottish firth. Once a student has decided on a sampling strategy, simulated data are provided for further analysis. This extends the range of teaching activity from the analysis of data collected by others to involvement in data collection and the need to grapple with issues of design. It is argued that this approach has significant bene fits in learning. The software which implements these tools is built on existing R packages, using rpanel controls for the geoR geostatistical simulation and modelling tools. The operation and construction of the software are described in detail. The software is made available as additional functions rp.geosim, rp.mururoa and rp.firth in the rpanel package.",WOS:000284004900001,JOURNAL OF STATISTICAL SOFTWARE,['PACKAGE'],Interactive Teaching Tools for Spatial Sampling,2010
1650,"The Generalized Hermite distribution (and the Hermite distribution as a particular case) is often used for fitting count data in the presence of overdispersion or multimodality. Despite this, to our knowledge, no standard software packages have implemented specific functions to compute basic probabilities and make simple statistical inference based on these distributions. We present here a set of computational tools that allows the user to face these difficulties by modelling with the Generalized Hermite distribution using the R package hermite. The package can also be used to generate random deviates from a Generalized Hermite distribution and to use basic functions to compute probabilities (density, cumulative density and quantile functions are available), to estimate parameters using the maximum likelihood method and to perform the likelihood ratio test for Poisson assumption against a Generalized Hermite alternative. In order to improve the density and quantile functions performance when the parameters are large, Edgeworth and Cornish-Fisher expansions have been used. Hermite regression is also a useful tool for modeling inflated count data, so its inclusion to a commonly used software like R will make this tool available to a wide range of potential users. Some examples of usage in several fields of application are also given.",WOS:000368551800020,R JOURNAL,"['MAXIMUM-LIKELIHOOD ESTIMATORS', 'COMPOUND']",Generalized Hermite Distribution Modelling with the R Package hermite,2015
1651,"The high-dimensionality typically associated with discretized approximations to Gaussian random fields is a considerable hinderance to computationally efficient methods for their simulation. Many direct approaches require spectral decompositions of the associated covariance matrix and so are unable to complete the solving process in a timely fashion, if at all. However under certain conditions, we may construct block-circulant versions of the covariance matrix at hand thereby allowing access to fast-Fourier methods to perform the required operations with impressive speed. We demonstrate how circulant embedding and subsequent simulation can be performed directly in the R language. The approach is currently implemented in C for the R package Random Fields, and used in the recently released package lgcp. Motivated by applications dealing with spatial point processes we restrict attention to stationary Gaussian fields on R-2, where sparsity of the covariance matrix cannot necessarily be assumed.",WOS:000328129900001,JOURNAL OF STATISTICAL SOFTWARE,"['MARKOV RANDOM-FIELDS', 'SIMULATION', 'PACKAGE']",On Circulant Embedding for Gaussian Random Fields in R,2013
1652,"This paper studies the minimax detection of a small submatrix of elevated mean in a large matrix contaminated by additive Gaussian noise. To investigate the tradeoff between statistical performance and computational cost from a complexity-theoretic perspective, we consider a sequence of discretized models which are asymptotically equivalent to the Gaussian model. Under the hypothesis that the planted clique detection problem cannot be solved in randomized polynomial time when the clique size is of smaller order than the square root of the graph size, the following phase transition phenomenon is established: when the size of the large matrix p -> infinity, if the submatrix size k = Theta(p(alpha)) for any alpha is an element of (0, 2/3), computational complexity constraints can incur a severe penalty on the statistical performance in the sense that any randomized polynomial-time test is minimax suboptimal by a polynomial factor in p; if k = Theta(p(alpha)) for any alpha is an element of (2/3, 1), minimax optimal detection can be attained within constant factors in linear time. Using Schatten norm loss as a representative example, we show that the hardness of attaining the minimax estimation rate can crucially depend on the loss function. Implications on the hardness of support recovery are also obtained.",WOS:000355768700006,ANNALS OF STATISTICS,"['LARGE-AVERAGE', 'CLIQUES', 'MATRIX']",COMPUTATIONAL BARRIERS IN MINIMAX SUBMATRIX DETECTION,2015
1653,,WOS:000368551800001,R JOURNAL,,Untitled,2015
1654,"Folded concave penalization methods have been shown to enjoy the strong oracle property for high-dimensional sparse estimation. However, a folded concave penalization problem usually has multiple local solutions and the oracle property is established only for one of the unknown local solutions. A challenging fundamental issue still remains that it is not clear whether the local optimum computed by a given optimization algorithm possesses those nice theoretical properties. To close this important theoretical gap in over a decade, we provide a unified theory to show explicitly how to obtain the oracle solution via the local linear approximation algorithm. For a folded concave penalized estimation problem, we show that as long as the problem is localizable and the oracle estimator is well behaved, we can obtain the oracle estimator by using the one-step local linear approximation. In addition, once the oracle estimator is obtained, the local linear approximation algorithm converges, namely it produces the same estimator in the next iteration. The general theory is demonstrated by using four classical sparse estimation problems, that is, sparse linear regression, sparse logistic regression, sparse precision matrix estimation and sparse quantile regression.",WOS:000338477800001,ANNALS OF STATISTICS,"['MULTISTAGE CONVEX RELAXATION', 'QUANTILE REGRESSION', 'VARIABLE SELECTION', 'DANTZIG SELECTOR', 'COVARIANCE ESTIMATION', 'NP-DIMENSIONALITY', 'MATRIX ESTIMATION', 'MM ALGORITHMS', 'LASSO', 'REGULARIZATION']",STRONG ORACLE OPTIMALITY OF FOLDED CONCAVE PENALIZED ESTIMATION,2014
1655,"Gaussian graphical models have become a well-recognized tool for the analysis of conditional independencies within a set of continuous random variables. From an inferential point of view, it is important to realize that they are composite exponential transformation families. We reveal this structure by explicitly describing, for any undirected graph, the (maximal) matrix group acting on the space of concentration matrices in the model. The continuous part of this group is captured by a poset naturally associated to the graph, while automorphisms of the graph account for the discrete part of the group. We compute the dimension of the space of orbits of this group on concentration matrices, in terms of the combinatorics of the graph; and for dimension zero we recover the characterization by Letac and Massam of models that are transformation families. Furthermore, we describe the maximal invariant of this group on the sample space, and we give a sharp lower bound on the sample size needed for the existence of equivariant estimators of the concentration matrix. Finally, we address the issue of robustness of these estimators by computing upper bounds on finite sample breakdown points.",WOS:000326991200009,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD ESTIMATOR', 'CONDITIONAL-INDEPENDENCE', 'COVARIANCE MATRICES', 'BREAKDOWN', 'DISTRIBUTIONS', 'INFERENCE']",GROUPS ACTING ON GAUSSIAN GRAPHICAL MODELS,2013
1656,"This paper provides a rigorous study of the nonparametric estimation of filaments or ridge lines of a probability density f. Points on the filament are considered as local extrema of the density when traversing the support of f along the integral curve driven by the vector field of second eigenvectors of the Hessian of f. We ""parametrize"" points on the filaments by such integral curves, and thus both the estimation of integral curves and of filaments will be considered via a plug-in method using kernel density estimation. We establish rates of convergence and asymptotic distribution results for the estimation of both the integral curves and the filaments. The main theoretical result establishes the asymptotic distribution of the uniform deviation of the estimated filament from its theoretical counterpart. This result utilizes the extreme value behavior of nonstationary Gaussian processes indexed by manifolds M-h, h is an element of (0, 1] as h -> 0.",WOS:000375175200013,ANNALS OF STATISTICS,"['PRINCIPAL CURVES', 'DENSITY-FUNCTION', 'MEAN SHIFT', 'UNIFORM', 'CONSISTENCY', 'GALAXIES', 'GRADIENT']",THEORETICAL ANALYSIS OF NONPARAMETRIC FILAMENT ESTIMATION,2016
1657,"An unknown m by n matrix X-0 is to be estimated from noisy measurements Y = X-0 + Z, where the noise matrix Z has i.i.d. Gaussian entries. A popular matrix denoising scheme solves the nuclear norm penalization problem min(X) parallel to Y - X parallel to(2)(F)/2 + lambda parallel to X parallel to* where parallel to X parallel to(*) denotes the nuclear norm (sum of singular values). This is the analog, for matrices, of l(1) penalization in the vector case. It has been empirically observed that if X-0 has low rank, it may be recovered quite accurately from the noisy measurement Y.
In a proportional growth framework where the rank r(n), number of rows m(n) and number of columns n all tend to infinity proportionally to each other (r(n)/m(n) -> rho, m(n)/n -> beta), we evaluate the asymptotic minimax MSE M (rho, beta) = lim(mn),n ->infinity inf(lambda) sup(rank(X)<= rn) MSE(X-0, (X) over cap). Our formulas involve incomplete moments of the quarter- and semi-circle laws (beta = 1, square case) and the Mareenko-Pastur law (beta < 1, nonsquare case). For finite m and n, we show that MSE increases as the nonzero singular values of X-0 grow larger. As a result, the finite-n worst-case MSE, a quantity which can be evaluated numerically, is achieved when the signal X-0 is ""infinitely strong.""
The nuclear norm penalization problem is solved by applying soft thresholding to the singular values of Y. We also derive the minimax threshold, namely the value lambda* (rho), which is the optimal place to threshold the singular values.
All these results are obtained for general (nonsquare, nonsymmetric) real matrices. Comparable results are obtained for square symmetric nonnegative-definite matrices.",WOS:000345884900009,ANNALS OF STATISTICS,"['MINIMIZATION', 'COMPLETION']",MINIMAX RISK OF MATRIX DENOISING BY SINGULAR VALUE THRESHOLDING,2014
1658,"dlmap is a software package capable of mapping quantitative trait loci (QTL) in a variety of genetic studies. Unlike most other QTL mapping packages, dlmap is built on a linear mixed model platform, and thus can simultaneously handle multiple sources of genetic and environmental variation. Furthermore, it can accommodate both experimental crosses and association mapping populations within a versatile modeling framework. The software implements a mapping algorithm with separate detection and localization stages in a user-friendly manner. It accepts data in various common formats, has a flexible modeling environment, and summarizes results both graphically and numerically.",WOS:000307476000001,JOURNAL OF STATISTICAL SOFTWARE,"['QUANTITATIVE TRAIT LOCI', 'EXPERIMENTAL CROSSES', 'LINKAGE', 'POPULATIONS', 'SAMPLES', 'TRIALS', 'MAPS']",dlmap: An R Package for Mixed Model QTL and Association Analysis,2012
1659,"The first part of this paper describes a series of loglinear preference models based on paired comparisons, a method of measurement whose aim is to order a set of objects according to an attribute of interest by asking subjects to compare pairs of objects. Based on the basic Bradley-Terry specification, two types of models, the loglinear Bradley-Terry model and a pattern approach are presented. Both methods are extended to include subject and object-specific covariates and some further structural effects. In addition, models for derived paired comparisons (based on rankings and ratings) are also included. Latent classes and missing values can be included. The second part of the paper describes the package prefmod that implements the above models in R. Illustrational applications are provided in the last part of the paper.",WOS:000305118400001,JOURNAL OF STATISTICAL SOFTWARE,['GENERALIZED LINEAR-MODELS'],"prefmod: An R Package for Modeling Preferences Based on Paired Comparisons, Rankings, or Ratings",2012
1660,"We present a set of functions in S-PLUS to implement the clustered data generalized additive marginal modelling (CDGAM) strategy proposed by Berhane and Tibshirani (1998). A variety of working correlation structures are supported, and the regression basis may include components from the family of smoothing splines. Keywords: generalized estimating equations, clustered data analysis.",WOS:000232891300001,JOURNAL OF STATISTICAL SOFTWARE,"['SEMIPARAMETRIC REGRESSION', 'NONPARAMETRIC REGRESSION', 'LONGITUDINAL DATA', 'LINEAR-MODELS']",An algorithm for clustered data generalized additive modelling with S-PLUS,2005
1661,"For right-censored data perhaps the most commonly used tests are weighted logrank tests, such as the logrank and Wilcoxon-type tests. In this paper we review several generalizations of those weighted logrank tests to interval-censored data and present an R package, interval, to implement many of them. The interval package depends on the perm package, also presented here, which performs exact and asymptotic linear permutation tests. The perm package performs many of the tests included in the already available coin package, and provides an independent validation of coin. We review analysis methods for interval-censored data, and we describe and show how to use the interval and perm packages.",WOS:000281593100001,JOURNAL OF STATISTICAL SOFTWARE,"['FAILURE TIME DATA', 'PROPORTIONAL HAZARDS MODEL', 'NONPARAMETRIC-ESTIMATION', 'SURVIVAL-DATA', 'RANK-TESTS', 'INFERENCE', 'IMPUTATION']",Exact and Asymptotic Weighted Logrank Tests for Interval Censored Data: The interval R Package,2010
1662,"The crop water requirement is a key factor in the agricultural process. It is usually estimated throughout actual evapotranspiration (ETa). This parameter is the key to develop irrigation strategies, to improve water use efficiency and to understand hydrological, climatic, and ecosystem processes. Currently, it is calculated with classical methods, which are difficult to extrapolate, or with land surface energy balance models (LSEB), such as METRIC and SEBAL, which are based on remote sensing data. This paper describes water, an open implementation of LSEB. The package provides several functions to estimate the parameters of the LSEB equation from satellite data and proposes a new object class to handle weather station data. One of the critical steps in METRIC is the selection of ""cold"" and ""hot"" pixels, which water solves with an automatic method. The water package can process a batch of satellite images and integrates most of the already published sub-models for METRIC. Although water implements METRIC, it will be expandable to SEBAL and others in the near future. Finally, two different procedures are demonstrated using data that is included in water package.",WOS:000395669800023,R JOURNAL,"['IRRIGATED MERLOT VINEYARD', 'MAPPING EVAPOTRANSPIRATION', 'TEMPERATURE RETRIEVAL', 'CROP COEFFICIENTS', 'LEAF-AREA', 'SATELLITE', 'CALIBRATION', 'ALGORITHM', 'PARAMETERIZATION', 'VARIABILITY']",water: Tools and Functions to Estimate Actual Evapotranspiration Using Land Surface Energy Balance Models in R,2016
1663,"We introduce a powerful and flexible MCMC algorithm for stochastic simulation. The method builds on a pseudo-marginal method originally introduced in [Genetics 164 (2003) 1139-1160], showing how algorithms which are approximations to an idealized marginal algorithm, can share the same marginal stationary distribution as the idealized method. Theoretical results are given describing the convergence properties of the proposed method, and simple numerical examples are given to illustrate the promising empirical characteristics of the technique. Interesting comparisons with a more obvious, but inexact, Monte Carlo approximation to the marginal algorithm, are also given.",WOS:000265500500006,ANNALS OF STATISTICS,"['MODEL DETERMINATION', 'GIBBS SAMPLER', 'LINEAR-MODELS', 'HASTINGS', 'SCHEMES']",THE PSEUDO-MARGINAL APPROACH FOR EFFICIENT MONTE CARLO COMPUTATIONS,2009
1664,"Fully nonparametric methods for regression from functional data have poor accuracy from a statistical viewpoint, reflecting the fact that their convergence rates are slower than nonparametric rates for the estimation of high-dimensional functions. This difficulty has led to an emphasis on the so-called functional linear model, which is much more flexible than common linear models in finite dimension, but nevertheless imposes structural constraints on the relationship between predictors and responses. Recent advances have extended the linear approach by using it in conjunction with link functions, and by considering multiple indices, but the flexibility of this technique is still limited. For example, the link may be modeled parametrically or on a grid only, or may be constrained by an assumption such as monotonicity; multiple indices have been modeled by making finite-dimensional assumptions. In this paper we introduce a new technique for estimating the link function nonparametrically, and we suggest an approach to multi-index modeling using adaptively defined linear projections of functional data. We show that our methods enable prediction with polynomial convergence rates. The finite sample performance of our methods is studied in simulations, and is illustrated by an application to a functional regression problem.",WOS:000293716500014,ANNALS OF STATISTICS,"['SMOOTHING SPLINES ESTIMATORS', 'LINEAR-REGRESSION', 'PREDICTION', 'VARIABLES', 'CONSTANTS']",SINGLE AND MULTIPLE INDEX FUNCTIONAL REGRESSION MODELS WITH NONPARAMETRIC LINK,2011
1665,"We consider nonparametric estimation of a regression function for a situation where precisely measured predictors are used to estimate the regression curve for coarsened, that is, less precise or contaminated predictors. Specifically, while one has available a sample (W(1), Y(1)),..., (W(n) ,Y(n)) of independent and identically distributed data, representing observations with precisely measured predictors, where E(Y(i)/W(i)) = g (W(i)), instead of the smooth regression function g, the target of interest is another smooth regression function m that pertains to predictors Xi that are noisy versions of the Wi. Our target is then the regression function m(x) = E(Y/X = x), where X is a contaminated version of W, that is, X = W + delta. It is assumed that either the density of the errors is known, or replicated data are available resembling, but not necessarily the same as, the variables X. In either case, and under suitable conditions, we obtain root n-rates of convergence of the proposed estimator and its derivatives, and establish a functional limit theorem. Weak convergence to a Gaussian limit process implies pointwise and uniform confidence intervals and /i-i-consistent estimators of extrema and zeros of m. It is shown that these results are preserved under more general models in which X is determined by an explanatory variable. Finite sample performance is investigated in simulations and illustrated by a real data example.",WOS:000253077800018,ANNALS OF STATISTICS,"['ERRORS-IN-VARIABLES', 'SIMULATION-EXTRAPOLATION', 'MIXING PROCESSES', 'DECONVOLUTION', 'MODEL']",Accelerated convergence for nonparametric regression with coarsened predictors,2007
1666,"Serial correlation in the residuals of time series models can cause bias in both model estimation and prediction. However, models with such serially correlated residuals are difficult to estimate, especially when the regression function is nonlinear. Existing estimation methods require strong assumption for the relation between the residuals and the regressors, which excludes the commonly used autoregressive models in time series analysis. By extending the Whittle likelihood estimation, this paper investigates in details a semi-parametric autoregressive model with ARMA sequence of residuals. Asymptotic normality of the estimators is established, and a model selection procedure is proposed. Numerical examples are employed to illustrate the performance of the proposed estimation method and the necessity of incorporating the serial correlation in the residuals.",WOS:000379972900009,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'AUTOCORRELATED ERRORS', 'AUTOREGRESSIVE MODELS', 'CORRELATED ERRORS', 'KERNEL REGRESSION', 'IDENTIFICATION', 'LIKELIHOOD', 'SPLINES']",ESTIMATION OF SEMIVARYING COEFFICIENT TIME SERIES MODELS WITH ARMA ERRORS,2016
1667,"Simultaneous clustering of rows and columns, usually designated by bi-clustering, co-clustering or block clustering, is an important technique in two way data analysis. A new standard and efficient approach has been recently proposed based on the latent block model (Govaert and Nadif 2003) which takes into account the block clustering problem on both the individual and variable sets. This article presents our R package blockcluster for co-clustering of binary, contingency and continuous data based on these very models. In this document, we will give a brief review of the model-based block clustering methods, and we will show how the R package blockcluster can be used for co-clustering.",WOS:000398466700001,JOURNAL OF STATISTICAL SOFTWARE,"['MIXTURE-MODELS', 'EM ALGORITHM']",blockcluster: An R Package for Model-Based Co-Clustering,2017
1668,"Markov chains are well-established probabilistic models of a wide variety of real systems that evolve along time. Countless examples of applications of Markov chains that successfully capture the probabilistic nature of real problems include areas as diverse as biology, medicine, social science, and engineering. One interesting feature which characterizes certain kinds of Markov chains is their stationary distribution, which stands for the global fraction of time the system spends in each state. The computation of the stationary distribution requires precise knowledge of the transition probabilities. When the only information available is a sequence of observations drawn from the system, such probabilities have to be estimated. Here we review an existing method to estimate fuzzy transition probabilities from observations and, with them, obtain the fuzzy stationary distribution of the resulting fuzzy Markov chain. The method also works when the user directly provides fuzzy transition probabilities. We provide an implementation in the R environment that is the first available to the community and serves as a proof of concept. We demonstrate the usefulness of our proposal with computational experiments on a toy problem, namely a time-homogeneous Markov chain that guides the randomized movement of an autonomous robot that patrols a small area.",WOS:000384915500001,JOURNAL OF STATISTICAL SOFTWARE,"['TRANSITION-PROBABILITIES', 'DIFFERENTIAL EVOLUTION', 'GLOBAL OPTIMIZATION', 'MODELS', 'DISEASE']",FuzzyStatProb: An R Package for the Estimation of Fuzzy Stationary Probabilities from a Sequence of Observations of an Unknown Markov Chain,2016
1669,"When using optimal linear prediction to interpolate point observations of a mean square continuous stationary spatial process, one often finds that the interpolant mostly depends on those observations located nearest to the predictand. This phenomenon is called the screening effect. However, there are situations in which a screening effect does not hold in a reasonable asymptotic sense, and theoretical support for the screening effect is limited to some rather specialized settings for the observation locations. This paper explores conditions on the observation locations and the process model under which an asymptotic screening effect holds. A series of examples shows the difficulty in formulating a general result, especially for processes with different degrees of smoothness in different directions, which can naturally occur for spatial-temporal processes. These examples lead to a general conjecture and two special cases of this conjecture are proven. The key condition on the process is that its spectral density should change slowly at high frequencies. Models not satisfying this condition of slow high-frequency change should be used with caution.",WOS:000300383200001,ANNALS OF STATISTICS,"['COVARIANCE FUNCTIONS', 'RANDOM-FIELDS']",2010 RIETZ LECTURE WHEN DOES THE SCREENING EFFECT HOLD?,2011
1670,"The representation of biochemical knowledge in terms of fluxes ( transformation rates) in a metabolic network is often a crucial step in the development of new drugs and efficient bioreactors. Mass spectroscopy ( MS) and nuclear magnetic resonance spectroscopy ( NMRS) in combination with C-13 labeled substrates are experimental techniques resulting in data that may be used to quantify fluxes in the metabolic network underlying a process. The massive amount of data generated by spectroscopic experiments increasingly requires software which models the dynamics of the underlying biological system. In this work we present an approach to handle isotopomer distributions in metabolic networks using an object-oriented programming approach, implemented using S4 classes in R. The developed package is called FluxSimulator and provides a user friendly interface to specify the topological information of the metabolic network as well as carbon atom transitions in plain text files. The package automatically derives the mathematical representation of the formulated network, and assembles a set of ordinary differential equations ( ODEs) describing the change of each isotopomer pool over time. These ODEs are subsequently solved numerically. In a case study FluxSimulator was applied to an example network. Our results indicate that the package is able to reproduce exact changes in isotopomer compositions of the metabolite pools over time at given flux rates.",WOS:000244067900001,JOURNAL OF STATISTICAL SOFTWARE,"['BIDIRECTIONAL REACTION STEPS', 'FROZEN TISSUE SAMPLES', 'FLUX ANALYSIS', 'LABELING EXPERIMENTS', 'C-13', 'CYCLE']",FluxSimulator: An R package to simulate isotopomer distributions in metabolic networks,2007
1671,"Semiparametric models are often considered for analyzing longitudinal data for a good balance between flexibility and parsimony. In this paper, we study a class of marginal partially linear quantile models with possibly varying coefficients. The functional coefficients are estimated by basis function approximations. The estimation procedure is easy to implement, and it requires no specification of the error distributions. The asymptotic properties of the proposed estimators are established for the varying coefficients as well as for the constant coefficients. We develop rank score tests for hypotheses on the coefficients, including the hypotheses on the constancy of a subset of the varying coefficients. Hypothesis testing of this type is theoretically challenging, as the dimensions of the parameter spaces under both the null and the alternative hypotheses are growing with the sample size. We assess the finite sample performance of the proposed method by Monte Carlo simulation studies, and demonstrate its value by the analysis of an AIDS data set, where the modeling of quantiles provides more comprehensive information than the usual least squares approach.",WOS:000271673700005,ANNALS OF STATISTICS,"['LONGITUDINAL DATA', 'PARAMETERS']",QUANTILE REGRESSION IN PARTIALLY LINEAR VARYING COEFFICIENT MODELS,2009
1672,"Nonparametric estimation of mean and covariance functions is important in functional data analysis. We investigate the performance of local linear smoothers for both mean and covariance functions with a general weighing scheme, which includes two commonly used schemes, equal weight per observation (OBS), and equal weight per subject (SUBJ), as two special cases. We provide a comprehensive analysis of their asymptotic properties on a unified platform for all types of sampling plan, be it dense, sparse or neither. Three types of asymptotic properties are investigated in this paper: asymptotic normality, L-2 convergence and uniform convergence. The asymptotic theories are unified on two aspects: (1) the weighing scheme is very general; (2) the magnitude of the number N-i of measurements for the ith subject relative to the sample size n can vary freely. Based on the relative order of Ni to n, functional data are partitioned into three types: non-dense, dense and ultra dense functional data for the OBS and SUBJ schemes. These two weighing schemes are compared both theoretically and numerically. We also propose a new class of weighing schemes in terms of a mixture of the OBS and SUBJ weights, of which theoretical and numerical performances are examined and compared.",WOS:000384397200020,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'VARYING-COEFFICIENT MODELS', 'LOCAL LINEAR-REGRESSION', 'MIXED-EFFECTS MODELS', 'LONGITUDINAL DATA', 'NONPARAMETRIC REGRESSION', 'CONVERGENCE-RATES', 'CURVES', 'CONSISTENCY', 'ESTIMATORS']",FROM SPARSE TO DENSE FUNCTIONAL DATA AND BEYOND,2016
1673,"Suppose that a sequence of unknown parameters is observed subject to independent Gaussian noise. The EbayesThresh package in the S language implements a class of Empirical Bayes thresholding methods that can take advantage of possible sparsity in the sequence, to improve the quality of estimation.
The prior for each parameter in the sequence is a mixture of an atom of probability at zero and a heavy-tailed density. Within the package, this can be either a Laplace ( double exponential) density or else a mixture of normal distributions with tail behavior similar to the Cauchy distribution. The mixing weight, or sparsity parameter, is chosen automatically by marginal maximum likelihood. If estimation is carried out using the posterior median, this is a random thresholding procedure; the estimation can also be carried out using other thresholding rules with the same threshold, and the package provides the posterior mean, and hard and soft thresholding, as additional options.
This paper reviews the method, and gives details ( far beyond those previously published) of the calculations needed for implementing the procedures. It explains and motivates both the general methodology, and the use of the EbayesThresh package, through simulated and real data examples.
When estimating the wavelet transform of an unknown function, it is appropriate to apply the method level by level to the transform of the observed data. The package can carry out these calculations for wavelet transforms obtained using various packages in R and S-PLUS. Details, including a motivating example, are presented, and the application of the method to image estimation is also explored.
The final topic considered is the estimation of a single sequence that may become progressively sparser along the sequence. An iterated least squares isotone regression method allows for the choice of a threshold that depends monotonically on the order in which the observations are made. An alternative possibility, also discussed in detail, is a particular parametric dependence of the sparsity parameter on the position in the sequence.",WOS:000232807000001,JOURNAL OF STATISTICAL SOFTWARE,['WAVELET SHRINKAGE'],EbayesThresh: R programs for empirical Bayes thresholding,2005
1674,"We develop an efficient estimation procedure for identifying and estimating the central subspace. Using a new way of parameterization, we convert the problem of identifying the central subspace to the problem of estimating a finite dimensional parameter in a semiparametric model. This conversion allows us to derive an efficient estimator which reaches the optimal semiparametric efficiency bound. The resulting efficient estimator can exhaustively estimate the central subspace without imposing any distributional assumptions. Our proposed efficient estimation also provides a possibility for making inference of parameters that uniquely identify the central subspace. We conduct simulation studies and a real data analysis to demonstrate the finite sample performance in comparison with several existing methods.",WOS:000317451200010,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'SEMIPARAMETRIC ESTIMATORS', 'DISTRIBUTED PREDICTORS', 'MEASUREMENT ERROR', 'MODELS']",EFFICIENT ESTIMATION IN SUFFICIENT DIMENSION REDUCTION,2013
1675,"We study a random sampling technique to approximate integrals f[0,1](s) f (x) dx by averaging the function at some sampling points. We focus on cases where the integrand is smooth, which is a problem which occurs in statistics.
The convergence rate of the approximation error depends on the smoothness of the function f and the sampling technique. For instance, Monte Carlo (MC) sampling yields a convergence of the root mean square error (RMSE) of order N(-1/2) (where N is the number of samples) for functions f with finite variance. Randomized QMC (RQMC), a combination of MC and quasi-Monte Carlo (QMC), achieves a RMSE of order N(-3/2+epsilon) under the stronger assumption that the integrand has bounded variation. A combination of RQMC with local antithetic sampling achieves a convergence of the RMSE of order N(-3/2-1/s+epsilon) (where s >= 1 is the dimension) for functions with mixed partial derivatives up to order two.
Additional smoothness of the integrand does not improve the rate of convergence of these algorithms in general. On the other hand, it is known that without additional smoothness of the integrand it is not possible to improve the convergence rate.
This paper introduces a new RQMC algorithm, for which we prove that it achieves a convergence of the root mean square error (RMSE) of order N(-alpha-1/2+epsilon) provided the integrand satisfies the strong assumption that it has square integrable partial mixed derivatives up to order alpha > 1 in each variable. Known lower bounds on the RMSE show that this rate of convergence cannot be improved in general for integrands with this smoothness. We provide numerical examples for which the RMSE converges approximately with order N(-5/2) and N(-7/2), in accordance with the theoretical upper bound.",WOS:000293716500002,ANNALS OF STATISTICS,"['VARIANCE', 'RULES']",HIGHER ORDER SCRAMBLED DIGITAL NETS ACHIEVE THE OPTIMAL RATE OF THE ROOT MEAN SQUARE ERROR FOR SMOOTH INTEGRANDS,2011
1676,"The multivariate linear model is
[GRAPHICS]
=
[GRAPHICS]
[GRAPHICS]
+
[GRAPHICS]
The multivariate linear model can be fit with the lm function in R, where the left-hand side of the model comprises a matrix of response variables, and the right-hand side is specified exactly as for a univariate linear model (i.e., with a single response variable). This paper explains how to use the Anova and linear Hypothesis functions in the car package to perform convenient hypothesis tests for parameters in multivariate linear models, including models for repeated-measures data.",WOS:000321944400005,R JOURNAL,"['REPEATED-MEASURES DESIGNS', 'HE PLOTS']",Hypothesis Tests for Multivariate Linear Models Using the car Package,2013
1677,"This paper deals with the density estimation on R-d under sup-norm loss. We provide a fully data-driven estimation procedure and establish for it a so-called sup-norm oracle inequality. The proposed estimator allows us to take into account not only approximation properties of the underlying density, but eventual independence structure as well. Our results contain, as a particular case, the complete solution of the bandwidth selection problem in the multivariate density model. Usefulness of the developed approach is illustrated by application to adaptive estimation over anisotropic Nikolskii classes.",WOS:000320488200022,ANNALS OF STATISTICS,"['ADAPTIVE ESTIMATION', 'INEQUALITIES', 'POINTWISE', 'SELECTION', 'WAVELET']","MULTIVARIATE DENSITY ESTIMATION UNDER SUP-NORM LOSS: ORACLE APPROACH, ADAPTATION AND INDEPENDENCE STRUCTURE",2013
1678,"Many problems in statistics, finance, biology, pharmacology, physics, mathematics, economics, and chemistry involve determination of the global minimum of multidimensional functions. R packages for different stochastic methods such as genetic algorithms and differential evolution have been developed and successfully used in the R community. Based on Tsallis statistics, the R package GenSA was developed for generalized simulated annealing to process complicated non-linear objective functions with a large number of local minima. In this paper we provide a brief introduction to the R package and demonstrate its utility by solving a non-convex portfolio optimization problem in finance and the Thomson problem in physics. GenSA is useful and can serve as a complementary tool to, rather than a replacement for, other widely used R packages for optimization.",WOS:000321944400003,R JOURNAL,"['BOUND-CONSTRAINED OPTIMIZATION', 'MULTIPLE-MINIMA PROBLEM', 'DIFFERENTIAL EVOLUTION', 'GENETIC ALGORITHM', 'TABOO SEARCH', 'CHARGES']",Generalized Simulated Annealing for Global Optimization: The GenSA Package,2013
1679,"Phylogenetic analysis of DNA or other data commonly gives rise to a collection or sample of inferred evolutionary trees. Principal Components Analysis (PCA) cannot be applied directly to collections of trees since the space of evolutionary trees on a fixed set of taxa is not a vector space. This paper describes a novel geometrical approach to PCA in tree-space that constructs the first principal path in an analogous way to standard linear Euclidean PCA. Given a data set of phylogenetic trees, a geodesic principal path is sought that maximizes the variance of the data under a form of projection onto the path. Due to the high dimensionality of tree-space and the nonlinear nature of this problem, the computational complexity is potentially very high, so approximate optimization algorithms are used to search for the optimal path. Principal paths identified in this way reveal and quantify the main sources of variation in the original collection of trees in terms of both topology and branch lengths. The approach is illustrated by application to simulated sets of trees and to a set of gene trees from metazoan (animal) species.",WOS:000299186500020,ANNALS OF STATISTICS,"['ALGORITHM', 'DISTANCE']",PRINCIPAL COMPONENTS ANALYSIS IN THE SPACE OF PHYLOGENETIC TREES,2011
1680,"Consider the problem of testing s hypotheses simultaneously. In this paper, we derive methods which control the generalized family-wise error rate given by the probability of k or more false rejections, abbreviated k-FWER. We derive both single-step and step-down procedures that control the k-FWER in finite samples or asymptotically, depending on the situation. Moreover, the procedures are asymptotically balanced in an appropriate sense. We briefly consider control of the average number of false rejections. Additionally, we consider the false discovery proportion (FDP), defined as the number of false rejections divided by the total number of rejections (and defined to be 0 if there are no rejections). Here, the goal is to construct methods which satisfy, for given gamma and alpha, P{FDP > gamma} <= alpha, at least asymptotically. Special attention is paid to the construction of methods which implicitly take into account the dependence structure of the individual test statistics in order to further increase the ability to detect false null hypotheses. A general resampling and subsampling approach is presented which achieves these objectives, at least asymptotically.",WOS:000273800100018,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'MULTIPLE TESTING PROCEDURES', 'TEST STATISTICS', 'BOOTSTRAP']",BALANCED CONTROL OF GENERALIZED ERROR RATES,2010
1681,"Empirical likelihood is a popular nonparametric or semi-parametric statistical method with many nice statistical properties. Yet when the sample size is small, or the dimension of the accompanying estimating function is high, the application of the empirical likelihood method can be hindered by low precision of the chi-square approximation and by nonexistence of solutions to the estimating equations. In this paper, we show that the adjusted empirical likelihood is effective at addressing both problems. With a specific level of adjustment, the adjusted empirical likelihood achieves the high-order precision of the Bartlett correction, in addition to the advantage of a guaranteed solution to the estimating equations. Simulation results indicate that the confidence regions constructed by the adjusted empirical likelihood have coverage probabilities comparable to or substantially more accurate than the original empirical likelihood enhanced by the Bartlett correction.",WOS:000277471000003,ANNALS OF STATISTICS,"['GENERALIZED-METHOD', 'MOMENTS ESTIMATORS', 'CONFIDENCE-REGIONS', 'SAMPLE PROPERTIES', 'MODELS', 'INFERENCE', 'TESTS', 'GMM']",ADJUSTED EMPIRICAL LIKELIHOOD WITH HIGH-ORDER PRECISION,2010
1682,This paper establishes that so-called instrumental variables enable the identification and the estimation of a fully nonparametric regression model with Berkson-type measurement error in the regressors. An estimator is proposed and proven to be consistent. Its practical performance and feasibility are investigated via Monte Carlo simulations as well as through an epidemiological application investigating the effect of particulate air pollution on respiratory health. These examples illustrate that Berkson errors can clearly not be neglected in nonlinear regression models and that the proposed method represents an effective remedy.,WOS:000323271500010,ANNALS OF STATISTICS,"['IN-VARIABLES MODELS', 'PARTICULATE AIR-POLLUTION', 'NONLINEAR MODELS', 'IDENTIFICATION', 'MORTALITY', 'CITIES']",REGRESSIONS WITH BERKSON ERRORS IN COVARIATES-A NONPARAMETRIC APPROACH,2013
1683,"The solution of a (stochastic) differential equation can be locally approximated by a (stochastic) expansion. If the vector field of the differential equation is a polynomial, the corresponding expansion is a linear combination of iterated integrals of the drivers and can be calculated using Picard Iterations. However, such expansions grow exponentially fast in their number of terms, due to their specific algebra, rendering their practical use limited.
We present a Mathematica procedure that addresses this issue by reparametrizing the polynomials and distributing the load in as small as possible parts that can be processed and manipulated independently, thus alleviating large memory requirements and being perfectly suited for parallelized computation. We also present an iterative implementation of the shuffle product (as opposed to a recursive one, more usually implemented) as well as a fast way for calculating the expectation of iterated Stratonovich integrals for Brownian motion.",WOS:000320041200001,JOURNAL OF STATISTICAL SOFTWARE,['INTEGRALS'],A Distributed Procedure for Computing Stochastic Expansions with Mathematica,2013
1684,"Exact inference is based on the conditional distribution of the sufficient statistics for the parameters of interest given the observed values for the remaining sufficient statistics. Exact inference for logistic regression can be problematic when data sets are large and the support of the conditional distribution cannot be represented in memory. Additionally, these methods are not widely implemented except in commercial software packages such as LogXact and SAS. Therefore, we have developed elrm, software for R implementing ( approximate) exact inference for binomial regression models from large data sets. We provide a description of the underlying statistical methods and illustrate the use of elrm with examples. We also evaluate elrm by comparing results with those obtained using other methods.",WOS:000252428300001,JOURNAL OF STATISTICAL SOFTWARE,,elrm: Software implementing exact-like inference for logistic regression models,2007
1685,"I present the small R package digitize, designed to extract data from scatterplots with a simple method and suited to small datasets. I present an application of this method to the extraction of data from a graph whose source is not available.",WOS:000208590100005,R JOURNAL,,The digitize Package: Extracting Numerical Data from Scatterplots,2011
1686,,WOS:000330193300001,R JOURNAL,,Untitled,2013
1687,"In this paper we give a general framework for isotone optimization. First we discuss a generalized version of the pool-adjacent-violators algorithm (PAVA) to minimize a separable convex function with simple chain constraints. Besides of general convex functions we extend existing PAVA implementations in terms of observation weights, approaches for tie handling, and responses from repeated measurement designs. Since isotone optimization problems can be formulated as convex programming problems with linear constraints we then develop a primal active set method to solve such problem. This methodology is applied on specific loss functions relevant in statistics. Both approaches are implemented in the R package isotone.",WOS:000271534000001,JOURNAL OF STATISTICAL SOFTWARE,"['CONVEX-FUNCTIONS SUBJECT', 'ORDERED ALTERNATIVES', 'REGRESSION', 'HOMOGENEITY']",Isotone Optimization in R: Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods,2009
1688,"One of the most widely used goodness-of-fit tests is the two-sided one sample Kolmogorov-Smirnov (K-S) test which has been implemented by many computer statistical software packages. To calculate a two-sided p value (evaluate the cumulative sampling distribution), these packages use various methods including recursion formulae, limiting distributions, and approximations of unknown accuracy developed over thirty years ago. Based on an extensive literature search for the two-sided one sample K-S test, this paper identifies an exact formula for sample sizes up to 31, six recursion formulae, and one matrix formula that can be used to calculate a p value. To ensure accurate calculation by avoiding catastrophic cancelation and eliminating rounding error, each of these formulae is implemented in rational arithmetic. For the six recursion formulae and the matrix formula, computational experience for sample sizes up to 500 shows that computational times are increasing functions of both the sample size and the number of digits in the numerator and denominator integers of the rational number test statistic. The computational times of the seven formulae vary immensely but the Durbin recursion formula is almost always the fastest. Linear search is used to calculate the inverse of the cumulative sampling distribution (find the confidence interval half-width) and tables of calculated half-widths are presented for sample size up to 500. Using calculated half-widths as input, computational times for the fastest formula, the Durbin recursion formula, are given for sample sizes up to two thousand.",WOS:000257322400001,JOURNAL OF STATISTICAL SOFTWARE,"['DISTRIBUTION FUNCTION LIES', 'STATISTICS', 'PROBABILITY']",Rational arithmetic mathematica functions to evaluate the two-sided one sample K-S cumulative sampling distribution,2008
1689,"Hedeker and Nordgren (2013) present the stand-alone MIXREGLS program for fitting the mixed-effects location scale model to continuous longitudinal and other clustered data. This model can be used when interest lies in joint modeling the mean and dispersion of subjects' responses over time. The model extends the standard two-level random-intercept mixed model by allowing both the within- and between-subject variances to be influenced by the covariates and for the within-subject variance to additionally depend on a subject random-scale effect. In this article we present the runmixregls command to run MIXREGLS seamlessly from within Stata. We illustrate the notable advantages of using runmixregls by replicating and extending the two example analyses presented in Hedeker and Nordgren (2013). We then use runmixregls to demonstrate a new and important research finding. Namely, that ignoring the random-scale effect in the within-subject variance function will lead to the regression coefficients in this function to be estimated with spurious precision, especially the regression coefficients of subject-level covariates.",WOS:000341808300001,JOURNAL OF STATISTICAL SOFTWARE,"['WINBUGS', 'MODEL']",runmixregls: A Program to Run the MIXREGLS Mixed-Effects Location Scale Software from within Stata,2014
1690,"Comparing two proportions through the difference is a basic problem in statistics and has applications in many fields. More than twenty confidence intervals (Newcombe, 1998a,b) have been proposed. Most of them are approximate intervals with an asymptotic infimum coverage probability much less than the nominal level. In addition, large sample may be costly in practice. So exact optimal confidence intervals become critical for drawing valid statistical inference with accuracy and precision. Recently, Wang (2010, 2012) derived the exact smallest (optimal) one-sided 1 - alpha confidence intervals for the difference of two paired or independent proportions. His intervals, however, are computer-intensive by nature. In this article, we provide an R package ExactCIdiff to implement the intervals when the sample size is not large. This would be the first available package in R to calculate the exact confidence intervals for the difference of proportions. Exact two-sided 1 - alpha interval can be easily obtained by taking the intersection of the lower and upper one-sided 1 - alpha/2 intervals. Readers may jump to Examples 1 and 2 to obtain these intervals.",WOS:000330193300007,R JOURNAL,"['BINOMIAL PROPORTIONS', 'CONSTRUCTION']",ExactCIdiff: An R Package for Computing Exact Confidence Intervals for the Difference of Two Proportions,2013
1691,One of the key challenges in changepoint analysis is the ability to detect multiple changes within a given time series or sequence. The changepoint package has been developed to provide users with a choice of multiple changepoint search methods to use in conjunction with a given changepoint method and in particular provides an implementation of the recently proposed PELT algorithm. This article describes the search methods which are implemented in the package as well as some of the available test statistics whilst highlighting their application with simulated and practical examples. Particular emphasis is placed on the PELT algorithm and how results differ from the binary segmentation approach.,WOS:000341583800001,JOURNAL OF STATISTICAL SOFTWARE,"['CHANGE-POINT', 'STRUCTURAL-CHANGES', 'CLUSTER-ANALYSIS', 'MODELS', 'ALGORITHMS', 'SELECTION', 'VARIANCE']",changepoint: An R Package for Changepoint Analysis,2014
1692,"In a large class of statistical inverse problems it is necessary to suppose that the transformation that is inverted is known. Although, in many applications, it is unrealistic to make this assumption, the problem is often insoluble without it. However, if additional data are available, then it is possible to estimate consistently the unknown error density. Data are seldom available directly on the transformation, but repeated, or replicated, measurements increasingly are becoming available. Such data consist of ""intrinsic"" values that are measured several times, with errors that are generally independent. Working in this setting we treat the nonparametric deconvolution problems of density estimation with observation errors, and regression with errors in variables. We show that, even if the number of repeated measurements is quite small, it is possible for modified kernel estimators to achieve the same level of performance they would if the error distribution were known. Indeed, density and regression estimators can be constructed from replicated data so that they have the same first-order properties as conventional estimators in the known-error case, without any replication, but with sample size equal to the sum of the numbers of replicates. Practical methods for constructing estimators with these properties are suggested, involving empirical rules for smoothing-parameter choice.",WOS:000254502700007,ANNALS OF STATISTICS,"['ERRORS-IN-VARIABLES', 'NONPARAMETRIC DECONVOLUTION', 'OPTIMAL RATES', 'MODELS', 'DENSITY', 'REGRESSION', 'ROBUST', 'CONVERGENCE']",On deconvolution with repeated measurements,2008
1693,"Multi-state models provide a relevant tool for studying the observations of a continuous-time process at arbitrary times. Markov models are often considered even if semi-Markov are better adapted in various situations. Such models are still not frequently applied mainly due to lack of available software. We have developed the R package SemiMarkov to fit homogeneous semi-Markov models to longitudinal data. The package performs maximum likelihood estimation in a parametric framework where the distributions of the sojourn times can be chosen between exponential, Weibull or exponentiated Weibull. The package computes and displays the hazard rates of sojourn times and the hazard rates of the semi-Markov process. The effects of covariates can be studied with a Cox proportional hazards model for the sojourn times distributions. The number of covariates and the distribution of sojourn times can be specified for each possible transition providing a great flexibility in a model's definition. This article presents parametric semi-Markov models and gives a detailed description of the package together with an application to asthma control.",WOS:000365977900001,JOURNAL OF STATISTICAL SOFTWARE,"['NONPARAMETRIC-ESTIMATION', 'REGRESSION-MODELS']",SemiMarkov: An R Package for Parametric Estimation in Multi-State Semi-Markov Models,2015
1694,"We discuss how news in R and addon packages can be disseminated. R 2.10.0 has added facilities for computing on news in a common plain text format. In R 2.12.0, the Rd format has been further enhanced so that news can be very conveniently recorded in Rd, allowing for both improved rendering and the development of new news services.",WOS:000208590000012,R JOURNAL,,What's New?,2010
1695,"Persistent homology is a method for probing topological properties of point clouds and functions. The method involves tracking the birth and death of topological features (2000) as one varies a tuning parameter. Features with short lifetimes are informally considered to be ""topological noise,"" and those with a long lifetime are considered to be ""topological signal."" In this paper, we bring some statistical ideas to persistent homology. In particular, we derive confidence sets that allow us to separate topological signal from topological noise.",WOS:000345884900006,ANNALS OF STATISTICS,"['NONPARAMETRIC-ESTIMATION', 'DENSITY ESTIMATORS', 'TOPOLOGY', 'HOMOLOGY', 'SUPPORT']",CONFIDENCE SETS FOR PERSISTENCE DIAGRAMS,2014
1696,"Multidimensional item response models have been developed to incorporate a general trait and several specific trait dimensions. Depending on the structure of these latent traits, different models can be considered. This paper provides the requisite information and description of software that implement the Gibbs sampling procedures for three such models with a normal ogive form. The software developed is written in the MATLAB package IRTm2noHA. The package is flexible enough to allow a user the choice to simulate binary response data with a latent structure involving general and specific traits, specify prior distributions for model parameters, check convergence of the MCMC chain, and obtain Bayesian fit statistics. Illustrative examples are provided to demonstrate and validate the use of the software package.",WOS:000276707300001,JOURNAL OF STATISTICAL SOFTWARE,"['ITEM RESPONSE THEORY', 'CHAIN MONTE-CARLO', 'POSTERIOR PREDICTIVE ASSESSMENT', 'IRT MODELS', 'DISTRIBUTIONS']",Bayesian Estimation of MIRT Models with General and Specific Latent Traits in MATLAB,2010
1697,"This paper presents the R package gRapHD for efficient selection of high-dimensional undirected graphical models. The package provides tools for selecting trees, forests, and decomposable models minimizing information criteria such as AIC or BIC, and for displaying the independence graphs of the models. It has also some useful tools for analysing graphical structures. It supports the use of discrete, continuous, or both types of variables.",WOS:000284596800001,JOURNAL OF STATISTICAL SOFTWARE,"['NETWORK THEORY', 'CONTINGENCY-TABLES', 'DISTRIBUTIONS', 'VARIABLES']",High-Dimensional Graphical Model Search with the gRapHD R Package,2010
1698,"We propose statistical tests to discriminate between the finite and infinite activity of jumps in a semimartingale discretely observed at high frequency. The two statistics allow for a symmetric treatment of the problem: we can either take the null hypothesis to be finite activity, or infinite activity. When implemented on high-frequency stock returns, both tests point toward the presence of infinite-activity jumps in the data.",WOS:000293716500013,ANNALS OF STATISTICS,"['STOCK RETURNS', 'DIFFUSION', 'MARKETS', 'OPTIONS', 'NOISE', 'MODEL']",TESTING WHETHER JUMPS HAVE FINITE OR INFINITE ACTIVITY,2011
1699,"Two-phase designs, in which for a large study a dichotomous outcome and partial or proxy information on risk factors is available, whereas precise or complete measurements on covariates have been obtained only in a stratified sub-sample, extend the standard case-control design and have been proven useful in practice. The application of two-phase designs, however, seems to be hampered by the lack of appropriate, easy-to-use software. This paper introduces sas-twophase-package, a collection of SAS-macros, to fulfill this task. sas-twophase-package implements weighted likelihood, pseudo likelihood and semi-parametric maximum likelihood estimation via the EM algorithm and via pro file likelihood in two-phase settings with dichotomous outcome and a given stratification.",WOS:000341020200001,JOURNAL OF STATISTICAL SOFTWARE,"['MAXIMUM-LIKELIHOOD', 'REGRESSION-MODELS', 'EM ALGORITHM', 'DESIGN']",A SAS Package for Logistic Two-Phase Studies,2014
1700,"ads is an R package that performs multi-scale spatial point pattern analyses through methods derived from Ripley's K-function. These methods apply to univariate, multivariate or marked point data mapped in a rectangular, circular or irregular-shaped sampling window. Specific tests of statistical significance based on Monte Carlo simulations are associated to these methods. The main features of ads is to call fast C subroutines for computing Ripley's unbiased local correction of edge effects for various sampling window con figurations and for performing Monte Carlo simulations. It thus allows one to analyze large datasets and to compute robust confidence envelopes. This paper is an introduction to ads version 1.5, focusing on its complementarity with the other R packages for spatial point pattern analysis, and on recent original developments towards the introduction of multivariate functions for analyzing spatial pattern of species diversity.",WOS:000349845400001,JOURNAL OF STATISTICAL SOFTWARE,"['NULL-MODELS', 'DIVERSITY', 'COMMUNITIES', 'STATISTICS', 'FOREST', 'TREE']",ads Package for R: A Fast Unbiased Implementation of the K-function Family for Studying Spatial Point Patterns in Irregular-Shaped Sampling Windows,2015
1701,The tolerance package for R provides a set of functions for estimating and plotting tolerance limits. This package provides a wide-range of functions for estimating discrete and continuous tolerance intervals as well as for estimating regression tolerance intervals. An additional tool of the tolerance package is the plotting capability for the univariate and regression settings as well as for the multivariate normal setting. The tolerance package's capabilities are illustrated using simulated data sets. Formulas used for the estimation procedures are also presented.,WOS:000281593400001,JOURNAL OF STATISTICAL SOFTWARE,"['PARAMETER EXPONENTIAL-DISTRIBUTION', 'LIMITS', 'DISTRIBUTIONS', 'RELIABILITY', 'CONFIDENCE']",tolerance: An R Package for Estimating Tolerance Intervals,2010
1702,"We introduce a wavelet-based model of local stationarity. This model enlarges the class of locally stationary, wavelet processes and contains processes whose spectral density function may change very suddenly in time. A notion of time-varying wavelet spectrum is uniquely defined as a wavelet-type transform of the autocovariance function with respect to so-called autocorrelation wavelets. This leads to a natural representation of the autocovariance which is localized on scales. We propose a pointwise adaptive estimator of the time-varying spectrum. The behavior of the estimator studied in homogeneous and inhomogeneous regions of the wavelet spectrum.",WOS:000258243000017,ANNALS OF STATISTICS,"['TIME-SERIES ANALYSIS', 'STATIONARY-PROCESSES', 'MODELS']",Locally adaptive estimation of evolutionary wavelet spectra,2008
1703,"We consider the testing and estimation of change-points-locations where the distribution abruptly changes-in a data sequence. A new approach, based on scan statistics utilizing graphs representing the similarity between observations, is proposed. The graph-based approach is nonparametric, and can be applied to any data set as long as an informative similarity measure on the sample space can be defined. Accurate analytic approximations to the significance of graph-based scan statistics for both the single change-point and the changed interval alternatives are provided. Simulations reveal that the new approach has better power than existing approaches when the dimension of the data is moderate to high. The new approach is illustrated on two applications: The determination of authorship of a classic novel, and the detection of change in a network over time.",WOS:000349738500006,ANNALS OF STATISTICS,"['LIKELIHOOD RATIO TESTS', 'CONFIDENCE-REGIONS', 'MULTIVARIATE', 'SEQUENCES', 'NETWORK']",GRAPH-BASED CHANGE-POINT DETECTION,2015
1704,"In this paper, we study the detection boundary for minimax hypothesis testing in the context of high-dimensional, sparse binary regression models. Motivated by genetic sequencing association studies for rare variant effects, we investigate the complexity of the hypothesis testing problem when the design matrix is sparse. We observe a new phenomenon in the behavior of detection boundary which does not occur in the case of Gaussian linear regression. We derive the detection boundary as a function of two components: a design matrix sparsity index and signal strength, each of which is a function of the sparsity of the alternative. For any alternative, if the design matrix sparsity index is too high, any test is asymptotically powerless irrespective of the magnitude of signal strength. For binary design matrices with the sparsity index that is not too high, our results are parallel to those in the Gaussian case. In this context, we derive detection boundaries for both dense and sparse regimes. For the dense regime, we show that the generalized likelihood ratio is rate optimal; for the sparse regime, we propose an extended Higher Criticism Test and show it is rate optimal and sharp. We illustrate the finite sample properties of the theoretical results using simulation studies.",WOS:000349738500013,ANNALS OF STATISTICS,"['HIGHER CRITICISM', 'VARIANTS', 'MIXTURES', 'SAMPLE']",HYPOTHESIS TESTING FOR HIGH-DIMENSIONAL SPARSE BINARY REGRESSION,2015
1705,"For random samples of size n obtained from p-variate normal distributions, we consider the classical likelihood ratio tests (LRT) for their means and covariance matrices in the high-dimensional setting. These test statistics have been extensively studied in multivariate analysis, and their limiting distributions under the null hypothesis were proved to be chi-square distributions as n goes to infinity and p remains fixed. In this paper, we consider the high-dimensional case where both p and n go to infinity with p/n -> y is an element of (0, 1]. We prove that the likelihood ratio test statistics under this assumption will converge in distribution to normal distributions with explicit means and variances. We perform the simulation study to show that the likelihood ratio tests using our central limit theorems outperform those using the traditional chi-square approximations for analyzing high-dimensional data.",WOS:000326991200012,ANNALS OF STATISTICS,"['SAMPLE CORRELATION-MATRICES', 'COVARIANCE MATRICES', 'ASYMPTOTIC-DISTRIBUTION', 'LARGEST ENTRIES', 'EQUALITY', 'INDEPENDENCE', 'UNBIASEDNESS', 'SPHERICITY', 'COHERENCE', 'CRITERIA']",CENTRAL LIMIT THEOREMS FOR CLASSICAL LIKELIHOOD RATIO TESTS FOR HIGH-DIMENSIONAL NORMAL DISTRIBUTIONS,2013
1706,"Variance function estimation in nonparametric regression is considered and the minimax rate of convergence is derived. We are particularly interested in the effect of the unknown mean on the estimation of the variance function. Our results indicate that, contrary to the common practice, it is not desirable to base the estimator of the variance function on the residuals from an optimal estimator of the mean when the mean function is not smooth. Instead it is more desirable to use estimators of the mean with minimal bias. On the other hand, when the mean function is very smooth, our numerical results show that the residual-based method performs better, but not substantial better than the first-order-difference-based estimator. In addition our asymptotic results also correct the optimal rate claimed in Hall and Carroll [J. Roy. Statist. Soc. Ser. B 51 (1989) 3-14].",WOS:000254502700006,ANNALS OF STATISTICS,"['SQUARE SUCCESSIVE DIFFERENCE', 'RATIO']",Effect of mean on variance function estimation in nonparametric regression,2008
1707,"R has gained explicit text mining support with the tm package enabling statisticians to answer many interesting research questions via statistical analysis or modeling of (text) corpora. However, we typically face two challenges when analyzing large corpora: (1) the amount of data to be processed in a single machine is usually limited by the available main memory (i.e., RAM), and (2) the more data to be analyzed the higher the need for efficient procedures for calculating valuable results. Fortunately, adequate programming models like MapReduce facilitate parallelization of text mining tasks and allow for processing data sets beyond what would fit into memory by using a distributed file system possibly spanning over several machines, e. g., in a cluster of workstations. In this paper we present a plug-in package to tm called tm.plugin.dc implementing a distributed corpus class which can take advantage of the Hadoop MapReduce library for large scale text mining tasks. We show on the basis of an application in culturomics that we can efficiently handle data sets of significant size.",WOS:000312288800001,JOURNAL OF STATISTICAL SOFTWARE,,A tm Plug-In for Distributed Text Mining in R,2012
1708,"Let (Y-k)(k is an element of Z) be a stationary sequence on a probability space (Omega, A, P) taking values in a standard Borel space Y. Consider the associated maximum likelihood estimator with respect to a parametrized family of hidden Markov models such that the law of the observations (Y-k)(k is an element of Z) is not assumed to be described by any of the hidden Markov models of this family. In this paper we investigate the consistency of this estimator in such misspecified models under mild assumptions.",WOS:000321844300012,ANNALS OF STATISTICS,"['STATE-SPACE-MODELS', 'PROBABILISTIC FUNCTIONS', 'FILTERS', 'CHAINS']",ASYMPTOTIC PROPERTIES OF THE MAXIMUM LIKELIHOOD ESTIMATION IN MISSPECIFIED HIDDEN MARKOV MODELS,2012
1709,"It is shown that a necessary and sufficient condition for an Archimedean copula generator to generate a d-dimensional copula is that the generator is a d-monotone function. The class of d-dimensional Archimedean copulas is shown to coincide with the class of survival copulas of d-dimensional l(1)-norm symmetric distributions that place no point mass at the origin, The d-monotone Archimedean copula generators may be characterized using a little-known integral transform of Williamson [Duke Math. J. 23 (1956) 189207] in an analogous manner to the well-known Bernstein-Widder characterization of completely monotone generators in terms of the Laplace transform. These insights allow the construction of new Archimedean Copula families and provide a general solution to the problem of sampling multivariate Archimedean copulas. They also yield useful expressions for the d-dimensional Kendall function and Kendall's rank correlation coefficients and facilitate the derivation of results On the existence of densities and the description of singular components for Archimedean copulas. The existence of a sharp lower bound for Archimedean copulas with respect to the positive tower orthant dependence ordering is shown.",WOS:000268605000016,ANNALS OF STATISTICS,"['FAMILIES', 'DERIVATIVES', 'DEPENDENCE', 'MARGINALS']","MULTIVARIATE ARCHIMEDEAN COPULAS, d-MONOTONE FUNCTIONS AND l(1)-NORM SYMMETRIC DISTRIBUTIONS",2009
1710,Homogeneity analysis combines the idea of maximizing the correlations between variables of a multivariate data set with that of optimal scaling. In this article we present methodological and practical issues of the R package homals which performs homogeneity analysis and various extensions. By setting rank constraints nonlinear principal component analysis can be performed. The variables can be partitioned into sets such that homogeneity analysis is extended to nonlinear canonical correlation analysis or to predictive models which emulate discriminant analysis and regression models. For each model the scale level of the variables can be taken into account by setting level constraints. All algorithms allow for missing values.,WOS:000268700700001,JOURNAL OF STATISTICAL SOFTWARE,"['VARIABLES', 'SETS']",Gifi Methods for Optimal Scaling in R: The Package homals,2009
1711,"This paper presents the reshape package for R, which provides a common framework for many types of data reshaping and aggregation. It uses a paradigm of 'melting' and 'casting', where the data are 'melted' into a form which distinguishes measured and identifying variables, and then 'cast' into a new shape, whether it be a data frame, list, or high dimensional array. The paper includes an introduction to the conceptual framework, practical advice for melting and casting, and a case study.",WOS:000252429400001,JOURNAL OF STATISTICAL SOFTWARE,,Reshaping data with the reshape package,2007
1712,"Stochastic processes are often used to model complex scientific problems in fields ranging from biology and finance to engineering and physical science. This paper investigates rate-optimal estimation of the volatility matrix of a high-dimensional Ito process observed with measurement errors at discrete time points. The minimax rate of convergence is established for estimating sparse volatility matrices. By combining the multi-scale and threshold approaches we construct a volatility matrix estimator to achieve the optimal convergence rate. The minimax lower bound is derived by considering a subclass of Ito processes for which the minimax lower bound is obtained through a novel equivalent model of covariance matrix estimation for independent but nonidentically distributed observations and through a delicate construction of the least favorable parameters. In addition, a simulation study was conducted to test the finite sample performance of the optimal estimator, and the simulation results were found to support the established asymptotic theory.",WOS:000326991200005,ANNALS OF STATISTICS,"['HIGH-FREQUENCY DATA', 'LARGE COVARIANCE MATRICES', 'MICROSTRUCTURE NOISE', 'DIFFUSION-PROCESSES', 'FINANCIAL DATA', 'OPTIMAL RATES', 'CONVERGENCE', 'SELECTION', 'TIME', 'LIKELIHOOD']",OPTIMAL SPARSE VOLATILITY MATRIX ESTIMATION FOR HIGH-DIMENSIONAL ITO PROCESSES WITH MEASUREMENT ERRORS,2013
1713,"Sequential monitoring in clinical trials is often employed to allow for early stopping and other interim decisions, while maintaining the type I error rate. However, sequential monitoring is typically described only in the context of a population model. We describe a computational method to implement sequential monitoring in a randomization-based context. In particular, we discuss a new technique for the computation of approximate conditional tests following restricted randomization procedures and then apply this technique to approximate the joint distribution of sequentially computed conditional randomization tests. We also describe the computation of a randomization-based analog of the information fraction. We apply these techniques to a restricted randomization procedure, Efron's [Biometrika 58 (1971) 403-417] biased coin design. These techniques require derivation of certain conditional probabilities;and conditional covariances of the randomization procedure. We employ combinatoric techniques to derive these for the biased coin design.",WOS:000304684900002,ANNALS OF STATISTICS,"['CLINICAL-TRIALS', 'INFERENCE', 'DESIGNS', 'RULES']",SEQUENTIAL MONITORING WITH CONDITIONAL RANDOMIZATION TESTS,2012
1714,"Our ltsa package implements the Durbin-Levinson and Trench algorithms and provides a general approach to the problems of fitting, forecasting and simulating linear time series models as well as fitting regression models with linear time series errors. For computational efficiency both algorithms are implemented in C and interfaced to R. Examples are given which illustrate the efficiency and accuracy of the algorithms. We provide a second package FGN which illustrates the use of the ltsa package with fractional Gaussian noise (FGN). It is hoped that the ltsa will provide a base for further time series software.",WOS:000252431500001,JOURNAL OF STATISTICAL SOFTWARE,"['INTERVENTION ANALYSIS', 'MODELS', 'INVERSION', 'SPECTRUM', 'MATRIX']",Algorithms for linear time series analysis: With R package,2007
1715,"We obtain a sharp convergence rate for banded covariance matrix estimates of stationary processes. A precise order of magnitude is derived for spectral radius of sample covariance matrices. We also consider a thresholded covariance matrix estimator that can better characterize sparsity if the true covariance matrix is sparse. As our main tool, we implement Toeplitz [Math. Ann. 70 (1911) 351-376] idea and relate eigenvalues of covariance matrices to the spectral densities or Fourier transforms of the covariances. We develop a large deviation result for quadratic forms of stationary processes using m-dependence approximation, under the framework of causal representation and physical dependence measures.",WOS:000304684900018,ANNALS OF STATISTICS,"['CENTRAL-LIMIT-THEOREM', 'LARGE DEVIATIONS', 'QUADRATIC-FORMS', 'LARGEST EIGENVALUE', 'AUTOCOVARIANCE MATRICES', 'MODERATE DEVIATIONS', 'GAUSSIAN-PROCESSES', 'RANDOM-VARIABLES', 'PERIODOGRAM', 'MAXIMUM']",COVARIANCE MATRIX ESTIMATION FOR STATIONARY TIME SERIES,2012
1716,"Panel data are observations of a continuous-time process at arbitrary times, for example, visits to a hospital to diagnose disease status. Multi-state models for such data are generally based on the Markov assumption. This article reviews the range of Markov models and their extensions which can be fitted to panel-observed data, and their implementation in the msm package for R. Transition intensities may vary between individuals, or with piecewise-constant time-dependent covariates, giving an inhomogeneous Markov model. Hidden Markov models can be used for multi-state processes which are misclassified or observed only through a noisy marker. The package is intended to be straightforward to use, flexible and comprehensively documented. Worked examples are given of the use of msm to model chronic disease progression and screening. Assessment of model fit, and potential future developments of the software, are also discussed.",WOS:000285981900001,JOURNAL OF STATISTICAL SOFTWARE,"['HIDDEN MARKOV-MODELS', 'OF-FIT TEST', 'DISEASE PROGRESSION', 'CONTINUOUS-TIME', 'HEPATITIS-C', 'RATES', 'DISABILITY']",Multi-State Models for Panel Data: The msm Package for R,2011
1717,"Suppose that Y is a scalar and X is a second-order stochastic process, where Y and X are conditionally independent given the random variables xi(1), ..., xi(p) which belong to the closed span L-X(2) of X. This paper investigates a unified framework for the inverse regression dimension-reduction problem. It is found that the identification of L-X(2) with the reproducing kernel Hilbert space of X provides a platform for a seamless extension from the finite- to infinite-dimensional settings. It also facilitates convenient computational algorithms that can be applied to a variety of models.",WOS:000265500500007,ANNALS OF STATISTICS,"['SAMPLE PATHS', 'PRINCIPAL', 'SPACES']",AN RKHS FORMULATION OF THE INVERSE REGRESSION DIMENSION-REDUCTION PROBLEM,2009
1718,"Global optimization is an important field of research both in mathematics and computer sciences. It has applications in nearly all fields of modern science and engineering. Memetic algorithms are powerful problem solvers in the domain of continuous optimization, as they offer a trade-off between exploration of the search space using an evolutionary algorithm scheme, and focused exploitation of promising regions with a local search algorithm. In particular, we describe the memetic algorithms with local search chains (MA-LS-Chains) paradigm, and the R package Rmalschains, which implements them. MA-LS-Chains has proven to be effective compared to other algorithms, especially in high-dimensional problem solving. In an experimental study, we demonstrate the advantages of using Rmalschains for high-dimension optimization problems in comparison to other optimization methods already available in R.",WOS:000392704300001,JOURNAL OF STATISTICAL SOFTWARE,"['DIFFERENTIAL EVOLUTION', 'GLOBAL OPTIMIZATION', 'GENETIC ALGORITHM', 'MINIMIZATION', 'DEOPTIM']",Memetic Algorithms with Local Search Chains in R: The Rmalschains Package,2016
1719,"The fgui R package is designed for developers of R packages, to help rapidly, and sometimes fully automatically, create a graphical user interface for command line R package. The interface is built upon the Tcl/Tk graphical interface include in R. The package further facilitates the developer by loading in the help files from the command line functions to provide context sensitive help to the user with no additional effort from the developer. Passing a function as argument to the routines in the fgui package creates a graphical interface for the function, and further options are available to tweak this interface for those who want more flexibility.",WOS:000266310300001,JOURNAL OF STATISTICAL SOFTWARE,,fgui: A Method for Automatically Creating Graphical User Interfaces for Command-Line R Packages,2009
1720,"Inhomogeneous random graph models encompass many network models such as stochastic block models and latent position models. We consider the problem of statistical estimation of the matrix of connection probabilities based on the observations of the adjacency matrix of the network. Taking the stochastic block model as an approximation, we construct estimators of network connection probabilities the ordinary block constant least squares estimator, and its restricted version. We show that they satisfy oracle inequalities with respect to the block constant oracle. As a consequence, we derive optimal rates of estimation of the probability matrix. Our results cover the important setting of sparse networks. Another consequence consists in establishing upper bounds on the minimax risks for graphon estimation in the L-2 norm when the probability matrix is sampled according to a graphon model. These bounds include an additional term accounting for the ""agnostic"" error induced by the variability of the latent unobserved variables of the graphon model. In this setting, the optimal rates are influenced not only by the bias and variance components as in usual nonparametric problems but also include the third component, which is the agnostic error. The results shed light on the differences between estimation under the empirical loss (the probability matrix estimation) and under the integrated loss (the graphon estimation).",WOS:000396804900010,ANNALS OF STATISTICS,,ORACLE INEQUALITIES FOR NETWORK MODELS AND SPARSE GRAPHON ESTIMATION,2017
1721,"Probability forecasts of events are routinely used in climate predictions, in forecasting default probabilities on bank loans or in estimating the probability of a patient's positive response to treatment. Scoring rules have long been used to assess the efficacy of the forecast probabilities after observing the occurrence, or nonoccurrence, of the predicted events. We develop herein a statistical theory for scoring rules and propose an alternative approach to the evaluation of probability forecasts. This approach uses loss functions relating the predicted to the actual probabilities of the events and applies martingale theory to exploit the temporal structure between the forecast and the subsequent occurrence or nonoccurrence of the event.",WOS:000299186500007,ANNALS OF STATISTICS,"['SCORING RULES', 'PREDICTIVE ACCURACY', 'STATISTICS', 'ABILITY']",EVALUATING PROBABILITY FORECASTS,2011
1722,,WOS:000258243000003,ANNALS OF STATISTICS,"['SURROGATE OBJECTIVE FUNCTIONS', 'EM ALGORITHM', 'CONVERGENCE']",Discussion: One-step sparse estimates in nonconcave penalized likelihood models: Who cares if it is a white cat or a black cat?,2008
1723,"For functional data lying on an unknown nonlinear low-dimensional space, we study manifold learning and introduce the notions of manifold mean, manifold modes of functional variation and of functional manifold components. These constitute nonlinear representations of functional data that complement classical linear representations such as eigenfunctions and functional principal components. Our manifold learning procedures borrow ideas from existing nonlinear dimension reduction methods, which we modify to address functional data settings. In simulations and applications, we study examples of functional data which lie on a manifold and validate the superior behavior of manifold mean and functional manifold components over traditional cross-sectional mean and functional principal components. We also include consistency proofs for our estimators under certain assumptions.",WOS:000304684900001,ANNALS OF STATISTICS,"['DIMENSIONALITY REDUCTION', 'SAMPLE CURVES', 'REGRESSION', 'EIGENMAPS', 'MODES']",NONLINEAR MANIFOLD REPRESENTATIONS FOR FUNCTIONAL DATA,2012
1724,"Clustering is the partitioning of a set of objects into groups (clusters) so that objects within a group are more similar to each others than objects in different groups. Most of the clustering algorithms depend on some assumptions in order to define the subgroups present in a data set. As a consequence, the resulting clustering scheme requires some sort of evaluation as regards its validity.
The evaluation procedure has to tackle difficult problems such as the quality of clusters, the degree with which a clustering scheme fits a specific data set and the optimal number of clusters in a partitioning. In the literature, a wide variety of indices have been proposed to find the optimal number of clusters in a partitioning of a data set during the clustering process. However, for most of indices proposed in the literature, programs are unavailable to test these indices and compare them.
The R package NbClust has been developed for that purpose. It provides 30 indices which determine the number of clusters in a data set and it offers also the best clustering scheme from different results to the user. In addition, it provides a function to perform k-means and hierarchical clustering with different distance measures and aggregation methods. Any combination of validation indices and clustering methods can be requested in a single function call. This enables the user to simultaneously evaluate several clustering schemes while varying the number of clusters, to help determining the most appropriate number of clusters for the data set of interest.",WOS:000349840900001,JOURNAL OF STATISTICAL SOFTWARE,"['GROUPING DATA', 'VALIDATION', 'CRITERION', 'INDEXES']",Nbclust: An R Package for Determining the Relevant Number of Clusters in a Data Set,2014
1725,"In this paper, we introduce an asymptotic test procedure to assess the stability of volatilities and cross-volatilites of linear and nonlinear multivariate time series models, The test is very flexible as it can be applied, for example, to many of the multivariate GARCH models established in the literature, and also works well in the case of high dimensionality of the underlying data. Since it is nonparametric, the procedure avoids the difficulties associated with parametric model selection, model fitting and parameter estimation. We provide the theoretical foundation for the test and demonstrate its applicability via a Simulation study and an analysis of financial data. Extensions to multiple changes and the case of infinite fourth moments are also discussed.",WOS:000271673700012,ANNALS OF STATISTICS,"['CONDITIONAL HETEROSKEDASTICITY', 'AUTOREGRESSIVE PROCESSES', 'WEAK DEPENDENCE', 'GARCH PROCESSES', 'ASSET RETURNS', 'ARCH MODELS', 'STATIONARITY', 'SEQUENCES', 'VARIANCE', 'SQUARES']",BREAK DETECTION IN THE COVARIANCE STRUCTURE OF MULTIVARIATE TIME SERIES MODELS,2009
1726,"The datasets being produced by high-throughput biological experiments, such as microarrays, have forced biologists to turn to sophisticated statistical analysis and visualization tools in order to understand their data. We address the particular need for an open-source exploratory data analysis tool that applies numerical methods in coordination with interactive graphics to the analysis of experimental data. The software package, known as explorase, provides a graphical user interface (GUI) on top of the R platform for statistical computing and the GGobi software for multivariate interactive graphics. The GUI is designed for use by biologists, many of whom are unfamiliar with the R language. It displays metadata about experimental design and biological entities in tables that are sortable and filterable. There are menu shortcuts to the analysis methods implemented in R, including graphical interfaces to linear modeling tools. The GUI is linked to data plots in GGobi through a brush tool that simultaneously colors rows in the entity information table and points in the GGobi plots.
explorase is an R package publicly available from Bioconductor and is a tool in the MetNet platform for the analysis of systems biology data.",WOS:000255794600001,JOURNAL OF STATISTICAL SOFTWARE,['SOFTWARE'],explorase: Multivariate exploratory analysis and visualization for systems biology,2008
1727,The objective of the present paper is to develop a minimax theory for the varying coefficient model in a nonasymptotic setting. We consider a high-dimensional sparse varying coefficient model where only few of the covariates are present and only some of those covariates are time dependent. Our analysis allows the time-dependent covariates to have different degrees of smoothness and to be spatially inhomogeneous. We develop the minimax lower bounds for the quadratic risk and construct an adaptive estimator which attains those lower bounds within a constant (if all time-dependent covariates are spatially homogeneous) or logarithmic factor of the number of observations.,WOS:000355768700011,ANNALS OF STATISTICS,"['LONGITUDINAL DATA', 'ORACLE INEQUALITIES', 'REGRESSION-MODELS', 'SPLINE ESTIMATION', 'LINEAR-MODELS', 'GROUP LASSO', 'INFERENCE', 'SELECTION', 'VARIABLES']",SPARSE HIGH-DIMENSIONAL VARYING COEFFICIENT MODEL: NONASYMPTOTIC MINIMAX STUDY,2015
1728,"To simulate fractional Brownian motion indexed by a manifold poses serious numerical problems: storage, computing time and choice of an appropriate grid. We propose an effective and fast method, valid not only for fractional Brownian fields indexed by a manifold, but for any Gaussian fields indexed by a manifold. The performance of our method is illustrated with different manifolds (sphere, hyperboloid).",WOS:000281593300001,JOURNAL OF STATISTICAL SOFTWARE,,On Simulation of Manifold Indexed Fractional Gaussian Fields,2010
1729,"Many data analysis problems involve the application of a split-apply-combine strategy, where you break up a big problem into manageable pieces, operate on each piece independently and then put all the pieces back together. This insight gives rise to a new R package that allows you to smoothly apply this strategy, without having to worry about the type of structure in which your data is stored.
The paper includes two case studies showing how these insights make it easier to work with batting records for veteran baseball players and a large 3d array of spatio-temporal ozone measurements.",WOS:000289228100001,JOURNAL OF STATISTICAL SOFTWARE,,The Split-Apply-Combine Strategy for Data Analysis,2011
1730,"We study the absolute penalized maximum partial likelihood estimator in sparse, high-dimensional Cox proportional hazards regression models where the number of time-dependent covariates can be larger than the sample size. We establish oracle inequalities based on natural extensions of the compatibility and cone invertibility factors of the Hessian matrix at the true regression coefficients. Similar results based on an extension of the restricted eigenvalue can be also proved by our method. However, the presented oracle inequalities are sharper since the compatibility and cone invertibility factors are always greater than the corresponding restricted eigenvalue. In the Cox regression model, the Hessian matrix is based on time-dependent covariates in censored risk sets, so that the compatibility and cone invertibility factors, and the restricted eigenvalue as well, are random variables even when they are evaluated for the Hessian at the true regression coefficients. Under mild conditions, we prove that these quantities are bounded from below by positive constants for time-dependent covariates, including cases where the number of covariates is of greater order than the sample size. Consequently, the compatibility and cone invertibility factors can be treated as positive constants in our oracle inequalities.",WOS:000321847600004,ANNALS OF STATISTICS,"['PROPORTIONAL HAZARDS MODEL', 'NONCONCAVE PENALIZED LIKELIHOOD', 'VARIABLE SELECTION', 'DANTZIG SELECTOR', 'REGRESSION-MODEL', 'LARGE-SAMPLE', 'SPARSITY', 'REGULARIZATION', 'CONSISTENCY']",ORACLE INEQUALITIES FOR THE LASSO IN THE COX MODEL,2013
1731,"This paper introduces a Projected Principal Component Analysis (Projected-PCA), which employs principal component analysis to the projected (smoothed) data matrix onto a given linear space spanned by covariates. When it applies to high-dimensional factor analysis, the projection removes noise components. We show that the unobserved latent factors can be more accurately estimated than the conventional PCA if the projection is genuine, or more precisely, when the factor loading matrices are related to the projected linear space. When the dimensionality is large, the factors can be estimated accurately even when the sample size is finite. We propose a flexible semiparametric factor model, which decomposes the factor loading matrix into the component that can be explained by subject-specific covariates and the orthogonal residual component. The covariates' effects on the factor loadings are further modeled by the additive model via sieve approximations. By using the newly proposed Projected-PCA, the rates of convergence of the smooth factor loading matrices are obtained, which are much faster than those of the conventional factor analysis. The convergence is achieved even when the sample size is finite and is particularly appealing in the high-dimension-low-sample-size situation. This leads us to developing nonparametric tests on whether observed covariates have explaining powers on the loadings and whether they fully explain the loadings. The proposed method is illustrated by both simulated data and the returns of the components of the S&P 500 index.",WOS:000368022000008,ANNALS OF STATISTICS,"['DYNAMIC-FACTOR MODEL', 'CONSISTENT COVARIANCE-MATRIX', 'APPROXIMATE FACTOR MODELS', 'HIGH-DIMENSION', 'OPTIMAL RATES', 'SPARSE PCA', 'NUMBER', 'DEPENDENCE', 'HETEROSKEDASTICITY', 'IDENTIFICATION']",PROJECTED PRINCIPAL COMPONENT ANALYSIS IN FACTOR MODELS,2016
1732,"The R package gdistance provides classes and functions to calculate various distance measures and routes in heterogeneous geographic spaces represented as grids. Least-cost distances as well as more complex distances based on (constrained) random walks can be calculated. Also the corresponding routes or probabilities of passing each cell can be determined. The package implements classes to store the data about the probability or cost of transitioning from one cell to another on a grid in a memory-efficient sparse format. These classes make it possible to manipulate the values of cell-to-cell movement directly, which offers flexibility and the possibility to use asymmetric values. The novel distances implemented in the package are used in geographical genetics (applying circuit theory), but also have applications in other fields of geospatial analysis.",WOS:000398467300001,JOURNAL OF STATISTICAL SOFTWARE,,R Package gdistance: Distances and Routes on Geographical Grids,2017
1733,"The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the bene fits of parallel processing with several types of models.",WOS:000260799600001,JOURNAL OF STATISTICAL SOFTWARE,['ALGORITHMS'],Building Predictive Models in R Using the caret Package,2008
1734,"Pattern-mixture models have gained considerable interest in recent years. Pattern mixture modeling allows the analysis of incomplete longitudinal outcomes under a variety of missingness mechanisms. In this manuscript, we describe a SAS program which combines R functionalities to fit pattern-mixture models, considering the cases that missingness mechanisms are at random and not at random. Patterns are defined based on missingness at every time point and parameter estimation is based on a full group-by time interaction. The program implements a multiple imputation method under so-called identifying restrictions. The code is illustrated using data from a placebo-controlled clinical trial. This manuscript and the program are directed to SAS users with minimal knowledge of the R language.",WOS:000384910000001,JOURNAL OF STATISTICAL SOFTWARE,"['MISSING DATA', 'FIT']",A SAS Program Combining R Functionalities to Implement Pattern-Mixture Models,2015
1735,"Most classification algorithms deal with datasets which have a set of input features, the variables to be used as predictors, and only one output class, the variable to be predicted. However, in late years many scenarios in which the classifier has to work with several outputs have come to life. Automatic labeling of text documents, image annotation or protein classification are among them. Multilabel datasets are the product of these new needs, and they have many specific traits. The mldr package allows the user to load datasets of this kind, obtain their characteristics, produce specialized plots, and manipulate them. The goal is to provide the exploratory tools needed to analyze multilabel datasets, as well as the transformation and manipulation functions that will make possible to apply binary and multiclass classification models to this data or the development of new multilabel classifiers. Thanks to its integrated user interface, the exploratory functions will be available even to non-specialized R users.",WOS:000368551800012,R JOURNAL,['CLASSIFICATION'],Working with Multilabel Datasets in R: The mldr Package,2015
1736,"This paper studies estimation in functional linear quantile regression in which the dependent variable is scalar while the covariate is a function, and the conditional quantile for each fixed quantile index is modeled as a linear functional of the covariate. Here we suppose that covariates are discretely observed and sampling points may differ across subjects, where the number of measurements per subject increases as the sample size. Also, we allow the quantile index to vary over a given subset of the open unit interval, so the slope function is a function of two variables: (typically) time and quantile index. Likewise, the conditional quantile function is a function of the quantile index and the covariate. We consider an estimator for the slope function based on the principal component basis. An estimator for the conditional quantile function is obtained by a plug-in method. Since the so-constructed plug-in estimator not necessarily satisfies the monotonicity constraint with respect to the quantile index, we also consider a class of monotonized estimators for the conditional quantile function. We establish rates of convergence for these estimators under suitable norms, showing that these rates are optimal in a minimax sense under some smoothness assumptions on the covariance kernel of the covariate and the slope function. Empirical choice of the cutoff level is studied by using simulations.",WOS:000321845400013,ANNALS OF STATISTICS,"['SMOOTHING SPLINES', 'MODEL', 'CONVERGENCE', 'METHODOLOGY', 'RATES']",ESTIMATION IN FUNCTIONAL LINEAR QUANTILE REGRESSION,2012
1737,"Let f be a nonincreasing function defined on [0, 1]. Under standard regularity conditions, we derive the asymptotic distribution of the supremum norm of the difference between f and its Grenander-type estimator on sub-intervals of [0, 1]. The rate of convergence is found to be of order (n/logn)(-1/3) and the limiting distribution to be Gumbel.",WOS:000310650900011,ANNALS OF STATISTICS,"['MONOTONE DENSITY', 'ASYMPTOTIC NORMALITY', 'SMOOTH MONOTONE', 'REGRESSION']",THE LIMIT DISTRIBUTION OF THE L-infinity-ERROR OF GRENANDER-TYPE ESTIMATORS,2012
1738,"We address the issue of variable selection in the regression model with very high ambient dimension, that is, when the number of variables is very large. The main focus is on the situation where the number of relevant variables, called intrinsic dimension, is much smaller than the ambient dimension d. Without assuming any parametric form of the underlying regression function, we get tight conditions making it possible to consistently estimate the set of relevant variables. These conditions relate the intrinsic dimension to the ambient dimension and to the sample size. The procedure that is provably consistent under these tight conditions is based on comparing quadratic functionals of the empirical Fourier coefficients with appropriately chosen threshold values.
The asymptotic analysis reveals the presence of two quite different regimes. The first regime is when the intrinsic dimension is fixed. In this case the situation in nonparametric regression is the same as in linear regression, that is, consistent variable selection is possible if and only if log d is small compared to the sample size n. The picture is different in the second regime, that is, when the number of relevant variables denoted by s tends to infinity as n -> infinity. Then we prove that consistent variable selection in nonparametric set- up is possible only if s + log log d is small compared to log n. We apply these results to derive minimax separation rates for the problem of variable selection.",WOS:000321844300011,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'NONPARAMETRIC REGRESSION', 'MODEL SELECTION', 'ASYMPTOTIC EQUIVALENCE', 'ADAPTIVE ESTIMATION', 'GROUP SPARSITY', 'WHITE-NOISE', 'MULTIVARIATE', 'REDUCTION', 'RECOVERY']",TIGHT CONDITIONS FOR CONSISTENCY OF VARIABLE SELECTION IN THE CONTEXT OF HIGH DIMENSIONALITY,2012
1739,"We analyze a class of estimators based on convex relaxation for solving high-dimensional matrix decomposition problems. The observations are noisy realizations of a linear transformation (sic) of the sum of an (approximately) low rank matrix Theta(star) with a second matrix Gamma(star) endowed with a complementary form of low-dimensional structure; this set-up includes many statistical models of interest, including factor analysis, multi-task regression and robust covariance estimation. We derive a general theorem that bounds the Frobenius norm error for an estimate of the pair (Theta(star), Gamma(star)) obtained by solving a convex optimization problem that combines the nuclear norm with a general decomposable regularizer. Our results use a ""spikiness"" condition that is related to, but milder than, singular vector incoherence. We specialize our general result to two cases that have been studied in past work: low rank plus an entrywise sparse matrix, and low rank plus a columnwise sparse matrix. For both models, our theory yields nonasymptotic Frobenius error bounds for both deterministic and stochastic noise matrices, and applies to matrices Theta(star) that can be exactly or approximately low rank, and matrices Gamma(star) that can be exactly or approximately sparse. Moreover, for the case of stochastic noise matrices and the identity observation operator, we establish matching lower bounds on the minimax error. The sharpness of our nonasymptotic predictions is confirmed by numerical simulations.",WOS:000307608000020,ANNALS OF STATISTICS,"['LOW-RANK MATRICES', 'LINEAR-REGRESSION']",NOISY MATRIX DECOMPOSITION VIA CONVEX RELAXATION: OPTIMAL RATES IN HIGH DIMENSIONS,2012
1740,"A web interface, named WebBUGS, is developed to conduct Bayesian analysis online over the Internet through Open BUGS and R. WebBUGS can be used with the minimum requirement of a web browser both remotely and locally. WebBUGS has many collaborative features such as email notification and sharing. WebBUGS also eases the use of Open BUGS by providing built-in model templates, data management module, and other useful modules. In this paper, the use of WebBUGS is illustrated and discussed.",WOS:000349841100001,JOURNAL OF STATISTICAL SOFTWARE,"['GROWTH CURVE MODELS', 'CHAIN MONTE-CARLO']",WebBUGS : Conducting Bayesian Statistical Analysis Online,2014
1741,"This paper considers the problem of testing for the presence of a continuous part in a semimartingale sampled at high frequency. We provide two tests, one where the null hypothesis is that a continuous component is present, the other where the continuous component is absent, and the model is then driven by a pure jump process. When applied to high-frequency individual stock data, both tests point toward the need to include a continuous component in the model.",WOS:000282402800016,ANNALS OF STATISTICS,"['STOCK RETURNS', 'JUMPS']",IS BROWNIAN MOTION NECESSARY TO MODEL HIGH-FREQUENCY DATA?,2010
1742,"The R function kofnGA conducts a genetic algorithm search for the best subset of k items from a set of n alternatives, given an objective function that measures the quality of a subset. The function fills a gap in the presently available subset selection software, which typically searches over a range of subset sizes, restricts the types of objective functions considered, or does not include freely available code. The new function is demonstrated on two types of problem where a fixed-size subset search is desirable: design of environmental monitoring networks, and D-optimal design of experiments. Additionally, the performance is evaluated on a class of constructed test problems with a novel design that is interesting in its own right.",WOS:000366015100001,JOURNAL OF STATISTICAL SOFTWARE,['CONSTRUCTION'],A Genetic Algorithm for Selection of Fixed-Size Subsets with Application to Design Problems,2015
1743,"Motivated by recent work studying massive imaging data in the neuroimaging literature, we propose multivariate varying coefficient models (MVCM) for modeling the relation between multiple functional responses and a set of covariates. We develop several statistical inference procedures for MVCM and systematically study their theoretical properties. We first establish the weak convergence of the local linear estimate of coefficient functions, as well as its asymptotic bias and variance, and then we derive asymptotic bias and mean integrated squared error of smoothed individual functions and their uniform convergence rate. We establish the uniform convergence rate of the estimated covariance function of the individual functions and its associated eigenvalue and eigenfunctions. We propose a global test for linear hypotheses of varying coefficient functions, and derive its asymptotic distribution under the null hypothesis. We also propose a simultaneous confidence band for each individual effect curve. We conduct Monte Carlo simulation to examine the finite-sample performance of the proposed procedures. We apply MVCM to investigate the development of white matter diffusivities along the genu tract of the corpus callosum in a clinical study of neurodevelopment.",WOS:000321844300010,ANNALS OF STATISTICS,"['PRINCIPAL-COMPONENTS-ANALYSIS', 'SIMULTANEOUS CONFIDENCE BANDS', 'LONGITUDINAL DATA', 'STATISTICAL-ANALYSIS', 'DIFFUSION TENSOR', 'LINEAR-MODELS', 'REGRESSION', 'INFERENCE']",MULTIVARIATE VARYING COEFFICIENT MODEL FOR FUNCTIONAL RESPONSES,2012
1744,"This special volume of the Journal of Statistical Software on political methodology includes 14 papers, with wide-ranging software contributions of political scientists to their own field, and more generally to statistical data analysis in the the social sciences and beyond. Special emphasis is given to software that is written in or can cooperate with the R system for statistical computing.",WOS:000292096500001,JOURNAL OF STATISTICAL SOFTWARE,,"An Introduction to the Special Volume on ""Political Methodology""",2011
1745,In this paper we present the R package gRain for propagation in graphical independence networks (for which Bayesian networks is a special instance). The paper includes a description of the theory behind the computations. The main part of the paper is an illustration of how to use the package. The paper also illustrates how to turn a graphical model and data into an independence network.,WOS:000301073100001,JOURNAL OF STATISTICAL SOFTWARE,['SYSTEMS'],Graphical Independence Networks with the gRain Package for R,2012
1746,"We consider settings where data are available on a nonparametric function and various partial derivatives. Such circumstances arise in practice, for example in the joint estimation of cost and input functions in economics. We show that when derivative data are available, local averages can be replaced in certain dimensions by nonlocal averages, thus reducing the nonparametric dimension of the problem. We derive optimal rates of convergence and conditions under which dimension reduction is achieved. Kernel estimators and their properties are analyzed, although other estimators, such as local polynomial, spline and nonparametric least squares, may also be used. Simulations and an application to the estimation of electricity distribution costs are included.",WOS:000247498100012,ANNALS OF STATISTICS,"['PROJECTION PURSUIT', 'ADDITIVE-MODELS', 'REGRESSION', 'CONVERGENCE', 'RATES']",Nonparametric estimation when data on derivatives are available,2007
1747,"Clustering streams of continuously arriving data has become an important application of data mining in recent years and efficient algorithms have been proposed by several researchers. However, clustering alone neglects the fact that data in a data stream is not only characterized by the proximity of data points which is used by clustering, but also by a temporal component. The extensible Markov model (EMM) adds the temporal component to data stream clustering by superimposing a dynamically adapting Markov chain. In this paper we introduce the implementation of the R extension package rEMM which implements EMM and we discuss some examples and applications.",WOS:000281587600001,JOURNAL OF STATISTICAL SOFTWARE,['SEQUENCES'],rEMM: Extensible Markov Model for Data Stream Clustering in R,2010
1748,"When a linear model is rank-deficient, then predictions based on that model become questionable because not all predictions are uniquely estimable. However, some of them are, and the estimability package provides tools that package developers can use to tell which is which. With the use of these tools, a model object's predict method could return estimable predictions as-is while flagging non-estimable ones in some way, so that the user can know which predictions to believe. The estimability package also provides, as a demonstration, an estimability-enhanced epredict method to use in place of predict for models fitted using the stats package.",WOS:000357431900017,R JOURNAL,,Estimability Tools for Package Developers,2015
1749,"Canonical correlation analysis is a widely used multivariate statistical technique for exploring the relation between two sets of variables. This paper considers the problem of estimating the leading canonical correlation directions in high-dimensional settings. Recently, under the assumption that the leading canonical correlation directions are sparse, various procedures have been proposed for many high-dimensional applications involving massive data sets. However, there has been few theoretical justification available in the literature. In this paper, we establish rate-optimal nonasymptotic minimax estimation with respect to an appropriate loss function for a wide range of model spaces. Two interesting phenomena are observed. First, the minimax rates are not affected by the presence of nuisance parameters, namely the covariance matrices of the two sets of random variables, though they need to be estimated in the canonical correlation analysis problem. Second, we allow the presence of the residual canonical correlation directions. However, they do not influence the minimax rates under a mild condition on eigengap. A generalized sin-theta theorem and an empirical process bound for Gaussian quadratic forms under rank constraint are used to establish the minimax upper bounds, which may be of independent interest.",WOS:000362697700011,ANNALS OF STATISTICS,"['PRINCIPAL-COMPONENTS-ANALYSIS', 'LARGEST EIGENVALUE', 'HIGH DIMENSIONS', 'CONVERGENCE', 'RATES', 'PCA']",MINIMAX ESTIMATION IN SPARSE CANONICAL CORRELATION ANALYSIS,2015
1750,"Quaternary code (QC) designs form an attractive class of nonregular factorial fractions. We develop a complementary set theory for characterizing optimal QC designs that are highly fractionated in the sense of accommodating a large number of factors. This is in contrast to existing theoretical results which work only for a relatively small number of factors. While the use of imaginary numbers to represent the Gray map associated with QC designs facilitates the derivation, establishing a link with foldovers of regular fractions helps in presenting our results in a neat form.",WOS:000330204900003,ANNALS OF STATISTICS,"['FRACTIONAL FACTORIAL-DESIGNS', 'MINIMUM ABERRATION', 'NONREGULAR DESIGNS', 'RESOLUTION', 'ONE-8TH']",A COMPLEMENTARY SET THEORY FOR QUATERNARY CODE DESIGNS,2013
1751,"Missing data are a common occurrence in real datasets. For epidemiological and prognostic factors studies in medicine, multiple imputation is becoming the standard route to estimating models with missing covariate data under a missing-at-random assumption. We describe ice, an implementation in Stata of the MICE approach to multiple imputation. Real data from an observational study in ovarian cancer are used to illustrate the most important of the many options available with ice. We remark briefly on the new database architecture and procedures for multiple imputation introduced in releases 11 and 12 of Stata.",WOS:000298032600001,JOURNAL OF STATISTICAL SOFTWARE,"['FULLY CONDITIONAL SPECIFICATION', 'MISSING VALUES', 'UPDATE', 'EMPHASIS', 'MODEL', 'ICE']",Multiple Imputation by Chained Equations (MICE): Implementation in Stata,2011
1752,"The inverse Gaussian distribution (IGD) is a well known and often used probability distribution for which fully reliable numerical algorithms have not been available. We develop fast, reliable basic probability functions (dinvgauss, pinvgauss, qinvgauss and rinvgauss) for the IGD that work for all possible parameter values and which achieve close to full machine accuracy. The most challenging task is to compute quantiles for given cumulative probabilities and we develop a simple but elegant mathematical solution to this problem. We show that Newton's method for finding the quantiles of a IGD always converges monotonically when started from the mode of the distribution. Simple Taylor series expansions are used to improve accuracy on the log-scale. The IGD probability functions provide the same options and obey the same conventions as do probability functions provided in the stats package.",WOS:000385276100025,R JOURNAL,['GENERALIZED LINEAR-MODELS'],statmod: Probability Calculations for the Inverse Gaussian Distribution,2016
1753,"In this paper, we deal with the data-driven selection of multidimensional and possibly anisotropic bandwidths in the general framework of kernel empirical risk minimization. We propose a universal selection rule, which leads to optimal adaptive results in a large variety of statistical models such as nonparametric robust regression and statistical learning with errors in variables. These results are stated in the context of smooth loss functions, where the gradient of the risk appears as a good criterion to measure the performance of our estimators. The selection rule consists of a comparison of gradient empirical risks. It can be viewed as a nontrivial improvement of the so-called Goldenshluger-Lepski method to nonlinear estimators. Furthermore, one main advantage of our selection rule is the nondependency on the Hessian matrix of the risk, usually involved in standard adaptive procedures.",WOS:000357441000013,ANNALS OF STATISTICS,"['ORACLE INEQUALITIES', 'ADAPTIVE ESTIMATION', 'DENSITY-ESTIMATION', 'FAST RATES', 'ADAPTATION', 'DECONVOLUTION', 'PERFORMANCE', 'REGRESSION', 'ROBUST', 'BOUNDS']",BANDWIDTH SELECTION IN KERNEL EMPIRICAL RISK MINIMIZATION VIA THE GRADIENT,2015
1754,"This article surveys currently available implementations in R for continuous global optimization problems. A new R package globalOptTests is presented that provides a set of standard test problems for continuous global optimization based on C functions by Ali, Khompatraporn, and Zabinsky (2005). 48 of the objective functions contained in the package are used in empirical comparison of 18 R implementations in terms of the quality of the solutions found and speed.",WOS:000345289800001,JOURNAL OF STATISTICAL SOFTWARE,"['DIFFERENTIAL EVOLUTION', 'PACKAGE', 'ALGORITHMS', 'DEOPTIM']",Continuous Global Optimization in R,2014
1755,"Beta regression -an increasingly popular approach for modeling rates and proportions - is extended in various directions: (a) bias correction/reduction of the maximum likelihood estimator, (b) beta regression tree models by means of recursive partitioning, (c) latent class beta regression by means of finite mixture models. All three extensions may be of importance for enhancing the beta regression toolbox in practice to provide more reliable inference and capture both observed and unobserved/latent heterogeneity in the data. Using the analogy of Smithson and Verkuilen (2006), these extensions make beta regression not only ""a better lemon squeezer"" (compared to classical least squares regression) but a full-fledged modern juicer offering lemon-based drinks: shaken and stirred (bias correction and reduction), mixed (finite mixture model), or partitioned (tree model). All three extensions are provided in the R package betareg (at least 2.4-0), building on generic algorithms and implementations for bias correction/reduction, model-based recursive partioning, and finite mixture models, respectively. Specifically, the new functions betatree () and betamix () reuse the object-oriented flexible implementation from the R packages party and flexmix, respectively.",WOS:000305118500001,JOURNAL OF STATISTICAL SOFTWARE,"['STRUCTURAL-CHANGE', 'TESTS', 'MODEL', 'PARAMETERS', 'INFERENCE', 'VARIABLES', 'POINT']","Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned",2012
1756,"This paper discusses asymptotically distribution free tests for the classical goodness-of-fit hypothesis of all error distribution in nonparametric regression models. These tests are based on the same martingale transform of the residual empirical process as used in the one sample location model. This transformation eliminates extra randomization due to covariates but not due the errors, which is intrinsically present in the estimators of the regression function. Thus, tests based on the transformed process have, generally, better power. The results of this paper are applicable as soon as asymptotic uniform linearity of nonparametric residual empirical process is available. In particular they are applicable under the conditions Stipulated in recent papers of Akritas and Van Keilegom and Muller, Schick and Wefelmeyer.",WOS:000271673500003,ANNALS OF STATISTICS,"['TESTS', 'MODELS']",GOODNESS-OF-FIT PROBLEM FOR ERRORS IN NONPARAMETRIC REGRESSION: DISTRIBUTION FREE APPROACH,2009
1757,"This special volume celebrates the 20th anniversary of the Journal of Statistical Software (JSS) and is a Festschrift for its founding editor Jan de Leeuw. Jan recently retired from his long-held position as founding chair of the Department of Statistics at the University of California, Los Angeles. The contributions to this special volume look back at some of his research interests and accomplishments during the half-century that he has been active in psychometrics and statistics. In this introduction, the guest editors also reminisce on their own first encounters with Jan, ten years ago. Since that time JSS has solidified its place as a leading journal of computational statistics, a fact that has a lot to do with Jan's stewardship. We include a brief history of JSS.",WOS:000389126500001,JOURNAL OF STATISTICAL SOFTWARE,,Honoring the Lion: A Festschrift for Jan de Leeuw,2016
1758,"This paper studies the estimation of a high-dimensional Gaussian graphical model (GGM). Typically, the existing methods depend on regularization techniques. As a result, it is necessary to choose the regularized parameter. However, the precise relationship between the regularized parameter and the number of false edges in GGM estimation is unclear. In this paper we propose an alternative method by a multiple testing procedure. Based on our new test statistics for conditional dependence, we propose a simultaneous testing procedure for conditional dependence in GGM. Our method can control the false discovery rate (FDR) asymptotically. The numerical performance of the proposed method shows that our method works quite well.",WOS:000330204900009,ANNALS OF STATISTICS,"['COVARIANCE-MATRIX ESTIMATION', 'DANTZIG SELECTOR', 'LASSO', 'RECOVERY']",GAUSSIAN GRAPHICAL MODEL ESTIMATION WITH FALSE DISCOVERY RATE CONTROL,2013
1759,"Joint models for longitudinal and time-to-event data constitute an attractive modeling framework that has received a lot of interest in the recent years. This paper presents the capabilities of the R package JMbayes for fitting these models under a Bayesian approach using Markov chain Monte Carlo algorithms. JMbayes can fit a wide range of joint models, including among others joint models for continuous and categorical longitudinal responses, and provides several options for modeling the association structure between the two outcomes. In addition, this package can be used to derive dynamic predictions for both outcomes, and offers several tools to validate these predictions in terms of discrimination and calibration. All these features are illustrated using a real data example on patients with primary biliary cirrhosis.",WOS:000389072600001,JOURNAL OF STATISTICAL SOFTWARE,"['SURVIVAL-DATA', 'PREDICTIVE ACCURACY', 'PROSTATE-CANCER', 'BIOMARKERS', 'REGRESSION', 'SPLINES']",The R Package JMbayes for Fitting Joint Models for Longitudinal and Time-to-Event Data Using MCMC,2016
1760,"True to their functional roots, most R functions are side-effect-free, and users expect datatypes to be persistent. However, these semantics complicate the creation of efficient and dynamic data structures. Here, we describe the implementation of stack and queue data structures satisfying these conditions in R, available in the CRAN package rstackdeque. Guided by important work in purely functional languages, we look at both partially-and fully-persistent versions of queues, comparing their performance characteristics. Finally, we illustrate the usefulness of such dynamic structures with examples of generating and solving mazes.",WOS:000357431900011,R JOURNAL,,Implementing Persistent O(1) Stacks and Queues in R,2015
1761,"This paper introduces the R package SAVE which implements statistical methodology for the analysis of computer models. Namely, the package includes routines that perform emulation, calibration and validation of this type of models. The methodology is Bayesian and is essentially that of Bayarri, Berger, Paulo, Sacks, Cafeo, Cavendish, Lin, and Tu (2007). The package is available through the Comprehensive R Archive Network. We illustrate its use with a real data example and in the context of a simulated example.",WOS:000352916400001,JOURNAL OF STATISTICAL SOFTWARE,['CALIBRATION'],SAVE: An R Package for the Statistical Analysis of Computer Models,2015
1762,"We derive randomization-based models for experiments with a chain of randomizations. Estimation theory for these models leads to formulae for the estimators of treatment effects, their standard errors and expected mean squares in the analysis of variance. We discuss the practicalities in fitting these models and outline the difficulties that can occur, many of which do not arise in two-tiered experiments.",WOS:000375175200009,ANNALS OF STATISTICS,"['ORTHOGONAL BLOCK STRUCTURE', 'DECOMPOSITION TABLES', 'PERMUTATION-GROUPS', 'VARIANCE', 'DESIGNS', 'INFORMATION', 'FORMULAS']",RANDOMIZATION-BASED MODELS FOR MULTITIERED EXPERIMENTS: I. A CHAIN OF RANDOMIZATIONS,2016
1763,"The R package spikeSlabGAM implements Bayesian variable selection, model choice, and regularized estimation in (geo-) additive mixed models for Gaussian, binomial, and Poisson responses. Its purpose is to (1) choose an appropriate subset of potential covariates and their interactions, (2) to determine whether linear or more flexible functional forms are required to model the effects of the respective covariates, and (3) to estimate their shapes. Selection and regularization of the model terms is based on a novel spike-and-slab-type prior on coefficient groups associated with parametric and semi-parametric effects.",WOS:000294837100001,JOURNAL OF STATISTICAL SOFTWARE,"['SMOOTHING SPLINE ANOVA', 'NONPARAMETRIC REGRESSION']","spikeSlabGAM: Bayesian Variable Selection, Model Choice and Regularization for Generalized Additive Mixed Models in R",2011
1764,"This paper provides parametric and rank-based optimal tests for eigenvectors and eigenvalues of covariance or scatter matrices in elliptical families. The parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963) and their pseudo-Gaussian robustifications by Davis (1977) and Tyler (1981, 1983). The rank-based tests address a much broader class of problems, where covariance matrices need not exist and principal components are associated with more general scatter matrices. The proposed tests are shown to outperform daily practice both from the point of view of validity as from the point of view of efficiency. This is achieved by utilizing the Le Cam theory of locally asymptotically normal experiments, in the nonstandard context, however, of a curved parametrization. The results we derive for curved experiments are of independent interest, and likely to apply in other contexts.",WOS:000283792900002,ANNALS OF STATISTICS,"['ASYMPTOTIC DISTRIBUTIONS', 'COVARIANCE-STRUCTURES', 'CORRELATION MATRIX', 'SHAPE', 'INFERENCE', 'EIGENVECTORS', 'ESTIMATORS', 'SCATTER', 'MODEL']",OPTIMAL RANK-BASED TESTING FOR PRINCIPAL COMPONENTS,2010
1765,"We propose a self-tuning root Lasso method that simultaneously resolves three important practical problems in high-dimensional regression analysis, namely it handles the unknown scale, heteroscedasticity and (drastic) non-Gaussianity of the noise. In addition, our analysis allows for badly behaved designs, for example, perfectly collinear regressors, and generates sharp bounds even in extreme cases, such as the infinite variance case and the noiseless case, in contrast to Lasso. We establish various nonasymptotic bounds for root Lasso including prediction norm rate and sparsity. Our analysis is based on new impact factors that are tailored for bounding prediction norm. In order to cover heteroscedastic non-Gaussian noise, we rely on moderate deviation theory for self-normalized sums to achieve Gaussian-like results under weak conditions. Moreover, we derive bounds on the performance of ordinary least square (ols) applied to the model selected by root Lasso accounting for possible misspecification of the selected model. Under mild conditions, the rate of convergence of ols post root Lasso is as good as root Lasso's rate. As an application, we consider the use of root Lasso and ols post root Lasso as estimators of nuisance parameters in a generic semiparametric problem (nonlinear moment condition or Z-problem), resulting in a construction of root n-consistent and asymptotically normal estimators of the main parameters.",WOS:000336888400017,ANNALS OF STATISTICS,"['DIMENSIONAL LINEAR-REGRESSION', 'MODEL-SELECTION', 'SPARSE MODELS', 'DANTZIG SELECTOR', 'LEAST-SQUARES', 'RECOVERY', 'INEQUALITIES', 'MINIMIZATION']",PIVOTAL ESTIMATION VIA SQUARE-ROOT LASSO IN NONPARAMETRIC REGRESSION,2014
1766,"In this paper, we investigate frailty models for clustered survival data that are subject to both left- and right-censoring, termed ""doubly-censored data"". This model extends current survival literature by broadening the application of frailty models from right-censoring to a more complicated situation with additional left-censoring.
Our approach is motivated by a recent Hepatitis B study where the sample consists of families. We adopt a likelihood approach that aims at the non parametric maximum likelihood estimators (NPMLE). A new algorithm is proposed, which not only works well for clustered data but also improve over existing algorithm for independent and doubly-censored data, a special case when the frailty variable is a constant equal to one. This special case is well known to be a computational challenge due to the left-censoring feature of the data. The new algorithm not only resolves this challenge but also accommodates the additional frailty variable effectively.
Asymptotic properties of the NPMLE are established along with semi parametric efficiency of the NPMLE for the finite-dimensional parameters. The consistency of Bootstrap estimators for the standard errors of the NPMLE is also discussed. We conducted some simulations to illustrate the numerical performance and robustness of the proposed algorithm, which is also applied to the Hepatitis B data.",WOS:000375175200014,ANNALS OF STATISTICS,"['PROPORTIONAL HAZARDS MODEL', 'CARLO EXPECTATION-MAXIMIZATION', 'MAXIMUM-LIKELIHOOD ESTIMATORS', 'ASYMPTOTIC THEORY', 'REGRESSION-ANALYSIS', 'SURVIVAL FUNCTION', 'SELF-CONSISTENT', 'FAILURE TIME', 'CONVERGENCE', 'ALGORITHMS']",SEMIPARAMETRIC EFFICIENT ESTIMATION FOR SHARED-FRAILTY MODELS WITH DOUBLY-CENSORED CLUSTERED DATA,2016
1767,"R offers several extension packages that allow it to perform parallel computations. These operate on fixed points in the program flow and make it difficult to deal with nested parallelism and to organize parallelism in complex computations in general. In this article we discuss, first, of how to detect parallelism in functions, and second, how to minimize user intervention in that process. We present a solution that requires minimal code changes and enables to flexibly and dynamically choose the degree of parallelization in the resulting computation. An implementation is provided by the R package parallelize. dynamic and practical issues are discussed with the help of examples.
The R language (Ihaka and Gentleman, 1996) can be used to program in the functional paradigm, i.e. return values of functions only depend on their arguments and values of variables bound at the moment of function definition. Assuming a functional R program, it follows that calls to a given set of functions are independent as long as their arguments do not involve return values of each other. This property of function calls can be exploited and several R packages allow to compute function calls in parallel, e. g. packages parallel, Rsge (Bode, 2012) or for each (Michael et al., 2013; Revolution Analytics and Weston, 2013). A natural point in the program flow where to employ parallelization is where use of the apply-family of functions is made. These functions take a single function (here called the compute-function) as their first argument together with a set of values as their second argument (here called the compute-arguments) each member of which is passed to the compute-function. The calling mechanism guarantees that function calls cannot see each others return values and are thereby independent. This family includes the apply, sapply, lapply, and tapply functions called generically Apply in the following. Examples of packages helping to parallelize Apply functions include parallel and Rsge among others and we will focus on these functions in this article as well.
In these packages, a given Apply function is replaced by a similar function from the package that performs the same computation in a parallel way. Fixing a point of parallelism introduces some potential problems. For example, the bootstrap package boot (Davison and Hinkley, 1997; Canty and Ripley, 2013) allows implicit use of the parallel package. If bootstrap computations become nested within larger computations the parallelization option of the boot function potentially has to be changed to allow parallelization at a higher level once the computation scenario changes. In principle, the degree of parallelism could depend on parameter values changing between computations thereby making it difficult to choose an optimal code point at which to parallelize. Another shortcoming of existing solutions is that only a single Apply function gets parallelized thereby ignoring parallelism that spans different Apply calls in nested computations. The aim of this paper is to outline solutions that overcome these limitations. This implies that the parallelization process should be as transparent as possible, i.e. requiring as little user intervention as necessary. An ideal solution would therefore allow the user to ask for parallelization of a certain piece of code and we will try to approximate this situation. Potential benefits for the user are that less technical knowledge is required to make use of parallelization, computations can become more efficient by better control over the scaling of parallelization, and finally programs can better scale to different resources, say the local machine compared to a computer cluster.
This article is organized as follows. We first give further motivation by an example that highlights the problems this approach seeks to address. We then outline the technical strategy needed to determine the parallelism in a given function call. After that, trade-offs introduced by such a strategy are discussed. We conclude by benchmarking two examples and discussing important practical issues such as deviations of R programs from the functional programming style.",WOS:000330193300010,R JOURNAL,,Dynamic Parallelization of R Functions,2013
1768,"The X-12-ARIMA seasonal adjustment program of the US Census Bureau extracts the different components (mainly: seasonal component, trend component, outlier component and irregular component) of a monthly of quarterly time series. It is the state-of-the-art technology for seasonal adjustment used in many statisfical offices. It is possible to include a moving holiday effect, a trading day effect and user-defined regressors, and additionally incorporates automatic and creates an output data set containing the adjusted time series and intermediate calculation.
The original output from X-12-ARIMA is somehow static and it not always an easy task for users to extract the required information for further processing. The R package x12 provides wrapper functions and an abstraction layer for batch processing of X-12-ARIMA. It allows summarizing, modifying and storing the output from X-12-ARIMA within a well-defined class-oriented implementation. On top of the class-oriented (command line) implementation the graphical user interface allows access to the R package x12 without requiring too much R knowledge. User can interactively select additive outlines, level shifts and temporary changes and see the impact immediately.
The provision of the powerful X-12-ARIMA seasonal adjustment program available directly from within R, as well as of the new facilities for marking outliers, batch processing and change tracking, makes the package a potent and functional tool.",WOS:000349843100001,JOURNAL OF STATISTICAL SOFTWARE,['MODELS'],Seasonal Adjustment with the R Packages x12 and x12GUI,2014
1769,"Traditionally, assessing the accuracy of inference based on regression quantiles has relied on the Bahadur representation. This provides an error of order n(-1/4) in normal approximations, and suggests that inference based on regression quantiles may not be as reliable as that based on other (smoother) approaches, whose errors are generally of order n(-1/2) (or better in special symmetric cases). Fortunately, extensive simulations and empirical applications show that inference for regression quantiles shares the smaller error rates of other procedures. In fact, the ""Hungarian"" construction of Komlos, Major and Tusnady [Z. Wahrsch. Verw. Gebiete 32 (1975) 111-131, Z. Wahrsch. Verw. Gebiete 34 (1976) 33-58] provides an alternative expansion for the one-sample quantile process with nearly the root-n error rate (specifically, to within a factor of log n). Such an expansion is developed here to provide a theoretical foundation for more accurate approximations for inference in regression quantile models. One specific application of independent interest is a result establishing that for conditional inference, the error rate for coverage probabilities using the Hall and Sheather [J. R. Stat. Soc. Ser B Stat. Methodol. 50 (1988) 381-391] method of sparsity estimation matches their one-sample rate.",WOS:000310650900016,ANNALS OF STATISTICS,"['PARTIAL SUMS', 'BOOTSTRAP', 'MODELS']",NEARLY ROOT-n APPROXIMATION FOR REGRESSION QUANTILE PROCESSES,2012
1770,"The heuristic k-means algorithm, widely used for cluster analysis, does not guarantee optimality. We developed a dynamic programming algorithm for optimal one-dimensional clustering. The algorithm is implemented as an R package called Ckmeans.1d.dp. We demonstrate its advantage in optimality and runtime over the standard iterative k-means algorithm.",WOS:000208590200006,R JOURNAL,,Ckmeans.1d.dp: Optimal k-means Clustering in One Dimension by Dynamic Programming,2011
1771,,WOS:000313198000001,R JOURNAL,,Untitled,2012
1772,"Stimulated by the Boston house price data, in this paper, we propose a semiparametric spatial dynamic model, which extends the ordinary spatial autoregressive models to accommodate the effects of some covariates associated with the house price. A profile likelihood based estimation procedure is proposed. The asymptotic normality of the proposed estimators are derived. We also investigate how to identify the parametric/nonparametric components in the proposed semiparametric model. We show how many unknown parameters an unknown bivariate function amounts to, and propose an AIC/BIC of nonparametric version for model selection. Simulation studies are conducted to examine the performance of the proposed methods. The simulation results show our methods work very well. We finally apply the proposed methods to analyze the Boston house price data, which leads to some interesting findings.",WOS:000336888400015,ANNALS OF STATISTICS,"['VARYING-COEFFICIENT MODELS', 'REGRESSION']",A SEMIPARAMETRIC SPATIAL DYNAMIC MODEL,2014
1773,"We study the existence of algorithms generating almost surely nonnegative unbiased estimators. We show that given a nonconstant real-valued function f and a sequence of unbiased estimators of lambda is an element of R, there is no algorithm yielding almost surely nonnegative unbiased estimators of f(lambda) is an element of R+. The study is motivated by pseudo-marginal Monte Carlo algorithms that rely on such nonnegative unbiased estimators. These methods allow ""exact inference"" in intractable models, in the sense that integrals with respect to a target distribution can be estimated without any systematic error, even though the associated probability density function cannot be evaluated pointwise. We discuss the consequences of our results on the applicability of pseudo-marginal algorithms and thus on the possibility of exact inference in intractable models. We illustrate our study with particular choices of functions f corresponding to known challenges in statistics, such as exact simulation of diffusions, inference in large datasets and doubly intractable distributions.",WOS:000352757100011,ANNALS OF STATISTICS,"['OBSERVED DIFFUSION-PROCESSES', 'MONTE-CARLO METHOD', 'PARTICLE FILTERS', 'EXACT SIMULATION', 'DISTRIBUTIONS', 'NOISE']",ON NONNEGATIVE UNBIASED ESTIMATORS,2015
1774,"The uniform distribution on matrices with specified row and column sums is often a natural choice of null model when testing for structure in two-way tables (binary or nonnegative integer). Due to the difficulty of sampling from this distribution, many approximate methods have been developed. We will show that by exploiting certain symmetries, exact sampling and counting is in fact possible in many nontrivial real-world cases. We illustrate with real datasets including ecological co-occurrence matrices and contingency tables.",WOS:000323271500008,ANNALS OF STATISTICS,"['PRECISE NUMBER', 'ASYMPTOTIC ENUMERATION', 'UNIFORM GENERATION', 'CONTINGENCY-TABLES', 'COLUMN SUMS', '(0,1)-MATRICES', 'GRAPHS', 'MODEL', 'ROW']",EXACT SAMPLING AND COUNTING FOR FIXED-MARGIN MATRICES,2013
1775,This paper presents two illustrations of state space modeling in S-PLUS using the Ssf-Pack 3.0 routines implemented in S+FinMetrics 3.0. The state space modeling functions in S+FinMetrics/SsfPack are extremely flexible and powerful and can be used for a wide variety of linear Gaussian state space models and for some non-linear and non-Gaussian state space models.,WOS:000290526900001,JOURNAL OF STATISTICAL SOFTWARE,['TERM-STRUCTURE'],State Space Modeling Using SsfPack in S plus FinMetrics 3.0,2011
1776,"Measurement invariance is an important assumption in the Rasch model and mixture models constitute a flexible way of checking for a violation of this assumption by detecting unobserved heterogeneity in item response data. Here, a general class of Rasch mixture models is established and implemented in R, using conditional maximum likelihood estimation of the item parameters (given the raw scores) along with flexible specification of two model building blocks: (1) Mixture weights for the unobserved classes can be treated as model parameters or based on covariates in a concomitant variable model. (2) The distribution of raw score probabilities can be parametrized in two possible ways, either using a saturated model or a specification through mean and variance. The function raschmix () in the R package psychomix provides these models, leveraging the general infrastructure for fitting mixture models in the flexmix package. Usage of the function and its associated methods is illustrated on artificial data as well as empirical data from a study of verbally aggressive behavior.",WOS:000305117800001,JOURNAL OF STATISTICAL SOFTWARE,,Flexible Rasch Mixture Models with Package psychomix,2012
1777,"This paper considers a class of nonparametric autoregressive models with nonstationarity. We propose a nonparametric kernel test for the conditional mean and then establish an asymptotic distribution of the proposed test. Both the setting and the results differ from earlier work on nonparametric autoregression with stationarity. In addition, we develop a new bootstrap simulation scheme for the selection of a suitable bandwidth parameter involved in the kernel test as well as the choice of a simulated critical value. The finite-sample performance of the proposed test is assessed using one simulated example and one real data example.",WOS:000271673700007,ANNALS OF STATISTICS,"['NONPARAMETRIC-ESTIMATION', 'DIFFUSION-MODELS', 'REGRESSION']",SPECIFICATION TESTING IN NONLINEAR AND NONSTATIONARY TIME SERIES AUTOREGRESSION,2009
1778,"Most programming language communities have naming conventions that are generally agreed upon, that is, a set of rules that governs how functions and variables are named. This is not the case with R, and a review of unofficial style guides and naming convention usage on CRAN shows that a number of different naming conventions are currently in use. Some naming conventions are, however, more popular than others and as a newcomer to the R community or as a developer of a new package this could be useful to consider when choosing what naming convention to adopt.",WOS:000313198000010,R JOURNAL,,The State of Naming Conventions in R,2012
1779,"The apc package includes functions for age-period-cohort analysis based on the canonical parametrisation of Kuang et al. (2008a). The package includes functions for organizing the data, descriptive plots, a deviance table, estimation of (sub-models of) the age-period-cohort model, a plot for specification testing, plots of estimated parameters, and sub-sample analysis.",WOS:000368551800005,R JOURNAL,"['CHAIN-LADDER MODEL', 'MESOTHELIOMA MORTALITY', 'TEMPORAL VARIATION', 'CANCER RATES']",apc: An R Package for Age-Period-Cohort Analysis,2015
1780,"We introduce a quantile-adaptive framework for nonlinear variable screening with high-dimensional heterogeneous data. This framework has two distinctive features: (1) it allows the set of active variables to vary across quantiles, thus making it more flexible to accommodate heterogeneity; (2) it is model-free and avoids the difficult task of specifying the form of a statistical model in a high dimensional space. Our nonlinear independence screening procedure employs spline approximations to model the marginal effects at a quantile level of interest. Under appropriate conditions on the quantile functions without requiring the existence of any moments, the new procedure is shown to enjoy the sure screening property in ultra-high dimensions. Furthermore, the quantile-adaptive framework can naturally handle censored data arising in survival analysis. We prove that the sure screening property remains valid when the response variable is subject to random right censoring. Numerical studies confirm the fine performance of the proposed method for various semiparametric models and its effectiveness to extract quantile-specific information from heteroscedastic data.",WOS:000317451200014,ANNALS OF STATISTICS,"['MEDIAN REGRESSION', 'SURVIVAL ANALYSIS', 'LINEAR-MODELS', 'SELECTION', 'SPLINES', 'DISTRIBUTIONS']",QUANTILE-ADAPTIVE MODEL-FREE VARIABLE SCREENING FOR HIGH-DIMENSIONAL HETEROGENEOUS DATA,2013
1781,"The envelope model is a new paradigm to address estimation and prediction in multivariate analysis. Using sufficient dimension reduction techniques, it has the potential to achieve substantial efficiency gains compared to standard models. This model was first introduced by [Statist. Sinica 20 (2010) 927-960] for multivariate linear regression, and has since been adapted to many other contexts. However, a Bayesian approach for analyzing envelope models has not yet been investigated in the literature. In this paper, we develop a comprehensive Bayesian framework for estimation and model selection in envelope models in the context of multivariate linear regression. Our framework has the following attractive features. First, we use the matrix Bingham distribution to construct a prior on the orthogonal basis matrix of the envelope subspace. This prior respects the manifold structure of the envelope model, and can directly incorporate prior information about the envelope subspace through the specification of hyperparamaters. This feature has potential applications in the broader Bayesian sufficient dimension reduction area. Second, sampling from the resulting posterior distribution can be achieved by using a block Gibbs sampler with standard associated conditionals. This in turn facilitates computationally efficient estimation and model selection. Third, unlike the current frequentist approach, our approach can accommodate situations where the sample size is smaller than the number of responses. Lastly, the Bayesian approach inherently offers comprehensive uncertainty characterization through the posterior distribution. We illustrate the utility of our approach on simulated and real datasets.",WOS:000396804900006,ANNALS OF STATISTICS,['MULTIVARIATE LINEAR-REGRESSION'],A BAYESIAN APPROACH FOR ENVELOPE MODELS,2017
1782,"Variational methods for parameter estimation are an active research area, potentially offering computationally tractable heuristics with theoretical performance bounds. We build on recent work that applies such methods to network data, and establish asymptotic normality rates for parameter estimates of stochastic blockmodel data, by either maximum likelihood or variational estimation. The result also applies to various sub-models of the stochastic blockmodel found in the literature.",WOS:000326991200008,ANNALS OF STATISTICS,"['RANDOM GRAPHS', 'DEGREE DISTRIBUTIONS', 'NETWORK MODELS', 'BLOCK-MODELS']",ASYMPTOTIC NORMALITY OF MAXIMUM LIKELIHOOD AND ITS VARIATIONAL APPROXIMATION FOR STOCHASTIC BLOCKMODELS,2013
1783,"We consider maximin and Bayesian D-optimal designs for nonlinear regression models. The maximin criterion requires the specification of a region for the nonlinear parameters in the model, while the Bayesian optimality criterion assumes that a prior for these parameters is available. On interval parameter spaces, it was observed empirically by many authors that an increase of uncertainty in the prior information (i.e., a larger range for the parameter space in the maximin criterion or a larger variance of the prior in the Bayesian criterion) yields a larger number of support points of the corresponding optimal designs. In this paper, we present analytic tools which are used to prove this phenomenon in concrete situations. The proposed methodology can be used to explain many empirically observed results in the literature. Moreover, it explains why maximin D-optimal designs are usually supported at more points than Bayesian D-optimal designs.",WOS:000248987600013,ANNALS OF STATISTICS,"['EXPONENTIAL REGRESSION-MODELS', 'EFFICIENT DESIGNS', 'CONSTRUCTION', 'ROBUST']",On the number of support points of maximin and bayesian optimal designs,2007
1784,"We extend the isotonic analysis for Wicksell's problem to estimate a regression function, which is motivated by the problem of estimating dark matter distribution in astronomy. The main result is a version of the Kiefer-Wolfowitz theorem comparing the empirical distribution to its least concave majorant, but with a convergence rate n(-1) log n faster than n(-2/3) log n. The main result is useful in obtaining asymptotic distributions for estimators, such as isotonic and smooth estimators.",WOS:000249568000009,ANNALS OF STATISTICS,['SPHERE SIZE DISTRIBUTIONS'],A Kiefer-Wolfowitz comparison theorem for wicksell's problem,2007
1785,,WOS:000343788100001,R JOURNAL,,Untitled,2014
1786,"When considering a graphical Gaussian model N-G Markov with respect to a decomposable graph G, the parameter space of interest for the precision parameter is the cone PG of positive definite matrices with fixed zeros corresponding to the missing edges of G. The parameter space for the scale parameter of N-G is the cone Q(G), dual to P-G, of incomplete matrices with submatrices corresponding to the cliques of G being positive definite. In this paper we construct on the cones Q(G) and P-G two families of Wishart distributions, namely the Type I and Type II Wisharts. They can be viewed as generalizations of the hyper Wishart and the inverse of the hyper inverse Wishart as defined by Dawid and Lauritzen [Ann. Statist. 21 (1993) 1272-1317]. We show that the Type I and II Wisharts have properties similar to those of the hyper and hyper inverse Wishart. Indeed, the inverse of the Type II Wishart forms a conjugate family of priors for the covariance parameter of the graphical Gaussian model and is strong directed hyper Markov for every direction given to the graph by a perfect order of its cliques, while the Type I Wishart is weak hyper Markov. Moreover, the inverse Type II Wishart as a conjugate family presents the advantage of having a multidimensional shape parameter, thus offering flexibility for the choice of a prior. Both Type I and II Wishart distributions depend on multivariate shape parameters. A shape parameter is acceptable if and only if it satisfies a certain eigenvalue property. We show that the sets of acceptable shape parameters for a noncomplete G have dimension equal to at least one plus the number of cliques in G. These families, as conjugate families, are richer than the traditional Diaconis-Ylvisaker conjugate families which all have a shape parameter set of dimension one. A decomposable graph which does not contain a three-link chain as an induced subgraph is said to be homogeneous. In this case, our Wisharts are particular cases of the Wisharts on homogeneous cones as defined by Andersson and Wojnar [J. Theoret. Probab. 17 (2004) 781-818] and the dimension of the shape parameter set is even larger than in the nonhomogeneous case: it is indeed equal to the number of cliques plus the number of distinct minimal separators. Using the model where G is a three-link chain, we show by computing a 7-tuple integral that in general we cannot expect the shape parameter sets to have dimension larger than the number of cliques plus one.",WOS:000248692700014,ANNALS OF STATISTICS,"['EXPONENTIAL-FAMILIES', 'ENRICHED CONJUGATE', 'PRIORS', 'CONES']",Wishart distributions for decomposable graphs,2007
1787,"GLIMMPSE is a free, web-based software tool that calculates power and sample size for the general linear multivariate model with Gaussian errors (http://glimmpse.SampleSizeShop.org/). GLIMMPSE provides a user-friendly interface for the computation of power and sample size. We consider models with fixed predictors, and models with fixed predictors and a single Gaussian covariate. Validation experiments demonstrate that GLIMMPSE matches the accuracy of previously published results, and performs well against simulations. We provide several online tutorials based on research in head and neck cancer. The tutorials demonstrate the use of GLIMMPSE to calculate power and sample size.",WOS:000324372000001,JOURNAL OF STATISTICAL SOFTWARE,['SAMPLE-SIZE CALCULATIONS'],GLIMMPSE: Online Power Computation for Linear Models with and without a Baseline Covariate,2013
1788,"Over the last few years, the power law distribution has been used as the data generating mechanism in many disparate fields. However, at times the techniques used to fit the power law distribution have been inappropriate. This paper describes the poweRlaw R package, which makes fitting power laws and other heavy-tailed distributions straight forward. This package contains R functions for fitting, comparing and visualizing heavy tailed distributions. Overall, it provides a principled approach to power law fitting.",WOS:000352910600001,JOURNAL OF STATISTICAL SOFTWARE,"['LAW', 'NETWORKS']",Fitting Heavy Tailed Distributions: The poweRlaw Package,2015
1789,"We develop minimax optimal risk bounds for the general learning task consisting in predicting as well as the best function in a reference set g up to the smallest possible additive term, called the convergence rate. When the reference set is finite and when n denotes the size of the training data, we provide minimax convergence rates of the form C(log|g|/n)(nu) with tight evaluation of the positive constant C and with exact 0 < nu <= 1, the latter value depending on the convexity of the loss function and on the level of noise in the output distribution.
The risk upper bounds are based on a sequential randomized algorithm, which at each step concentrates on functions having both low risk and low variance with respect to the previous step prediction function. Our analysis puts forward the links between the probabilistic and worst-case viewpoints, and allows to obtain risk bounds unachievable with the standard statistical learning approach. One of the key ideas of this work is to use probabilistic inequalities with respect to appropriate (Gibbs) distributions on the prediction function space instead of using them with respect to the distribution generating the data.
The risk lower bounds are based on refinements of the Assouad lemma taking particularly into account the properties of the loss function. Our key example to illustrate the upper and lower bounds is to consider the L-q-regression setting for which an exhaustive analysis of the convergence rates is given while q ranges in [1; + infinity].",WOS:000268113500001,ANNALS OF STATISTICS,"['USE EXPERT ADVICE', 'INDIVIDUAL SEQUENCES', 'LOWER BOUNDS', 'PREDICTION', 'CLASSIFICATION', 'REGRESSION', 'CONVERGENCE', 'CLASSIFIERS', 'COMPLEXITY', 'RISK']",FAST LEARNING RATES IN STATISTICAL INFERENCE THROUGH AGGREGATION,2009
1790,"Progress in molecular high-throughput techniques has led to the opportunity of a comprehensive monitoring of biomolecules in medical samples. In the era of personalized medicine, these data form the basis for the development of diagnostic, prognostic and predictive tests for cancer. Because of the high number of features that are measured simultaneously in a relatively low number of samples, supervised learning approaches are sensitive to overfitting and performance overestimation. Bioinformatic methods were developed to cope with these problems including control of accuaracy and precision. However, there is demand for easy-to-use software that integrates methods for classifier construction, performance assessment and development of diagnostic tests. To contribute to filling of this gap, we developed a comprehensive R package for the development and validation of diagnostic tests from high-dimensional molecular data. An important focus of the package is a careful validation of the classification results. To this end, we implemented an extended version of the multiple random validation protocol, a validation method that was introduced before. The package includes methods for continuous prediction scores. This is important in a clinical setting, because scores can be converted to probabilities and help to distinguish between clear-cut and borderline classification results. The functionality of the package is illustrated by the analysis of two cancer microarray data sets.",WOS:000341792300001,JOURNAL OF STATISTICAL SOFTWARE,"['GENE-EXPRESSION PROFILES', 'BREAST-CANCER', 'CROSS-VALIDATION', 'THYMIDINE KINASE', 'MICROARRAY DATA', 'OVARIAN-CANCER', 'CLASSIFICATION', 'CHEMOTHERAPY', 'PREDICTORS', 'SIGNATURE']",cancerclass: An R Package for Development and Validation of Diagnostic Tests from High-Dimensional Molecular Data,2014
1791,"We first study the properties of solutions of quadratic programs with linear equality constraints whose parameters are estimated from data in the high-dimensional setting where p, the number of variables in the problem, is of the same order of magnitude as n, the number of observations used to estimate the parameters. The Markowitz problem in Finance is a subcase of our study. Assuming normality and independence of the observations we relate the efficient frontier computed empirically to the ""true"" efficient frontier. Our computations show that there is a separation of the errors induced by estimating the mean of the observations and estimating the covariance matrix. In particular, the price paid for estimating the covariance matrix is an underestimation of the variance by a factor roughly equal to 1 - p/n. Therefore the risk of the optimal population solution is underestimated when we estimate it by solving a similar quadratic program with estimated parameters.
We also characterize the statistical behavior of linear functionals of the empirical optimal vector and show that they are biased estimators of the corresponding population quantities.
We investigate the robustness of our Gaussian results by extending the study to certain elliptical models and models where our n observations are correlated (in ""time""). We show a lack of robustness of the Gaussian results, but are still able to get results concerning first order properties of the quantities of interest, even in the case of relatively heavy-tailed data (we require two moments). Risk underestimation is still present in the elliptical case and more pronounced than in the Gaussian case.
We discuss properties of the nonparametric and parametric bootstrap in this context. We show several results, including the interesting fact that standard applications of the bootstrap generally yield inconsistent estimates of bias.
We propose some strategies to correct these problems and practically validate them in some simulations. Throughout this paper, we will assume that p, n and n - p tend to infinity, and p < n.
Finally, we extend our study to the case of problems with more general linear constraints, including, in particular, inequality constraints.",WOS:000290231500006,ANNALS OF STATISTICS,"['COVARIANCE MATRICES', 'PORTFOLIO OPTIMIZATION', 'LARGEST EIGENVALUE', 'ESTIMATOR', 'SPECTRA']",HIGH-DIMENSIONALITY EFFECTS IN THE MARKOWITZ PROBLEM AND OTHER QUADRATIC PROGRAMS WITH LINEAR CONSTRAINTS: RISK UNDERESTIMATION,2010
1792,"We consider the generic regularized optimization problem (beta) over cap(lambda) = arg min(beta) L (y, X beta) + lambda J (beta). Efron, Hastie, Johnstone and Tibshirani [Ann. Statist. 32 (2004) 407-499] have shown that for the LASSO-that is, if L is squared error loss and J(beta) = vertical bar vertical bar beta vertical bar vertical bar(1) is the if l(1) norm of beta-the optimal coefficient path is piecewise linear, that is, is piecewise constant. We derive a general characterization of the properties of (loss L, penalty J) pairs which give piecewise linear coefficient paths. Such pairs allow for efficient generation of the full regularized coefficient paths. We investigate the nature of efficient path following algorithms which arise. We use our results to suggest robust versions of the LASSO for regression and classification, and to develop new, efficient algorithms for existing problems in the literature, including Mammen and van de Geer's locally adaptive regression splines.",WOS:000248692700004,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'REGRESSION', 'LASSO', 'SHRINKAGE', 'SELECTION', 'SPLINES']",Piecewise linear regularized solution paths,2007
1793,"We provide a language for formulating a range of state space models with response densities within the exponential family. The described methodology is implemented in the R-package sspir. A state space model is specified similarly to a generalized linear model in R, and then the time-varying terms are marked in the formula. Special functions for specifying polynomial time trends, harmonic seasonal patterns, unstructured seasonal patterns and time-varying covariates can be used in the formula. The model is fitted to data using iterated extended Kalman filtering, but the formulation of models does not depend on the implemented method of inference. The package is demonstrated on three datasets.",WOS:000237294400001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED LINEAR-MODELS', 'TIME-SERIES', 'MONTE-CARLO']",Formulating state space models in R with focus on longitudinal regression models,2006
1794,"The paper considers nonparametric specification tests of quantile curves for a general class of nonstationary processes. Using Bahadur representation and Gaussian approximation results for nonstationary time series, simultaneous confidence bands and integrated squared difference tests are proposed to test various parametric forms of the quantile curves with asymptotically correct type I error rates. A wild bootstrap procedure is implemented to alleviate the problem of slow convergence of the asymptotic results. In particular, our results can be used to test the trends of extremes of climate variables, an important problem in understanding climate change. Our methodology is applied to the analysis of the maximum speed of tropical cyclone winds. It was found that an inhomogeneous upward trend for cyclone wind speeds is pronounced at high quantile values. However, there is no trend in the mean lifetime-maximum wind speed. This example shows the effectiveness of the quantile regression technique.",WOS:000280359400009,ANNALS OF STATISTICS,"['SIMULTANEOUS CONFIDENCE BANDS', 'REGRESSION-MODEL', 'DENSITY-ESTIMATION', 'LINEAR-REGRESSION', 'OF-FIT', 'BOOTSTRAP', 'CHECKS', 'TESTS']",NONPARAMETRIC INFERENCE OF QUANTILE CURVES FOR NONSTATIONARY TIME SERIES,2010
1795,"The R package extracat provides two new graphical methods for displaying categorical data extending the concepts of multiple barcharts and parallel coordinates plots. The first method called rmb plot uses a crossover of mosaicplots and multiple barcharts to display the frequencies of a data table split up into conditional relative frequencies of one target variable and the absolute frequencies of the corresponding combinations of the remaining explanatory variables. It provides a well-structured representation of the data which is easy to interpret and allows precise comparisons. The graphic can additionally be used as a generalization of spineplots or with barcharts for the conditional relative frequencies. Several options, including ceiling censored zooming, residual shadings and a choice of color palettes, are provided. An interactive version based on the R package iWidgets is also presented. The second graphic cpcp uses the interactive parallel coordinates plots in the iplots package to visualize categorical data. Sequences of points are used to represent each of the variable categories, while ordering algorithms are applied to represent a hierarchical structure in the data and keep the arrangement clear. This interactive graphic is well-suited for exploratory analysis and allows a visual interpretation even for a higher number of variables and a mixture of categorical and numeric scales.",WOS:000320040300001,JOURNAL OF STATISTICAL SOFTWARE,"['DISPLAYS', 'MODELS']",New Approaches in Visualization of Categorical Data: R Package extracat,2013
1796,"influence.ME provides tools for detecting influential data in mixed effects models. The application of these models has become common practice, but the development of diagnostic tools has lagged behind. influence. ME calculates standardized measures of influential data for the point estimates of generalized mixed effects models, such as DFBETAS, Cook's distance, as well as percentile change and a test for changing levels of significance. influence. ME calculates these measures of influence while accounting for the nesting structure of the data. The package and measures of influential data are introduced, a practical example is given, and strategies for dealing with influential data are suggested.",WOS:000313198000006,R JOURNAL,,influence.ME: Tools for Detecting Influential Data in Mixed Effects Models,2012
1797,"Visual analysis of data is important to understand the main characteristics, main trends and relationships in data sets and it can be used to assess the data quality. Using the R package sparkTable, statistical tables holding quantitative information can be enhanced by including spark-type graphs such as sparklines
[GRAPHICS]
and sparkbars
[GRAPHICS]
.
These kind of graphics are well-known in literature and are considered as simple, intense and illustrative graphs that are small enough to fit in a single line. Thus, they can easily enrich tables and texts with additional information in a comprehensive visual way.
The R package sparkTable uses a clean S4 class design and provides methods to create different types of sparkgraphs that can be used in websites, presentations and documents. We also implemented an easy way for non-experts to create highly complex tables. In this case, graphical parameters can be interactively changed, variables can be sorted, graphs can be added and removed in an interactive manner. Thereby it is possible to produce custom-tailored graphical tables - standard tables that are enriched with graphs - that can be displayed in a browser and exported to various formats.",WOS:000357431900004,R JOURNAL,['INFORMATION'],sparkTable: Generating Graphical Tables for Websites and Documents with R,2015
1798,"Within a Bayesian decision theoretic framework we investigate some asymptotic optimality properties of a large class of multiple testing rules. A parametric setup is considered, in which observations come from a normal scale mixture model and the total loss is assumed to be the sum of losses for individual tests. Our model can be used for testing point null hypotheses, as well as to distinguish large signals from a multitude of very small effects. A rule is defined to be asymptotically Bayes optimal under sparsity (ABOS), if within our chosen asymptotic framework the ratio of its Bayes risk and that of the Bayes oracle (a rule which minimizes the Bayes risk) converges to one. Our main interest is in the asymptotic scheme where the proportion p of ""true"" alternatives converges to zero.
We fully characterize the class of fixed threshold multiple testing rules which are ABOS, and hence derive conditions for the asymptotic optimality of rules controlling the Bayesian False Discovery Rate (BFDR). We finally provide conditions under which the popular Benjamini-Hochberg (BH) and Bonferroni procedures are ABOS and show that for a wide class of sparsity levels, the threshold of the former can be approximated by a nonrandom threshold.
It turns out that while the choice of asymptotically optimal FDR levels for BH depends on the relative cost of a type I error, it is almost independent of the level of sparsity. Specifically, we show that when the number of tests in increases to infinity, then BH with FDR level chosen in accordance with the assumed loss function is ABOS in the entire range of sparsity parameters p proportional to m(-beta), with beta is an element of (0, 1].",WOS:000293716500008,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'EMPIRICAL-BAYES', 'DECISION-PROBLEMS', 'GENE-EXPRESSION', 'RATES', 'NULL', 'MICROARRAYS', 'PROPORTION', 'HYPOTHESES', 'MIXTURES']",ASYMPTOTIC BAYES-OPTIMALITY UNDER SPARSITY OF SOME MULTIPLE TESTING PROCEDURES,2011
1799,"Principal Components Analysis (PCA) is a common way to study the sources of variation in a high-dimensional data set. Typically, the leading principal components are used to understand the variation in the data or to reduce the dimension of the data for subsequent analysis. The remaining principal components are ignored since they explain little of the variation in the data. However, the space spanned by the low variation principal components may contain interesting structure, structure that PCA cannot find. Prinsimp is an R package that looks for interesting structure of low variability. ""Interesting"" is defined in terms of a simplicity measure. Looking for interpretable structure in a low variability space has particular importance in evolutionary biology, where such structure can signify the existence of a genetic constraint.",WOS:000348651700004,R JOURNAL,"['FUNCTIONAL DATA-ANALYSIS', 'LONGITUDINAL DATA']",Prinsimp,2014
1800,We developed the R package SimCorMultRes to facilitate simulation of correlated categorical (binary and multinomial) responses under a desired marginal model specification. The simulated correlated categorical responses are obtained by applying threshold approaches to correlated continuous responses of underlying regression models and the dependence structure is parametrized in terms of the correlation matrix of the latent continuous responses. This article provides an elaborate introduction to the SimCorMultRes package demonstrating its design and usage via three examples. The package can be obtained via CRAN.,WOS:000395669800006,R JOURNAL,"['GENERALIZED ESTIMATING EQUATIONS', 'ORDINAL DATA', 'REGRESSION-MODELS', 'R PACKAGE', 'DISTRIBUTIONS', 'ASSOCIATION', 'GENERATION', 'LIKELIHOOD']",Simulating Correlated Binary and Multinomial Responses under Marginal Model Specification: The SimCorMultRes Package,2016
1801,"In this paper we present the normalp package, a package for the statistical environment R that has a set of tools for dealing with the exponential power distribution. In this package there are functions to compute the density function, the distribution function and the quantiles from an exponential power distribution and to generate pseudo-random numbers from the same distribution. Moreover, methods concerning the estimation of the distribution parameters are described and implemented. It is also possible to estimate linear regression models when we assume the random errors distributed according to an exponential power distribution. A set of functions is designed to perform simulation studies to see the suitability of the estimators used. Some examples of use of this package are provided.",WOS:000232806600001,JOURNAL OF STATISTICAL SOFTWARE,"['PARAMETER ORTHOGONALITY', 'LIKELIHOOD']",A software tool for the exponential power distribution: The normalp package,2005
1802,"We study estimation of multivariate densities p of the form p(x) = h(g(x)) for x is an element of R-d and for a fixed monotone function h and an unknown convex function g. The canonical example is h(y) = e(-y) for y is an element of R; in this case, the resulting class of densities
P(e(-y)) = {p = exp(-g) : g is convex}
is well known as the class of log-concave densities. Other functions h allow for classes of densities with heavier tails than the log-concave class.
We first investigate when the maximum likelihood estimator (p) over cap exists for the class P(h) for various choices of monotone transformations h, including decreasing and increasing functions h. The resulting models for increasing transformations h extend the classes of log-convex densities studied previously in the econometrics literature, corresponding to h(y) = exp(y).
We then establish consistency of the maximum likelihood estimator for fairly general functions h, including the log-concave class P(e(-y)) and many others. In a final section, we provide asymptotic minimax lower bounds for the estimation of p and its vector of derivatives at a fixed point x(0) under natural smoothness hypotheses on h and g. The proofs rely heavily on results from convex analysis.",WOS:000290231500013,ANNALS OF STATISTICS,"['LOG-CONCAVE DENSITY', 'MAXIMUM-LIKELIHOOD-ESTIMATION', 'CONVERGENCE', 'CONSISTENCY', 'RATES']",NONPARAMETRIC ESTIMATION OF MULTIVARIATE CONVEX-TRANSFORMED DENSITIES,2010
1803,"R package mixAK originally implemented routines primarily for Bayesian estimation of finite normal mixture models for possibly interval-censored data. The functionality of the package was considerably enhanced by implementing methods for Bayesian estimation of mixtures of multivariate generalized linear mixed models proposed in Komarek and Komarkova (2013). Among other things, this allows for a cluster analysis (classification) based on multivariate continuous and discrete longitudinal data that arise whenever multiple outcomes of a different nature are recorded in a longitudinal study. This package also allows for a data-driven selection of a number of clusters as methods for selecting a number of mixture components were implemented. A model and clustering methodology for multivariate continuous and discrete longitudinal data is overviewed. Further, a step-by-step cluster analysis based jointly on three longitudinal variables of different types (continuous, count, dichotomous) is given, which provides a user manual for using the package for similar problems.",WOS:000341807200001,JOURNAL OF STATISTICAL SOFTWARE,"['DISCRIMINANT-ANALYSIS', 'MIXTURE-MODELS', 'MISSING DATA', 'DISTRIBUTIONS']",Capabilities of R Package mixAK for Clustering Based on Multivariate Continuous and Discrete Longitudinal Data,2014
1804,"We propose two tests for the equality of covariance matrices between two high-dimensional populations. One test is on the whole variance covariance matrices, and the other is on off-diagonal sub-matrices, which define the covariance between two nonoverlapping segments of the high-dimensional random vectors. The tests are applicable (i) when the data dimension is much larger than the sample sizes, namely the ""large p, small n"" situations and (ii) without assuming parametric distributions for the two populations. These two aspects surpass the capability of the conventional likelihood ratio test. The proposed tests can be used to test on covariances associated with gene ontology terms.",WOS:000307608000011,ANNALS OF STATISTICS,"['LARGEST EIGENVALUE', 'HYPOTHESIS TESTS', 'GENE-EXPRESSION', 'MICROARRAY DATA', 'MODEL', 'REGULARIZATION', 'NORMALIZATION', 'CONSISTENCY', 'CATEGORIES', 'SELECTION']",TWO SAMPLE TESTS FOR HIGH-DIMENSIONAL COVARIANCE MATRICES,2012
1805,"This paper presents the R package bcrm for conducting and assessing Bayesian continual reassessment method (CRM) designs in Phase I dose-escalation trials. CRM designs are a class of adaptive design that select the dose to be given to the next recruited patient based on accumulating toxicity data from patients already recruited into the trial, often using Bayesian methodology. Despite the original CRM design being proposed in 1990, the methodology is still not widely implemented within oncology Phase I trials. The aim of this paper is to demonstrate, through example of the bcrm package, how a variety of possible designs can be easily implemented within the R statistical software, and how properties of the designs can be communicated to trial investigators using simple textual and graphical output obtained from the package. This in turn should facilitate an iterative process to allow a design to be chosen that is suitable to the needs of the investigator. Our bcrm package is the first to offer a large comprehensive choice of CRM designs, priors and escalation procedures, which can be easily compared and contrasted within the package through the assessment of operating characteristics",WOS:000324372600001,JOURNAL OF STATISTICAL SOFTWARE,"['CLINICAL-TRIALS', 'CANCER', 'ESCALATION']",bcrm: Bayesian Continual Reassessment Method Designs for Phase I Dose-Finding Trials,2013
1806,"The posterior distribution in a nonparametric inverse problem is shown to contract to the true parameter at a rate that depends on the smoothness of the parameter, and the smoothness and scale of the prior. Correct combinations of these characteristics lead to the minimax rate. The frequentist coverage of credible sets is shown to depend on the combination of prior and true parameter, with smoother priors leading to zero coverage and rougher priors to conservative coverage. In the latter case credible sets are of the correct order of magnitude. The results are numerically illustrated by the problem of recovering a function from observation of a noisy version of its primitive.",WOS:000299186500017,ANNALS OF STATISTICS,"['POSTERIOR DISTRIBUTIONS', 'ASYMPTOTIC NORMALITY', 'CONVERGENCE-RATES', 'HILBERT SCALES', 'REGULARIZATION', 'FUNCTIONALS', 'PARAMETERS', 'INFERENCE']",BAYESIAN INVERSE PROBLEMS WITH GAUSSIAN PRIORS,2011
1807,"In the class of normal regression models with a finite number of regressors, and for a wide class of prior distributions, a Bayesian model selection procedure based on the Bayes factor is consistent [Casella and Moreno J. Amer Statist. Assoc. 104 (2009) 1261-1271]. However, in models where the number of parameters increases as the sample size increases, properties of the Bayes factor are not totally understood. Here we study consistency of the Bayes factors for nested normal linear models when the number of regressors increases with the sample size. We pay attention to two successful tools for model selection [Schwarz Ann. Statist. 6 (1978) 461-464] approximation to the Bayes factor, and the Bayes factor for intrinsic priors [Berger and Pericchi I. Amer Statist. Assoc. 91 (1996) 109-122, Moreno, Bertolino and Racugno J. Amer Statist. Assoc. 93 (1998) 1451-1460].
We find that the the Schwarz approximation and the Bayes factor for intrinsic priors are consistent when the rate of growth of the dimension of the bigger model is O(n(b)) for b < 1. When b = 1 the Schwarz approximation is always inconsistent under the alternative while the Bayes factor for intrinsic priors is consistent except for a small set of alternative models which is characterized.",WOS:000280359400001,ANNALS OF STATISTICS,['VARIABLE SELECTION'],CONSISTENCY OF OBJECTIVE BAYES FACTORS AS THE MODEL DIMENSION GROWS,2010
1808,"The transition density of a diffusion process does not admit an explicit expression in general, which prevents the full maximum likelihood estimation (MLE) based on discretely observed sample paths. Ait-Sahalia [J. Finance 54 (1999) 1361-1395; Econometrica 70 (2002) 223-262] proposed asymptotic expansions to the transition densities of diffusion processes, which lead to an approximate maximum likelihood estimation (AMLE) for parameters. Built on Ait-Sahalia's [Econometrica 70 (2002) 223-262; Ann. Statist. 36 (2008) 906-937] proposal and analysis on the AMLE, we establish the consistency and convergence rate of the AMLE, which reveal the roles played by the number of terms used in the asymptotic density expansions and the sampling interval between successive observations. We find conditions under which the AMLE has the same asymptotic distribution as that of the full MLE. A first order approximation to the Fisher information matrix is proposed.",WOS:000300383200002,ANNALS OF STATISTICS,"['CLOSED-FORM APPROXIMATION', 'DISCRETE OBSERVATIONS', 'TERM STRUCTURE', 'HIGH-FREQUENCY', 'MODELS']",ON THE APPROXIMATE MAXIMUM LIKELIHOOD ESTIMATION FOR DIFFUSION PROCESSES,2011
1809,"This article describes the R package CountsEPPM and its use in determining maximum likelihood estimates of the parameters of extended Poisson process models. These provide a Poisson process based family of flexible models that can handle both underdispersion and overdispersion in observed count data, with the negative binomial and Poisson distributions being special cases. Within CountsEPPM models with mean and variance related to covariates are constructed to match a generalized linear model formulation. Use of the package is illustrated by application to several published datasets.",WOS:000373917600001,JOURNAL OF STATISTICAL SOFTWARE,"['SUCCESS PROBABILITY', 'BETA REGRESSION', 'NUMBER', 'DEPENDENCE', 'TRIALS']",Mean and Variance Modeling of Under- and Overdispersed Count Data,2016
1810,"Missing values are common in data, and usually require attention in order to conduct the statistical analysis. One of the first steps is to explore the structure of the missing values, and how missingness relates to the other collected variables. This article describes an R package, that provides a graphical user interface (GUI) designed to help explore the missing data structure and to examine the results of different imputation methods. The GUI provides numerical and graphical summaries conditional on missingness, and includes imputations using fixed values, multiple imputations and nearest neighbors.",WOS:000384909500001,JOURNAL OF STATISTICAL SOFTWARE,"['DATA VISUALIZATION', 'MULTIPLE IMPUTATION', 'XGOBI']",Visually Exploring Missing Values in Multivariable Data Using a Graphical User Interface,2015
1811,The maximum entropy bootstrap is an algorithm that creates an ensemble for time series inference. Stationarity is not required and the ensemble satisfies the ergodic theorem and the central limit theorem. The meboot R package implements such algorithm. This document introduces the procedure and illustrates its scope by means of several guided applications.,WOS:000263105400001,JOURNAL OF STATISTICAL SOFTWARE,,Maximum Entropy Bootstrap for Time Series: The meboot R Package,2009
1812,"Change-point models are widely used by statisticians to model drastic changes in the pattern of observed data. Least squares/maximum likelihood based estimation of change-points leads to curious asymptotic phenomena. When the change-point model is correctly specified, such estimates generally converge at a fast rate (n) and are asymptotically described by minimizers of a jump process. Under complete mis-specification by a smooth curve, that is, when a change-point model is fitted to data described by a smooth curve, the rate of convergence slows down to n(1/3) and the limit distribution changes to that of the minimizer of a continuous Gaussian process. In this paper, we provide a bridge between these two extreme scenarios by studying the limit behavior of change-point estimates under varying degrees of model mis-specification by smooth curves, which can be viewed as local alternatives. We find that the limiting regime depends on how quickly the alternatives approach a change-point model. We unravel a family of ""intermediate"" limits that can transition, at least qualitatively, to the limits in the two extreme scenarios. The theoretical results are illustrated via a set of carefully designed simulations. We also demonstrate how inference for the change-point parameter can be performed in absence of knowledge of the underlying scenario by resorting to sub-sampling techniques that involve estimation of the convergence rate.",WOS:000368022000006,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'GRADUAL CHANGES', 'THRESHOLD', 'ESTIMATORS', 'COVARIATE', 'CURVES']",ASYMPTOTICS FOR CHANGE-POINT MODELS UNDER VARYING DEGREES OF MIS-SPECIFICATION,2016
1813,This article describes the R package copas which is an add-on package to the R package meta. The R package copas can be used to fit the Copas selection model to adjust for bias in meta-analysis. A clinical example is used to illustrate fitting and interpreting the Copas selection model.,WOS:000208589800006,R JOURNAL,,copas: An R package for Fitting the Copas Selection Model,2009
1814,"In this paper we introduce and investigate a new rejection curve for asymptotic control of the false discovery rate (FDR) in multiple hypotheses testing problems. We first give a heuristic motivation for this new curve and propose some procedures related to it. Then we introduce a set of possible assumptions and give a unifying short proof of FDR control for procedures based on Simes' critical values, whereby certain types of dependency are allowed. This methodology of proof is then applied to other fixed rejection curves including the proposed new curve. Among others, we investigate the problem of finding least favorable parameter configurations such that the FDR becomes largest. We then derive a series of results concerning asymptotic FDR control for procedures based on the new curve and discuss several example procedures in more detail. A main result will be an asymptotic optimality statement for various procedures based on the new curve in the class of fixed rejection curves. Finally, we briefly discuss strict FDR control for a finite number of hypotheses.",WOS:000265500500002,ANNALS OF STATISTICS,"['MULTIPLE TESTING PROCEDURES', 'I ERRORS', 'HYPOTHESES', 'DEPENDENCY', 'NUMBER']",ON THE FALSE DISCOVERY RATE AND AN ASYMPTOTICALLY OPTIMAL REJECTION CURVE,2009
1815,"This article contains two main theoretical results on neural spike train models, using the counting or point process on the real line as a model for the spike train. The first part of this article considers template matching of multiple spike trains. P-values for the occurrences of a given template or pattern in a set of spike trains are computed using a general scoring system. By identifying the pattern with an experimental stimulus, multiple spike trains can be deciphered to provide useful information.
The second part of the article assumes that the counting process has a conditional intensity function that is a product of a free firing rate function s, which depends only on the stimulus, and a recovery function r, which depends only on the time since the last spike. If s and r belong to a q-smooth class of functions, it is proved that sieve maximum likelihood estimators for s and r achieve the optimal convergence rate (except for a logarithmic factor) under L-1 loss.",WOS:000253077800020,ANNALS OF STATISTICS,"['DISCHARGE PATTERNS', 'LIMIT-THEOREM', 'ALGORITHMS', 'SIGNALS', 'REPLAY', 'SLEEP', 'RATES']",Some theoretical results on neural spike train probability models,2007
1816,"The optimal rate of convergence of estimators of the integrated volatility, for a discontinuous Ito semimartingale sampled at regularly spaced times and over a fixed time interval, has been a long-standing problem, at least when the jumps are not summable In this paper, we study this optimal rate, in the minimax sense and for appropriate ""bounded"" nonparametric classes of semimartingales. We show that, if the rth powers of the jumps are sununable for some r is an element of [0, 2), the minimax rate is equal to min(root 2, (n log n)((2-r)/2)), where n is the number of observations.",WOS:000338477800011,ANNALS OF STATISTICS,,A REMARK ON THE RATES OF CONVERGENCE FOR INTEGRATED VOLATILITY ESTIMATION IN THE PRESENCE OF JUMPS,2014
1817,"The GEEQBOX toolbox analyzes correlated data via the method of generalized estimating (GEE) and quasi-least squares (QLS), an approach based on GEE that overcomes some limitations of GEE that have been noted in the literature. GEEQBOX is currently able to handle correlated data that follows a normal, Bernoulii or Poisson distribution, and that is assumed to have an AR(1), Markov, tri-diagonal, equicorrelated, unstructured or working independence correlation structure. This toolbox is for use with MATLAB.",WOS:000255795200001,JOURNAL OF STATISTICAL SOFTWARE,"['LONGITUDINAL DATA', 'LINEAR-MODELS', 'FRAMEWORK']",GEEQBOX: A MATLAB toolbox for generalized estimating equations and quasi-least squares,2008
1818,"Let A(p) = YY*/m and B-p = XX*/n be two independent random matrices where X = (X-ij)(pxn) and Y = (Y-ij)(pxm) respectively consist of real (or complex) independent random variables with EXij = EYij = 0, E vertical bar X-ij vertical bar(2) = E vertical bar Y-ij vertical bar(2) = 1. Denote by lambda(1) the largest root of the determinantal equation det(lambda A(p) - B-p) = 0. We establish the Tracy-Widom type universality for lambda(1) under some moment conditions on X-ij and Y-ij when p/m and p/n approach positive constants as p -> infinity.",WOS:000379972900007,ANNALS OF STATISTICS,"['SAMPLE COVARIANCE MATRICES', 'MULTIVARIATE-ANALYSIS', 'UNIVERSALITY', 'STATISTICS', 'FLUCTUATIONS', 'POPULATION', 'ENSEMBLES', 'LIMIT']",THE TRACY-WIDOM LAW FOR THE LARGEST EIGENVALUE OF F TYPE MATRICES,2016
1819,"This article introduces new software, the games package, for estimating strategic statistical models in R. In these models, the probability distribution over outcomes corresponds to the equilibrium of an underlying game form. We review such models and provide derivations for one example, including discussion of alternative motivations for the stochastic component of the models. We introduce the basic functionality of the games package, such as how to estimate players' utilities for outcomes as a function of covariates. The package implements maximum likelihood estimation for the most commonly used models of strategic choice, including three extensive form games and an ultimatum bargaining model. The software also includes functions for bootstrapping, plotting fitted values with their confidence intervals, performing non-nested model comparisons, and checking global convergence failures. We use the new software to replicate Leblang's (2003) analysis of speculative currency attacks.",WOS:000332110000001,JOURNAL OF STATISTICAL SOFTWARE,"['DISCRETE-CHOICE MODELS', 'CANDIDATE COMPETITION', 'STRATEGIC INTERACTION', '1ST-PRICE AUCTIONS', 'REGRESSION-MODELS', 'DYNAMIC-GAMES', 'US HOUSE', 'SELECTION', 'ELECTIONS', 'TESTS']",Estimating Extensive Form Games in R,2014
1820,This paper establishes a necessary and sufficient condition for the asymptotic normality of the nonparametric estimator of sample coverage proposed by Good [Biometrica 40 (1953) 237-264]. This new necessary and sufficient condition extends the validity of the asymptotic normality beyond the previously proven cases.,WOS:000268604900019,ANNALS OF STATISTICS,"['NUMBER', 'PROBABILITY', 'POPULATION', 'BOUNDS']",ASYMPTOTIC NORMALITY OF A NONPARAMETRIC ESTIMATOR OF SAMPLE COVERAGE,2009
1821,"This paper presents the R package anacor for the computation of simple and canonical correspondence analysis with missing values. The canonical correspondence analysis is specified in a rather general way by imposing covariates on the rows and/or the columns of the two-dimensional frequency table. The package allows for scaling methods such as standard, Benzecri, centroid, and Goodman scaling. In addition, along with well-known two- and three-dimensional joint plots including confidence ellipsoids, it offers alternative plotting possibilities in terms of transformation plots, Benzecri plots, and regression plots.",WOS:000268700800001,JOURNAL OF STATISTICAL SOFTWARE,['CONTINGENCY-TABLES'],Simple and Canonical Correspondence Analysis Using the R Package anacor,2009
1822,"We consider the problem of locating a jump discontinuity (chan-e-point) in a smooth parametric regression model with a bounded covariate. It is assumed that one can sample the covariate at different values and measure the corresponding responses. Budget constraints dictate that a total of n such measurements can be obtained. A multistage adaptive procedure is proposed, where at each stage an estimate of the change point is obtained and new points are sampled from its appropriately chosen neighborhood. It is shown that such procedures accelerate the rate of convergence of the least squares estimate of the change-point. Further, the asymptotic distribution of the estimate is derived using empirical processes techniques. The latter result provides guidelines on how to choose the tuning parameters of the multistage procedure in practice. The improved efficiency of the procedure is demonstrated using real and synthetic data. This problem is primarily motivated by applications in engineering systems.",WOS:000268113500006,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'COVARIATE', 'THRESHOLD', 'CURVES', 'MODELS']",CHANGE-POINT ESTIMATION UNDER ADAPTIVE SAMPLING,2009
1823,"This paper generalizes recent proposals of density forecasting models and it develops theory for this class of models. In density forecasting, the density of observations is estimated in regions where the density is not observed. Identification of the density in such regions is guaranteed by structural assumptions on the density that allows exact extrapolation. In this paper, the structural assumption is made that the density is a product of one-dimensional functions. The theory is quite general in assuming the shape of the region where the density is observed. Such models naturally arise when the time point of an observation can be written as the sum of two terms (e.g., onset and incubation period of a disease). The developed theory also allows for a multiplicative factor of seasonal effects. Seasonal effects are present in many actuarial, biostatistical, econometric and statistical studies. Smoothing estimators are proposed that are based on backfitting. Full asymptotic theory is derived for them. A practical example from the insurance business is given producing a within year budget of reported insurance claims. A small sample study supports the theoretical results.",WOS:000352757100006,ANNALS OF STATISTICS,"['ADDITIVE-MODELS', 'REGRESSION', 'INTEGRATION', 'ESTIMATORS']",ASYMPTOTICS FOR IN-SAMPLE DENSITY FORECASTING,2015
1824,"In this article, the weighted empirical likelihood is applied to a general setting of two-sample semiparametric models, which includes biased sampling models and case-control logistic regression models as special cases. For various types of censored data, such as right censored data, doubly censored data, interval censored data and partly interval-censored data, the weighted empirical likelihood-based semiparametric maximum likelihood estimator ((theta) over tilde (n), (F) over tilde (n)) for the underlying parameter theta(0) and distribution F(0) is derived, and the strong consistency of ((theta) over tilde (n), (F) over tilde (n)) and the asymptotic normality of (theta) over tilde (n) are established. Under biased sampling models, the weighted empirical log-likelihood ratio is shown to have an asymptotic scaled chi-squared distribution for censored data aforementioned. For right censored data, doubly censored data and partly interval-censored data, it is shown that root n((F) over tilde (n) - F(0)) weakly converges to a centered Gaussian process, which leads to a consistent goodness-of-fit test for the case-control logistic regression models.",WOS:000253390000006,ANNALS OF STATISTICS,"['RATIO CONFIDENCE-INTERVALS', 'NONPARAMETRIC-ESTIMATION', 'ASYMPTOTIC PROPERTIES', 'SELF-CONSISTENT', 'ESTIMATORS', 'DISTRIBUTIONS', 'BIAS']",Weighted empirical likelihood in some two-sample semiparametric models with various types of censored data,2008
1825,"We present a new adaptive kernel density estimator based on linear diffusion processes. The proposed estimator builds on existing ideas for adaptive smoothing by incorporating information from a pilot density estimate. In addition, we propose a new plug-in bandwidth selection method that is free from the arbitrary normal reference rules used by existing methods. We present simulation examples in which the proposed approach outperforms existing methods in terms of accuracy and reliability.",WOS:000282402800011,ANNALS OF STATISTICS,"['BANDWIDTH SELECTION', 'BOUNDARY CORRECTION', 'VARIABLE LOCATION', 'PLUG-IN', 'BIAS']",KERNEL DENSITY ESTIMATION VIA DIFFUSION,2010
1826,,WOS:000275510800002,ANNALS OF STATISTICS,,Multivariate quantiles and multiple-output regression quantiles: From L-1 optimization to halfspace depth DISCUSSION,2010
1827,"A class of nonlinear models combining a pharmacokinetic compartmental model and a pharmacodynamic Emax model is introduced. The locally D-optimal (LD) design for a four-parameter composed model is found to be a saturated four-point uniform LD design with the two boundary points of the design space in the LD design support. For a five-parameter composed model, a sufficient condition for the LD design to require the minimum number of sampling time points is derived. Robust LD designs are also investigated for both models. It is found that an LD design with k parameters is equivalent to an LD design with k - 1 parameters if the linear parameter in the two composed models is a nuisance parameter. Assorted examples of LD designs are presented.",WOS:000253390000017,ANNALS OF STATISTICS,"['EXPONENTIAL REGRESSION-MODELS', 'OPTICAL-DENSITY DATA', 'NONLINEAR MODELS', 'POPULATION', 'PHARMACOKINETICS', 'QUINIDINE', 'DYNAMICS']",Locally D-optimal designs based on a class of composed models resulted from blending Emax and one-compartment models,2008
1828,"We introduce the R package DBKGrad, conceived to facilitate the use of kernel smoothing in graduating mortality rates. The package implements univariate and bivariate adaptive discrete beta kernel estimators. Discrete kernels have been preferred because, in this context, variables such as age, calendar year and duration, are pragmatically considered as discrete and the use of beta kernels is motivated since it reduces boundary bias. Furthermore, when data on exposures to the risk of death are available, the use of adaptive bandwidth, that may be selected by cross-validation, can provide additional benefits. To exemplify the use of the package, an application to Italian mortality rates, for different ages and calendar years, is presented.",WOS:000340587300001,JOURNAL OF STATISTICAL SOFTWARE,['AUTODEPENDOGRAM'],DBKGrad: An R Package for Mortality Rates Graduation by Discrete Beta Kernel Techniques,2014
1829,"The integrated nested Laplace approximation (INLA) provides an interesting way of approximating the posterior marginals of a wide range of Bayesian hierarchical models. This approximation is based on conducting a Laplace approximation of certain functions and numerical integration is extensively used to integrate some of the models parameters out.
The R-INLA package offers an interface to INLA, providing a suitable framework for data analysis. Although the INLA methodology can deal with a large number of models, only the most relevant have been implemented within R-INLA. However, many other important models are not available for R-INLA yet.
In this paper we show how to fit a number of spatial models with R-INLA, including its interaction with other R packages for data analysis. Secondly, we describe a novel method to extend the number of latent models available for the model parameters. Our approach is based on conditioning on one or several model parameters and fit these conditioned models with R-INLA. Then these models are combined using Bayesian model averaging to provide the final approximations to the posterior marginals of the model.
Finally, we show some examples of the application of this technique in spatial statistics. It is worth noting that our approach can be extended to a number of other fields, and not only spatial statistics.",WOS:000349847700001,JOURNAL OF STATISTICAL SOFTWARE,"['APPROXIMATE BAYESIAN-INFERENCE', 'NESTED LAPLACE APPROXIMATIONS', 'MODELS', 'PACKAGE']",Spatial Data Analysis with R-INLA with Some Extensions,2015
1830,,WOS:000208589700003,R JOURNAL,,"Collaborative Software Development Using R-Forge Special invited paper on ""The Future of R""",2009
1831,"Gaussian covariance graph models encode marginal independence among the components of a multivariate random vector by means of a graph G. These models are distinctly different from the traditional concentration graph models (often also referred to as Gaussian graphical models or covariance selection models) since the zeros in the parameter are now reflected in the covariance matrix E, as compared to the concentration matrix Omega = Sigma(-1) The parameter space of interest for covariance graph models is the cone PG of positive definite matrices with fixed zeros corresponding to the missing edges of G. As in Letac and Massam [Ann. Statist. 35 (2007) 1278-1323], we consider the case where G is decomposable. In this paper, we construct on the cone PG a family of Wishart distributions which serve a similar purpose in the covariance graph setting as those constructed by Letac and Massam [Ann. Statist. 35 (2007) 1278-1323] and Dawid and Lauritzen [Ann. Statist. 21 (1993) 1272-1317] do in the concentration graph setting. We proceed to undertake a rigorous study of these ""covariance"" Wishart distributions and derive several deep and useful properties of this class. First, they form a rich conjugate family of priors with multiple shape parameters for covariance graph models. Second, we show how to sample from these distributions by using a block Gibbs sampling algorithm and prove convergence of this block Gibbs sampler. Development of this class of distributions enables Bayesian inference, which, in turn, allows for the estimation of Sigma, even in the case when the sample size is less than the dimension of the data (i.e., when ""n < p""), otherwise not generally possible in the maximum likelihood framework. Third, we prove that when G is a homogeneous graph, our covariance priors correspond to standard conjugate priors for appropriate directed acyclic graph (DAG) models. This correspondence enables closed form expressions for normalizing constants and expected values, and also establishes hyper-Markov properties for our class of priors. We also note that when G is homogeneous, the family IW(QG) of Letac and Massam [Ann. Statist. 35 (2007) 1278-1323] is a special case of our covariance Wishart distributions. Fourth, and finally, we illustrate the use of our family of conjugate priors on real and simulated data.",WOS:000288183800017,ANNALS OF STATISTICS,"['GAUSSIAN MODELS', 'MATRIX', 'COMPLETIONS', 'EXPRESSION', 'CONJUGATE', 'INFERENCE', 'PRIORS', 'CONES']",WISHART DISTRIBUTIONS FOR DECOMPOSABLE COVARIANCE GRAPH MODELS,2011
1832,"The performance of multiple hypothesis testing is known to be affected by the statistical dependence among random variables involved. The mechanisms responsible for this, however, are not well understood. We study the effects of the dependence structure of a finite state hidden Markov model (HMM) on the likelihood ratios critical for optimal multiple testing on the hidden states. Various convergence results are obtained for the likelihood ratios as the observations of the HMM form an increasing long chain. Analytic expansions of the first and second order derivatives are obtained for the case of binary states, explicitly showing the effects of the parameters of the HMM on the likelihood ratios.",WOS:000288183800015,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'MAXIMUM-LIKELIHOOD ESTIMATOR', 'GEOMETRIC ERGODICITY', 'STABILITY', 'CHAINS']",EFFECTS OF STATISTICAL DEPENDENCE ON MULTIPLE TESTING UNDER A HIDDEN MARKOV MODEL,2011
1833,"ROC curve analysis is a fundamental tool for evaluating the performance of a marker in a number of research areas, e.g., biomedicine, bioinformatics, engineering etc., and is frequently used for discriminating cases from controls. There are a number of analysis tools which are used to guide researchers through their analysis. Some of these tools are commercial and provide basic methods for ROC curve analysis while others offer advanced analysis techniques and a command-based user interface, such as the R environment. The R environmentg includes comprehensive tools for ROC curve analysis; however, using a command-based interface might be challenging and time consuming when a quick evaluation is desired; especially for non-R users, physicians etc. Hence, a quick, comprehensive, free and easy-to-use analysis tool is required. For this purpose, we developed a user-friendly web-tool based on the R language. This tool provides ROC statistics, graphical tools, optimal cutpoint calculation, comparison of several markers, and sample size estimation to support researchers in their decisions without writing R codes. easyROC can be used via any device with an internet connection independently of the operating system. The web interface of easyROC is constructed with the R package shiny. This tool is freely available through www.biosoft.hacettepe.edu.tr/easyROC.",WOS:000395669800014,R JOURNAL,"['OPERATING CHARACTERISTIC CURVES', 'AREAS', 'PROPORTIONS', 'INTERVAL', 'PACKAGE']",easyROC: An Interactive Web-tool for ROC Curve Analysis Using R Language Environment,2016
1834,Here we present and discuss the R package modTempEff including a set of functions aimed at modelling temperature effects on mortality with time series data. The functions fit a particular log linear model which allows to capture the two main features of mortality-temperature relationships: nonlinearity and distributed lag effect. Penalized splines and segmented regression constitute the core of the modelling framework. We briefly review the model and illustrate the functions throughout a simulated dataset.,WOS:000273882100001,JOURNAL OF STATISTICAL SOFTWARE,"['AMBIENT-TEMPERATURE', 'BREAK-POINTS', 'MODELS', 'REGRESSION', 'SELECTION']",Analyzing Temperature Effects on Mortality Within the R Environment: The Constrained Segmented Distributed Lag Parameterization,2010
1835,"State Space Models (SSM) is a MATLAB toolbox for time series analysis by state space methods. The software features fully interactive construction and combination of models, with support for univariate and multivariate models, complex time-varying (dynamic) models, non-Gaussian models, and various standard models such as ARIMA and structural time-series models. The software includes standard functions for Kalman filtering and smoothing, simulation smoothing, likelihood evaluation, parameter estimation, signal extraction and forecasting, with incorporation of exact initialization for filters and smoothers, and support for missing observations and multiple time series input with common analysis structure. The software also includes implementations of TRAMO model selection and Hillmer-Tiao decomposition for ARIMA models. The software will provide a general toolbox for time series analysis on the MATLAB platform, allowing users to take advantage of its readily available graph plotting and general matrix computation capabilities.",WOS:000290527100001,JOURNAL OF STATISTICAL SOFTWARE,,The State Space Models Toolbox for MATLAB,2011
1836,"We consider two nonparametric procedures for estimating a concave distribution function based on data corrupted with additive noise generated by a bounded decreasing density on (0, infinity). For the maximum likelihood (ML) estimator and least squares (LS) estimator, we state qualitative properties, prove consistency and propose a computational algorithm. For the LS estimator and its derivative, we also derive the pointwise asymptotic distribution. Moreover, the rate n(-2/5) achieved by the LS estimator is shown to be minimax for estimating the distribution function at a fixed point.",WOS:000265500500009,ANNALS OF STATISTICS,"['NONPARAMETRIC DECONVOLUTION', 'OPTIMAL RATES', 'DENSITY', 'CONVERGENCE']",ESTIMATING A CONCAVE DISTRIBUTION FUNCTION FROM DATA CORRUPTED WITH ADDITIVE NOISE,2009
1837,"In many social, economical, biological and medical studies, one objective is to classify a subject into one of several classes based on a set of variables observed from the subject. Because the probability distribution of the variables is usually unknown, the rule of classification is constructed using a training sample. The well-known linear discriminant analysis (LDA) works well for the situation where the number of variables used for classification is much smaller than the training sample size. Because of the advance in technologies, modern statistical studies often face classification problems with the number of variables much larger than the sample size, and the LDA may perform poorly. We explore when and why the LDA has poor performance and propose a sparse LDA that is asymptotically optimal under some sparsity conditions on the unknown parameters. For illustration of application, we discuss an example of classifying human cancer into two classes of leukemia based on a set of 7,129 genes and a training sample of size 72. A simulation is also conducted to check the performance of the proposed method.",WOS:000291183300019,ANNALS OF STATISTICS,"['WAVELET SHRINKAGE', 'CLASSIFICATION', 'SELECTION']",SPARSE LINEAR DISCRIMINANT ANALYSIS BY THRESHOLDING FOR HIGH DIMENSIONAL DATA,2011
1838,"This paper is devoted to the explicit construction of optimal designs for discrimination between two polynomial regression models of degree n - 2 and n. In a fundamental paper, Atkinson and Fedorov [Biometrika 62 (1975a) 57-70] proposed the T-optimality criterion for this purpose. Recently, Atkinson [MODA 9, Advances in Model-Oriented Design and Analysis (2010) 9-16] determined T-optimal designs for polynomials up to degree 6 numerically and based on these results he conjectured that the support points of the optimal design are cosines of the angles that divide half of the circle into equal parts if the coefficient of x(n-1) in the polynomial of larger degree vanishes. In the present, paper we give a strong justification of the conjecture and determine all T-optimal designs explicitly for any degree n is an element of N. In particular, we show that there exists a one-dimensional class of T-optimal designs. Moreover, we also present a generalization to the case when the ratio between the coefficients of x(n-1) and x(n) is smaller than a certain critical value. Because of the complexity of the optimization problem, T-optimal designs have only been determined numerically so far, and this paper provides the first explicit solution of the T-optimal design problem since its introduction by Atkinson and Fedorov [Biometrika 62 (1975a) 57-70]. Finally, for the remaining cases (where the ratio of coefficients is larger than the critical value), we propose a numerical procedure to calculate the T-optimal designs. The results are also illustrated in an example.",WOS:000304684900008,ANNALS OF STATISTICS,"['RIVAL MODELS', 'REGRESSION']",T-OPTIMAL DESIGNS FOR DISCRIMINATION BETWEEN TWO POLYNOMIAL MODELS,2012
1839,"Structural equation models (SEM) are very popular in many disciplines. The partial least squares (PLS) approach to SEM offers an alternative to covariance-based SEM, which is especially suited for situations when data is not normally distributed. PLS path modelling is referred to as soft-modeling-technique with minimum demands regarding measurement scales, sample sizes and residual distributions. These m PLS package provides the capability to estimate PLS path models within the R programming environment. Different setups for the estimation of factor scores can be used. Furthermore it contains modular methods for computation of bootstrap confidence intervals, model parameters and several quality indices. Various plot functions help to evaluate the model. The well known mobile phone dataset from marketing research is used to demonstrate the features of the package.",WOS:000305117200001,JOURNAL OF STATISTICAL SOFTWARE,"['PLS', 'INDICATORS']",semPLS: Structural Equation Modeling Using Partial Least Squares,2012
1840,"In analyzing data deriving from the administration of a questionnaire to a group of individuals, Item Response Theory (IRT) models provide a flexible framework to account for several aspects involved in the response process, such as the existence of multiple latent traits. In this paper, we focus on a class of semi-parametric multidimensional IRT models, in which these traits are represented through one or more discrete latent variables; these models allow us to cluster individuals into homogeneous latent classes and, at the same time, to properly study item characteristics. In particular, we follow a within-item multidimensional formulation similar to that adopted in the two-tier models, with each item measuring one or two latent traits. The proposed class of models may be estimated through the package MLCIRTwithin, whose functioning is illustrated in this paper with examples based on data about quality-of-life measurement and about the propensity to commit a crime.",WOS:000395669800010,R JOURNAL,"['ITEM BIFACTOR ANALYSIS', 'RESPONSE THEORY']",Two-Tier Latent Class IRT Models in R,2016
1841,"Standard statistical techniques often require transforming data to have mean 0 and standard deviation I. Typically, this process of ""standardization"" or ""normalization"" is applied across subjects when each subject produces a single number. High throughput genomic and financial data often come as rectangular arrays where each coordinate in one direction concerns subjects who might have different status (case or control, say), and each coordinate in the other designates ""outcome"" for a specific feature, for example, ""gene,"" ""polymorphic site"" or some aspect of financial profile. It may happen, when analyzing data that arrive as a rectangular array, that one requires BOTH the subjects and the features to be ""on the same fooling."" Thus there may be a need to standardize across rows and columns of the rectangular matrix. There arises the question as to how to achieve this double normalization. We propose and investigate the convergence of what seems to us a natural approach to successive normalization which we learned from our colleague Bradley Efron. We also study the implementation of the method on simulated data and also on data that arose from scientific experimentation.",WOS:000277471000013,ANNALS OF STATISTICS,,SUCCESSIVE NORMALIZATION OF RECTANGULAR ARRAYS,2010
1842,"In standardized testing it is important to equate tests in order to ensure that the test takers, regardless of the test version given, obtain a fair test. Recently, the kernel method of test equating, which is a conjoint framework of test equating, has gained popularity. The kernel method of test equating includes five steps: (1) pre-smoothing, (2) estimation of the score probabilities, (3) continuization, (4) equating, and (5) computing the standard error of equating and the standard error of equating difference. Here, an implementation has been made for six different equating designs: equivalent groups, single group, counter balanced, non-equivalent groups with anchor test using either chain equating or post-stratification equating, and non-equivalent groups using covariates. An R package for the kernel method of test equating called kequate is presented. Included in the package are also diagnostic tools aiding in the search for a proper log-linear model in the pre-smoothing step for use in conjunction with the R function glm",WOS:000325948000001,JOURNAL OF STATISTICAL SOFTWARE,,Performing the Kernel Method of Test Equating with the Package kequate,2013
1843,"We develop estimation for potentially high-dimensional additive structural equation models. A key component of our approach is to decouple order search among the variables from feature or edge selection in a directed acyclic graph encoding the causal structure. We show that the former can be done with nonregularized (restricted) maximum likelihood estimation while the latter can be efficiently addressed using sparse regression techniques. Thus, we substantially simplify the problem of structure search and estimation for an important class of causal models. We establish consistency of the (restricted) maximum likelihood estimator for low- and high-dimensional scenarios, and we also allow for misspecification of the error distribution: Furthermore, we develop an efficient computational algorithm which can deal with many variables, and the new method's accuracy and performance is illustrated on simulated and real data.",WOS:000345884900013,ANNALS OF STATISTICS,"['DIRECTED ACYCLIC GRAPHS', 'SELECTION', 'LASSO', 'DISCOVERY', 'VARIABLES', 'SPARSITY']","CAM: CAUSAL ADDITIVE MODELS, HIGH-DIMENSIONAL ORDER SEARCH AND PENALIZED REGRESSION",2014
1844,,WOS:000299186500003,ANNALS OF STATISTICS,,REMEMBERING ERICH LEHMANN,2011
1845,"In many applications, such as physiology and finance, large time series data bases are to be analyzed requiring the computation of linear, nonlinear and other measures. Such measures have been developed and implemented in commercial and freeware softwares rather selectively and independently. The Measures of Analysis of Time Series (MATS) MATLAB toolkit is designed to handle an arbitrary large set of scalar time series and compute a large variety of measures on them, allowing for the specification of varying measure parameters as well. The variety of options with added facilities for visualization of the results support different settings of time series analysis, such as the detection of dynamics changes in long data records, resampling (surrogate or bootstrap) tests for independence and linearity with various test statistics, and discrimination power of different measures and for different combinations of their parameters. The basic features of MATS are presented and the implemented measures are briefly described. The usefulness of MATS is illustrated on some empirical examples along with screenshots.",WOS:000275203600001,JOURNAL OF STATISTICAL SOFTWARE,"['SURROGATE DATA', 'EPILEPTIC SEIZURES', 'NONLINEARITY', 'COMPLEXITY', 'PREDICTABILITY', 'SYSTEM', 'TESTS']",Measures of Analysis of Time Series (MATS): A MATLAB Toolkit for Computation of Multiple Measures on Time Series Data Bases,2010
1846,"TableToLongForm automatically converts hierarchical Tables intended for a human reader into a simple LongForm dataframe that is machine readable, making it easier to access and use the data for analysis. It does this by recognising positional cues present in the hierarchical Table (which would normally be interpreted visually by the human brain) to decompose, then reconstruct the data into a LongForm dataframe. The article motivates the benefit of such a conversion with an example Table, followed by a short user manual, which includes a comparison between the simple one argument call to TableToLongForm, with code for an equivalent manual conversion. The article then explores the types of Tables the package can convert by providing a gallery of all recognised patterns. It finishes with a discussion of available diagnostic methods and future work.",WOS:000348651700003,R JOURNAL,['PACKAGE'],Automatic Conversion of Tables to LongForm Dataframes,2014
1847,"A scoring rule is a loss function measuring the quality of a quoted probability distribution Q for a random variable X, in the light of the realized outcome x of X; it is proper if the expected score, under any distribution P for X, is minimized by quoting Q = P. Using the fact that any differentiable proper scoring rule on a finite sample space X is the gradient of a concave homogeneous function, we consider when such a rule can be local in the sense of depending only on the probabilities quoted for points in a nominated neighborhood of x. Under mild conditions, we characterize such a proper local scoring rule in terms of a collection of homogeneous functions on the cliques of an undirected graph on the space X. A useful property of such rules is that the quoted distribution Q need only be known up to a scale factor. Examples of the use of such scoring rules include Besag's pseudo-likelihood and Hyvarinen's method of ratio matching.",WOS:000304684900022,ANNALS OF STATISTICS,['INFORMATION'],PROPER LOCAL SCORING RULES ON DISCRETE SAMPLE SPACES,2012
1848,"The increasing popularity and complexity of random number intensive methods such as simulation and bootstrapping in econometrics requires researchers to have a good grasp of random number generation in general, and the specific generators that they employ in particular. Here, we discuss the random number generation options, their specifications, and their implementations in gretl. We also assess the performance and the reliability of gretl in this department by conducting extensive empirical testing using the TestU01 library. Our results show that the available alternatives are soundly implemented and should be sufficient for most econometric applications.",WOS:000326871800001,JOURNAL OF STATISTICAL SOFTWARE,"['STATISTICAL PROCEDURES', 'MICROSOFT-EXCEL', 'ACCURACY', 'SOFTWARE', 'TESTS']",Random Number Generation in gretl,2012
1849,"We extend deconvolution in a periodic setting to deal with functional data. The resulting functional deconvolution model can be viewed as a generalization of a multitude of inverse problems in mathematical physics where one needs to recover initial or boundary conditions on the basis of observations from a noisy solution of a partial differential equation. In the case when it is observed at a finite number of distinct points, the proposed functional deconvolution model can also be viewed as a multichannel deconvolution model.
We derive minimax lower bounds for the L-2-risk in the proposed functional deconvolution model when f(.) is assumed to belong to a Besov ball and the blurring function is assumed to possess some smoothness properties, including both regular-smooth and super-smooth convolutions. Furthermore, we propose an adaptive wavelet estimator of f (.) that is asymptotically optimal (in the minimax sense), or near-optimal within a logarithmic factor, in a wide range of Besov balls.
In addition, we consider a discretization of the proposed functional deconvolution model and investigate when the availability of continuous data gives advantages over observations at the asymptotically large number of points. As an illustration, we discuss particular examples for both continuous and discrete settings.",WOS:000263129000003,ANNALS OF STATISTICS,"['WAVELET DECONVOLUTION', 'DENSITY DECONVOLUTION', 'INVERSE PROBLEMS', 'DECOMPOSITION', 'SYSTEMS', 'RISK']",FUNCTIONAL DECONVOLUTION IN A PERIODIC SETTING: UNIFORM CASE,2009
1850,"Asymptotic equivalence results for nonparametric regression experiments have always assumed that the variances of the observations are known. In practice, however the variance of each observation is generally considered to be an unknown nuisance parameter. We establish an asymptotic approximation to the nonparametric regression experiment when the value of the variance is an additional parameter to be estimated or tested. This asymptotically equivalent experiment has two components: the first contains all the information about the variance and the second has all the information about the mean. The result can be extended to regression problems where the variance varies slowly from observation to observation.",WOS:000249568000012,ANNALS OF STATISTICS,"['WAVELET SHRINKAGE', 'HETEROSCEDASTICITY', 'EQUIVALENCE']",Asymptotic approximation of nonparametric regression experiments with unknown variances,2007
1851,"We consider maximum likelihood estimation for both causal and noncausal autoregressive time series processes with non-Gaussian alpha-stable noise. A nondegenerate limiting distribution is given for maximum likelihood estimators of the parameters of the autoregressive model equation and the parameters of the stable noise distribution. The estimators for the autoregressive parameters are n(1/alpha)-consistent and converge in distribution to the maximizer of a random function. The form of this limiting distribution is intractable, but the shape of the distribution for these estimators can be examined using the bootstrap procedure. The bootstrap is asymptotically valid under general Conditions. The estimators for the parameters of the stable noise distribution have the traditional n(1/2) rate of convergence and are asymptotically normal. The behavior of the estimators for finite samples is studied via simulation, and we use maximum likelihood estimation to fit a noncausal autoregressive model to the natural logarithms of volumes of Wal-Mart stock traded daily on the New York Stock Exchange.",WOS:000268113500011,ANNALS OF STATISTICS,"['ABSOLUTE DEVIATION ESTIMATION', 'INFINITE VARIANCE', 'RANDOM-VARIABLES', 'MOVING AVERAGES', 'LIMIT THEORY', 'MODELS']",MAXIMUM LIKELIHOOD ESTIMATION FOR alpha-STABLE AUTOREGRESSIVE PROCESSES,2009
1852,We introduce a new R package called glmperm for inference in generalized linear models especially for small and moderate-sized data sets. The inference is based on the permutation of regressor residuals test introduced by Potter (2005). The implementation of glmperm outperforms currently available permutation test software as glmperm can be applied in situations where more than one covariate is involved.,WOS:000208589900007,R JOURNAL,,glmperm: A Permutation of Regressor Residuals Test for Inference in Generalized Linear Models,2010
1853,Deheuvels [J. Multivariate Anal. 11 (1981) 102-113] and Genest and Remillard [Test 13 (2004) 335-369] have shown that powerful rank tests of multivariate independence can be based on combinations of asymptotically independent Cramer-von Mises statistics derived from a Mobius decomposition of the empirical copula process. A result on the large-sample behavior of this process under contiguous sequences of alternatives is used here to give a representation of the limiting distribution of such test statistics and to compute their relative local asymptotic efficiency. Local power curves and asymptotic relative efficiencies are compared under familiar classes of copula alternatives.,WOS:000247498100008,ANNALS OF STATISTICS,"['NONPARAMETRIC TEST', 'KIEFER', 'BLUM']",Asymptotic local efficiency of Cramer-von Mises tests for multivariate independence,2007
1854,"In a smooth semiparametric estimation problem, the marginal posterior for the parameter of interest is expected to be asymptotically normal and satisfy frequentist criteria of optimality if the model is endowed with a suitable prior. It is shown that, under certain straightforward and interpretable conditions, the assertion of Le Cam's acclaimed, but strictly parametric, Bernstein-von Mises theorem [Univ. California Publ. Statist. 1 (1953) 277-329] holds in the semiparametric situation as well. As a consequence, Bayesian point-estimators achieve efficiency, for example, in the sense of Hajek's convolution theorem [Z. Wahrsch. Verw. Gebiete 14 (1970) 323-330]. The model is required to satisfy differentiability and metric entropy conditions, while the nuisance prior must assign nonzero mass to certain Kullback-Leibler neighborhoods [Ghosal, Ghosh and van der Vaart Ann. Statist. 28 (2000) 500-531]. In addition, the marginal posterior is required to converge at parametric rate, which appear to be the most stringent condition in examples. The results are applied to estimation of the linear coefficient in partial linear regression, with a Gaussian prior on a smoothness class for the nuisance.",WOS:000304684900009,ANNALS OF STATISTICS,"['POSTERIOR DISTRIBUTIONS', 'NONPARAMETRIC REGRESSION', 'PROFILE LIKELIHOOD', 'LINEAR-MODELS', 'RATES', 'CONVERGENCE', 'CONSISTENCY', 'PRIORS']",THE SEMIPARAMETRIC BERNSTEIN-VON MISES THEOREM,2012
1855,"In climate science, teleconnection analysis has a long standing history as a means for describing regions that exhibit above average capability of explaining variance over time within a certain spatial domain (e.g., global). The most prominent example of a global coupled ocean-atmosphere teleconnection is the El Nino Southern Oscillation. There are numerous signal decomposition methods for identifying such regions, the most widely used of which are (rotated) empirical orthogonal functions. First introduced by van den Dool, Saha, and Johansson (2000), empirical orthogonal teleconnections (EOT) denote a regression based approach that allows for straight-forward interpretation of the extracted modes. In this paper we present the R implementation of the original algorithm in the remote package. To highlight its usefulness, we provide three examples of potential use-case scenarios for the method including the replication of one of the original examples from van den Dool et al. (2000). Furthermore, we highlight the algorithm's use for cross-correlations between two different geographic fields (identifying sea surface temperature drivers for precipitation), as well as statistical downscaling from coarse to fine grids (using Normalized Difference Vegetation Index fields).",WOS:000365975100001,JOURNAL OF STATISTICAL SOFTWARE,"['RAINFALL VARIABILITY', 'SUPPORT VECTOR', 'PRECIPITATION', 'CLIMATE', 'MODEL', 'AUSTRALIA', 'DRIVERS', 'PROJECT', 'EUROPE']",remote: Empirical Orthogonal Teleconnections in R,2015
1856,"We consider the problem of computing an approximation to the integral I = integral([0, 1])d f (x) dx. Monte Carlo (MC) sampling typically attains a root mean squared error (RMSE) of O(n(-1/2)) from n independent random function evaluations. By contrast, quasi-Monte Carlo (QMC) sampling using carefully equispaced evaluation points can attain the rate O(n(-1+epsilon)) for any epsilon > 0 and randomized QMC (RQMC) can attain the RMSE O(n(-3/2+epsilon)), both under mild conditions on f.
Classical variance reduction methods for MC call be adapted to QMC. Published results combining QMC with importance sampling and with control variates have found worthwhile improvements, but no change in the error rate. This paper extends the classical variance reduction method of antithetic sampling and combines it with RQMC. One Such method is shown to bring a modest improvement in the RMSE rate, attaining O(n(-3/2-1/d+epsilon)) for any epsilon > 0, for smooth enough f.",WOS:000260554100012,ANNALS OF STATISTICS,"['MONOMIAL CUBATURE RULES', 'DIGITAL NETS', 'MONTE-CARLO', 'INTEGRALS', 'VARIANCE', 'COMPILATION', 'DISCREPANCY', 'SEQUENCES', 'STROUD']",LOCAL ANTITHETIC SAMPLING WITH SCRAMBLED NETS,2008
1857,"The R package vdg provides a flexible interface for producing various graphical summaries of the prediction variance associated with specific linear model specifications and experimental designs. These methods include variance dispersion graphs, fraction of design space plots and quantile plots which can assist in choosing between a catalogue of candidate experimental designs. Instead of restrictive optimization methods used in traditional software to explore design regions, vdg utilizes sampling methods to introduce more flexibility. The package takes advantage of R's modern graphical abilities via gg-plot2 (Wickham 2009), adds facilities for using a variety of distance methods, allows for more flexible model specifications and incorporates quantile regressions to help with model comparison.",WOS:000392513200001,JOURNAL OF STATISTICAL SOFTWARE,"['RESPONSE-SURFACE DESIGNS', 'PREDICTION CAPABILITY', 'VARIANCE', 'FRACTION', 'PLOTS', 'SPACE']",Flexible Graphical Assessment of Experimental Designs in R: The vdg Package,2016
1858,"The R package clValid contains functions for validating the results of a clustering analysis. There are three main types of cluster validation measures available, ""internal"", ""stability"", and ""biological"". The user can choose from nine clustering algorithms in existing R packages, including hierarchial, K-means, self-organizing maps (SOM), and model-based clustering. In addition, we provide a function to perform the self-organizing maps (SOM), and model-based clustering. In addition, we provide a function to perform the self-organizing tree algorithm (SOTA) method of clustering. Any combination of validation measures and clustering methods can be requested in a single function call. This allows the user to simultaneouly evaluate several clustering algorithms while varying the number of clusters, to help determine the most appropriate method and number of clusters for the dataset of interest. Additionally, the package can automatically make use of the biological information contained in the Gene Ontology (GO) database to calculate the biological validation measures, via the annotation packages available in Bioconductor. The function returns an object of S4 class ""clValid"", which has summary, plot, print, and additional methods which allow the user to display the optimal validation scores and extract clustering results.",WOS:000254619700001,JOURNAL OF STATISTICAL SOFTWARE,"['GENE-EXPRESSION DATA', 'GROWING NEURAL-NETWORK', 'MICROARRAY DATA', 'ANNOTATION', 'PATTERNS']",clValid: An R package for cluster validation,2008
1859,"Cluster ensembles are collections of individual solutions to a given clustering problem which are useful or necessary to consider in a wide range of applications. The R package clue provides an extensible computational environment for creating and analyzing cluster ensembles, with basic data structures for representing partitions and hierarchies, and facilities for computing on these, including methods for measuring proximity and obtaining consensus and ""secondary"" clusterings.",WOS:000232928500001,JOURNAL OF STATISTICAL SOFTWARE,"['2 HIERARCHICAL CLUSTERINGS', 'ASSIGNMENT PROBLEM', 'PARTITIONS', 'CLASSIFICATIONS', 'CONSENSUS', 'INDEX']",A CLUE for CLUster ensembles,2005
1860,"Independent Component Analysis (ICA) models are very popular semi-parametric models in which we observe independent copies of a random vector X = AS, where A is a non-singular matrix and S has independent components. We propose a new way of estimating the unmixing matrix W = A(-1) and the marginal distributions of the components of S using nonparametric maximum likelihood. Specifically, we study the projection of the empirical distribution onto the subset of ICA distributions having log-concave marginals. We show that, from the point of view of estimating the unmixing matrix, it makes no difference whether or not the log-concavity is correctly specified. The approach is further justified by both theoretical results and a simulation study.",WOS:000321845400008,ANNALS OF STATISTICS,"['LOG-CONCAVE DENSITY', 'ICA', 'DISTRIBUTIONS', 'MODELS', 'SYSTEM']",INDEPENDENT COMPONENT ANALYSIS VIA NONPARAMETRIC MAXIMUM LIKELIHOOD ESTIMATION,2012
1861,"We consider the predictive problem of supervised ranking, where the task is to rank sets of candidate items returned in response to queries. Although there exist statistical procedures that come with guarantees of consistency in this setting, these procedures require that individuals provide a complete ranking of all items, which is rarely feasible in practice. Instead, individuals routinely provide partial preference information, such as pairwise comparisons of items, and more practical approaches to ranking have aimed at modeling this partial preference data directly. As we show, however, such an approach raises serious theoretical challenges. Indeed, we demonstrate that many commonly used surrogate losses for pairwise comparison data do not yield consistency; surprisingly, we show inconsistency even in low-noise settings. With these negative results as motivation, we present a new approach to supervised ranking based on aggregation of partial preferences, and we develop U-statistic-based empirical risk minimization procedures. We present an asymptotic analysis of these new procedures, showing that they yield consistency results that parallel those available for classification. We complement our theoretical results with an experiment studying the new procedures in a large-scale web-ranking task.",WOS:000327746100002,ANNALS OF STATISTICS,"['CLASSIFICATION METHODS', 'STATISTICAL-ANALYSIS', 'DECISION-MAKING', 'MINUS 2', 'MINIMIZATION', 'JUDGMENT', 'CAPACITY', 'ONLINE', 'PLUS']",THE ASYMPTOTICS OF RANKING ALGORITHMS,2013
1862,"We propose new concepts of statistical depth, multivariate quantiles, vector quantiles and ranks, ranks and signs, based on canonical transportation maps between a distribution of interest on R-d and a reference distribution on the d-dimensional unit ball. The new depth concept, called Monge Kantorovich depth, specializes to halfspace depth for d = 1 and in the case of spherical distributions, but for more general distributions, differs from the latter in the ability for its contours to account for non-convex features of the distribution of interest. We propose empirical counterparts to the population versions of those Monge Kantorovich depth contours, quantiles, ranks, signs and vector quantiles and ranks, and show their consistency by establishing a uniform convergence property for empirical (forward and reverse) transport maps, which is the main theoretical result of this paper.",WOS:000396804900007,ANNALS OF STATISTICS,"['HALF-SPACE DEPTH', 'TUKEY DEPTH', 'MULTIVARIATE-ANALYSIS', 'REGRESSION QUANTILES', 'OPTIMAL TESTS', 'INFERENCE', 'NOTION', 'PLANE', 'MODEL', 'SHAPE']","MONGE-KANTOROVICH DEPTH, QUANTILES, RANKS AND SIGNS",2017
1863,"This paper is devoted to the R package fda.usc which includes some utilities for functional data analysis. This package carries out exploratory and descriptive analysis of functional data analyzing its most important features such as depth measurements or functional outliers detection, among others. The R package fda.usc also includes functions to compute functional regression models, with a scalar response and a functional explanatory data via non-parametric functional regression, basis representation or functional principal components analysis. There are natural extensions such as functional linear models and semi-functional partial linear models, which allow non-functional covariates and factors and make predictions. The functions of this package complement and incorporate the two main references of functional data analysis: The R package fda and the functions implemented by Ferraty and Vieu (2006).",WOS:000310774500001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR-MODEL', 'REGRESSION']",Statistical Computing in Functional Data Analysis: The R Package fda.usc,2012
1864,"We consider the model
y = X theta* + xi,
Z = X + Xi,
where the random vector y is an element of R(n) and the random n x p matrix Z are observed, the n x p matrix X is unknown, Xi is an n x p random noise matrix, xi is an element of R(n) is a noise independent of Xi, and theta* is a vector of unknown parameters to be estimated. The matrix uncertainty is in the fact that X is observed with additive error. For dimensions p that can be much larger than the sample size n, we consider the estimation of sparse vectors theta*. Under matrix uncertainty, the Lasso and Dantzig selector turn out to be extremely unstable in recovering the sparsity pattern (i.e., of the set of nonzero components of theta*), even if the noise level is very small. We suggest new estimators called matrix uncertainty selectors (or, shortly, the MU-selectors) which are close to theta* in different norms and in the prediction risk if the restricted eigenvalue assumption on X is satisfied. We also show that under somewhat stronger assumptions, these estimators recover correctly the sparsity pattern.",WOS:000282402800002,ANNALS OF STATISTICS,"['DANTZIG SELECTOR', 'INVERSE PROBLEMS', 'LASSO', 'AGGREGATION', 'REGRESSION', 'OPERATORS']",SPARSE RECOVERY UNDER MATRIX UNCERTAINTY,2010
1865,"We describe the R package ipw for estimating inverse probability weights. We show how to use the package to fit marginal structural models through inverse probability weighting, to estimate causal effects. Our package can be used with data from a point treatment situation as well as with a time-varying exposure and time-varying confounders. It can be used with binomial, categorical, ordinal and continuous exposure variable.",WOS:000294837000001,JOURNAL OF STATISTICAL SOFTWARE,"['MARGINAL STRUCTURAL MODELS', 'CAUSAL INFERENCE', 'SURVIVAL']",ipw: An R Package for Inverse Probability Weighting,2011
1866,"We formalize the problem of detecting a community in a network into testing whether in a given (random) graph there is a subgraph that is unusually dense. Specifically, we observe an undirected and unweighted graph on N nodes. Under the null hypothesis, the graph is a realization of an Erdos-Renyi graph with probability p(0). Under the (composite) alternative, there is an unknown subgraph of n nodes where the probability of connection is P-1 > p(0). We derive a detection lower bound for detecting such a subgraph in terms of N, n, p(0), P1 and exhibit a test that achieves that lower bound. We do this both when p(0) is known and unknown. We also consider the problem of testing in polynomial-time. As an aside, we consider the problem of detecting a clique, which is intimately related to the planted clique problem. Our focus in this paper is in the quasi-normal regime where np(0) is either bounded away from zero, or tends to zero slowly.",WOS:000338477800005,ANNALS OF STATISTICS,"['HIGHER CRITICISM', 'ANOMALY DETECTION', 'MIXTURES', 'MODELS', 'GRAPH']",COMMUNITY DETECTION IN DENSE RANDOM NETWORKS,2014
1867,"We assume that we have observational data generated from an unknown underlying directed acyclic graph (DAG) model. A DAG is typically not identifiable from observational data, but it is possible to consistently estimate the equivalence class of a DAG. Moreover, for any given DAG, causal effects can be estimated using intervention calculus. In this paper, we combine these two parts. For each DAG in the estimated equivalence class, we use intervention calculus to estimate the causal effects of the covariates on the response. This yields a collection of estimated causal effects for each covariate. We show that the distinct values in this set can be consistently estimated by an algorithm that uses only local information of the graph. This local approach is computationally fast and feasible in high-dimensional problems. We propose to use summary measures of the set of possible causal effects to determine variable importance. In particular, we use the minimum absolute value of this set, since that is a lower bound on the size of the causal effect. We demonstrate the merits of our methods in a simulation study and on a data set about riboflavin production.",WOS:000271673500002,ANNALS OF STATISTICS,"['CAUSAL INFERENCE', 'GRAPHS', 'CONSISTENCY', 'STATISTICS', 'SELECTION', 'DIAGRAMS', 'LASSO', 'MODEL']",ESTIMATING HIGH-DIMENSIONAL INTERVENTION EFFECTS FROM OBSERVATIONAL DATA,2009
1868,"We study behavior of the restricted maximum likelihood (REML) estimator under a misspecified linear mixed model (LMM) that has received much attention in recent genome-wide association studies. The asymptotic analysis establishes consistency of the REML estimator of the variance of the errors in the LMM, and convergence in probability of the REML estimator of the variance of the random effects in the LMM to a certain limit, which is equal to the true variance of the random effects multiplied by the limiting proportion of the nonzero random effects present in the LMM. The asymptotic results also establish convergence rate (in probability) of the REML estimators as well as a result regarding convergence of the asymptotic conditional variance of the REML estimator. The asymptotic results are fully supported by the results of empirical studies, which include extensive simulation studies that compare the performance of the REML estimator (under the misspecified LMM) with other existing methods, and real data applications (only one example is presented) that have important genetic implications.",WOS:000384397200015,ANNALS OF STATISTICS,"['HUMAN HEIGHT', 'MISSING HERITABILITY', 'REML ESTIMATION', 'COMMON SNPS', 'PROPORTION', 'ESTIMATORS', 'DISEASES', 'MATRIX', 'LIMIT', 'LOCI']",ON HIGH-DIMENSIONAL MISSPECIFIED MIXED MODEL ANALYSIS IN GENOME-WIDE ASSOCIATION STUDY,2016
1869,"We present the cacher package for R, which provides tools for caching statistical analyses and for distributing these analyses to others in an efficient manner. The cacher package takes objects created by evaluating R expressions and stores them in key-value databases. These databases of cached objects can subsequently be assembled into packages for distribution over the web. The cacher package also provides tools to help readers examine the data and code in a statistical analysis and reproduce, modify, or improve upon the results. In addition, readers can easily conduct alternate analyses of the data. We describe the design and implementation of the cacher package and provide two examples of how the package can be used for reproducible research.",WOS:000258204100001,JOURNAL OF STATISTICAL SOFTWARE,['AIR-POLLUTION'],Caching and distributing statistical analyses in R,2008
1870,"To simulate Gaussian fields poses serious numerical problems: storage and computing time. The midpoint displacement method is often used for simulating the fractional Brownian fields because it is fast. We propose an effective and fast method, valid not only for fractional Brownian fields, but for any Gaussian fields. First, our method is compared with midpoint for fractional Brownian fields. Second, the performance of our method is illustrated by simulating several Gaussian fields. The software FieldSim is an R package developed in R and C and that implements the procedures on which this paper focuses.",WOS:000252430900001,JOURNAL OF STATISTICAL SOFTWARE,"['MOTIONS', 'MODELS']",On fractional Gaussian random fields simulations,2007
1871,,WOS:000267708500002,JOURNAL OF STATISTICAL SOFTWARE,,Computational details,2009
1872,"Large-scale precision matrix estimation is of fundamental importance yet challenging in many contemporary applications for recovering Gaussian graphical models. In this paper, we suggest a new approach of innovated scalable efficient estimation (ISEE) for estimating large precision matrix. Motivated by the innovated transformation, we convert the original problem into that of large covariance matrix estimation. The suggested method combines the strengths of recent advances in high-dimensional sparse modeling and large covariance matrix estimation. Compared to existing approaches, our method is scalable and can deal with much larger precision matrices with simple tuning. Under mild regularity conditions, we establish that this procedure can recover the underlying graphical structure with significant probability and provide efficient estimation of link strengths. Both computational and theoretical advantages of the procedure are evidenced through simulation and real data examples.",WOS:000384397200014,ANNALS OF STATISTICS,"['COVARIANCE-MATRIX ESTIMATION', 'NONCONCAVE PENALIZED LIKELIHOOD', 'FALSE DISCOVERY RATE', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'DANTZIG SELECTOR', 'ADAPTIVE LASSO', 'REGRESSION', 'CLASSIFICATION']",INNOVATED SCALABLE EFFICIENT ESTIMATION IN ULTRA-LARGE GAUSSIAN GRAPHICAL MODELS,2016
1873,"Multi-regional clinical trials have been widely used for efficient global new drug developments. Due to potential heterogeneity of patient populations, it is critical to evaluate consistency of treatment effects across different regions in a multi-regional trial in order to determine the applicability of the overall treatment effect to the patients in individual regions. Quan et al. (2010) proposed definitions for the assessments of consistency of treatment effects in multi-regional trials. To facilitate the application of their ideas to design multi-regional trials, in this paper, we provide the corresponding R functions for calculating the unconditional and conditional probabilities for demonstrating consistency in relationship with the overall/regional sample sizes and the anticipated treatment effects. Detailed step by step instructions and trial examples are also provided to illustrate the applications of these R functions.",WOS:000326871000001,JOURNAL OF STATISTICAL SOFTWARE,,R Functions for Sample Size and Probability Calculations for Assessing Consistency of Treatment Effects in Multi-Regional Clinical Trials,2012
1874,"The coneproj package contains routines for cone projection and quadratic programming, plus applications in estimation and inference for constrined parametric regression and shape-restricted regression problems. A short routine check_irred is included to chek the irreducibility of a matrix, whose rows are supposed to be a set of cone edges used by coneA OR coneB. For the coneA and coneB functions, the vector to project is provided by the user, along with the cone specifications, the vector. For coneB, the cone edges are provided. The coneA and coneB algorithms have been coded and compiled in C++, and are called by R. The qprog function transforms a quadratic programming problems into a cone projection problem and calls coneA. The constreg function does estimation and inference for parametric least-squares regression with constrainst on the parameters (using coneA). A p value for the ""one-sided"" test is provided. The shapereg function uses coneB to provide a least a least-squares estimator for a regression function with several choices of constraints including isotonic and convex regression function, as well as significance of the effects are also provided. This package is now available from the Comprehensive R Archive Network at http://CRAN.R-project.org/package=coneproj.",WOS:000349842000001,JOURNAL OF STATISTICAL SOFTWARE,"['SHAPE-RESTRICTED REGRESSION', 'BASES ALGORITHM', 'INEQUALITIES']",coneproj: An R Package for the Primal or Dual Cone Projections with Routines for Constrained Regression,2014
1875,"We consider linear transformation models applied to right censored survival data with a change-point in the regression coefficient based on a covariate threshold. We establish consistency and weak convergence of the nonparametric maximum likelihood estimators. The change-point parameter is shown to be n-consistent, while the remaining parameters are shown to have the expected root-n consistency. We show that the procedure is adaptive in the sense that the nonthreshold parameters are estimable with the same precision as if the true threshold value were known. We also develop Monte Carlo methods of inference for model parameters and score tests for the existence of a change-point. A key difficulty here is that some of the model parameters are not identifiable under the null hypothesis of no change-point. Simulation studies establish the validity of the proposed score tests for finite sample sizes.",WOS:000248692700002,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'COX REGRESSION', 'NUISANCE PARAMETER', 'HYPOTHESIS', 'TESTS']",Inference under right censoring for transformation models with a change-point based on a covariate threshold,2007
1876,"The openair package contains data analysis tools for the air quality community. This paper provides an overview of data importers, main functions, and selected utilities and workhorse functions within the package and the function output class, as of package version 0.4-14. It is intended as an explanation of the rationale for the package and a technical description for those wishing to work more interactively with the main functions or develop additional functions to support 'higher level' use of openair and R.",WOS:000313197700004,R JOURNAL,,openair - Data Analysis Tools for the Air Quality Community,2012
1877,,WOS:000357441000004,ANNALS OF STATISTICS,,"DISCUSSION OF ""FREQUENTIST COVERAGE OF ADAPTIVE NONPARAMETRIC BAYESIAN CREDIBLE SETS""",2015
1878,"Principal component analysis (PCA) is a standard tool for dimensional reduction of a set of n observations (samples), each with p variables. In this paper, using a matrix perturbation approach, we study the nonasymptotic relation between the eigenvalues and eigenvectors of PCA computed on a finite sample of size n, and those of the limiting population PCA as n -> infinity. As in machine learning, we present a finite sample theorem which holds with high probability for the closeness between the leading eigenvalue and eigenvector of sample PCA and population PCA under a spiked covariance model. In addition, we also consider the relation between finite sample PCA and the asymptotic results in the joint limit p, n -> infinity, with p/n = c. We present a matrix perturbation view of the ""phase transition phenomenon,"" and a simple linear-algebra based derivation of the eigenvalue and eigenvector overlap in this asymptotic limit. Moreover, our analysis also applies for finite p, n where we show that although there is no sharp phase transition as in the infinite case, either as a function of noise level or as a function of sample size n, the eigenvector of sample PICA may exhibit a sharp ""loss of tracking,"" suddenly losing its relation to the (true) eigenvector of the population PCA matrix. This occurs due to a crossover between the eigenvalue due to the signal and the largest eigenvalue due to noise, whose eigenvector points in a random direction.",WOS:000262731400008,ANNALS OF STATISTICS,"['DIMENSIONAL RANDOM MATRICES', 'COVARIANCE MATRICES', 'LARGEST EIGENVALUE', 'EMPIRICAL DISTRIBUTION', 'ROOTS', 'MODEL']",FINITE SAMPLE APPROXIMATION RESULTS FOR PRINCIPAL COMPONENT ANALYSIS: A MATRIX PERTURBATION APPROACH,2008
1879,"In this work the software application called Glotaran is introduced as a Java-based graphical user interface to the R package TIMP, a problem solving environment for fitting superposition models to multi-dimensional data. TIMP uses a command-line user interface for the interaction with data, the specification of models and viewing of analysis results. Instead, Glotaran provides a graphical user interface which features interactive and dynamic data inspection, easier assisted by the user interface model specification and interactive viewing of results. The interactivity component is especially helpful when working with large, multi-dimensional datasets as often result from time-resolved spectroscopy measurements, allowing the user to easily pre-select and manipulate data before analysis and to quickly zoom in to regions of interest in the analysis results. Glotaran has been developed on top of the NetBeans rich client platform and communicates with R through the Java-to-R interface R serve. The background and the functionality of the application are described here. In addition, the design, development and implementation process of Glotaran is documented in a generic way.",WOS:000305989900001,JOURNAL OF STATISTICAL SOFTWARE,['TARGET ANALYSIS'],Glotaran: A Java-Based Graphical User Interface for the R Package TIMP,2012
1880,"We describe the R package sae for small area estimation. This package can be used to obtain model-based estimates for small areas based on a variety of models at the area and unit levels, along with basic direct and indirect estimates. Mean squared errors are estimated by analytical approximations in simple models and applying bootstrap procedures in more complex models. We describe the package functions and show how to use them through examples.",WOS:000357431900008,R JOURNAL,"['MEAN SQUARED ERROR', 'MODEL', 'PREDICTION', 'BOOTSTRAP', 'EBLUP']",sae: An R Package for Small Area Estimation,2015
1881,"We consider the problem of inference on a regression function at a point when the entire function satisfies a sign or shape restriction under the null. We propose a test that achieves the optimal minimax rate adaptively over a range of Holder classes, up to a log log n term, which we show to be necessary for adaptation. We apply the results to adaptive one-sided tests for the regression discontinuity parameter under a monotonicity restriction, the value of a monotone regression function at the boundary and the proportion of true null hypotheses in a multiple testing problem.",WOS:000362697700008,ANNALS OF STATISTICS,"['NONPARAMETRIC CONFIDENCE-INTERVALS', 'ASYMPTOTIC EQUIVALENCE', 'WHITE-NOISE', 'IDENTIFICATION', 'HYPOTHESES', 'MIXTURES']",ADAPTIVE TESTING ON A REGRESSION FUNCTION AT A POINT,2015
1882,"The paper considers functional linear regression, where scalar responses Y(1), ... , Y(n) are modeled in dependence of random functions X(1), ... , X(n). We propose a smoothing splines estimator for the functional slope parameter based on a slight modification of the usual penalty. Theoretical analysis concentrates on the error in all out-of-sample prediction of the response for a new random function X(n+1). It is shown that rates of convergence of the prediction error depend on the smoothness of the slope function and on the Structure Of the predictors. We then prove that these rates are optimal in the sense that they are minimax over large classes of possible slope functions and distributions of the predictive curves. For the case of models with errors-in-variables the smoothing spline estimator is modified by using a denoising correction of the covariance matrix of discretized curves. The methodology is then applied to a real case study where the aim is to predict the maximum of the concentration of ozone by using the curve of this concentration measured the preceding day.",WOS:000263129000002,ANNALS OF STATISTICS,"['MODELS', 'NOISY']",SMOOTHING SPLINES ESTIMATORS FOR FUNCTIONAL LINEAR REGRESSION,2009
1883,"This paper describes graphical methods for multiple-response data within the framework of the multivariate linear model (MLM), aimed at understanding what is being tested in a multivariate test, and how factor/predictor effects are expressed across multiple response measures.
In particular, we describe and illustrate a collection of SAS macro programs for: ( a) Data ellipses and low-rank biplots for multivariate data, (b) HE plots, showing the hypothesis and error covariance matrices for a given pair of responses, and a given effect, ( c) HE plot matrices, showing all pairwise HE plots, and (d) low-rank analogs of HE plots, showing all observations, group means, and their relations to the response variables.",WOS:000242545800001,JOURNAL OF STATISTICAL SOFTWARE,['COVARIANCE'],"Data ellipses, HE plots and reduced-rank displays for multivariate linear models: SAS software and examples",2006
1884,"In a context of multiple hypothesis testing, we provide several new exact calculations related to the false discovery proportion (FDP) of step-up and step-down procedures. For step-up procedures, we show that the number of erroneous rejections conditionally on the rejection number is simply a binomial variable, which leads to explicit computations of the c.d.f., the sth moment and the mean of the FDP, the latter corresponding to the false discovery rate (FDR). For step-down procedures, we derive what is to our knowledge the first explicit formula for the FDR valid for any alternative c.d.f. of the p-values. We also derive explicit computations of the power for both step-up and step-down procedures. These formulas are ""explicit"" in the sense that they only involve the parameters of the model and the c.d.f. of the order statistics of i.i.d. uniform variables. The p-values are assumed either independent or coming from an equicorrelated multivariate normal model and an additional mixture model for the true/false hypotheses is used. Our approach is then used to investigate new results which are of interest in their own right, related to least/most favorable configurations for the FDR and the variance of the FDP.",WOS:000288183800019,ANNALS OF STATISTICS,"['FDR CONTROL', 'EMPIRICAL BAYES', 'INDEPENDENCE', 'DEPENDENCY', 'TESTS']",EXACT CALCULATIONS FOR FALSE DISCOVERY PROPORTION WITH APPLICATION TO LEAST FAVORABLE CONFIGURATIONS,2011
1885,"We introduce and examine dbEmpLikeGOF, an R package for performing goodness-of-fit tests based on sample entropy. This package also performs the two sample distribution comparison test. For a given vector of data observations, the provided function dbEmpLikeGOF tests the data for the proposed null distributions, or tests for distribution equality between two vectors of observations. The proposed methods represent a distribution-free density-based empirical likelihood technique applied to nonparametric testing. The proposed procedure performs exact and very efficient p values for each test statistic obtained from a Monte Carlo (MC) resampling scheme. Note by using an MC scheme, we are assured exact level alpha tests that approximate nonparametrically most powerful Neyman-Pearson decision rules. Although these entropy based tests are known in the theoretical literature to be very efficient, they have not been well addressed in statistical software. This article briefly presents the proposed tests and introduces the package, with applications to real data. We apply the methods to produce a novel analysis of a recently published dataset related to coronary heart disease.",WOS:000323909600001,JOURNAL OF STATISTICAL SOFTWARE,"['KOLMOGOROV-SMIRNOV', 'INCOMPLETE DATA', 'NORMALITY', 'PNEUMONIA']",dbEmpLikeGOF: An R Package for Nonparametric Likelihood Ratio Tests for Goodness-of-Fit and Two-Sample Comparisons Based on Sample Entropy,2013
1886,"Randomized experiments are the ""gold standard"" for estimating causal effects, yet often in practice, chance imbalances exist in covariate distributions between treatment groups. If covariate data are available before units are exposed to treatments, these chance imbalances can be mitigated by first checking covariate balance before the physical experiment takes place. Provided a precise definition of imbalance has been specified in advance, unbalanced randomizations can be discarded, followed by a rerandomization, and this process can continue until a randomization yielding balance according to the definition is achieved. By improving covariate balance, rerandomization provides more precise and trustworthy estimates of treatment effects.",WOS:000307608000023,ANNALS OF STATISTICS,"['CLINICAL-TRIALS', 'CAUSAL INFERENCE', 'RESTRICTED RANDOMIZATION', 'ADAPTIVE ALLOCATION', 'MATCHING METHODS', 'DESIGN', 'BIAS', 'ASSIGNMENT', 'PURSUIT', 'SCIENCE']",RERANDOMIZATION TO IMPROVE COVARIATE BALANCE IN EXPERIMENTS,2012
1887,"The basic purpose of the economic design of the control charts is to find the optimum control charts parameters to minimize the process cost. In this paper, an R package, edcc (economic design of control charts), which provides a numerical method to find the optimum chart parameters is presented using the unified approach of the economic design. Also, some examples are given to illustrate how to use this package. The types of the control chart available in the edcc package are (X) over bar, CUSUM (cumulative sum), and EWMA (exponentially-weighted moving average) control charts.",WOS:000315019600001,JOURNAL OF STATISTICAL SOFTWARE,['XBAR-CHARTS'],edcc: An R Package for the Economic Design of the Control Chart,2013
1888,"Streaming data, consisting of indefinitely evolving sequences, are becoming ubiquitous in many branches of science and in various applications. Computer scientists have developed streaming applications such as Storm and the S4 distributed stream computing platform to deal with data streams. However, in current production packages testing and evaluating streaming algorithms is cumbersome. This paper presents RStorm for the development and evaluation of streaming algorithms analogous to these production packages, but implemented fully in R. RStorm allows developers of streaming algorithms to quickly test, iterate, and evaluate various implementations of streaming algorithms. The paper provides both a canonical computer science example, the streaming word count, and examples of several statistical applications of RStorm.",WOS:000343788100013,R JOURNAL,,RStorm: Developing and Testing Streaming Algorithms in R,2014
1889,"The multivariate normal and the multivariate t distributions belong to the most widely used multivariate distributions in statistics, quantitative risk management, and insurance. In contrast to the multivariate normal distribution, the parameterization of the multivariate t distribution does not correspond to its moments. This, paired with a non-standard implementation in the R package mvtnorm, provides traps for working with the multivariate t distribution. In this paper, common traps are clarified and corresponding recent changes to mvtnorm are presented.",WOS:000330193300014,R JOURNAL,,On Sampling from the Multivariate t Distribution,2013
1890,"We propose a general method for constructing confidence intervals and statistical tests for single or low-dimensional components of a large parameter vector in a high-dimensional model. It can be easily adjusted for multiplicity taking dependence among tests into account. For linear models, our method is essentially the same as in Zhang and Zhang [J. R. Stat. Soc. Ser. B Stat. Methodol. 76 (2014) 217-242]: we analyze its asymptotic properties and establish its asymptotic optimality in terms of semiparametric efficiency. Our method naturally extends to generalized linear models with convex loss functions. We develop the corresponding theory which includes a careful analysis for Gaussian, sub-Gaussian and bounded correlated designs.",WOS:000338477800013,ANNALS OF STATISTICS,"['GENERALIZED LINEAR-MODELS', 'VARIABLE SELECTION', 'STABILITY SELECTION', 'DANTZIG SELECTOR', 'LASSO ESTIMATORS', 'SPARSE SIGNALS', 'FEATURE SPACE', 'REGRESSION', 'RECOVERY', 'INEQUALITIES']",ON ASYMPTOTICALLY OPTIMAL CONFIDENCE REGIONS AND TESTS FOR HIGH-DIMENSIONAL MODELS,2014
1891,Topic models allow the probabilistic modeling of term frequency occurrences in documents. The fitted model can be used to estimate the similarity between documents as well as between a set of specified keywords using an additional layer of latent variables which are referred to as topics. The R package topicmodels provides basic infrastructure for fitting topic models based on data structures from the text mining package tm. The package includes interfaces to two algorithms for fitting topic models: the variational expectation-maximization algorithm provided by David M. Blei and co-authors and an algorithm using Gibbs sampling by Xuan-Hieu Phan and co-authors.,WOS:000290391200001,JOURNAL OF STATISTICAL SOFTWARE,"['LIKELIHOOD', 'EM']",Topicmodels: An R Package for Fitting Topic Models,2011
1892,"A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.",WOS:000341806800001,JOURNAL OF STATISTICAL SOFTWARE,,Tidy Data,2014
1893,"The software package B A M P provides a method of analyzing incidence or mortality data on the Lexis diagram, using a Bayesian version of an age-period-cohort model. A hierarchical model is assumed with a binomial model in the first-stage. As smoothing priors for the age, period and cohort parameters random walks of first and second order, with and without an additional unstructured component are available. Unstructured heterogeneity can also be included in the model.
In order to evaluate the model fit, posterior deviance, DIC and predictive deviances are computed. By projecting the random walk prior into the future, future death rates can be predicted.",WOS:000252428900001,JOURNAL OF STATISTICAL SOFTWARE,"['PROJECTIONS', 'MORTALITY', 'TRENDS', 'RATES']",Bayesian age-period-cohort modeling and prediction - BAMP,2007
1894,"The googleVis package provides an interface between R and the Google Visualisation API to create interactive charts which can be embedded into web pages. The best known of these charts is probably the Motion Chart, popularised by Hans Rosling in his TED talks. With the googleVis package users can easily create web pages with interactive charts based on R data frames and display them either via the local R HTTP help server or within their own sites.",WOS:000208590200008,R JOURNAL,,Using the Google Visualisation API with R,2011
1895,An autoregressive-moving average model in which all roots of the autoregressive polynomial are reciprocals of roots of the moving average polynomial and vice versa is called an all-pass time series model. All-pass models are useful for identifying and modeling noncausal and noninvertible autoregressive-moving average processes. We establish asymptotic normality and consistency for rank-based estimators of all-pass model parameters. The estimators are obtained by minimizing the rank-based residual dispersion function given by Jaeckel [Ann. Math. Statist. 43 (1972) 1449-1458]. These estimators can have the same asymptotic efficiency as maximum likelihood estimators and are robust. The behavior of the estimators for finite samples is studied via simulation and rank estimation is used in the deconvolution of a simulated water gun seismogram.,WOS:000248987600016,ANNALS OF STATISTICS,"['R-ESTIMATION', 'LIKELIHOOD-ESTIMATION', 'ARMA MODELS', 'AUTOREGRESSION', 'SYSTEMS']",Rank-based estimation for all-pass time series models,2007
1896,"Community detection is a fundamental problem in network analysis, with applications in many diverse areas. The stochastic block model is a common tool for model-based community detection, and asymptotic tools for checking consistency of community detection under the block model have been recently developed. However, the block model is limited by its assumption that all nodes within a community are stochastically equivalent, and provides a poor fit to networks with hubs or highly varying node degrees within communities, which are common in practice. The degree-corrected stochastic block model was proposed to address this shortcoming and allows variation in node degrees within a community while preserving the overall block community structure. In this paper we establish general theory for checking consistency of community detection under the degree-corrected stochastic block model and compare several community detection criteria under both the standard and the degree-corrected models. We show which criteria are consistent under which models and constraints, as well as compare their relative performance in practice. We find that methods based on the degree-corrected block model, which includes the standard block model as a special case, are consistent under a wider class of models and that modularity-type methods require parameter constraints for consistency, whereas likelihood-based methods do not. On the other hand, in practice, the degree correction involves estimating many more parameters, and empirically we find it is only worth doing if the node degrees within communities are indeed highly variable. We illustrate the methods on simulated networks and on a network of political blogs.",WOS:000321842400005,ANNALS OF STATISTICS,"['SOCIAL NETWORKS', 'BLOCKMODELS', 'PREDICTION', 'GRAPHS']",CONSISTENCY OF COMMUNITY DETECTION IN NETWORKS UNDER DEGREE-CORRECTED STOCHASTIC BLOCK MODELS,2012
1897,"We obtain a limit of a hierarchical Bayes estimator of a finite population mean when the sample size is large. The limit is in the sense of ordinary calculus, where the sample observations are treated as fixed quantities. Our result suggests a simple way to correct the hierarchical Bayes estimator to achieve design-consistency, a well-known property in the traditional randomization approach to finite population sampling. We also suggest three different measures of uncertainty of our proposed estimator.",WOS:000248987600010,ANNALS OF STATISTICS,"['SMALL-AREA ESTIMATION', 'PREDICTION', 'MODELS', 'ERROR']",On the design-consistency property of hierarchical bayes estimators in finite population sampling,2007
1898,"Least-squares means are predictions from a linear model, or averages thereof. They are useful in the analysis of experimental data for summarizing the effects of factors, and for testing linear contrasts among predictions. The lsmeans package (Lenth 2016) provides a simple way of obtaining least-squares means and contrasts thereof. It supports many models fitted by R (R Core Team 2015) core packages (as well as a few key contributed ones) that fit linear or mixed models, and provides a simple way of extending it to cover more model classes.",WOS:000373914900001,JOURNAL OF STATISTICAL SOFTWARE,['LINEAR MIXED MODELS'],Least-Squares Means: The R Package lsmeans,2016
1899,"The demand for data from surveys, censuses or registers containing sensible information on people or enterprises has increased significantly over the last years. However, before data can be provided to the public or to researchers, confidentiality has to be respected for any data set possibly containing sensible information about individual units. Confidentiality can be achieved by applying statistical disclosure control (SDC) methods to the data in order to decrease the disclosure risk of data.
The R package sdcMicro serves as an easy-to-handle, object-oriented S4 class implementation of SDC methods to evaluate and anonymize confidential micro-data sets. It includes all popular disclosure risk and perturbation methods. The package performs automated recalculation of frequency counts, individual and global risk measures, information loss and data utility statistics after each anonymization step. All methods are highly optimized in terms of computational costs to be able to work with large data sets. Reporting facilities that summarize the anonymization process can also be easily used by practitioners. We describe the package and demonstrate its functionality with a complex household survey test data set that has been distributed by the International Household Survey Network.",WOS:000365982100001,JOURNAL OF STATISTICAL SOFTWARE,['MASKING'],Statistical Disclosure Control for Micro-Data Using the R Package sdcMicro,2015
1900,"Network analysis is one of the most widely used techniques in many areas of modern science. Most existing tools for that purpose are limited to drawing networks and computing their basic general characteristics. The user is not able to interactively and graphically manipulate the networks, select and explore subgraphs using other statistical and data mining techniques, add and plot various other data within the graph, and so on. In this paper we present a tool that addresses these challenges, an add-on for exploration of networks within the general component-based environment Orange.",WOS:000318237500001,JOURNAL OF STATISTICAL SOFTWARE,,Interactive Network Exploration with Orange,2013
1901,"Quantile regression for censored survival (duration) data offers a more flexible alternative to the Cox proportional hazard model for some applications. We describe three estimation methods for such applications that have been recently incorporated into the R package quantreg: the Powell (1986) estimator for fixed censoring, and two methods for random censoring, one introduced by Portnoy (2003), and the other by Peng and Huang (2008). The Portnoy and Peng-Huang estimators can be viewed, respectively, as generalizations to regression of the Kaplan-Meier and Nelson-Aalen estimators of univariate quantiles for censored observations. Some asymptotic and simulation comparisons are made to highlight advantages and disadvantages of the three methods.",WOS:000258206300001,JOURNAL OF STATISTICAL SOFTWARE,"['ESTIMATORS', 'EQUATIONS']",Censored quantile regression redux,2008
1902,"This article describes the many capabilities offered by the TraMineR toolbox for categorical sequence data. It focuses more specifically on the analysis and rendering of state sequences. Addressed features include the description of sets of sequences by means of transversal aggregated views, the computation of longitudinal characteristics of individual sequences and the measure of pairwise dissimilarities. Special emphasis is put on the multiple ways of visualizing sequences. The core element of the package is the state sequence object in which we store the set of sequences together with attributes such as the alphabet, state labels and the color palette. The functions can then easily retrieve this information to ensure presentation homogeneity across all printed and graphical displays. The article also demonstrates how TraMineR's outcomes give access to advanced analyses such as clustering and statistical modeling of sequence data.",WOS:000289228500001,JOURNAL OF STATISTICAL SOFTWARE,"['OPTIMAL MATCHING METHODS', 'TIME-SERIES', 'TRAJECTORIES', 'SOCIOLOGY', 'PATTERNS', 'PROSPECT', 'WORK']",Analyzing and Visualizing State Sequences in R with TraMineR,2011
1903,"actuar is a package providing additional Actuarial Science functionality to the R statistical system. The project was launched in 2005 and the package is available on the Comprehensive R Archive Network since February 2006. The current version of the package contains functions for use in the fields of loss distributions modeling, risk theory ( including ruin theory), simulation of compound hierarchical models and credibility theory. This paper presents in detail but with few technical terms the most recent version of the package.",WOS:000254620000001,JOURNAL OF STATISTICAL SOFTWARE,,actuar: An R package for actuarial science,2008
1904,"This paper aims at developing a quasi-Bayesian analysis of the nonparametric instrumental variables model, with a focus on the asymptotic properties of quasi-posterior distributions. In this paper, instead of assuming a distributional assumption on the data generating process, we consider a quasi-likelihood induced from the conditional moment restriction, and put priors on the function-valued parameter. We call the resulting posterior quasi-posterior, which corresponds to ""Gibbs posterior"" in the literature. Here we focus on priors constructed on slowly growing finite-dimensional sieves. We derive rates of contraction and a nonparametric Bernstein-von Mises type result for the quasi-posterior distribution, and rates of convergence for the quasi-Bayes estimator defined by the posterior expectation. We show that, with priors suitably chosen, the quasi-posterior distribution (the quasi-Bayes estimator) attains the minimax optimal rate of contraction (convergence, resp.). These results greatly sharpen the previous related work.",WOS:000327746100004,ANNALS OF STATISTICS,"['POSED INVERSE PROBLEMS', 'POSTERIOR DISTRIBUTIONS', 'CONVERGENCE-RATES', 'EXPONENTIAL-FAMILIES', 'ASYMPTOTIC NORMALITY', 'DENSITY-ESTIMATION', 'LINEAR-MODELS', 'REGRESSION', 'INFERENCE', 'IDENTIFICATION']",QUASI-BAYESIAN ANALYSIS OF NONPARAMETRIC INSTRUMENTAL VARIABLES MODELS,2013
1905,"Social Network Analysis (SNA) provides tools to examine relationships between people. Text Mining (TM) allows capturing the text they produce in Web 2.0 applications, for example, however it neglects their social structure. This paper applies an approach to combine the two methods named ""content-based SNA"". Using the R mailing lists, R-help and R-devel, we show how this combination can be used to describe people's interests and to find out if authors who have similar interests actually communicate. We find that the expected positive relationship between sharing interests and communicating gets stronger as the centrality scores of authors in the communication networks increase.",WOS:000208590100003,R JOURNAL,,Content-Based Social Network Analysis of Mailing Lists,2011
1906,"We provide theoretical analysis of the statistical and computational properties of penalized M-estimators that can be formulated as the solution to a possibly nonconvex optimization problem. Many important estimators fall in this category, including least squares regression with nonconvex regularization, generalized linear models with nonconvex.regularization and sparse elliptical random design regression. For these problems, it is intractable to calculate the global solution due to the nonconvex formulation. In this paper, we propose an approximate regularization path-following method for solving a variety of learning problems with nonconvex objective functions. Under a unified analytic framework, we simultaneously provide explicit statistical and computational rates of convergence for any local solution attained by the algorithm. Computationally, our algorithm attains a global geometric rate of convergence for calculating the full regularization path, which is optimal among all first-order algorithms. Unlike most existing methods that only attain geometric rates of convergence for one single regularization parameter, our algorithm calculates the full regularization path with the same iteration complexity. In particular, we provide a refined iteration complexity bound to sharply characterize the performance of each stage along the regularization path. Statistically, we provide sharp sample complexity analysis for all the approximate local solutions along the regularization path. In particular, our analysis improves upon existing results by providing a more refined sample complexity bound as well as an exact support recovery result for the final estimator. These results show that the final estimator attains an oracle statistical property due to the usage of nonconvex penalty.",WOS:000345884900002,ANNALS OF STATISTICS,"['GENERALIZED LINEAR-MODELS', 'NONCONCAVE PENALIZED LIKELIHOOD', 'MULTISTAGE CONVEX RELAXATION', 'ULTRA-HIGH DIMENSION', 'VARIABLE SELECTION', 'COORDINATE DESCENT', 'GRADIENT METHODS', 'REGRESSION', 'LASSO', 'REGULARIZATION']",OPTIMAL COMPUTATIONAL AND STATISTICAL RATES OF CONVERGENCE FOR SPARSE NONCONVEX LEARNING PROBLEMS,2014
1907,"The False Discovery Rate (FDR) paradigm aims to attain certain control on Type I errors with relatively high power for multiple hypothesis testing. The Benjamini-Hochberg (BH) procedure is a well-known FDR controlling procedure. Under a random effects model, we show that, in general, unlike the FDR, the positive FDR (pFDR) of the BH procedure cannot be controlled at an arbitrarily low level due to the limited evidence provided by the observations to separate false and true nulls. This results in a criticality phenomenon, which is characterized by a transition of the procedure's power from being positive to asymptotically 0 without any reduction in the pFDR, once the target FDR control level is below a positive critical value. To address the constraints on the power and pFDR control imposed by the criticality phenomenon, we propose a procedure which applies BH-type procedures at multiple locations in the domain of p-values. Both analysis and simulations show that the proposed procedure can attain substantially improved power and pFDR control.",WOS:000249568000003,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'RATES', 'HYPOTHESES', 'STATISTICS', 'NUMBER']",On the performance of FDR control: Constraints and a partial solution,2007
1908,"This article introduces the showtext package that makes it easy to use system fonts in R graphics. Unlike other methods to embed fonts into graphics, showtext converts text into raster images or polygons, and then adds them to the plot canvas. This method produces platform-independent image files that do not rely on the fonts that create them. It supports a large number of font formats and R graphics devices, and meanwhile provides convenient features such as using web fonts and integrating with knitr. This article provides an elaborate introduction to the showtext package, including its design, usage, and examples.",WOS:000357431900009,R JOURNAL,,showtext: Using System Fonts in R Graphics,2015
1909,,WOS:000208589800011,R JOURNAL,,Conference Review: DSC 2009,2009
1910,"This paper provides conditions under which subsampling and the bootstrap can be used to construct estimators of the quantiles of the distribution of a root that behave well uniformly over a large class of distributions P. These results are then applied (i) to construct confidence regions that behave well uniformly over P in the sense that the coverage probability tends to at least the nominal level uniformly over P and (ii) to construct tests that behave well uniformly over P in the sense that the size tends to no greater than the nominal level uniformly over P. Without these stronger notions of convergence, the asymptotic approximations to the coverage probability or size may be poor, even in very large samples. Specific applications include the multivariate mean, testing moment inequalities, multiple testing, the empirical process and U-statistics.",WOS:000321845400002,ANNALS OF STATISTICS,"['IDENTIFIED ECONOMETRIC-MODELS', 'MOMENT INEQUALITIES', 'INFERENCE', 'PARAMETERS', 'SELECTION', 'ESTIMATORS', 'RISK', 'SET']",ON THE UNIFORM ASYMPTOTIC VALIDITY OF SUBSAMPLING AND THE BOOTSTRAP,2012
1911,"Mixture models have received considerable attention recently and Newton [Sankhya Ser A 64 (2002) 306-322] proposed a fast recursive algorithm for estimating a mixing distribution. We prove almost sure consistency of this recursive estimate in the weak topology under mild conditions on the family of densities being mixed. This recursive estimate depends on the data ordering and a permutation-invariant modification is proposed, which is an average of the original over permutations of the data sequence. A Rao-Blackwell argument is used to prove consistency in probability of this alternative estimate. Several Simulations are presented, comparing the finite-sample performance of the recursive estimate and a Monte Carlo approximation to the permutation-invariant alternative along with that of the nonparametric maximum likelihood estimate and a nonparametric Bayes estimate.",WOS:000268604900015,ANNALS OF STATISTICS,"['MIXTURE MODEL', 'POSTERIOR DISTRIBUTIONS', 'NONPARAMETRIC PROBLEMS', 'DENSITY-ESTIMATION', 'BAYESIAN-ANALYSIS', 'EXPRESSION DATA', 'RATES']",CONSISTENCY OF A RECURSIVE ESTIMATE OF MIXING DISTRIBUTIONS,2009
1912,"We consider regression models with parametric (linear or nonlinear) regression function and allow responses to be ""missing at random."" We assume that the errors have mean zero and are independent of the covariates. In order to estimate expectations of functions of covariate and response we use a fully imputed estimator, namely all empirical estimator based oil estimators of conditional expectations given the covariate. We exploit the independence of covariates and errors by writing the conditional expectations as unconditional expectations, which call now be estimated by empirical plug-in estimators. The mean zero constraint oil the error distribution is exploited by adding Suitable residual-based weights. We prove that the estimator is efficient (in the sense of Hajek and Le Cam) if an efficient estimator of the parameter is used. Our results give rise to new efficient estimators of smooth transformations of expectations. Estimation of the mean response is discussed as a special (degenerate) case.",WOS:000268604900006,ANNALS OF STATISTICS,"['LIKELIHOOD-BASED INFERENCE', 'EMPIRICAL-LIKELIHOOD', 'EFFICIENT ESTIMATION', 'MEAN FUNCTIONALS', 'MODELS', 'IMPUTATION']",ESTIMATING LINEAR FUNCTIONALS IN NONLINEAR REGRESSION WITH RESPONSES MISSING AT RANDOM,2009
1913,"In this work we deal with correlated failure time (age at onset) data arising from population-based, case-control studies, where case and control probands are selected by population-based sampling and all array of risk factor measures is collected for both cases and controls and their relatives. Parameters of interest are effects of risk factors on the failure time hazard function and within-family dependencies among failure times after adjusting for the risk factors. Due to the retrospective sampling scheme, large sample theory for existing methods has not been established. We develop a novel technique for estimating the parameters of interest under a general semiparametric shared frailty model. We also present a simple, easily computed, and noniterative nonparametric estimator for the cumulative baseline hazard function. We provide rigorous large sample theory for the proposed method. We also present simulation results and a real data example for illustrating the utility of the proposed method.",WOS:000265619700014,ANNALS OF STATISTICS,"['CASE-CONTROL-FAMILY', 'COUNTING-PROCESSES', 'ASYMPTOTIC THEORY', 'ASSOCIATION', 'REGRESSION', 'POPULATION', 'MUTATIONS', 'INFERENCE', 'SAMPLE']",CASE-CONTROL SURVIVAL ANALYSIS WITH A GENERAL SEMIPARAMETRIC SHARED FRAILTY MODEL: A PSEUDO FULL LIKELIHOOD APPROACH,2009
1914,"Multivariate analyses are well known and widely used to identify and understand structures of ecological communities. The ade4 package for the R statistical environment proposes a great number of multivariate methods. Its implementation follows the tradition of the French school of ""Analyse des Donnees"" and is based on the use of the duality diagram. We present the theory of the duality diagram and discuss its implementation in ade4. Classes and main functions are presented. An example is given to illustrate the ade4 philosophy.",WOS:000252429800001,JOURNAL OF STATISTICAL SOFTWARE,"['PRINCIPAL COMPONENT ANALYSIS', 'ORDINATION', 'TRANSFORMATIONS', 'APPROXIMATION', 'VARIABLES', 'MATRICES']",The ade4 package: Implementing the duality diagram for ecologists,2007
1915,"We introduce a semiparametric ""tubular neighborhood"" of a parametric model in the multinomial setting. It consists of all multinomial distributions lying in a distance-based neighborhood of the parametric model of interest. Fitting such a tubular model allows one to use a parametric model while treating it as an approximation to the true distribution. In this paper, the Kullback-Leibler distance is used to build the tubular region. Based on this idea one can define the distance between the true multinomial distribution and the parametric model to be the index of fit. The paper develops a likelihood ratio test procedure for testing the magnitude of the index. A semiparametric bootstrap method is implemented to better approximate the distribution of the LRT statistic. The approximation permits more accurate construction of a lower confidence limit for the model fitting index.",WOS:000271673500020,ANNALS OF STATISTICS,"['DISTANCE', 'TABLES']",BUILDING AND USING SEMIPARAMETRIC TOLERANCE REGIONS FOR PARAMETRIC MULTINOMIAL MODELS,2009
1916,"This paper introduces an R package for spatial and spatio-temporal prediction and forecasting for log-Gaussian Cox processes. The main computational tool for these models is Markov chain Monte Carlo (MCMC) and the new package, lgcp, therefore also provides an extensible suite of functions for implementing MCMC algorithms for processes of this type. The modelling framework and details of inferential procedures are first presented before a tour of lgcp functionality is given via a walk-through data-analysis. Topics covered include reading in and converting data, estimation of the key components and parameters of the model, specifying output and simulation quantities, computation of Monte Carlo expectations, post-processing and simulation of data sets.",WOS:000315019100001,JOURNAL OF STATISTICAL SOFTWARE,['POINT PATTERNS'],lgcp: Inference with Spatial and Spatio-Temporal Log-Gaussian Cox Processes in R,2013
1917,"Copulas have become a popular tool in multivariate modeling successfully applied in many fields. A good open-source implementation of copulas is much needed for more practitioners to enjoy the joy of copulas. This article presents the design, features, and some implementation details of the R package copula. The package provides a carefully designed and easily extensible platform for multivariate modeling with copulas in R. S4 classes for most frequently used elliptical copulas and Archimedean copulas are implemented, with methods for density/distribution evaluation, random number generation, and graphical display. Fitting copula-based models with maximum likelihood method is provided as template examples. With the classes and methods in the package, the package can be easily extended by user-defined copulas and margins to solve problems.",WOS:000252428400001,JOURNAL OF STATISTICAL SOFTWARE,"['BIVARIATE DISTRIBUTIONS', 'ARCHIMEDEAN COPULAS', 'INDEPENDENCE', 'ASSOCIATION', 'DEPENDENCE', 'INFERENCE', 'MODELS', 'TESTS']",Enjoy the joy of copulas: With a package copula,2007
1918,"In 1985, for detecting a change in distribution, Pollak introduced a specific minimax performance metric and a randomized version of the Shiryaev-Roberts procedure where the zero initial condition is replaced by a random variable sampled from the quasi-stationary distribution of the Shiryaev-Roberts statistic. Pollak proved that this procedure is third-order asymptotically optimal as the mean time to false alarm becomes large. The question of whether Pollak's procedure is strictly minimax for any false alarm rate has been open for more than two decades, and there were several attempts to prove this strict optimality. In this paper, we provide a counterexample which shows that Pollak's procedure is not optimal and that there is a strictly optimal procedure which is nothing but the Shiryaev-Roberts procedure that starts with a specially designed deterministic point.",WOS:000290231500004,ANNALS OF STATISTICS,,ON OPTIMALITY OF THE SHIRYAEV-ROBERTS PROCEDURE FOR DETECTING A CHANGE IN DISTRIBUTION,2010
1919,"The bayesTFR package for R provides a set of functions to produce probabilistic projections of the total fertility rate (TFR) for all countries. In the model, a random walk with drift is used to project the TFR during the fertility transition, using a Bayesian hierarchical model to estimate the parameters of the drift term. The TFR is modeled with a first order autoregressive process during the post-transition phase. The computationally intensive part of the projection model is a Markov chain Monte Carlo algorithm for estimating the parameters of the drift term. This article summarizes the projection model and describes the basic steps to generate probabilistic projections, as well as other functionalities such as projecting aggregate outcomes and dealing with missing values.",WOS:000293389700001,JOURNAL OF STATISTICAL SOFTWARE,,bayesTFR: An R Package for Probabilistic Projections of the Total Fertility Rate,2011
1920,"This article introduces yaImpute, an R package for nearest neighbor search and imputation. Although nearest neighbor imputation is used in a host of disciplines, the methods implemented in the yaImpute package are tailored to imputation-based forest attribute estimation and mapping. The impetus to writing the yaImpute is a growing interest in nearest neighbor imputation methods for spatially explicit forest inventory, and a need within this research community for software that facilitates comparison among different nearest neighbor search algorithms and subsequent imputation techniques. yaImpute provides directives for defining the search space, subsequent distance calculation, and imputation rules for a given number of nearest neighbors. Further, the package offers a suite of diagnostics for comparison among results generated from different imputation analyses and a set of functions for mapping imputation results.",WOS:000252432000001,JOURNAL OF STATISTICAL SOFTWARE,"['VECTOR QUANTIZATION', 'NEIGHBOR', 'ALGORITHM', 'DENSITY', 'AREA']",yaImpute: An R package for kNN imputation,2008
1921,"The study of good nonregular fractional factorial designs has received significant attention over the last two decades. Recent research indicates that designs constructed from quaternary codes (QC) are very promising in this regard. The present paper shows how a trigonometric approach can facilitate a systematic understanding of such QC designs and lead to new theoretical results covering hitherto unexplored situations. We focus attention on one-eighth and one-sixteenth fractions of two-level factorials and show that optimal QC designs often have larger generalized resolution and projectivity than comparable regular designs. Moreover, some of these designs are found to have maximum projectivity among all designs.",WOS:000291183300009,ANNALS OF STATISTICS,"['2-LEVEL FACTORIAL-DESIGNS', 'MINIMUM ABERRATION', 'NONREGULAR DESIGNS', 'ORTHOGONAL ARRAYS', 'G(2)-ABERRATION', 'PROJECTION']",A TRIGONOMETRIC APPROACH TO QUATERNARY CODE DESIGNS WITH APPLICATION TO ONE-EIGHTH AND ONE-SIXTEENTH FRACTIONS,2011
1922,"We describe a Monte Carlo method to approximate the maximum likelihood estimate (MLE), when there are missing data and the observed data likelihood is not available in closed form. This method uses simulated missing data that are independent and identically distributed and independent of the observed data. Our Monte Carlo approximation to the MLE is a consistent and asymptotically normal estimate of the minimizer theta* of the Kullback-Leibler information, as both Monte Carlo and observed data sample sizes go to infinity simultaneously. Plug-in estimates of the asymptotic variance are provided for constructing confidence regions for theta*. We give Logit-Normal generalized linear mixed model examples, calculated using an R package.",WOS:000248692700003,ANNALS OF STATISTICS,"['GENERALIZED LINEAR-MODELS', 'CONVEX SETS CONES', 'MAXIMUM-LIKELIHOOD', 'MIXED MODELS', 'EM ALGORITHM', 'CONVERGENCE', 'SEQUENCES', 'PEDIGREES']",Monte Carlo likelihood inference for missing data models,2007
1923,"In this paper we present the Stata package stgenreg for the parametric analysis of survival data. Any user-defined hazard function can be specified, with the model estimated using maximum likelihood utilising numerical quadrature. Models that can be fitted range from the Weibull proportional hazards model to the generalized gamma model, mixture models, cure rate models, accelerated failure time models and relative survival models. We illustrate the features of stgenreg through application to a cohort of women diagnosed with breast cancer with outcome all-cause death.",WOS:000320041300001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION-MODELS', 'RELATIVE SURVIVAL', 'CANCER']",stgenreg: A Stata Package for General Parametric Survival Analysis,2013
1924,"We present a new computing environment for authoring mixed natural and computer language documents. In this environment a single hierarchically-organized plain text source file may contain a variety of elements such as code in arbitrary programming languages, raw data, links to external resources, project management data, working notes, and text for publication. Code fragments may be executed in situ with graphical, numerical and textual output captured or linked in the file. Export to LATEX, HTML, LATEX beamer, DocBook and other formats permits working reports, presentations and manuscripts for publication to be generated from the file. In addition, functioning pure code files can be automatically extracted from the file. This environment is implemented as an extension to the Emacs text editor and provides a rich set of features for authoring both prose and code, as well as sophisticated project management capabilities.",WOS:000301069900001,JOURNAL OF STATISTICAL SOFTWARE,,A Multi-Language Computing Environment for Literate Programming and Reproducible Research,2012
1925,"We describe a new package called pseval that implements the core methods for the evaluation of principal surrogates in a single clinical trial. It provides a flexible interface for defining models for the risk given treatment and the surrogate, the models for integration over the missing counterfactual surrogate responses, and the estimation methods. Estimated maximum likelihood and pseudo-score can be used for estimation, and the bootstrap for inference. A variety of post-estimation methods are provided, including print, summary, plot, and testing. We summarize the main statistical methods that are implemented in the package and illustrate its use from the perspective of a novice R user.",WOS:000395669800018,R JOURNAL,"['END-POINTS', 'VACCINE TRIALS', 'BIOMARKERS', 'DESIGNS']",An Introduction to Principal Surrogate Evaluation with the pseval Package,2016
1926,"The R package micromap is used to create linked micromaps, which display statistical summaries associated with areal units, or polygons. Linked micromaps provide a mesns to simultaneously summarize and display both statistical and geographic distributions by linking statistical summaries to a series of small maps. The package contains functions dependent on the ggplot2 package to produce a row-oriented graph composed of different panels, or columns, of information. These panels at a minimum typically contain maps, a legend, and statistical summaries, with the color-coded legend linking the maps and statistical summaries. We first describe the layout of linked with the statistical dataset. Highly detailed polygons are not appropriate for display in linked micromaps so we describe how polygon boundaries can be simplified, decreasing the time required so we describe how polygon boundaries can be simplified, describing the time required to draw the graphs, while retaining adequate detail for detection of spatial patterns. Our worked examples of linked micromaps use public health data as well as environment data collected from spatially balanced probabilistic surveys.",WOS:000349844800001,JOURNAL OF STATISTICAL SOFTWARE,"['PLOTS', 'GRAPHICS', 'INFOVIS', 'VISUALIZATION']",micromap: A Package for Linked Micromaps,2015
1927,"The R graphics engine has new support for drawing complex paths via the functions polypath() and grid.path(). This article explains what is meant by a complex path and demonstrates the usefulness of complex paths in drawing non-trivial shapes, logos, customised data symbols, and maps.",WOS:000313198000003,R JOURNAL,,"It's Not What You Draw, It's What You Don't Draw Drawing Paths with Holes in R Graphics",2012
1928,"Parameter and structural learning on continuous time Bayesian network classifiers are challenging tasks when you are dealing with big data. This paper describes an efficient scalable parallel algorithm for parameter and structural learning in the case of complete data using the MapReduce framework. Two popular instances of classifiers are analyzed, namely the continuous time naive Bayes and the continuous time tree augmented naive Bayes. Details of the proposed algorithm are presented using Hadoop, an open-source implementation of a distributed file system and the MapReduce framework for distributed data processing. Performance evaluation of the designed algorithm shows a robust parallel scaling.",WOS:000349843200001,JOURNAL OF STATISTICAL SOFTWARE,,Learning Continuous Time Bayesian Network Classifiers Using MapReduce,2014
1929,"We consider the estimation of integrated covariance (ICV) matrices of high dimensional diffusion processes based on high frequency observations. We start by studying the most commonly used estimator, the realized covariance (RCV) matrix. We show that in the high dimensional case when the dimension p and the observation frequency n grow in the same rate, the limiting spectral distribution (LSD) of RCV depends on the covolatility process not only through the targeting ICV, but also on how the covolatility process varies in time. We establish a Marcenko-Pastur type theorem for weighted sample covariance matrices, based on which we obtain a Marcenko-Pastur type theorem for RCV for a class C of diffusion processes. The results explicitly demonstrate how the time variability of the covolatility process affects the LSD of RCV. We further propose an alternative estimator, the time-variation adjusted realized covariance (TVARCV) matrix. We show that for processes in class C, the TVARCV possesses the desirable property that its LSD depends solely on that of the targeting ICV through the Marcenko-Pastur equation, and hence, in particular, the TVARCV can be used to recover the empirical spectral distribution of the ICV by using existing algorithms.",WOS:000300383200012,ANNALS OF STATISTICS,"['LIMITING SPECTRAL DISTRIBUTION', 'EMPIRICAL DISTRIBUTION', 'EIGENVALUES', 'VOLATILITY']",ON THE ESTIMATION OF INTEGRATED COVARIANCE MATRICES OF HIGH DIMENSIONAL DIFFUSION PROCESSES,2011
1930,"For better or for worse, rankings of institutions, such as universities, schools and hospitals, play an important role today in conveying information about relative performance. They inform policy decisions and budgets, and are often reported in the media. While overall rankings can vary markedly over relatively short time periods, it is not unusual to find that the ranks of a small number of ""highly performing"" institutions remain fixed, even when the data on which the rankings are based are extensively revised, and even when a large number of new institutions are added to the competition. In the present paper, we endeavor to model this phenomenon. In particular, we interpret as a random variable the value of the attribute on which the ranking should ideally be based. More precisely, if p items are to be ranked then the true, but unobserved, attributes are taken to be values of p independent and identically distributed variates. However, each attribute value is observed only with noise, and via a sample of size roughly equal to n, say. These noisy approximations to the true attributes are the quantities that are actually ranked. We show that, if the distribution of the true attributes is light-tailed (e.g., normal or exponential) then the number of institutions whose ranking is correct, even after recalculation using new data and even after many new institutions are added, is essentially fixed. Formally, p is taken to be of order n(C) for any fixed C > 0, and the number of institutions whose ranking is reliable depends very little on p. On the other hand, cases where the number of reliable rankings increases significantly when new institutions are added are those for which the distribution of the true attributes is relatively heavy-tailed, for example, with tails that decay like x(-alpha) for some alpha > 0. These properties and others are explored analytically, under general conditions. A numerical study links the results to outcomes for real-data problems.",WOS:000282402800003,ANNALS OF STATISTICS,"['BOOTSTRAP', 'COVERAGE']",MODELING THE VARIABILITY OF RANKINGS,2010
1931,"The Rasch sampler is an efficient algorithm to sample binary matrices with given marginal sums. It is a Markov chain Monte Carlo (MCMC) algorithm. The program can handle matrices of up to 1024 rows and 64 columns. A special option allows to sample square matrices with given marginals and fixed main diagonal, a problem prominent in social network analysis. In all cases the stationary distribution is uniform. The user has control on the serial dependency.",WOS:000247011100001,JOURNAL OF STATISTICAL SOFTWARE,['TESTS'],The Rasch sampler,2007
1932,"We derive the precise asymptotic distributional behavior of Gaussian variational approximate estimators of the parameters in a single-predictor Poisson mixed model. These results are the deepest yet obtained concerning the statistical properties of a variational approximation method. Moreover, they give rise to asymptotically valid statistical inference. A simulation study demonstrates that Gaussian variational approximate confidence intervals possess good to excellent coverage properties, and have a similar precision to their exact likelihood counterparts.",WOS:000299186500012,ANNALS OF STATISTICS,"['BAYES', 'MODEL']",ASYMPTOTIC NORMALITY AND VALID INFERENCE FOR GAUSSIAN VARIATIONAL APPROXIMATION,2011
1933,"sphet is a package for estimating and testing spatial models with heteroskedastic innovations. We implement recent generalized moments estimators and semiparametric methods for the estimation of the coefficients variance-covariance matrix. This paper is a general description of sphet and all functionalities are illustrated by application to the popular Boston housing dataset. The package in its current version is limited to the estimators based on Arraiz, Drukker, Kelejian, and Prucha (2010); Kelejian and Prucha (2007, 2010). The estimation functions implemented in sphet are able to deal with virtually any sample size.",WOS:000281586900001,JOURNAL OF STATISTICAL SOFTWARE,"['AUTOREGRESSIVE MODELS', 'DISTURBANCES', 'ESTIMATORS', 'MATRICES', 'ERRORS', 'AIR']",sphet: Spatial Models with Heteroskedastic Innovations in R,2010
1934,"This paper introduces the hypergeo package of R routines for numerical calculation of hypergeometric functions. The package is focussed on efficient and accurate evaluation of the Gauss hypergeometric function over the whole of the complex plane within the constraints of fixed-precision arithmetic. The hypergeometric series is convergent only within the unit circle, so analytic continuation must be used to define the function outside the unit circle. This short document outlines the numerical and conceptual methods used in the package; and justifies the package philosophy, which is to maintain transparent and verifiable links between the software and Abramowitz and Stegun (1965). Most of the package functionality is accessed via the single function hypergeo(), which dispatches to one of several methods depending on the value of its arguments. The package is demonstrated in the context of game theory.",WOS:000368551800007,R JOURNAL,"['ANALYTIC CONTINUATION', 'PARAMETERS']",Numerical Evaluation of the Gauss Hypergeometric Function with the hypergeo Package,2015
1935,,WOS:000208589700008,R JOURNAL,,mvtnorm: New Numerical Algorithm for Multivariate Normal Probabilities,2009
1936,"extremely popular dimension reduction technique for high-dimensional data. The theoretical challenge, in the simplest case, is to estimate the leading eigenvector of a population covariance matrix under the assumption that this eigenvector is sparse. An impressive range of estimators have been proposed; some of these are fast to compute, while others are known to achieve the mini-max optimal rate over certain Gaussian or sub-Gaussian classes. In this paper, we show that, under a widely-believed assumption from computational complexity theory, there is a fundamental trade-off between statistical and computational performance in this problem. More precisely, working with new, larger classes satisfying a restricted covariance concentration condition, we show that there is an effective sample size regime in which no randomised polynomial time algorithm can achieve the minimax optimal rate. We also study the theoretical performance of a (polynomial time) variant of the well-known semidefinite relaxation estimator, revealing a subtle interplay between statistical and computational efficiency.",WOS:000384397200007,ANNALS OF STATISTICS,"['HIGH-DIMENSIONAL DATA', 'ADAPTIVE ESTIMATION', 'POWER METHOD', 'PCA', 'EIGENVALUE', 'MINIMIZATION', 'RELAXATIONS', 'CONSISTENCY', 'OPERATORS', 'CLIQUES']",STATISTICAL AND COMPUTATIONAL TRADE-OFFS IN ESTIMATION OF SPARSE PRINCIPAL COMPONENTS,2016
1937,Locally stationary process representations have recently been proposed and applied to both time series and image analysis applications. This article describes an implementation of the locally stationary two-dimensional wavelet process approach in R. This package permits construction of estimates of spatially localized spectra and localized autocovariance which can be used to characterize structure within images.,WOS:000293390000001,JOURNAL OF STATISTICAL SOFTWARE,"['ADAPTIVE ESTIMATION', 'TIME-SERIES', 'SPECTRA', 'MODEL']",LS2W: Implementing the Locally Stationary 2D Wavelet Process Approach in R,2011
1938,"This paper discusses a nonparametric regression model that naturally generalizes neural network models. The model is based on a finite number of one-dimensional transformations and can be estimated with a one-dimensional rate of convergence. The model contains the generalized additive model with unknown link function as a special case. For this case, it is shown that the additive components and link function can be estimated with the optimal rate by a smoothing spline that is the solution of a penalized least squares criterion.",WOS:000253077800016,ANNALS OF STATISTICS,"['ADDITIVE-MODELS', 'LINEAR-MODELS', 'LIKELIHOOD ESTIMATION', 'ASYMPTOTIC PROPERTIES', 'SPLINES', 'SERIES']",Rate-optimal estimation for a general class of nonparametric regression models with unknown link functions,2007
1939,,WOS:000348651700016,R JOURNAL,,Conference Report: R in Insurance 2014,2014
1940,,WOS:000299186500001,ANNALS OF STATISTICS,,INTRODUCTION TO THE LEHMANN SPECIAL SECTION,2011
1941,"In this paper, we give an explanation to the failure of two likelihood ratio procedures for testing about covariance matrices from Gaussian populations when the dimension p is large compared to the sample size n. Next, using recent central limit theorems for linear spectral statistics of sample covariance matrices and of random F-matrices, we propose necessary corrections for these LR tests to cope with high-dimensional effects. The asymptotic distributions of these corrected tests under the null are given. Simulations demonstrate that the corrected LR tests yield a realized size close to nominal level for both moderate p (around 20) and high dimension, while the traditional LR tests with chi(2) approximation fails.
Another contribution from the paper is that for testing the equality between two covariance matrices, the proposed correction applies equally for non-Gaussian populations yielding a valid pseudo-likelihood ratio test.",WOS:000271673700004,ANNALS OF STATISTICS,,CORRECTIONS TO LRT ON LARGE-DIMENSIONAL COVARIANCE MATRIX BY RMT,2009
1942,"Given n samples X-1, X-2,...,X-n from N(0, Sigma), we are interested in estimating the p x p precision matrix Omega = Sigma(-)1; we assume Omega is sparse in that each row has relatively few nonzeros.
We propose Partial Correlation Screening (PCS) as a new row -by -row approach. To estimate the ith row of Omega, 1 <= i <= p, PCS uses a Screen step and a Clean step. In the Screen step, PCS recruits a (small) subset of indices using a stage -wise algorithm, where in each stage, the algorithm updates the set of recruited indices by adding the index j that has the largest empirical partial correlation (in magnitude) with i, given the set of indices recruited so far. In the Clean step, PCS reinvestigates all recruited indices, removes false positives and uses the resultant set of indices to reconstruct the ith row.
PCS is computationally efficient and modest in memory use: to estimate a row of Omega, it only needs a few rows (determined sequentially) of the empirical covariance matrix. PCS is able to execute an estimation of a large Omega (e.g., p = 10K) in a few minutes.
Higher Criticism Thresholding (HCT) is a recent classifier that enjoys optimality, but to exploit its full potential, we need a good estimate of Omega. Note that given an estimate of Omega, we can always combine it with HCT to build a classifier (e.g., HCT-PCS, HCT-glasso).
We have applied HCT-PCS to two microarray data sets (p = 8K and 10K) for classification, where it not only significantly outperforms HCT-glasso, but also is competitive to the Support Vector Machine (SVM) and Random Forest (RF). These suggest that PCS gives more useful estimates of Omega than the glasso; we study this carefully and have gained some interesting insight.
We show that in a broad context, PCS fully recovers the support of Omega and HCT-PCS is optimal in classification. Our theoretical study sheds interesting light on the behavior of stage-wise procedures.",WOS:000384397200011,ANNALS OF STATISTICS,"['DIMENSIONAL VARIABLE SELECTION', 'HIGHER CRITICISM', 'GRAPHICAL LASSO', 'COVARIANCE ESTIMATION', 'MODEL', 'ALTERNATIVES', 'DISCRIMINANT', 'PERFORMANCE', 'REGRESSION', 'FEATURES']","PARTIAL CORRELATION SCREENING FOR ESTIMATING LARGE PRECISION MATRICES, WITH APPLICATIONS TO CLASSIFICATION",2016
1943,"The minimum aberration criterion has been frequently used in the selection of fractional factorial designs with nominal factors. For designs with quantitative factors, however, level permutation of factors could alter their geometrical structures and statistical properties. In this paper uniformity is used to further distinguish fractional factorial designs, besides the minimum aberration criterion. We show that minimum aberration designs have low discrepancies on average. An efficient method for constructing uniform minimum aberration designs is proposed and optimal designs with 27 and 81 runs are obtained for practical use. These designs have good uniformity and are effective for studying quantitative factors.",WOS:000307608000010,ANNALS OF STATISTICS,['MINIMUM ABERRATION'],UNIFORM FRACTIONAL FACTORIAL DESIGNS,2012
1944,"Trust region algorithms are nonlinear optimization tools that tend to be stable and reliable when the objective function is non-concave, ill-conditioned, or exhibits regions that are nearly at. Additionally, most freely-available optimization routines do not exploit the sparsity of the Hessian when such sparsity exists, as in log posterior densities of Bayesian hierarchical models. The trustOptim package for the R programming language addresses both of these issues. It is intended to be robust, scalable and e_cient for a large class of nonlinear optimization problems that are often encountered in statistics, such as _nding posterior modes. The user must supply the objective function, gradient and Hessian. However, when used in conjunction with the sparseHessianFD package, the user does not need to supply the exact sparse Hessian, as long as the sparsity structure is known in advance. For models with a large number of parameters, but for which most of the cross-partial derivatives are zero (i.e., the Hessian is sparse), trustOptim o_ers dramatic performance improvements over existing options, in terms of computational time and memory footprint.",WOS:000345289200001,JOURNAL OF STATISTICAL SOFTWARE,,trust Optim : An R Package for Trust Region Optimization with Sparse Hessians,2014
1945,"We propose a two-sample test for the means of high-dimensional data when the data dimension is much larger than the sample size. Hotelling's classical T(2) test does not work for this ""large p, small n"" situation. The proposed test does not require explicit conditions in the relationship between the data dimension and sample size. This offers much flexibility in analyzing high-dimensional data. An application of the proposed test is in testing significance for sets of genes which we demonstrate in an empirical study on a leukemia data set.",WOS:000275510800009,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'MICROARRAY DATA', 'COVARIANCE-MATRIX', 'HYPOTHESIS TESTS', 'NORMALIZATION', 'CONSISTENCY', 'CATEGORIES', 'EXPRESSION', 'LIMIT', 'MODEL']",A TWO-SAMPLE TEST FOR HIGH-DIMENSIONAL DATA WITH APPLICATIONS TO GENE-SET TESTING,2010
1946,"It is well known that using individual covariate information (such as body weight or gender) to model heterogeneity in capture-recapture (CR) experiments can greatly enhance inferences on the size of a closed population. Since individual covariates are only observable for captured individuals, complex conditional likelihood methods are usually required and these do not constitute a standard generalized linear model (GLM) family. Modern statistical techniques such as generalized additive models (GAMs), which allow a relaxing of the linearity assumptions on the covariates, are readily available for many standard GLM families. Fortunately, a natural statistical framework for maximizing conditional likelihoods is available in the Vector GLM and Vector GAM classes of models. We present several new R functions (implemented within the VGAM package) specifically developed to allow the incorporation of individual covariates in the analysis of closed population CR data using a GLM/GAM-like approach and the conditional likelihood. As a result, a wide variety of practical tools are now readily available in the VGAM object oriented framework. We discuss and demonstrate their advantages, features and flexibility using the new VGAM CR functions on several examples.",WOS:000365974200001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED ADDITIVE-MODELS', 'LINEAR-MODELS', 'NONPARAMETRIC-ESTIMATION', 'BEHAVIORAL-RESPONSE', 'AUXILIARY VARIABLES', 'QUANTITATIVE TRAIT', 'NATURAL-SELECTION', 'POPULATIONS', 'REGRESSION']",The VGAM Package for Capture-Recapture Data Using the Conditional Likelihood,2015
1947,"Accelerated failure time (AFT) models are alternatives to relative risk models which are used extensively to examine the covariate effects on event times in censored data regression. Nevertheless, AFT models have been much less utilized in practice due to lack of reliable computing methods and software. This paper describes an R package aft g gee that implements recently developed inference procedures for AFT models with both the rank-based approach and the least squares approach. For the rank-based approach, the package allows various weight choices and uses an induced smoothing procedure that leads to much more efficient computation than the linear programming method. With the rank-based estimator as an initial value, the generalized estimating equation approach is used as an extension of the least squares approach to the multivariate case. Additional sampling weights are incorporated to handle missing data needed as in case-cohort studies or general sampling schemes. A simulated dataset and two real life examples from biomedical research are employed to illustrate the usage of the package.",WOS:000349841900001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED ESTIMATING EQUATIONS', 'NATIONAL WILMS-TUMOR', 'LINEAR RANK-TESTS', 'CENSORED-DATA', 'REGRESSION-ANALYSIS', 'LARGE-SAMPLE']",Fitting Accelerated Failure Time Models in Routine Survival Analysis with R Package aftgee,2014
1948,"Let (X(n), i) 1 <= i <= n,m is an element of N be a triangular array of row-wise stationary R(d)-valued random variables. We use a ""blocks method"" to define clusters of extreme values: the rows of (X(n), i) are divided into m(n) blocks (Y(n), j), and if a block contains at least one extreme value, the block is considered to contain a cluster. The cluster starts at the first extreme value in the block and ends at the last one. The main results are uniform central limit theorems for empirical processes Z(n)(f) := 1/root nv(n) Sigma(mn)(j=1) (f(Y(n, j)) - Ef(Y(n, j))), for v(n) = P{X(n, i) not equal 0} and f belonging to classes of cluster functionals, that is, functions of the blocks Y(n, j) which only depend on the cluster values and which are equal to 0 if Y(n, j) does not contain a cluster. Conditions for finite-dimensional convergence include beta-mixing, suitable Lindeberg conditions and convergence of covariances. To obtain full uniform convergence, we use either ""bracketing entropy"" or bounds on covering numbers with respect to a random semi-metric. The latter makes it possible to bring the powerful Vapnik-Cervonenkis theory to bear. Applications include multivariate tail empirical processes and empirical processes of cluster values and of order statistics in clusters. Although our main field of applications is the analysis of extreme values, the theory can be applied more generally to rare events occurring, for example, in nonparametric curve estimation.",WOS:000280359400008,ANNALS OF STATISTICS,"['PANEL COUNT DATA', 'TAIL', 'CONVERGENCE', 'SEQUENCES', 'EXTREMES', 'UNIFORM']",LIMIT THEOREMS FOR EMPIRICAL PROCESSES OF CLUSTER FUNCTIONALS,2010
1949,"Network analysis is becoming one of the most active research areas in statistics Significant advances have been made recently on developing theories, methodologies and algorithms for analyzing networks. However, there has been little fundamental study on optimal estimation. In this paper, we establish optimal rate of convergence for graphon estimation. For the stochastic block model with k clusters, we show that the optimal rate under the mean squared error is n(-1) log k + k(2)/n(2). The minimax upper bound improves the existing results in literature through a technique of solving a quadratic equation. When k <= root n log n, as the number of the cluster k grows, the minimax rate grows slowly with only a logarithmic order n(-1) log k. A key step to establish the lower bound is to construct a novel subset of the parameter space and then apply Fano's lemma, from which we see a clear distinction of the non-parametric graphon estimation problem from classical nonparametric regression, due to the lack of identifiability of the order of nodes in exchangeable random graph models. As an immediate application, we consider nonparametric graphon estimation in a Holder class with smoothness alpha. When the smoothness alpha >= 1, the optimal rate of convergence is n(-1) log n, independent of alpha >= 1, while for alpha is an element of (0, 1), the rate is n(-2 alpha/(alpha+1)), which is, to our surprise, identical to the classical nonparametric rate.",WOS:000363437900011,ANNALS OF STATISTICS,"['STOCHASTIC BLOCK-MODELS', 'COMMUNITY DETECTION', 'EXCHANGEABLE ARRAYS', 'COMPLEX NETWORKS', 'BLOCKMODELS', 'MATRIX', 'APPROXIMATION', 'CONSISTENCY', 'PREDICTION', 'LIKELIHOOD']",RATE-OPTIMAL GRAPHON ESTIMATION,2015
1950,"The problem of structure estimation in graphical models with latent variables is considered. We characterize conditions for tractable graph estimation and develop efficient methods with provable guarantees. We consider models where the underlying Markov graph is locally tree-like, and the model is in the regime of correlation decay. For the special case of the Ising model, the number of samples n required for structural consistency of our method scales as n = Omega(theta(-delta eta(eta+1)-2)(min)log p), where p is the number of variables, theta(min) is the minimum edge potential, delta is the depth (i.e., distance from a hidden node to the nearest observed nodes), and eta is a parameter which depends on the bounds on node and edge potentials in the Ising model. Necessary conditions for structural consistency under any algorithm are derived and our method nearly matches the lower bound on sample requirements. Further, the proposed method is practical to implement and provides flexibility to control the number of latent variables and the cycle lengths in the output graph.",WOS:000320488200001,ANNALS OF STATISTICS,"['LOCAL SEPARATION CRITERION', 'MARKOV RANDOM-FIELDS', 'ISING-MODELS', 'SELECTION', 'TREE', 'NETWORKS', 'GIRTH']",LEARNING LOOPY GRAPHICAL MODELS WITH LATENT VARIABLES: EFFICIENT METHODS AND GUARANTEES,2013
1951,"Association models for a pair of random elements X and Y (e.g., vectors) are considered which specify the odds ratio function up to an unknown parameter theta. These models are shown to be semiparametric in the sense that they do not restrict the marginal distributions of X and Y. Inference for the odds ratio parameter theta may be obtained from sampling either Y conditionally on X or vice versa. Generalizing results from Prentice and Pyke, Weinberg and Wacholder and Scott and Wild, we show that asymptotic inference for theta under sampling conditional oil Y is the same as if sampling had been conditional on X. Common regression models, for example, generalized linear models with canonical link or multivariate linear, respectively, logistic models, are association models where the regression parameter beta is closely related to the odds ratio parameter theta. Hence inference for beta may be drawn from samples conditional oil Y using an association model.",WOS:000263129000017,ANNALS OF STATISTICS,"['CONTINGENCY-TABLES', 'REGRESSION']",ASYMPTOTIC INFERENCE FOR SEMIPARAMETRIC ASSOCIATION MODELS,2009
1952,"We review the theory and application of generalized linear autoregressive moving average observation-driven models for time series of counts with explanatory variables and describe the estimation of these models using the R package glarma. Forecasting, diagnostic and graphical methods are also illustrated by several examples.",WOS:000365983000001,JOURNAL OF STATISTICAL SOFTWARE,"['MOVING AVERAGE MODELS', 'GENERALIZED ADDITIVE-MODELS', 'POISSON COUNTS', 'US RECESSIONS', 'SEQUENCES', 'FORECASTS', 'MIXTURES']",The glarma Package for Observation-Driven Time Series Regression of Counts,2015
1953,"We present the R package mixsmsn, which implements routines for maximum likelihood estimation (via an expectation maximization EM-type algorithm) in finite mixture models with components belonging to the class of scale mixtures of the skew-normal distribution, which we call the FMSMSN models. Both univariate and multivariate responses are considered. It is possible to fix the number of components of the mixture to be fitted, but there exists an option that transfers this responsibility to an automated procedure, through the analysis of several models choice criteria. Plotting routines to generate histograms, plug-in densities and contour plots using the fitted models output are also available. The precision of the EM estimates can be evaluated through their estimated standard deviations, which can be obtained by the provision of an approximation of the associated information matrix for each particular model in the FMSMSN family. A function to generate artificial samples from several elements of the family is also supplied. Finally, two real data sets are analyzed in order to show the usefulness of the package.",WOS:000324372300001,JOURNAL OF STATISTICAL SOFTWARE,"['R PACKAGE', 'MODELS', 'REGRESSION', 'INFERENCE', 'FAMILIES']",mixsmsn: Fitting Finite Mixture of Scale Mixture of Skew-Normal Distributions,2013
1954,"Aligment of mass spectrometry (MS) chromatograms is sometimes required prior to sample comparison and data analysis. Without alignment, direct comparison of chromatograms would lead to inaccurate results. We demonstrate a new method for computing a high quality alignment of full length MS chromatograms using variable penalty dynamic time warping. This method aligns signals using local linear shifts without excessive warping that can alter the shape (and area) of chromatogram peaks. The software is available as the R package VPdtw on the Comprehensive R Archive Network and we highlight how one can use this package here.",WOS:000303804900001,JOURNAL OF STATISTICAL SOFTWARE,"['AUTOMATED ALIGNMENT', 'PEAK ALIGNMENT', 'LC-MS', 'ALGORITHM', 'PROFILES']",Variable Penalty Dynamic Time Warping Code for Aligning Mass Spectrometry Chromatograms in R,2012
1955,"To assess the goodness-of-fit of a sample to a continuous random distribution, the most popular approach has been based on measuring, using either L-infinity- or L-2-norms, the distance between the null hypothesis cumulative distribution function and the empirical cumulative distribution function. Indeed, as far as I know, almost all the tests currently available in R related to this issue (ks. test in package stats, ad.test in package AD-GofTest, and ad.test, ad2.test, ks.test, v.test and w2.test in package truncgof) use one of these two distances on cumulative distribution functions. This paper (i) proposes dgeometric.test, a new implementation of the test that measures the discrepancy between a sample kernel estimate of the density function and the null hypothesis density function on the L-1-norm, (ii) introduces the GoFKernel package, and (iii) performs a large simulation exercise to assess the calibration and sensitivity of the above listed tests as well as the Fan's test (Fan 1994), fan. test, also implemented in the GoFKernel package. In addition to dgeometric. test and f an. test, the GoFKernel package adds a couple of functions that R users might also find of interest: density. reflected extends density, allowing the computation of consistent kernel density estimates for bounded random variables, and random. function offers an ad-hoc and universal (although computational expensive and potentially inaccurate for long tail distributions) sampling method. In light of the simulation results, we can conclude that (i) the tests implemented in the truncgof package should not be used to assess goodness-of-fit (at least for non-truncated distributions), (ii) the test fan.test shows an over-tendency to not reject the null hypothesis, being visibly miscalibrated (at least in its default option, where the bandwidth parameter is estimated using dpik from package KernSmooth), (iii) the tests ks.test and ad. test show similar power, with ad. test being slightly preferable in large samples, and (iv) dgeometric.test represents a good alternative given its satisfactory calibration and its, in general, superior power in samples of medium and large sizes. As a counterpart it entails more computational burden when the random generator of the null hypothesis density function is not available in R and random. function must be used.",WOS:000365979800001,JOURNAL OF STATISTICAL SOFTWARE,"['LIKELIHOOD RATIO', 'STATISTICS']",Testing Goodness-of-Fit with the Kernel Density Estimator: GoFKernel,2015
1956,"The removal of blur from a signal, in the presence of noise, is readily accomplished if the blur can be described in precise mathematical terms. However, there is growing interest in problems where the extent of blur is known only approximately, for example in terms of a blur function which depends on unknown parameters that must be computed from data. More challenging still is the case where no parametric assumptions are made about the blur function. There has been a limited amount of work in this setting, but it invariably relies on iterative methods, sometimes under assumptions that are mathematically convenient but physically unrealistic (e.g., that the operator defined by the blur function has an integrable inverse). In this paper we suggest a direct, noniterative approach to nonparametric, blind restoration of a signal. Our method is based on a new, ridge-based method for deconvolution, and requires only mild restrictions on the blur function. We show that the convergence rate of the method is close to optimal, from some viewpoints, and demonstrate its practical performance by applying it to real images.",WOS:000249568000007,ANNALS OF STATISTICS,"['BLIND DECONVOLUTION', 'IMAGE-RESTORATION', 'CONVERGENCE', 'RECOVERY', 'RATES']",Nonparametric estimation of a point-spread function in multivariate problems,2007
1957,"In this paper we develop a nonparametric regression method that is simultaneously adaptive over a wide range of function classes for the regression function and robust over a large collection of error distributions, including those that are heavy-tailed, and may not even possess variances or means. Our approach is to first use local medians to turn the problem of nonparametric regression with unknown noise distribution into a standard Gaussian regression problem and then apply a wavelet block thresholding procedure to construct an estimator of the regression function. It is shown that the estimator simultaneously attains the optimal rate of convergence over a wide range of the Besov classes, without prior knowledge of the smoothness of the underlying functions or prior knowledge of the error distribution. The estimator also automatically adapts to the local smoothness of the underlying function, and attains the local adaptive minimax rate for estimating functions at a point.
A key technical result in our development is a quantile coupling theorem which gives a tight bound for the quantile coupling between the sample medians and a normal variable. This median Coupling inequality may be of independent interest.",WOS:000260554100002,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'ASYMPTOTIC EQUIVALENCE', 'MINIMAX ESTIMATION', 'SHRINKAGE', 'APPROXIMATIONS', 'INTERPOLATION', 'INEQUALITY']",ROBUST NONPARAMETRIC ESTIMATION VIA WAVELET MEDIAN REGRESSION,2008
1958,"We develop asymptotic theory for weighted likelihood estimators (WLE) under two-phase stratified sampling without replacement. We also consider several variants of WLEs involving estimated weights and calibration. A set of empirical process tools are developed including a Glivenko-Cantelli theorem, a theorem for rates of convergence of M-estimators, and a Donsker theorem for the inverse probability weighted empirical processes under two-phase sampling and sampling without replacement at the second phase. Using these general results, we derive asymptotic distributions of the WLE of a finite-dimensional parameter in a general semiparametric model where an estimator of a nuisance parameter is estimable either at regular or nonregular rates. We illustrate these results and methods in the Cox model with right censoring and interval censoring. We compare the methods via their asymptotic variances under both sampling without replacement and the more usual (and easier to analyze) assumption of Bernoulli sampling at the second phase.",WOS:000317451200011,ANNALS OF STATISTICS,"['PROPORTIONAL HAZARDS MODELS', 'SEMIPARAMETRIC MODELS', 'CASE-COHORT', 'CALIBRATION ESTIMATORS', 'EFFICIENT ESTIMATION', 'STRATIFIED SAMPLES', 'COX REGRESSION', 'THEOREMS', 'DISEASE', 'DESIGN']",WEIGHTED LIKELIHOOD ESTIMATION UNDER TWO-PHASE SAMPLING,2013
1959,"The System-Wide Monitoring Program (SWMP) was implemented in 1995 by the US National Estuarine Research Reserve System. This program has provided two decades of continuous monitoring data at over 140 fixed stations in 28 estuaries. However, the increasing quantity of data provided by the monitoring network has complicated broad-scale comparisons between systems and, in some cases, prevented simple trend analysis of water quality parameters at individual sites. This article describes the SWMPr package that provides several functions that facilitate data retrieval, organization, and analysis of time series data in the reserve estuaries. Previously unavailable functions for estuaries are also provided to estimate rates of ecosystem metabolism using the open-water method. The SWMPr package has facilitated a cross-reserve comparison of water quality trends and links quantitative information with analysis tools that have use for more generic applications to environmental time series.",WOS:000385276100016,R JOURNAL,,"SWMPr: An R Package for Retrieving, Organizing, and Analyzing Environmental Data for Estuaries",2016
1960,"In this paper, we study nonparametric estimation of the Levy density for Levy processes, with and without Brownian component. For this, we consider n discrete time observations with step Delta. The asymptotic framework is: n tends to infinity, Delta = Delta(n), tends to zero while n Delta(n) tends to infinity. We use a Fourier approach to construct an adaptive nonparametric estimator of the Levy density and to provide a bound for the global L(2)-risk. Estimators of the drift and of the variance of the Gaussian component are also studied. We discuss rates of convergence and give examples and simulation results for processes fitting in our framework.",WOS:000291183300005,ANNALS OF STATISTICS,"['NONPARAMETRIC-ESTIMATION', 'DISTRIBUTIONS']",ESTIMATION FOR LEVY PROCESSES FROM HIGH FREQUENCY DATA WITHIN A LONG TIME INTERVAL,2011
1961,"The clustering of variables is a strategy for deciphering the underlying structure of a data set. Adopting an exploratory data analysis point of view, the Clustering of Variables around Latent Variables (CLV) approach has been proposed by Vigneau and Qannari (2003). Based on a family of optimization criteria, the CLV approach is adaptable to many situations. In particular, constraints may be introduced in order to take account of additional information about the observations and/or the variables. In this paper, the CLV method is depicted and the R package ClustVarLV including a set of functions developed so far within this framework is introduced. Considering successively different types of situations, the underlying CLV criteria are detailed and the various functions of the package are illustrated using real case studies.",WOS:000368551800011,R JOURNAL,"['REGRESSION', 'SEGMENTATION', 'DIRECTIONS', 'COMPONENTS', 'PREFERENCE', 'CONSUMERS', 'PATTERNS']",ClustVarLV: An R Package for the Clustering of Variables Around Latent Variables,2015
1962,"The article introduces the cshapes R package, which includes our CShapes dataset of contemporary and historical country boundaries, as well as computational tools for computing geographical measures from these maps. We provide an overview of the need for considering spatial dependence in comparative research, how this requires appropriate historical maps, and detail how the cshapes associated R package cshapes can contribute to these ends. We illustrate the use of the package for drawing maps, computing spatial variables for countries, and generating weights matrices for spatial statistics.",WOS:000208589900004,R JOURNAL,,Mapping and Measuring Country Shapes The cshapes Package,2010
1963,"A complete assessment of population growth and viability from field census data often requires complex data manipulations, statistical routines, mathematical tools, programming environments, and graphical capabilities. We therefore designed an R package called popbio to facilitate both the construction and analysis of projection matrix models. The package consists primarily of the R translation of MATLAB code found in Caswell (2001) and Morris and Doak (2002) for the analysis of projection matrix models. The package also includes methods to estimate vital rates and construct projection matrix models from census data typically collected in plant demography studies. In these studies, vital rates can often be estimated directly from annual censuses of tagged individuals using transition frequency tables. Because the construction of projection matrix models requires careful management of census data, we describe the steps to construct a projection matrix in detail.",WOS:000252430500001,JOURNAL OF STATISTICAL SOFTWARE,"['STAGE', 'AGE']",Estimating and analyzing demographic models using the popbio package in R,2007
1964,"We introduce growcurves for R that performs analysis of repeated measures multiple membership (MM) data. This data structure arises in studies under which an intervention is delivered to each subject through the subject's participation in a set of multiple elements that characterize the intervention. In our motivating study design under which subjects receive a group cognitive behavioral therapy (CBT) treatment, an element is a group CBT session and each subject attends multiple sessions that, together, comprise the treatment. The sets of elements, or group CBT sessions, attended by subjects will partly overlap with some of those from other subjects to induce a dependence in their responses. The growcurves package offers two alternative sets of hierarchical models: 1. Separate terms are specified for multivariate subject and MM element random effects, where the subject effects are modeled under a Dirichlet process prior to produce a semi-parametric construction; 2. A single term is employed to model joint subject-by-MM effects. A fully non-parametric dependent Dirichlet process formulation allows exploration of differences in subject responses across different MM elements. This model allows for borrowing information among subjects who express similar longitudinal trajectories for flexible estimation. growcurves deploys ""estimation"" functions to perform posterior sampling under a suite of prior options. An accompanying set of ""plot"" functions allows the user to readily extract by-subject growth curves. The design approach intends to anticipate inferential goals with tools that fully extract information from repeated measures data. Computational efficiency is achieved by performing the sampling for estimation functions using compiled C++ code.",WOS:000334020200001,JOURNAL OF STATISTICAL SOFTWARE,"['SURVIVAL-DATA', 'GROUP-THERAPY', 'INFERENCE']",Bayesian Semi- and Non-Parametric Models for Longitudinal Data with Multiple Membership Effects in R,2014
1965,"We consider optimal sequential allocation in the context of the so-called stochastic multi-armed bandit model. We describe a generic index policy, in the sense of Gittins [J. R. Stat. Soc. Ser. B Stat. Methodol. 41 (1979) 148-177], based on upper confidence bounds of the arm payoffs computed using the Kullback-Leibler divergence. We consider two classes of distributions for which instances of this general idea are analyzed: the kl-UCB algorithm is designed for one-parameter exponential families and the empirical KL-UCB algorithm for bounded and finitely supported distributions. Our main contribution is a unified finite-time analysis of the regret of these algorithms that asymptotically matches the lower bounds of Lai and Robbins [Adv. in Appl. Math. 6 (1985) 4-22] and Burnetas and Katehakis [Adv. in Appl. Math. 17 (1996) 122-142], respectively. We also investigate the behavior of these algorithms when used with general bounded rewards, showing in particular that they provide significant improvements over the state-of-the-art.",WOS:000323271500006,ANNALS OF STATISTICS,"['OPTIMAL ADAPTIVE POLICIES', 'MULTIARMED BANDIT PROBLEM', 'REGRET', 'INDEX']",KULLBACK-LEIBLER UPPER CONFIDENCE BOUNDS FOR OPTIMAL SEQUENTIAL ALLOCATION,2013
1966,"Consider two p-variate populations, not necessarily Gaussian, with covariance matrices Sigma 1 and Sigma 2, respectively. Let S-1 and S-2 be the corresponding sample covariance matrices with degrees of freedom m and n. When the difference Delta between Sigma l and Sigma 2 is of small rank compared to p, m and n, the Fisher matrix S := S2-1S1 is called a spiked Fisher matrix. When p, m and n grow to infinity proportionally, we establish a phase transition for the extreme eigenvalues of the Fisher matrix: a displacement formula showing that when the eigenvalues of Delta (spikes) are above (or under) a critical value, the associated extreme eigenvalues of S will converge to some point outside the support of the global limit (LSD) of other eigenvalues (become outliers); otherwise, they will converge to the edge points of the LSD. Furthermore, we derive central limit theorems for those outlier eigenvalues of S. The limiting distributions are found to be Gaussian if and only if the corresponding population spike eigenvalues in Delta are simple. Two applications are introduced. The first application uses the largest eigenvalue of the Fisher matrix to test the equality between two high-dimensional covariance matrices, and explicit power function is found under the spiked alternative. The second application is in the field of signal detection, where an estimator for the number of signals is proposed while the covariance structure of the noise is arbitrary.",WOS:000396804900013,ANNALS OF STATISTICS,"['SAMPLE COVARIANCE MATRICES', 'LARGE WIGNER MATRICES', 'POPULATION-MODEL', 'NUMBER', 'DEFORMATIONS', 'CONVERGENCE', 'COMPONENTS']",EXTREME EIGENVALUES OF LARGE-DIMENSIONAL SPIKED FISHER MATRICES WITH APPLICATION,2017
1967,The stochastic block model is a popular tool for studying community structures in network data. We develop a goodness-of-fit test for the stochastic block model. The test statistic is based on the largest singular value of a residual matrix obtained by subtracting the estimated block mean effect from the adjacency matrix. Asymptotic null distribution is obtained using recent advances in random matrix theory. The test is proved to have full power against alternative models with finer structures. These results naturally lead to a consistent sequential testing estimate of the number of communities.,WOS:000368022000014,ANNALS OF STATISTICS,"['WIGNER MATRICES', 'NETWORKS', 'BLOCKMODELS']",A GOODNESS-OF-FIT TEST FOR STOCHASTIC BLOCK MODELS,2016
1968,"HPLC-DAD systems generate time intensity ( absorbance) matrices called spectrochromatograms. Under good experimental conditions, spectro-chromatograms of elution peaks of pure analytes are bilinear products of a time peak and an absorbance spectrum. Co-eluting impurities create deviations from this pure bilinear structure. Unfortunately, other imperfections, such as scan averaging, large optical windows, imperfect lamp alignment, mobile phase fluctuations, etc. also create departures from the pure bilinear structure. This makes it hard to distinguish low concentration impurities from artifacts and hampers safe detection of contaminants. There are two main ways to deal with such artifacts: removal and simulation, and ImpuR provides R functions to do both and to integrate both approaches. More specifically, ImpuR provides a set of tools to explore time-intensity matrices with respect to their bilinear structure and departures from it. It includes exploratory graphs for bilinear matrices ( bilinear residual graphs and singular value decompositions), spectral dissimilarity curves via window-evolving factor analysis with heteroscedasticity correction and the sine method, methods for removal of artifacts, and a comprehensive simulation tool to assess the impact of potential artifacts and to allow for the construction of guide curves for use with the sine method.",WOS:000244068200001,JOURNAL OF STATISTICAL SOFTWARE,,ImpuR: A collection of diagnostic tools developed in R in the context of peak impurity detection in HPLC-DAD but potentially useful with other types of time-intensity matrices,2007
1969,"We derive an asymptotic expansion for the excess risk (regret) of a weighted nearest-neighbour classifier. This allows us to find the asymptotically optimal vector of nonnegative weights, which has a rather simple form. We show that the ratio of the regret of this classifier to that of an unweighted k-nearest neighbour classifier depends asymptotically only on the dimension d of the feature vectors, and not on the underlying populations. The improvement is greatest when d = 4, but thereafter decreases as d -> infinity. The popular bagged nearest neighbour classifier can also be regarded as a weighted nearest neighbour classifier, and we show that its corresponding weights are somewhat suboptimal when d is small (in particular, worse than those of the unweighted k-nearest neighbour classifier when d = 1), but are close to optimal when d is large. Finally, we argue that improvements in the rate of convergence are possible under stronger smoothness assumptions, provided we allow negative weights. Our findings are supported by an empirical performance comparison on both simulated and real data sets.",WOS:000321844300013,ANNALS OF STATISTICS,"['CLASSIFICATION', 'REGRESSION', 'DISCRIMINATION', 'SELECTION', 'CHOICE', 'RULES', 'RATES']",OPTIMAL WEIGHTED NEAREST NEIGHBOUR CLASSIFIERS,2012
1970,"We consider a multi-armed bandit problem in a setting where each arm produces a noisy reward realization which depends on an observable random covariate. As opposed to the traditional static multi-armed bandit problem, this setting allows for dynamically changing rewards that better describe applications where side information is available We adopt a nonparametric model where the expected rewards are smooth functions of the covariate and where the hardness of the problem is captured by a margin parameter. To maximize the expected cumulative reward, we introduce a policy called Adaptively Binned Successive Elimination (AB SE) that adaptively decomposes the global problem into suitably ""localized"" static bandit problems. This policy constructs an adaptive partition using a variant of the Successive Elimination (SE) policy. Our results include sharper regret bounds for the SE policy in a static bandit problem and minimax optimal regret bounds for the ABSE policy in the dynamic problem.",WOS:000320488200011,ANNALS OF STATISTICS,"['RANDOMIZED ALLOCATION', 'REGRET BOUNDS', 'CLASSIFIERS']",THE MULTI-ARMED BANDIT PROBLEM WITH COVARIATES,2013
1971,The gWidgetsWWW package provides a framework for easily developing interactive web pages from within R. It uses the API developed in the gWidgets programming interface to specify the layout of the controls and the relationships between them. The web pages may be served locally under R's built-in web server for help pages or from an rApache-enabled web server.,WOS:000305992000001,JOURNAL OF STATISTICAL SOFTWARE,,gWidgetsWWW: Creating Interactive Web Pages within R,2012
1972,"We consider a two-stage procedure (TSP) for estimating an inverse regression function at a given point, where isotonic regression is used at stage one to obtain an initial estimate and a local linear approximation in the vicinity of this estimate is used at stage two. We establish that the convergence rate of the second-stage estimate can attain the parametric n(1/2) rate. Furthermore, a bootstrapped variant of TSP (BTSP) is introduced and its consistency properties studied. This variant manages to overcome the slow speed of the convergence in distribution and the estimation of the derivative of the regression function at the unknown target quantity. Finally, the finite sample performance of BTSP is studied through simulations and the method is illustrated on a data set.",WOS:000291183300010,ANNALS OF STATISTICS,"['DOSE-RESPONSE', 'ISOTONIC REGRESSION', 'RANDOM-VARIABLES', 'DOWN DESIGNS', 'CALIBRATION', 'INFERENCE', 'TRIALS', 'MODELS', 'ERROR', 'LAWS']",A TWO-STAGE HYBRID PROCEDURE FOR ESTIMATING AN INVERSE REGRESSION FUNCTION,2011
1973,"We derive the degrees of freedom of the lasso fit, placing no assumptions on the predictor matrix X. Like the well-known result of Zou, Hastie and Tibshirani [Ann. Statist. 35 (2007) 2173-2192], which gives the degrees of freedom of the lasso fit when X has full column rank, we express our result in terms of the active set of a lasso solution. We extend this result to cover the degrees of freedom of the generalized lasso fit for an arbitrary predictor matrix X (and an arbitrary penalty matrix D). Though our focus is degrees of freedom, we establish some intermediate results on the lasso and generalized lasso that may be interesting on their own.",WOS:000307608000021,ANNALS OF STATISTICS,"['LEAST ANGLE REGRESSION', 'VARIABLE SELECTION', 'PATH']",DEGREES OF FREEDOM IN LASSO PROBLEMS,2012
1974,"The celebrated de la Garza phenomenon states that for a polynomial regression model of degree p - 1 any optimal design can be based on at most p design points. In a remarkable paper, Yang [Ann. Statist. 38 (2010) 2499 - 2524] showed that this phenomenon exists in many locally optimal design problems for nonlinear models. In the present note, we present a different view point on these findings using results about moment theory and Chebyshev systems. In particular, we show that this phenomenon occurs in an even larger class of models than considered so far.",WOS:000291183300020,ANNALS OF STATISTICS,['RATIONAL MODELS'],A NOTE ON THE DE LA GARZA PHENOMENON FOR LOCALLY OPTIMAL DESIGNS,2011
1975,"We investigate the performance of robust estimates of multivariate location under nonstandard data contamination models such as componentwise outliers (i.e., contamination in each variable is independent from the other variables). This model brings up a possible new source of statistical error that we call ""propagation of outliers."" This source of error is Unusual in the sense that it is generated by the data processing itself and takes place after the data has been collected. We define and derive the influence function of robust multivariate location estimates under flexible contamination models and use it to investigate the effect of propagation of outliers. Furthermore, we show that standard high-breakdown affine equivariant estimators propagate outliers and therefore show poor breakdown behavior under componentwise contamination when the dimension d is high.",WOS:000263129000011,ANNALS OF STATISTICS,"['M-ESTIMATORS', 'LOCATION', 'SCATTER', 'COVARIANCE', 'REGRESSION', 'MATRICES', 'BIAS']",PROPAGATION OF OUTLIERS IN MULTIVARIATE DATA,2009
1976,"In this paper, we propose a class of Bayes estimators for the covariance matrix of graphical Gaussian models Markov with respect to a decomposable graph G. Working with the W(PG) family defined by Letac and Massam [Ann. Statist. 35 (2007) 1278-1323] we derive closed-form expressions for Bayes estimators under the entropy and squared-error losses. The W(PG) family includes the classical inverse of the hyper inverse Wishart but has many more shape parameters, thus allowing for flexibility in differentially shrinking various parts of the covariance matrix. Moreover, using this family avoids recourse to MCMC, often infeasible in high-dimensional problems. We illustrate the performance of our estimators through a collection of numerical examples where we explore frequentist risk properties and the efficacy of graphs in the estimation of high-dimensional covariance structures.",WOS:000262731400009,ANNALS OF STATISTICS,"['BAYESIAN-INFERENCE', 'WISHART DISTRIBUTIONS', 'PRECISION MATRIX', 'REFERENCE PRIORS', 'SELECTION', 'CONJUGATE']",FLEXIBLE COVARIANCE ESTIMATION IN GRAPHICAL GAUSSIAN MODELS,2008
1977,"We propose a consistent estimator of sharp bounds on the variance of the difference-in-means estimator in completely randomized experiments. Generalizing Robins [Stat. Med. 7 (1988) 773-785], our results resolve a well-known identification problem in causal inference posed by Neyman [Statist. Sci. 5 (1990) 465-472. Reprint of the original 1923 paper]. A practical implication of our results is that the upper bound estimator facilitates the asymptotically narrowest conservative Wald-type confidence intervals, with applications in randomized controlled and clinical trials.",WOS:000338477800002,ANNALS OF STATISTICS,"['REGRESSION ADJUSTMENTS', 'INFERENCE', 'DESIGN']",SHARP BOUNDS ON THE VARIANCE IN RANDOMIZED EXPERIMENTS,2014
1978,"This paper considers the problem of defining distributions over graphical structures. We propose an extension of the hyper Markov properties of Dawid and Lauritzen [Ann. Statist. 21 (1993) 1272-1317], which we term structural Markov properties, for both undirected decomposable and directed acyclic graphs, which requires that the structure of distinct components of the graph be conditionally independent given the existence of a separating component. This allows the analysis and comparison of multiple graphical structures, while being able to take advantage of the common conditional independence constraints. Moreover, we show that these properties characterise exponential families, which form conjugate priors under sampling from compatible Markov distributions.",WOS:000357441000014,ANNALS OF STATISTICS,"['DECOMPOSABLE GRAPHS', 'CONDITIONAL-INDEPENDENCE', 'EQUIVALENCE CLASSES', 'ACYCLIC DIGRAPHS', 'CHORDAL GRAPHS', 'CHAIN', 'DISTRIBUTIONS', 'NETWORKS']",STRUCTURAL MARKOV GRAPH LAWS FOR BAYESIAN MODEL UNCERTAINTY,2015
1979,"This work builds a unified framework for the study of quadratic form distance measures as they are used in assessing the goodness of fit of models. Many important procedures have this structure, but the theory for these methods is dispersed and incomplete. Central to the statistical analysis of these distances is the spectral decomposition of the kernel that generates the distance. We show how this determines the limiting distribution of natural goodness-of-fit tests. Additionally, we develop a new notion, the spectral degrees of freedom of the test, based on this decomposition. The degrees of freedom are easy to compute and estimate, and can be used as a guide in the construction of useful procedures in this class.",WOS:000254502700018,ANNALS OF STATISTICS,"['OF-FIT TESTS', 'STATISTICS']",Quadratic distances on probabilities: A unified foundation,2008
1980,"We generalize Pearl's back-door criterion for directed acyclic graphs (DAGs) to more general types of graphs that describe Markov equivalence classes of DAGs and/or allow for arbitrarily many hidden variables. We also give easily checkable necessary and sufficient graphical criteria for the existence of a set of variables that satisfies our generalized back-door criterion, when considering a single intervention and a single outcome variable. Moreover, if such a set exists, we provide an explicit set that fulfills the criterion. We illustrate the results in several examples. R-code is available in the R-package pcalg.",WOS:000355768700005,ANNALS OF STATISTICS,"['DIRECTED ACYCLIC GRAPHS', 'EQUIVALENCE CLASSES', 'MARKOV EQUIVALENCE', 'ANCESTRAL GRAPHS', 'CAUSAL INFERENCE', 'SELECTION', 'MODELS', 'LATENT']",A GENERALIZED BACK-DOOR CRITERION,2015
1981,"Robust estimators of location and dispersion are Often used in the elliptical model to obtain an uncontaminated and highly representative subsample by trimming the data Outside an ellipsoid based in the associated Mahalanobis distance. Here we analyze some one (or k)-step Maximum Likelihood Estimators computed on a subsample obtained with Such a procedure. We introduce different models which arise naturally from the ways in which the discarded data can be treated, leading to truncated or censored likelihoods. as well as to a likelihood based on an only outliers gross errors model. Results on existence, uniqueness, robustness and asymptotic properties of the proposed estimators are included. A remarkable fact is that the proposed estimators generally keep the breakdown point of the initial (robust) estimators, but they Could improve the rate of convergence of the initial estimator because our estimators always converge at rate n(1/2). independently of the rate of convergence of the initial estimator.",WOS:000260554100011,ANNALS OF STATISTICS,"['MULTIVARIATE LOCATION', 'ASYMPTOTIC-BEHAVIOR', 'MAXIMUM-LIKELIHOOD', 'INITIAL ESTIMATOR', 'REGRESSION', 'SCATTER', 'EFFICIENCY', 'MATRICES', 'POINTS']",TRIMMING AND LIKELIHOOD: ROBUST LOCATION AND DISPERSION ESTIMATION IN THE ELLIPTICAL MODEL,2008
1982,"The use of state space models and their inference is illustrated using the package SsfPack for Ox. After a rather long introduction that explains the use of SsfPack and many of its functions, four case-studies illustrate the practical implementation of the software to real world problems through short sample programs.
The first case consists in the analysis of the well-known (at least to time series analysis experts) Nile data with a local level model. The other case-studies deal with ARIMA and RegARIMA models applied to the (also well-known) Airline time series, structural time series models applied to the Italian industrial production index and stochastic volatility models applied to the FTSE100 index. In all applications inference on the model (hyper-) parameters is carried out by maximum likelihood, but in one case (stochastic volatility) also an MCMC-based approach is illustrated. Cubic splines are covered in a very short example as well.",WOS:000290526700001,JOURNAL OF STATISTICAL SOFTWARE,"['TIME-SERIES', 'MODELS']",State Space Methods in Ox/SsfPack,2011
1983,"In this paper, a uniform (over some parameter space) moment bound for the inverse of Fisher's information matrix is established. This result is then applied to develop moment bounds for the normalized least squares estimate in (nonlinear) stochastic regression models. The usefulness of these results is illustrated using time series models. In particular, an asymptotic expression for the mean squared prediction error of the least squares predictor in autoregressive moving average models is obtained. This asymptotic expression provides a solid theoretical foundation for some model selection criteria.",WOS:000293716500007,ANNALS OF STATISTICS,"['STOCHASTIC REGRESSION-MODELS', 'AUTOREGRESSIVE PROCESSES', 'PREDICTORS', 'SELECTION', 'IDENTIFICATION', 'CONSISTENCY', 'PRINCIPLES', 'ORDER']",UNIFORM MOMENT BOUNDS OF FISHER'S INFORMATION WITH APPLICATIONS TO TIME SERIES,2011
1984,"Theoretical developments on cross validation (CV) have mainly focused on selecting one among a list of finite-dimensional models (e.g., subset or order selection in linear regression) or selecting a smoothing parameter (e.g., bandwidth for kernel smoothing). However, little is known about consistency of cross validation when applied to compare between parametric and nonparametric methods or within nonparametric methods. We show that under some conditions, with an appropriate choice of data splitting ratio, cross validation is consistent in the sense of selecting the better procedure with probability approaching 1.
Our results reveal interesting behavior of cross validation. When comparing two models (procedures) converging at the same nonparametric rate, in contrast to the parametric case, it turns out that the proportion of data used for evaluation in CV does not need to be dominating in size. Furthermore, it can even be of a smaller order than the proportion for estimation while not affecting the consistency property.",WOS:000253077800011,ANNALS OF STATISTICS,"['LEARNING-TESTING METHODS', 'LINEAR-MODEL SELECTION', 'NONPARAMETRIC REGRESSION', 'OPTIMAL RATES', 'CONVERGENCE']",Consistency of cross validation for comparing regression procedures,2007
1985,"It is well known in the literature that the problem of learning the structure of Bayesian networks is very hard to tackle: Its computational complexity is super-exponential in the number of nodes in the worst case and polynomial in most real-world scenarios.
Efficient implementations of score-based structure learning benefit from past and current research in optimization theory, which can be adapted to the task by using the network score as the objective function to maximize. This is not true for approaches based on conditional independence tests, called constraint-based learning algorithms. The only optimization in widespread use, backtracking, leverages the symmetries implied by the definitions of neighborhood and Markov blanket.
In this paper we illustrate how backtracking is implemented in recent versions of the bnlearn R package, and how it degrades the stability of Bayesian network structure learning for little gain in terms of speed. As an alternative, we describe a software architecture and framework that can be used to parallelize constraint-based structure learning algorithms (also implemented in bnlearn) and we demonstrate its performance using four reference networks and two real-world data sets from genetics and systems biology. We show that on modern multi-core or multiprocessor hardware parallel implementations are preferable over backtracking, which was developed when single-processor machines were the norm.",WOS:000399023300001,JOURNAL OF STATISTICAL SOFTWARE,"['GRAPHICAL MODELS', 'PROBABILISTIC INFERENCE', 'BELIEF NETWORKS', 'CAUSAL']",Bayesian Network Constraint-Based Structure Learning Algorithms: Parallel and Optimized Implementations in the bnlearn R Package,2017
1986,"We discuss R package BB, in particular, its capabilities for solving a nonlinear system of equations. The function BBsolve in BB can be used for this purpose. We demonstrate the utility of these functions for solving: (a) large systems of nonlinear equations, (b) smooth, nonlinear estimating equations in statistical modeling, and (c) non-smooth estimating equations arising in rank-based regression modeling of censored failure time data. The function BBoptim can be used to solve smooth, box-constrained optimization problems. A main strength of BB is that, due to its low memory and storage requirements, it is ideally suited for solving high-dimensional problems with thousands of variables.",WOS:000270821800001,JOURNAL OF STATISTICAL SOFTWARE,"['GRADIENT-METHOD', 'MINIMIZATION', 'MODEL']",BB: An R Package for Solving a Large System of Nonlinear Equations and for Optimizing a High-Dimensional Nonlinear Objective Function,2009
1987,"robustloggamma is an R package for robust estimation and inference in the generalized loggamma model. We briefly introduce the model, the estimation procedures and the computational algorithms. Then, we illustrate the use of the package with the help of a real data set.",WOS:000384911700001,JOURNAL OF STATISTICAL SOFTWARE,"['GAMMA DISTRIBUTION', 'REGRESSION', 'INFERENCE']",Robust Estimation of the Generalized Loggamma Model: The R Package robustloggamma,2016
1988,,WOS:000275510800005,ANNALS OF STATISTICS,,Multivariate quantiles and multiple-output regression quantiles: From L-1 optimization to halfspace depth DISCUSSION,2010
1989,"Longitudinal data from factorial experiments frequently arise in various fields of study, ranging from medicine and biology to public policy and sociology. In most practical situations, the distribution of observed data is unknown and there may exist a number of atypical measurements and outliers. Hence, use of parametric and semiparametric procedures that impose restrictive distributional assumptions on observed longitudinal samples becomes questionable. This, in turn, has led to a substantial demand for statistical procedures that enable us to accurately and reliably analyze longitudinal measurements in factorial experiments with minimal conditions on available data, and robust nonparametric methodology offering such a possibility becomes of particular practical importance. In this article, we introduce a new R package nparLD which provides statisticians and researchers from other disciplines an easy and user-friendly access to the most up-to-date robust rank-based methods for the analysis of longitudinal data in factorial settings. We illustrate the implemented procedures by case studies from dentistry, biology, and medicine.",WOS:000308910400001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED ESTIMATING EQUATIONS', '2 ALTERNATIVE METHODOLOGIES', 'MIXED MODELS', 'DESIGNS', 'HYPOTHESES', 'TESTS']",nparLD: An R Software Package for the Nonparametric Analysis of Longitudinal Data in Factorial Experiments,2012
1990,"Item response theory (IRT) models are a class of statistical models used by researchers to describe the response behaviors of individuals to a set of categorically scored items. The most common IRT models can be classified as generalized linear fixed-and/or mixed-effect models. Although IRT models appear most often in the psychological testing literature, researchers in other fields have successfully utilized IRT-like models in a wide variety of applications. This paper discusses the three major methods of estimation in IRT and develops R functions utilizing the built-in capabilities of the R environment to find the marginal maximum likelihood estimates of the generalized partial credit model. The currently available R packages ltm is also discussed.",WOS:000247012000001,JOURNAL OF STATISTICAL SOFTWARE,"['LATENT TRAIT', 'EM ALGORITHM', 'RASCH MODEL', 'RATIO', 'SUM', 'IRT']",Marginal maximum likelihood estimation of item response models in R,2007
1991,"This paper applies Le Cam's asymptotic theory of statistical experiments to the signal detection problem in high dimension. We consider the problem of testing the null hypothesis of sphericity of a high-dimensional covariance matrix against an alternative of (unspecified) multiple symmetry-breaking directions (multispiked alternatives). Simple analytical expressions for the Gaussian asymptotic power envelope and the asymptotic powers of previously proposed tests are derived. Those asymptotic powers remain valid for non-Gaussian data satisfying mild moment restrictions. They appear to lie very substantially below the Gaussian power envelope, at least for small values of the number of symmetry-breaking directions. In contrast, the asymptotic power of Gaussian likelihood ratio tests based on the eigenvalues of the sample covariance matrix are shown to be very close to the envelope. Although based on Gaussian likelihoods, those tests remain valid under non-Gaussian densities satisfying mild moment conditions. The results of this paper extend to the case of multispiked alternatives and possibly non-Gaussian densities, the findings of an earlier study [Ann. Statist. 41 (2013) 1204-1231] of the single-spiked case. The methods we are using here, however, are entirely new, as the Laplace approximation methods considered in the single-spiked context do not extend to the multispiked case.",WOS:000334256100010,ANNALS OF STATISTICS,"['SAMPLE COVARIANCE MATRICES', 'LARGEST EIGENVALUE', 'TESTS', 'DISTRIBUTIONS', 'WISHART', 'MODELS']",SIGNAL DETECTION IN HIGH DIMENSION: THE MULTISPIKED CASE,2014
1992,"The problem of simultaneous inference and multiple comparison for comparing means of k(>= 3) populations has been long studied in the statistics literature and is widely available in statistics literature. However to-date, the problem of multiple comparison of regression models has not found its way to the software. It is only recently that the computational aspects of this problem have been resolved in a general setting. SimReg employs this new methodology and provides users with software for multiple regression of several regression models. The comparisons can be among any set of pairs, and moreover any number of predictors can be included in the model. More importantly predictors can be constrained to their natural boundaries, if known.
Computational methods for the problem of simultaneous confidence bands when predictors are constrained to intervals has also recently been addressed. SimReg utilizes this recent development to offer simultaneous confidence bands for regression models with any number of predictor variables. Again, the predictors can be constrained to their natural boundaries which results in narrower bands, as compared to the case where no restriction is imposed. A by-product of these confidence bands is a new method for comparing two regression surfaces, that is more informative than the usual partial F test.",WOS:000232806400001,JOURNAL OF STATISTICAL SOFTWARE,['BOUNDS'],SimReg: A software includinge some new developments in multiple comparison and simultaneous confidence bands for linear regression models,2005
1993,"In this article the package High-dimensional Metrics hdm is introduced. It is a collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. It focuses on providing confidence intervals and significance testing for ( possibly many) low-dimensional subcomponents of the high-dimensional parameter vector. Efficient estimators and uniformly valid confidence intervals for regression coefficients on target variables (e.g., treatment or policy variable) in a high-dimensional approximately sparse regression model, for average treatment effect (ATE) and average treatment effect for the treated (ATET), as well for extensions of these parameters to the endogenous setting are provided. Theory grounded, data-driven methods for selecting the penalization parameter in Lasso regressions under heteroscedastic and non-Gaussian errors are implemented. Moreover, joint/simultaneous confidence intervals for regression coefficients of a high-dimensional sparse regression are implemented. Data sets which have been used in the literature and might be useful for classroom demonstration and for testing new estimators are included.",WOS:000395669800012,R JOURNAL,"['SPARSE MODELS', 'SELECTION', 'INFERENCE']",hdm: High-Dimensional Metrics,2016
1994,"When using the bootstrap in the presence of measurement error, we must first estimate the target distribution function; we cannot directly resample since we don not have a sample from the target. These and other considerations motivate the development of estimators of distributions, and of related quantities such as moments and quantiles, in errors-in-variables settings. We show that such estimators have curious and unexpected properties. For example, if the distributions of the variable of interest, W, say, and of the observation error are both centered at zero, then the rate of convergence of an estimator of the distribution function of W can be slower at the origin than away from the origin. This is an intrinsic characteristic of the problem, not a quirk of particular estimators: the property holds true for optimal estimators.",WOS:000260554100004,ANNALS OF STATISTICS,"['KERNEL DENSITY-ESTIMATION', 'NONPARAMETRIC DECONVOLUTION', 'BANDWIDTH SELECTION', 'CONTAMINATED SAMPLE', 'OPTIMAL RATES', 'CONVERGENCE']","ESTIMATION OF DISTRIBUTIONS, MOMENTS AND QUANTILES IN DECONVOLUTION PROBLEMS",2008
1995,"The R package kerdiest has been designed for computing kernel estimators of the distribution function and other related functions. Because of its usefulness in real applications, the bandwidth parameter selection problem has been considered, and a cross-validation method and two of plug-in type have been implemented. Moreover, three relevant functions in nature hazards have also been programmed. The package is completed with two interesting data sets, one of geological type (a complete catalogue of the earthquakes occurring in the northwest of the Iberian Peninsula) and another containing the maximum peak flow levels of a river in the United States of America.",WOS:000307534500001,JOURNAL OF STATISTICAL SOFTWARE,"['SMOOTH DISTRIBUTION-FUNCTIONS', 'SELECTION', 'AREA', 'UNCERTAINTY', 'MAGNITUDE', 'SPAIN', 'NW']",Nonparametric Kernel Distribution Function Estimation with kerdiest: An R Package for Bandwidth Choice and Applications,2012
1996,"Results of ecological models differ, to some extent, more from measured data than from empirical knowledge. Existing techniques for validation based on quantitative assessments sometimes cause an underestimation of the performance of models due to time shifts, accelerations and delays or systematic differences between measurement and simulation. However, for the application of such models it is often more important to reproduce essential patterns instead of seemingly exact numerical values.
This paper presents techniques to identify patterns and numerical methods to measure the consistency of patterns between observations and model results. An orthogonal set of deviance measures for absolute, relative and ordinal scale was compiled to provide informations about the type of difference. Furthermore, two different approaches accounting for time shifts were presented. The first one transforms the time to take time delays and speed differences into account. The second one describes known qualitative criteria dividing time series into interval units in accordance to their main features. The methods differ in their basic concepts and in the form of the resulting criteria. Both approaches and the deviance measures discussed are implemented in an R package. All methods are demonstrated by means of water quality measurements and simulation data.
The proposed quality criteria allow to recognize systematic differences and time shifts between time series and to conclude about the quantitative and qualitative similarity of patterns.",WOS:000252430200001,JOURNAL OF STATISTICAL SOFTWARE,"['VALIDATION', 'PATTERN', 'ECOLOGY', 'INDEX']",Statistical methods for the qualitative assessment of dynamic models with time delay (R package qualV),2007
1997,"The R function glm uses step-halving to deal with certain types of convergence problems when using iteratively reweighted least squares to fit a generalized linear model. This works well in some circumstances but non-convergence remains a possibility, particularly with a nonstandard link function. In some cases this is because step-halving is never invoked, despite a lack of convergence. In other cases step-halving is invoked but is unable to induce convergence. One remedy is to impose a stricter form of step-halving than is currently available in glm, so that the deviance is forced to decrease in every iteration. This has been implemented in the glm2 function available in the glm2 package. Aside from a modified computational algorithm, glm2 operates in exactly the same way as glm and provides improved convergence properties. These improvements are illustrated here with an identity link Poisson model, but are also relevant in other contexts.",WOS:000208590200003,R JOURNAL,,glm2: Fitting Generalized Linear Models with Convergence Problems,2011
1998,"Consider a linear model Y = X beta + z, z similar to N(0, I-n). Here, X = X-n,X-p, where both p and n are large, but p > n. We model the rows of X as lid. samples from N(0, 1/n Omega), where Omega is a p x p correlation matrix, which is unknown to us but is presumably sparse. The vector beta is also unknown but has relatively few nonzero coordinates, and we are interested in identifying these nonzeros.
We propose the Univariate Penalization Screeing (UPS) for variable selection. This is a screen and clean method where we screen with univariate thresholding and clean with penalized MLE. It has two important properties: sure screening and separable after screening. These properties enable us to reduce the original regression problem to many small-size regression problems that can be fitted separately. The UPS is effective both in theory and in computation.
We measure the performance of a procedure by the Hamming distance, and use an asymptotic framework where p -> infinity and other quantities (e.g., n, sparsity level and strength of signals) are linked to p by fixed parameters. We find that in many cases, the UPS achieves the optimal rate of convergence. Also, for many different Omega, there is a common three-phase diagram in the two-dimensional phase space quantifying the signal sparsity and signal strength. In the first phase, it is possible to recover all signals. In the second phase, it is possible to recover most of the signals, but not all of them. In the third phase, successful variable selection is impossible. UPS partitions the phase space in the same way that the optimal procedures do, and recovers most of the signals as long as successful variable selection is possible.
The lasso and the subset selection are well-known approaches to variable selection. However, somewhat surprisingly, there are regions in the phase space where neither of them is rate optimal, even in very simple settings, such as Omega is tridiagonal, and when the tuning parameter is ideally set.",WOS:000304684900004,ANNALS OF STATISTICS,"['LASSO', 'MODEL', 'REGULARIZATION', 'REGRESSION']",UPS DELIVERS OPTIMAL PHASE DIAGRAM IN HIGH-DIMENSIONAL VARIABLE SELECTION,2012
1999,An increasing number of quantitative reviews of epidemiological data includes a dose-response analysis. Aims of this paper are to describe the main aspects of the methodology and to illustrate the novel R package dosresmeta developed for multivariate dose-response meta-analysis of summarized data. Specific topics covered are reconstructing covariances of correlated outcomes; pooling of study-specific trends; flexible modeling of the exposure; testing hypothesis; assessing statistical heterogeneity; and presenting in either a graphical or tabular way the overall dose-response association.,WOS:000389126300001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED LEAST-SQUARES', 'TREND ESTIMATION', 'META-REGRESSION', 'ALCOHOL', 'MODEL']",Multivariate Dose-Response Meta-Analysis: The dosresmeta R Package,2016
2000,"The copula-based modeling of multivariate distributions with continuous margins is presented as a succession of rank-based tests: a multivariate test of randomness followed by a test of mutual independence and a series of goodness-of-fit tests. All the tests under consideration are based on the empirical copula, which is a nonparametric rank-based estimator of the true unknown copula. The principles of the tests are recalled and their implementation in the copula R package is briefly described. Their use in the construction of a copula model from data is thoroughly illustrated on real insurance and financial data.",WOS:000281584000001,JOURNAL OF STATISTICAL SOFTWARE,"['NONPARAMETRIC TEST', 'TIME-SERIES', 'INDEPENDENCE', 'TESTS', 'DEPENDENCE', 'FIT']",Modeling Multivariate Distributions with Continuous Margins Using the copula R Package,2010
2001,"We study asymptotic properties of M-estimates of regression parameters in linear models in which errors are dependent. Weak and strong Bahadur representations of the M-estimates are derived and a central limit theorem is established. The results are applied to linear models with errors being short-range dependent linear processes, heavy-tailed linear processes and some widely used nonlinear time series.",WOS:000248987600002,ANNALS OF STATISTICS,"['ITERATED RANDOM FUNCTIONS', 'VARIANCE MOVING AVERAGES', 'INFINITE-VARIANCE', 'ASYMPTOTIC NORMALITY', 'ROBUST ESTIMATION', 'BAHADUR REPRESENTATION', 'REGRESSION PARAMETERS', 'RANDOM-VARIABLES', 'LIMIT-THEOREMS', 'QUANTILES']",M-estimation of linear models with dependent errors,2007
2002,"To the worker who only has a hammer, we are told, everything looks like a nail. Solutions to applied statistical problems are framed by the limitations imposed by statistical computing packages and languages. For better or worse, we can do what the packages do; we cannot do what the packages won't do. Statistical languages like R have basic tools that allow the analyst to design new hammers, but even in R we cannot build an arbitrary hammer, only ones within the limits imposed by the R language. XLISP-STAT imposes different limitations, so we can produce different hammers.
In this article, I look at some of the tools in XLISP-STAT that allow the user to think about graphics in ways that cannot be easily replicated in other statistical languages. The interactive graphical methods available in XLISP-STAT lead to very different methodology than would be developed without the tools that XLISP-STAT provides. The general approach to graphics and indeed to data analysis in general is quite different in a package like Arc that is built on top of XLISP-STAT, than it is in other statistical packages. We discuss why that might be true, and why this depends on design options created by XLISP-STAT.",WOS:000232807100001,JOURNAL OF STATISTICAL SOFTWARE,,Lost opportunities: Why we need a variety of statistical languages,2005
2003,"We study the effective degrees of freedom of the lasso in the framework of Stein's unbiased risk estimation (SURE). We show that the number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso-a conclusion that requires no special assumption on the predictors. In addition, the unbiased estimator is shown to be asymptotically consistent. With these results on hand, various model selection criteria-C-p, AIC and BIC-are available, which, along with the LARS algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit.",WOS:000251096100013,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'ADAPTIVE MODEL SELECTION', 'VARIABLE SELECTION', 'REGRESSION', 'SHRINKAGE']","On the ""degrees of freedom"" of the lasso",2007
2004,"Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependence, as well. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently nonconvex, and it is difficult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing nonconvex programs, we are able to both analyze the statistical error associated with any global optimum, and more surprisingly, to prove that a simple algorithm based on projected gradient descent will converge in polynomial time to a small neighborhood of the set of all global minimizers. On the statistical side, we provide nonasymptotic bounds that hold with high probability for the cases of noisy, missing and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm is guaranteed to converge at a geometric rate to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing close agreement with the predicted scalings.",WOS:000310650900013,ANNALS OF STATISTICS,"['LASSO', 'SELECTION', 'RECOVERY', 'MODELS']",HIGH-DIMENSIONAL REGRESSION WITH NOISY AND MISSING DATA: PROVABLE GUARANTEES WITH NONCONVEXITY,2012
2005,"Trimmed regions are a powerful tool of multivariate data analysis. They describe a probability distribution in Euclidean d-space regarding location, dispersion, and shape, and they order multivariate data with respect to their centrality. Dyckerhoff and Mosler (2011) have introduced the class of weighted-mean trimmed regions, which possess attractive properties regarding continuity, subadditivity, and monotonicity.
We present an exact algorithm to compute the weighted-mean trimmed regions of a given data cloud in arbitrary dimension d. These trimmed regions are convex polytopes in R-d. To calculate them, the algorithm builds on methods from computational geometry. A characterization of a region's facets is used, and information about the adjacency of the facets is extracted from the data. A key problem consists in ordering the facets. It is solved by the introduction of a tree-based order, by which the whole surface can be traversed efficiently with the minimal number of computations. The algorithm has been programmed in C++ and is available as the R package WMTregions.",WOS:000305065500001,JOURNAL OF STATISTICAL SOFTWARE,['DEPTH'],An Exact Algorithm for Weighted-Mean Trimmed Regions in Any Dimension,2012
2006,"RegComponent models are time series models with linear regression mean functions and error terms that follow ARIMA (autoregressive-integrated-moving average) component time series models. Bell (2004) discusses these models and gives some underlying theoretical and computational results. The REGCMPNT program is a Fortran program for performing Gaussian maximum likelihood estimation, signal extraction, and forecasting with RegComponent models. In this paper we briefly examine the nature of RegComponent models, provide an overview of the REGCMPNT program, and then use three examples to show some important features of the program and to illustrate its application to various different RegComponent models.",WOS:000290527200001,JOURNAL OF STATISTICAL SOFTWARE,"['NONSTATIONARY TIME-SERIES', 'STATE-SPACE MODELS', 'SIGNAL EXTRACTION', 'INITIAL CONDITIONS']",REGCMPNT - A Fortran Program for Regression Models with ARIMA Component Errors,2011
2007,"The main purpose of this paper is to present the main algorithms underlining the construction and implementation of the SMR package, whose aim is to compute studentized normal midrange distribution. Details on the externally studentized normal midrange and standardized normal midrange distributions are also given. The package follows the same structure as the probability functions implemented in R. That is: the probability density function (dSMR), the cumulative distribution function (pSMR), the quantile function (qSMR) and the random number generating function (rSMR). Pseudocode and illustrative examples of how to use the package are presented.",WOS:000348651700011,R JOURNAL,"['POPULATION', 'RANGE']",SMR: An R Package for Computing the Externally Studentized Normal Midrange Distribution,2014
2008,"This is a short overview of the R add-on package BradleyTerry2, which facilitates the specification and fitting of Bradley-Terry logit, probit or cauchit models to pair-comparison data. Included are the standard 'unstructured' Bradley-Terry model, structured versions in which the parameters are related through a linear predictor to explanatory variables, and the possibility of an order or 'home advantage' effect or other 'contest-specific' effects. Model fitting is either by maximum likelihood, by penalized quasi-likelihood (for models which involve a random effect), or by bias-reduced maximum likelihood in which the first-order asymptotic bias of parameter estimates is eliminated. Also provided are a simple and efficient approach to handling missing covariate data, and suitably-defined residuals for diagnostic checking of the linear predictor.",WOS:000305118200001,JOURNAL OF STATISTICAL SOFTWARE,"['PAIRED-COMPARISON EXPERIMENTS', 'TIES']",Bradley-Terry Models in R: The BradleyTerry2 Package,2012
2009,"Scattering moments provide nonparametric models of random processes with stationary increments. They are expected values of random variables computed with a nonexpansive operator, obtained by iteratively applying wavelet transforms and modulus nonlinearities, which preserves the variance. First- and second-order scattering moments are shown to characterize intermittency and self-similarity properties of multiscale processes. Scattering moments of Poisson processes, fractional Brownian motions, Levy processes and multifractal random walks are shown to have characteristic decay. The Generalized Method of Simulated Moments is applied to scattering moments to estimate data generating models. Numerical applications are shown on financial time-series and on energy dissipation of turbulent flows.",WOS:000349738500012,ANNALS OF STATISTICS,"['MULTIFRACTAL FORMALISM', 'INVARIANT SCATTERING', 'WAVELET-TRANSFORM', 'FRACTAL SIGNALS', 'ASSET RETURNS', 'MODEL', 'BOOTSTRAP', 'NETWORKS', 'SPECTRUM', 'LEADERS']",INTERMITTENT PROCESS ANALYSIS WITH SCATTERING MOMENTS,2015
2010,"Today, many experiments in the field of behavioral sciences are conducted using a computer. While there is a broad choice of computer programs facilitating the process of conducting experiments as well as programs for statistical analysis there are relatively few programs facilitating the intermediate step of data aggregation. ART has been developed in order to fill this gap and to provide a computer program for data aggregation that has a graphical user interface such that aggregation can be done more easily and without any programming. All ""rules"" that are necessary to extract variables can be seen ""at a glance"" which helps the user to conduct even complex aggregations with several hundreds of variables and makes aggregation more resistant against errors. ART runs with Windows XP, Vista, 7, and 8 and it is free.",WOS:000341792800001,JOURNAL OF STATISTICAL SOFTWARE,,ART: A Data Aggregation Program for the Behavioral Sciences,2014
2011,"We study the approximation of arbitrary distributions P on d-dimensional space by distributions with log-concave density. Approximation means minimizing a Kullback Leibler-type functional. We show that such an approximation exists if and only if P has finite first moments and is not supported by some hyperplane. Furthermore we show that this approximation depends continuously on P with respect to Mallows distance D(1)(.,.). This result implies consistency of the maximum likelihood estimator of a log-concave density under fairly general conditions. It also allows us to prove existence and consistency of estimators in regression models with a response Y = mu(X) + epsilon, where X and epsilon are independent, mu(.) belongs to a certain class of regression functions while E is a random error with log-concave density and mean zero.",WOS:000291183300002,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'DENSITY', 'CONSISTENCY', 'ESTIMATORS', 'INFERENCE']","APPROXIMATION BY LOG-CONCAVE DISTRIBUTIONS, WITH APPLICATIONS TO REGRESSION",2011
2012,"This paper presents some limit theorems for certain functionals of moving averages of semimartingales plus noise which are observed at high frequency. Our method generalizes the pre-averaging approach (see [Bernoulli 15 (2009) 634-658, Stochastic Process. Appl. 119 (2009) 2249-2276]) and provides consistent estimates for various characteristics of general semimartingales. Furthermore, we prove the associated multidimensional (stable) central limit theorems. As expected, we find central limit theorems with a convergence rate n(-1/4), if it is the number of observations.",WOS:000277471000008,ANNALS OF STATISTICS,"['HIGH-FREQUENCY DATA', 'MICROSTRUCTURE NOISE', 'VOLATILITY', 'FUNCTIONALS', 'JUMPS']",LIMIT THEOREMS FOR MOVING AVERAGES OF DISCRETIZED PROCESSES PLUS NOISE,2010
2013,"A general rate estimation method is proposed that is based on studying the in-sample evolution of appropriately chosen diverging/converging statistics. The proposed rate estimators are based on simple least squares arguments, and are shown to be accurate in a very general setting without requiring the choice of a tuning parameter. The notion of scanning is introduced with the purpose of extracting useful subsamples of the data series; the proposed rate estimation method is applied to different scans, and the resulting estimators are then combined to improve accuracy. Applications to heavy tail index estimation as well as to the problem of estimating the long memory parameter are discussed; a small simulation study complements our theoretical results.",WOS:000249568000019,ANNALS OF STATISTICS,"['TAIL INDEX', 'LONG MEMORY', 'CONVERGENCE']","Computer-intensive rate estimation, diverging statistics and scanning",2007
2014,"This paper deals with the factor modeling for high-dimensional time series based on a dimension-reduction viewpoint. Under stationary settings, the inference is simple in the sense that both the number of factors and the factor loadings are estimated in terms of an eigenanalysis for a nonnegative definite matrix, and is therefore applicable when the dimension of time series is on the order of a few thousands. Asymptotic properties of the proposed method are investigated under two settings: (i) the sample size goes to infinity while the dimension of time series is fixed; and (ii) both the sample size and the dimension of time series go to infinity together. In particular, our estimators for zero-eigenvalues enjoy faster convergence (or slower divergence) rates, hence making the estimation for the number of factors easier. In particular, when the sample size and the dimension of time series go to infinity together, the estimators for the eigenvalues are no longer consistent. However, our estimator for the number of the factors, which is based on the ratios of the estimated eigenvalues, still works fine. Furthermore, this estimation shows the so-called ""blessing of dimensionality"" property in the sense that the performance of the estimation may improve when the dimension of time series increases. A two-step procedure is investigated when the factors are of different degrees of strength. Numerical illustration with both simulated and real data is also reported.",WOS:000307608000003,ANNALS OF STATISTICS,['DYNAMIC-FACTOR MODEL'],FACTOR MODELING FOR HIGH-DIMENSIONAL TIME SERIES: INFERENCE FOR THE NUMBER OF FACTORS,2012
2015,"We study the asymptotic behavior of a class of methods for sufficient dimension reduction in high-dimension regressions, as the sample size and number of predictors grow in various alignments. It is demonstrated that these methods are consistent in a variety of settings, particularly in abundant regressions where most predictors contribute some information on the response, and oracle rates are possible. Simulation results are presented to support the theoretical conclusion.",WOS:000304684900014,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'COVARIANCE ESTIMATION', 'DISTRIBUTED PREDICTORS', 'VARIABLE SELECTION', 'COMPONENTS', 'MATRICES', 'MODELS']",ESTIMATING SUFFICIENT REDUCTIONS OF THE PREDICTORS IN ABUNDANT HIGH-DIMENSIONAL REGRESSIONS,2012
2016,"The problem we concentrate on is as follows: given (1) a convex compact set X in R(n), an affine mapping x bar right arrow A(x), a parametric family {p(mu)(.)} of probability densities and (2) N i.i.d. observations of the random variable omega, distributed with the density p(A(x)) (.) for some (unknown) x is an element of X, estimate the value g(T)x of a given linear form at x.
For several families {p(mu)(.)} with no additional assumptions on X and A, we develop computationally efficient estimation routines which are minimax optimal, within an absolute constant factor. We then apply these routines to recovering x itself in the Euclidean norm.",WOS:000268604900007,ANNALS OF STATISTICS,"['ADAPTIVE ESTIMATION', 'LINEAR FUNCTIONALS', 'CONVERGENCE', 'RATES']",NONPARAMETRIC ESTIMATION BY CONVEX PROGRAMMING,2009
2017,"Algorithms for binary classification based on adaptive tree partitioning are formulated and analyzed for both their risk performance and their friendliness to numerical implementation. The algorithms can be-viewed as generating a set approximation to the Bayes set and thus fall into the general category of set estimators. In contrast with the most studied tree-based algorithms, which utilize piecewise constant approximation on the generated partition [IEEE Trans. Inform. Theory 52 (2006) 1335-1353; Mach. Learn. 66 (2007) 209-242], we consider decorated trees, which allow us to derive higher order methods. Convergence rates for these methods are derived in terms the parameter alpha of margin conditions and a rate s of best approximation of the Bayes set by decorated adaptive partitions. They can also be expressed in terms of the Besov smoothness beta of the regression function that governs its approximability by piecewise polynomials on adaptive partition. The execution of the algorithms does not require knowledge of the smoothness or margin conditions. Besov smoothness conditions are weaker than the commonly used Holder conditions, which govern approximation by nonadaptive partitions, and therefore for a given regression function can result in a higher rate of convergence. This in turn mitigates the compatibility conflict between smoothness and margin parameters.",WOS:000345884900001,ANNALS OF STATISTICS,"['DYADIC DECISION TREES', 'CLASSIFIERS']",CLASSIFICATION ALGORITHMS USING ADAPTIVE PARTITIONING,2014
2018,"Distance correlation is a new measure of dependence between random vectors. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but unlike the classical definition of correlation, distance correlation is zero only if the random vectors are independent. The empirical distance dependence measures are based on certain Euclidean distances between sample elements rather than sample moments, yet have a compact representation analogous to the classical covariance and correlation. Asymptotic properties and applications in testing independence are discussed. Implementation of the test and Monte Carlo results are also presented.",WOS:000253077800022,ANNALS OF STATISTICS,,Measuring and testing dependence by correlation of distances,2007
2019,"Joint models for longitudinal and survival data now have a long history of being used in clinical trials or other studies in which the goal is to assess a treatment effect while accounting for a longitudinal biomarker such as patient-reported outcomes or immune responses. Although software has been developed for fitting the joint model, no software packages are currently available for simultaneously fitting the joint model and assessing the fit of the longitudinal component and the survival component of the model separately as well as the contribution of the longitudinal data to the fit of the survival model. To fulfill this need, we develop a SAS macro, called JMFit. JMFit implements a variety of popular joint models and provides several model assessment measures including the decomposition of AIC and BIC as well as Delta AIC and Delta BIC recently developed in Zhang, Chen, Ibrahim, Boye, Wang, and Shen (2014). Examples with real and simulated data are provided to illustrate the use of JMFit.",WOS:000384914300001,JOURNAL OF STATISTICAL SOFTWARE,,JMFit: A SAS Macro for Joint Models of Longitudinal and Survival Data,2016
2020,"We present two methods of calculating trimmed means without sorting the data in O(n) time. The existing method implemented in major statistical packages relies on sorting, which takes O (n log n) time. The proposed algorithm is based on the quickselect algorithm for calculating order statistics with O(n) expected running time. It is an order of magnitude faster than the existing method for large data sets.",WOS:000208805700001,JOURNAL OF STATISTICAL SOFTWARE,,Fast Computation of Trimmed Means,2011
2021,"flexmix provides infrastructure for fexible fitting of finite mixture models in R using the expectation-maximization (EM) algorithm or one of its variants. The functionality of the package was enhanced. Now concomitant variable models as well as varying and constant parameters for the component specific generalized linear regression models can be fitted. The application of the package is demonstrated on several examples, the implementation described and examples given to illustrate how new drivers for the component specific models and the concomitant variable models can be de fined.",WOS:000259947000001,JOURNAL OF STATISTICAL SOFTWARE,"['GENERALIZED LINEAR-MODELS', 'EM ALGORITHM', 'LOGISTIC-REGRESSION', 'MAXIMUM-LIKELIHOOD', 'VALUES']",FlexMix Version 2: Finite Mixtures with Concomitant Variables and Varying and Constant Parameters,2008
2022,"Structural equation modeling (SEM) is a vast field and widely used by many applied researchers in the social and behavioral sciences. Over the years, many software packages for structural equation modeling have been developed, both free and commercial. However, perhaps the best state-of-the-art software packages in this field are still closed-source and/or commercial. The R package lavaan has been developed to provide applied researchers, teachers, and statisticians, a free, fully open-source, but commercial-quality package for latent variable modeling. This paper explains the aims behind the development of the package, gives an overview of its most important features, and provides some examples to illustrate how lavaan works in practice.",WOS:000305117100001,JOURNAL OF STATISTICAL SOFTWARE,"['COVARIANCE STRUCTURE-ANALYSIS', 'TEST STATISTICS', 'SPECIFICATION']",lavaan: An R Package for Structural Equation Modeling,2012
2023,"The R package ALTopt has been developed with the aim of creating and evaluating optimal experimental designs of censored accelerated life tests (ALTs). This package takes the generalized linear model approach to ALT planning, because this approach can easily handle censoring plans and derive information matrices for evaluating designs. Three types of optimality criteria are considered: D-optimality for model parameter estimation, U-optimality for reliability prediction at a single use condition, and I-optimality for reliability prediction over a region of use conditions. The Weibull distribution is assumed for failure time data and more than one stress factor can be specified in the package. Several graphical evaluation tools are also provided for the comparison of different ALT test plans.",WOS:000368551800014,R JOURNAL,"['RESPONSE-SURFACE DESIGNS', 'PREDICTION CAPABILITY']",ALTopt: An R Package for Optimal Experimental Design of Accelerated Life Testing,2015
2024,"We continue the investigation of Bernstein- von Mises theorems for non-parametric Bayes procedures from [Ann. Statist. 41 (2013) 1999-2028]. We introduce multiscale spaces on which nonparametric priors and posteriors are naturally defined, and prove Bernstein- von Mises theorems for a variety of priors in the setting of Gaussian nonparametric regression and in the i.i.d. sampling model. From these results we deduce several applications where posterior-based inference coincides with efficient frequentist procedures, including Donsker- and Kolmogorov-Smimov theorems for the random posterior cumulative distribution functions. We also show that multiscale posterior credible bands for the regression or density function are optimal frequentist confidence bands.",WOS:000344632400009,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'POSTERIOR DISTRIBUTIONS', 'DENSITY', 'THEOREM', 'MODELS', 'REGRESSION', 'EXTREMES']",ON THE BERNSTEIN-VON MISES PHENOMENON FOR NONPARAMETRIC BAYES PROCEDURES,2014
2025,"The estimation of surface integrals on the boundary of an unknown body is a challenge for nonparametric methods in statistics, with powerful applications to physics and image analysis, among other fields. Provided that one can determine whether random shots hit the body, Cuevas et al. [Ann. Statist. 35 (2007) 1031-1051] estimate the boundary measure (the boundary length for planar sets and the surface area for 3-dimensional objects) via the consideration of shots at a box containing the body. The statistics considered by these authors, as well as those in subsequent papers, are based on the estimation of Minkowski content and depend on a smoothing parameter which must be carefully chosen. For the same sampling scheme, we introduce a new approach which bypasses this issue, providing strongly consistent estimators of both the boundary measure and the surface integrals of scalar functions, provided one can collect the function values at the sample points. Examples arise in experiments in which the density of the body can be measured by physical properties of the impacts, or in situations where such quantities as temperature and humidity are observed by randomly distributed sensors. Our method is based on random Delaunay triangulations and involves a simple procedure for surface reconstruction from a dense cloud of points inside and outside the body. We obtain basic asymptotics of the estimator, perform simulations and discuss, via Google Earth's data, an application to the image analysis of the Aral Sea coast and its cliffs.",WOS:000288183800007,ANNALS OF STATISTICS,"['GAUSSIAN LIMITS', 'GEOMETRIC PROBABILITY', 'LARGE NUMBERS', 'LAWS']",NONPARAMETRIC ESTIMATION OF SURFACE INTEGRALS,2011
2026,"We present a new package in R implementing Bayesian additive regression trees (BART). The package introduces many new features for data analysis using BART such as variable selection, interaction detection, model diagnostic plots, incorporation of missing data and the ability to save trees for future prediction. It is significantly faster than the current R implementation, parallelized, and capable of handling both large sample sizes and high-dimensional data.",WOS:000373922300001,JOURNAL OF STATISTICAL SOFTWARE,"['MISSING DATA', 'RESPONSE DATA', 'BINARY', 'BART']",bartMachine: Machine Learning with Bayesian Additive Regression Trees,2016
2027,"This paper describes the R add-on package BradleyTerry, which facilitates the specification and fitting of Bradley-Terry logit models to pair-comparison data. Included are the standard 'unstructured' Bradley-Terry model, structured versions in which the parameters are related through a linear predictor to explanatory variables, and the possibility of an order or 'home advantage' effect. Model fitting is either by maximum likelihood or by bias-reduced maximum likelihood in which the first-order asymptotic bias of parameter estimates is eliminated. Also provided are a simple and efficient approach to handling missing covariate data, and suitably-defined residuals for diagnostic checking of the linear predictor; these are new methodological contributions which will be discussed in greater detail elsewhere.",WOS:000232662100001,JOURNAL OF STATISTICAL SOFTWARE,,Bradley-Terry models in R,2005
2028,"This paper develops a theory for group Lasso using a concept called strong group sparsity. Our result shows that group Lasso is superior to standard Lasso for strongly group-sparse signals. This provides a convincing theoretical justification for using group sparse regularization when the underlying group structure is consistent with the data. Moreover, the theory predicts some limitations of the group Lasso formulation that are confirmed by simulation studies.",WOS:000280359400003,ANNALS OF STATISTICS,"['LASSO', 'REGRESSION', 'SELECTION']",THE BENEFIT OF GROUP SPARSITY,2010
2029,"This paper presents a software package that implements Bayesian model averaging for gretl, the GNU regression, econometrics and time-series library. Bayesian model averaging is a model-building strategy that takes account of model uncertainty in conclusions about estimated parameters. It is an efficient tool for discovering the most probable models and obtaining estimates of their posterior characteristics. In recent years we have observed an increasing number of software packages devoted to Bayesian model averaging for different statistical and econometric software. In this paper, we propose the BMA package for gretl, which is an increasingly popular free, open-source software for econometric analysis with an easy-to-use graphical user interface. We introduce the BMA package for linear regression models with jointness measures proposed by Ley and Steel (2007) and Doppelhofer and Weeks (2009).",WOS:000366014300001,JOURNAL OF STATISTICAL SOFTWARE,"['VARIABLE SELECTION', 'LINEAR-REGRESSION', 'GROWTH REGRESSION', 'GRAPHICAL MODELS', 'DETERMINANTS', 'UNCERTAINTY', 'CRITERION']",Bayesian Model Averaging and Jointness Measures for gretl,2015
2030,"We study a class of hypothesis testing problems in which, upon observing the realization of an n-dimensional Gaussian vector, one has to decide whether the vector was drawn from a standard normal distribution or, alternatively, whether there is a subset of the components belonging to a certain given class of sets whose elements have been ""contaminated,"" that is, have a mean different from zero. We establish some general conditions under which testing is possible and others under which testing is hopeless with a small risk. The combinatorial and geometric structure of the class of sets is shown to play a crucial role. The bounds are illustrated on various examples.",WOS:000282402800015,ANNALS OF STATISTICS,"['LARGE HIDDEN CLIQUE', 'HIGH DIMENSION', 'GRAPH', 'FORESTS']",ON COMBINATORIAL TESTING PROBLEMS,2010
2031,"In many areas, scientists deal with increasingly high-dimensional data sets. An important aspect for these scientists is to gain a qualitative understanding of the process or system from which the data is gathered. Often, both input variables and an outcome are observed and the data can be characterized as a sample from a high-dimensional scalar function. This work presents the R package msr for exploratory data analysis of multivariate scalar functions based on the Morse-Smale complex. The Morse-Smale complex provides a topologically meaningful decomposition of the domain. The msr package implements a discrete approximation of the Morse-Smale complex for data sets. In previous work this approximation has been exploited for visualization and partition-based regression, which are both supported in the msr package. The visualization combines the Morse-Smale complex with dimension-reduction techniques for a visual summary representation that serves as a guide for interactive exploration of the high-dimensional function. In a similar fashion, the regression employs a combination of linear models based on the Morse-Smale decomposition of the domain. This regression approach yields topologically accurate estimates and facilitates interpretation of general trends and statistical comparisons between partitions. In this manner, the msr package supports high-dimensional data understanding and exploration through the Morse-Smale complex.",WOS:000306914100001,JOURNAL OF STATISTICAL SOFTWARE,"['SCALAR FUNCTIONS', 'TREES']",Data Analysis with the Morse-Smale Complex: The msr Package for R,2012
2032,"The ROSE package provides functions to deal with binary classification problems in the presence of imbalanced classes. Artificial balanced samples are generated according to a smoothed bootstrap approach and allow for aiding both the phases of estimation and accuracy evaluation of a binary classifier in the presence of a rare class. Functions that implement more traditional remedies for the class imbalance and different metrics to evaluate accuracy are also provided. These are estimated by holdout, bootstrap, or cross-validation methods.",WOS:000343788100009,R JOURNAL,,ROSE: A Package for Binary Imbalanced Learning,2014
2033,"Researchers are often interested in drawing inferences regarding the order between two experimental groups on the basis of multivariate response data. Since standard multivariate methods are designed for two-sided alternatives, they may not be ideal for testing for. order between two groups. In this article we introduce the notion of the linear stochastic order and investigate its properties. Statistical theory and methodology are developed to both estimate the direction which best separates two arbitrary ordered distributions and to test for order between the two groups. The new methodology generalizes Roy's classical largest root test to the nonparametric setting and is applicable to random vectors with discrete and/or continuous components. The proposed methodology is illustrated using data obtained from a 90-day pre-chronic rodent cancer bioassay study conducted by the National Toxicology Program (NTP).",WOS:000317451200001,ANNALS OF STATISTICS,"['N-BOOTSTRAP', 'ESTIMATOR']",THE LINEAR STOCHASTIC ORDER AND DIRECTED INFERENCE FOR MULTIVARIATE ORDERED DISTRIBUTIONS,2013
2034,,WOS:000258243000004,ANNALS OF STATISTICS,"['VARIABLE SELECTION', 'ORACLE PROPERTIES', 'LASSO']",Discussion: One-step sparse estimates in nonconcave penalized likelihood models,2008
2035,"Software testing is important, but many of us don't do it because it is frustrating and boring. testthat is a new testing framework for R that is easy learn and use, and integrates with your existing workflow. This paper shows how, with illustrations from existing packages.",WOS:000208590100002,R JOURNAL,,testthat: Get Started with Testing,2011
2036,,WOS:000389620800003,ANNALS OF STATISTICS,"['PRINCIPAL-COMPONENTS-ANALYSIS', 'FEATURE-SELECTION', 'SPARSE PCA', 'CONSISTENCY']","DISCUSSION OF ""INFLUENTIAL FEATURES PCA FOR HIGH DIMENSIONAL CLUSTERING""",2016
2037,An efficient estimator is constructed for the quadratic covariation or integrated co-volatility matrix of a multivariate continuous martingale based on noisy and nonsynchronous observations under high-frequency asymptotics. Our approach relies on an asymptotically equivalent continuous-time observation model where a local generalised method of moments in the spectral domain turns out to be optimal. Asymptotic semi-parametric efficiency is established in the Cramer-Rao sense. Main findings are that nonsynchronicity of observation times has no impact on the asymptotics and that major efficiency gains are possible under correlation. Simulations illustrate the finite-sample behaviour.,WOS:000342481700003,ANNALS OF STATISTICS,"['HIGH-FREQUENCY DATA', 'VOLATILITY']",ESTIMATING THE QUADRATIC COVARIATION MATRIX FROM NOISY OBSERVATIONS: LOCAL METHOD OF MOMENTS AND EFFICIENCY,2014
2038,"Web access logs contain information on HTTP(S) requests and form a key part of both industry and academic explorations of human behaviour on the internet. But the preparation (reading, parsing and manipulation) of that data is just unique enough to make generalized tools unfit for the task, both in programming time and processing time which are compounded when dealing with large data sets common with web access logs. In this paper we explain and demonstrate a series of packages designed to efficiently read in, parse and munge access log data, allowing researchers to handle URLs and IP addresses easily. These packages are substantially faster than existing R methods - from a 3-500% speedup for file reading to a 57,000% speedup in URL parsing.",WOS:000385276100027,R JOURNAL,,R Packages to Aid in Handling Web Access Logs,2016
2039,"Consider a decision maker who is responsible to dynamically collect observations so as to enhance his information about an underlying phenomena of interest in a speedy manner while accounting for the penalty of wrong declaration. Due to the sequential nature of the problem, the decision maker relies on his current information state to adaptively select the most ""informative"" sensing action among the available ones. In this paper, using results in dynamic programming, lower bounds for the optimal total cost are established. The lower bounds characterize the fundamental limits on the maximum achievable information acquisition rate and the optimal reliability. Moreover, upper bounds are obtained via an analysis of two heuristic policies for dynamic selection of actions. It is shown that the first proposed heuristic achieves asymptotic optimality, where the notion of asymptotic optimality, due to Chernoff, implies that the relative difference between the total cost achieved by the proposed policy and the optimal total cost approaches zero as the penalty of wrong declaration (hence the number of collected samples) increases. The second heuristic is shown to achieve asymptotic optimality only in a limited setting such as the problem of a noisy dynamic search. However, by considering the dependency on the number of hypotheses, under a technical condition, this second heuristic is shown to achieve a nonzero information acquisition rate, establishing a lower bound for the maximum achievable rate and error exponent. In the case of a noisy dynamic search with size-independent noise, the obtained nonzero rate and error exponent are shown to be maximum.",WOS:000330204900001,ANNALS OF STATISTICS,"['WHEREABOUTS SEARCH', 'BLOCK-CODES', 'DESIGN', 'INFORMATION', 'FEEDBACK', 'DISCRIMINATION', 'EFFICIENCY', 'STRATEGY']",ACTIVE SEQUENTIAL HYPOTHESIS TESTING,2013
2040,"The spectral representation of stationary Gaussian processes via the Fourier basis provides a computationally efficient specification of spatial surfaces and nonparametric regression functions for use in various statistical models. I describe the representation in detail and introduce the spectralGP package in R for computations. Because of the large number of basis coefficients, some form of shrinkage is necessary; I focus on a natural Bayesian approach via a particular parameterized prior structure that approximates stationary Gaussian processes on a regular grid. I review several models from the literature for data that do not lie on a grid, suggest a simple model modification, and provide example code demonstrating MCMC sampling using the spectralGP package. I describe reasons that mixing can be slow in certain situations and provide some suggestions for MCMC techniques to improve mixing, also with example code, and some general recommendations grounded in experience.",WOS:000245823000001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR MIXED MODELS', 'MARKOV RANDOM-FIELDS', 'REGRESSION-MODELS', 'SPATIAL DATA', 'COUNT DATA', 'LIKELIHOODS']",Bayesian smoothing with Gaussian processes using Fourier basis functions in the spectralGP package,2007
2041,"We derive an asymptotic theory of nonparametric estimation for a time series regression model Z(t) = f (X-t) + W-t, where {X-t) and {Z(t)} are observed nonstationary processes and {W-t} is an unobserved stationary process. In econometrics, this can be interpreted as a nonlinear cointegration type relationship, but we believe that our results are of wider interest. The class of nonstationary processes allowed for {Xt} is a subclass of the class of null recurrent Markov chains. This subclass contains random walk, unit root processes and nonlinear processes. We derive the asymptotics of a nonparametric estimate of f (x) under the assumption that {W-t} is a Markov chain satisfying some mixing conditions. The finite-sample properties of (f) over cap (x) are studied by means of simulation experiments.",WOS:000247498100011,ANNALS OF STATISTICS,"['ERROR-CORRECTION MODELS', 'BERRY-ESSEEN THEOREM', 'UNITED-KINGDOM', 'MARKOV-CHAINS', 'MONEY DEMAND', 'SERIES']",Nonparametric estimation in a nonlinear cointegration type model,2007
2042,A constructive proof of identification of multilinear decompositions of multiway arrays is presented. It can be applied to show identification in a variety of multivariate latent structures. Examples are finite-mixture models and hidden Markov models. The key step to show identification is the joint diagonalization of a set of matrices in the same nonorthogonal basis. An estimator of the latent-structure model may then be based on a sample version of this joint-diagonalization problem. Algorithms are available for computation and we derive distribution theory. We further develop asymptotic theory for orthogonal-series estimators of component densities in mixture models and emission densities in hidden Markov models.,WOS:000372594300004,ANNALS OF STATISTICS,"['CANONICAL POLYADIC DECOMPOSITION', 'JOINT EIGENVALUE DECOMPOSITION', 'NONPARAMETRIC IDENTIFICATION', 'NONDEFECTIVE MATRICES', 'MIXTURE-MODELS', 'UNIQUENESS', 'TENSORS', 'DIAGONALIZATION', 'DISTRIBUTIONS', 'INFERENCE']",ESTIMATING MULTIVARIATE LATENT-STRUCTURE MODELS,2016
2043,"Cook's distance [Technometrics 19 (1977) 15-18] is one of the most important diagnostic tools for detecting influential individual or subsets of observations in linear regression for cross-sectional data. However, for many complex data structures (e.g., longitudinal data), no rigorous approach has been developed to address a fundamental issue: deleting subsets with different numbers of observations introduces different degrees of perturbation to the current model fitted to the data, and the magnitude of Cook's distance is associated with the degree of the perturbation. The aim of this paper is to address this issue in general parametric models with complex data structures. We propose a new quantity for measuring the degree of the perturbation introduced by deleting a subset. We use stochastic ordering to quantify the stochastic relationship between the degree of the perturbation and the magnitude of Cook's distance. We develop several scaled Cook's distances to resolve the comparison of Cook's distance for different subset deletions. Theoretical and numerical examples are examined to highlight the broad spectrum of applications of these scaled Cook's distances in a formal influence analysis.",WOS:000307608000006,ANNALS OF STATISTICS,"['LINEAR LONGITUDINAL MODELS', 'INFLUENCE DIAGNOSTICS', 'DELETION DIAGNOSTICS', 'LOCAL INFLUENCE', 'CHILDREN BORN', 'MIXED MODELS', 'MALTREATMENT', 'REGRESSION', 'MOTHERS']",PERTURBATION AND SCALED COOK'S DISTANCE,2012
2044,We provide a limit theory for a general class of kernel smoothed U-statistics that may be used for specification testing in time series regression with nonstationary data. The test framework allows for linear and nonlinear models with endogenous regressors that have autoregressive unit roots or near unit roots. The limit theory for the specification test depends on the self-intersection local time of a Gaussian process. A new weak convergence result is developed for certain partial sums of functions involving nonstationary time series that converges to the intersection local time process. This result is of independent interest and is useful in other applications. Simulations examine the finite sample performance of the test.,WOS:000307608000004,ANNALS OF STATISTICS,"['INTEGRATED TIME-SERIES', 'NONPARAMETRIC COINTEGRATING REGRESSION', 'DIMENSIONAL RANDOM POLYMERS', 'ASYMPTOTIC THEORY', 'FUNCTIONALS', 'AUTOREGRESSION', 'CONVERGENCE', 'SUMS']",A SPECIFICATION TEST FOR NONLINEAR NONSTATIONARY MODELS,2012
2045,"Influence diagnosis is important since presence of influential observations could lead to distorted analysis and misleading interpretations. For high-dimensional data, it is particularly so, as the increased dimensionality and complexity may amplify both the chance of an observation being influential, and its potential impact on the analysis. In this article, we propose a novel high-dimensional influence measure for regressions with the number of predictors far exceeding the sample size. Our proposal can be viewed as a high-dimensional counterpart to the classical Cook's distance. However, whereas the Cook's distance quantifies the individual observation's influence on the least squares regression coefficient estimate, our new diagnosis measure captures the influence on the marginal correlations, which in turn exerts serious influence on downstream analysis including coefficient estimation, variable selection and screening. Moreover, we establish the asymptotic distribution of the proposed influence measure by letting the predictor dimension go to infinity. Availability of this asymptotic distribution leads to a principled rule to determine the critical value for influential observation detection. Both simulations and real data analysis demonstrate usefulness of the new influence diagnosis measure.",WOS:000327746100012,ANNALS OF STATISTICS,"['LINEAR LONGITUDINAL MODELS', 'INFLUENCE DIAGNOSTICS', 'ADAPTIVE LASSO', 'MIXED MODELS', 'DELETION DIAGNOSTICS', 'REGRESSION SHRINKAGE', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'COOKS DISTANCE', 'PERTURBATION']",HIGH-DIMENSIONAL INFLUENCE MEASURE,2013
2046,"We consider estimation of quantile curves for a general class of nonstationary processes. Consistency and central limit results are obtained for local linear quantile estimates under a mild short-range dependence condition. Our results are applied to environmental data sets. In particular, our results can be used to address the problem of whether climate variability has changed, an important problem raised by IPCC (Intergovernmental Panel on Climate Change) in 2001.",WOS:000268605000005,ANNALS OF STATISTICS,"['BAHADUR REPRESENTATION', 'REGRESSION QUANTILES', 'MODELS', 'VARIABLES', 'KERNEL']",LOCAL LINEAR QUANTILE ESTIMATION FOR NONSTATIONARY TIME SERIES,2009
2047,"This paper provides a brief introduction to the R package bio. infer, a set of scripts that facilitates the use of maximum likelihood (ML) methods for predicting environmental conditions from assemblage composition. Environmental conditions can often be inferred from only biological data, and these inferences are useful when other sources of data are unavailable. ML prediction methods are statistically rigorous and applicable to a broader set of problems than more commonly used weighted averaging techniques. However, ML methods require a substantially greater investment of time to program algorithms and to perform computations. This package is designed to reduce the effort required to apply ML prediction methods.",WOS:000252429700001,JOURNAL OF STATISTICAL SOFTWARE,"['REGRESSION', 'DIATOMS', 'PH', 'CALIBRATION', 'INFERENCES', 'ERROR']",Maximum likelihood method for predicting environmental conditions from assemblage composition: The R package bio.infer,2007
2048,"We give answer to an open problem regarding consistency of the maximum likelihood estimators (MLEs) in generalized linear mixed models (GLMMs) involving crossed random effects. The solution to the open problem introduces an interesting, nonstandard approach to proving consistency of the MLEs in cases of dependent observations. Using the new technique, we extend the results to MLEs under a general GLMM. An example is used to further illustrate the technique.",WOS:000317451200007,ANNALS OF STATISTICS,"['LINEAR MIXED MODELS', 'LIKELIHOOD INFERENCE', 'DATA CLONING', 'CONDITIONAL INFERENCE', 'BIAS CORRECTION', '2 COMPONENTS', 'DISPERSION']",THE SUBSET ARGUMENT AND CONSISTENCY OF MLE IN GLMM: ANSWER TO AN OPEN PROBLEM AND BEYOND,2013
2049,"The R package phtt provides estimation procedures for panel data with large dimensions n, T, and general forms of unobservable heterogeneous effects. Particularly, the estimation procedures are those of Bai (2009) and Kneip, Sickles, and Song (2012), which complement one another very well: both models assume the unobservable heterogeneous effects to have a factor structure. Kneip et al. (2012) considers the case in which the time-varying common factors have relatively smooth patterns including strongly positively auto-correlated stationary as well as non-stationary factors, whereas the method of Bai (2009) focuses on stochastic bounded factors such as ARMA processes. Additionally, the phtt package provides a wide range of dimensionality criteria in order to estimate the number of the unobserved factors simultaneously with the remaining model parameters.",WOS:000341806300001,JOURNAL OF STATISTICAL SOFTWARE,"['APPROXIMATE FACTOR MODELS', 'NUMBER']",phtt: Panel Data Analysis with Heterogeneous Time Trends in R,2014
2050,"The 3-dimensional representation and inspection of complex data is a frequently used strategy in many data analysis domains. Existing data mining software often lacks functionality that would enable users to explore 3D data interactively, especially if one wishes to make dynamic graphical representations directly viewable on the web.
In this paper we present vrmlgen, a software package for the statistical programming language R to create 3D data visualizations in web formats like the Virtual Reality Markup Language (VRML) and Live Graphics 3D .vrmlgen can be used to generate 3D charts and bar plots, scatter plots with density estimation contour surfaces, and visualizations of height maps, 3D object models and parametric functions. For greater fexibility, the user can also access low-level plotting methods through a unified interface and freely group different function calls to gether to create new higher-level plotting methods. Additionally, we present a web tool allowing users to visualize 3D data online and test some of vrmlgen's features without the need to install any software on their computer.",WOS:000281593700001,JOURNAL OF STATISTICAL SOFTWARE,['XGOBI'],vrmlgen: An R Package for 3D Data Visualization on the Web,2010
2051,In this paper we present the R package gRc for statistical inference in graphical Gausian models in which symmetry restrictions have been imposed on the concentration or partial correlation matrix. The models are represented by coloured graphs where parameters associated with edges or vertices of same colour are restricted tobeing identical. We describe algorithms for maximum likelihood estimate and discuss model selection issues. The paper illustrates the practical use of the gRc package.,WOS:000252431600001,JOURNAL OF STATISTICAL SOFTWARE,,Inference in graphical Gaussian models with edge and vertex symmetries with the gRc package for R,2007
2052,"An increasing number of R packages include nonparametric tests for the interaction in two-way factorial designs. This paper briefly describes the different methods of testing and reports the resulting p-values of such tests on datasets for four types of designs: between, within, mixed, and pretest-posttest designs. Potential users are advised only to apply tests they are quite familiar with and not be guided by p-values for selecting packages and tests.",WOS:000385276100028,R JOURNAL,"['COVARIANCE', 'MODELS']",Nonparametric Tests for the Interaction in Two-way Factorial Designs Using R,2016
2053,"In regression analysis, we employ contour projection (CP) to develop a new dimension reduction theory. Accordingly, we introduce the notions of the central contour subspace and generalized contour subspace. We show that both of their structural dimensions are no larger than that of the central subspace Cook [Regression Graphics (1998b) Wiley]. Furthermore, we employ CP-sliced inverse regression, CP-sliced average variance estimation and CP-directional regression to estimate the generalized contour,subspace, and we subsequently obtain their theoretical properties. Monte Carlo studies demonstrate that the three CP-based dimension reduction methods outperform their corresponding non-CP approaches when the predictors have heavy-tailed elliptical distributions. An empirical example is also presented to illustrate the usefulness of the CP method.",WOS:000271673700002,ANNALS OF STATISTICS,"['SLICED INVERSE REGRESSION', 'PRINCIPAL HESSIAN DIRECTIONS', 'AVERAGE VARIANCE-ESTIMATION', 'BINARY RESPONSE', 'ASYMPTOTICS', 'MOMENTS']",CONTOUR PROJECTED DIMENSION REDUCTION,2009
2054,"The special volume on ""Magnetic Resonance Imaging in R"" features articles and packages related to a variety of imaging modalities: functional MRI, diffusion-weighted MRI, dynamic contrast-enhanced MRI, dynamic susceptibility-contrast MRI and structural MRI. The papers describe the methodology, software implementation and provide comprehensive examples and data.",WOS:000296228100001,JOURNAL OF STATISTICAL SOFTWARE,"['FMRI DATA', 'PACKAGE', 'IMPLEMENTATION', 'VISUALIZATION', 'BRAIN', 'MODEL']",Special Volume on Magnetic Resonance Imaging in R,2011
2055,"Spectral clustering is a technique that clusters elements using the top few eigenvectors of their (possibly normalized) similarity matrix. The quality of spectral clustering is closely tied to the convergence properties of these principal eigenvectors. This rate of convergence has been shown to be identical for both the normalized and unnormalized variants in recent random matrix theory literature. However, normalization for spectral clustering is commonly believed to be beneficial [Stat. Comput. 17 (2007) 395-416]. Indeed, our experiments show that normalization improves prediction accuracy. In this paper, for the popular stochastic blockmodel, we theoretically show that normalization shrinks the spread of points in a class by a constant fraction under a broad parameter regime. As a byproduct of our work, we also obtain sharp deviation bounds of empirical principal eigenvalues of graphs generated from a stochastic blockmodel.",WOS:000355768700002,ANNALS OF STATISTICS,"['GRAPHS', 'MATRICES']",ROLE OF NORMALIZATION IN SPECTRAL CLUSTERING FOR STOCHASTIC BLOCKMODELS,2015
2056,"We address the problem of adaptive minimax estimation in white Gaussian noise models under L-p-loss, 1 <= p <= infinity, on the anisotropic Nikol'skii classes. We present the estimation procedure based on a new data-driven selection scheme from the family of kernel estimators with varying bandwidths. For the proposed estimator we establish so-called L-p-norm oracle inequality and use it for deriving minimax adaptive results. We prove the existence of rate-adaptive estimators and fully characterize behavior of the minimax risk for different relationships between regularity parameters and norm indexes in definitions of the functional class and of the risk. In particular some new asymptotics of the minimax risk are discovered, including necessary and sufficient conditions for the existence of a uniformly consistent estimator. We provide also a detailed overview of existing methods and results and formulate open problems in adaptive minimax estimation.",WOS:000355768700009,ANNALS OF STATISTICS,"['MINIMAX DENSITY-ESTIMATION', 'NONPARAMETRIC REGRESSION', 'MODEL SELECTION', 'NONLINEAR ESTIMATION', 'COMPOSITE FUNCTIONS', 'LINEAR FUNCTIONALS', 'OPTIMAL RATES', 'AGGREGATION', 'INEQUALITIES', 'ADAPTATION']",ADAPTIVE ESTIMATION OVER ANISOTROPIC FUNCTIONAL CLASSES VIA ORACLE APPROACH,2015
2057,"Let {Z(n)} be a real nonstationary stochastic process such that E(Z(n)vertical bar Fn-1) <(a.s.) infinity and E(Z(n)(2)vertical bar Fn-1) <(a.s.) infinity, where {F-n} is an increasing sequence of sigma-algebras. Assuming that E(Z(n)vertical bar Fn-1) = gn(theta(0), nu(0)) = g(n)((1))(theta(0)) + g(n)((2))(theta(0), nu(0)), theta(0) is an element of R-p, p < infinity, nu(0) is an element of R-q and q <= infinity, we study the symptotic properties of <(theta)over cap>(n) := arg min(theta) Sigma(n)(k=1) (Z(k) = g(k)(theta, (nu) over cap))(2)lambda(-1)(k), where lambda(k) is Fk-1-measurable, (nu) over cap = {(nu) over cap (k)} is a sequence of estimations of nu(0), g(n)(theta, (nu) over cap) is Lipschits in theta and g(n)((2))(theta(0), (nu) over cap) - g(n)((2))(theta, (nu) over cap) is asymptotically negligible relative to g(n)((1))(theta(0)) - g(n)((1)) (theta). We first generalize to this nonlinear stochastic model the necessary and sufficient condition obtained for the strong consistency of {(theta) over cap (n)} in the linear model. For that, we prove a strong law of large numbers for a class of submartingales. Again using this strong law, we derive the general conditions leading to the asymptotic distribution of (theta) over cap (n). We illustrate the theoretical results with examples of branching processes, and extension to quasi-likelihood estimators is also considered.",WOS:000273800100017,ANNALS OF STATISTICS,"['DEPENDENT BRANCHING-PROCESS', 'POLYMERASE-CHAIN-REACTION', 'ASYMPTOTIC PROPERTIES', 'STRONG CONSISTENCY', 'QUASI-LIKELIHOOD', 'PCR', 'IDENTIFICATION', 'PARAMETERS']",CONDITIONAL LEAST SQUARES ESTIMATION IN NONSTATIONARY NONLINEAR STOCHASTIC REGRESSION MODELS,2010
2058,"Let (X. Y) be a random couple in S x T with unknown distribution P and (X(1), Y(1)),..., (X(n), Y(n),) be i.i.d. copies of (X, Y). Denote P(n) the empirical distribution of (X(1), Y(1)),..., (X(n), Y(n)). Let h(1),..., h(N): S bar right arrow [-1, 1] be a dictionary that consists of N functions. For lambda is an element of R(N), denote f(lambda) := Sigma(N)(j=1) lambda(j)h(j). Let l: T x R bar right arrow R be a given loss function and suppose it is convex with respect to the second variable. Let (l center dot f)(x, y) := l(y; f (x)). Finally, let &ULAMBDA subset of R(N) be the simplex of all probability distributions on {1,..., N}. Consider the following penalized empirical risk minimization problem
(lambda) over cap (epsilon) := argmin(lambda is an element of Lambda)[ P(n)(l center dot f(lambda)) + epsilon Sigma(N)(j=1)lambda(j)log lambda(j)]
along with its distribution dependent version
lambda(epsilon) := argmin(lambda is an element of Lambda)[ P(l center dot f(lambda)) + epsilon Sigma(N)(j=1)lambda(j)log lambda(j)],
where epsilon >= 0 is a regularization parameter. It is proved that the ""approximate sparsity"" of lambda(epsilon) implies the ""approximate sparsity"" of (lambda) over cap (epsilon) and the impact of ""sparsity"" oil bounding the excess risk of the empirical solution is explored. Similar results are also discussed in the case of entropy penalized density estimation.",WOS:000265619700009,ANNALS OF STATISTICS,"['ORACLE INEQUALITIES', 'STATISTICAL ESTIMATION', 'RECONSTRUCTION', 'LASSO']",SPARSE RECOVERY IN CONVEX HULLS VIA ENTROPY PENALIZATION,2009
2059,"The estimation of a log-concave density on R-d represents a central problem in the area of nonparametric inference under shape constraints. In this paper, we study the performance of log-concave density estimators with respect to global loss functions, and adopt a minimax approach. We first show that no statistical procedure based on a sample of size n can estimate a log-concave density with respect to the squared Hellinger loss function with supremum risk smaller than order n(-4/5), when d = 1, and order n(-2/(d+1)) when d >= 2. In particular, this reveals a sense in which, when d >= 3, log-concave density estimation is fundamentally more challenging than the estimation of a density with two bounded derivatives (a problem to which it has been compared). Second, we show that for d <= 3, the Hellinger e-bracketing entropy of a class of log-concave densities with small mean and covariance matrix close to the identity grows like max {epsilon(-d/2), epsilon(-(d-1))} (up to a logarithmic factor when d = 2). This enables us to prove that when d <= 3 the log-concave maximum likelihood estimator achieves the minimax optimal rate (up to logarithmic factors when d = 2, 3) with respect to squared Hellinger loss..",WOS:000389620800018,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'CONSISTENCY', 'NUMBERS', 'SETS']",GLOBAL RATES OF CONVERGENCE IN LOG-CONCAVE DENSITY ESTIMATION,2016
2060,"We consider the detection of multivariate spatial clusters in the Bernoulli model with N locations, where the design distribution has weakly dependent marginals. The locations are scanned with a rectangular window with sides parallel to the axes and with varying sizes and aspect ratios. Multivariate scan statistics pose a statistical. problem due to the multiple testing over many scan windows, as well as a computational problem because statistics have to be evaluated on many windows. This paper introduces methodology that leads to both statistically optimal inference and computationally efficient algorithms. The main difference to the traditional calibration of scan statistics is the concept of grouping scan windows according to their sizes, and then applying different critical values to different groups. It is shown that this calibration of the scan statistic results in optimal inference for spatial clusters on both small scales and on large scales, as well as in the case where the cluster lives on one of the marginals. Methodology is introduced that allows for an efficient approximation of the set of all rectangles while still guaranteeing the statistical optimality results described above. It is shown that the resulting scan statistic has a computational complexity that is almost linear in N.",WOS:000275510800015,ANNALS OF STATISTICS,,OPTIMAL AND FAST DETECTION OF SPATIAL CLUSTERS WITH SCAN STATISTICS,2010
2061,"In this paper we are interested in empirical likelihood (EL)as a method of estimation, and we address the following two problems: (1) selecting among Various empirical discrepancies in an EL framework and (2) demonstrating that El. has a well-defined probabilistic interpretation that would justify its use in a Bayesian context. Using the large deviations approach, a Bayesian law of large numbers is developed that implies that EL and the Bayesian maximum a posteriori probability (MAP) estimators are consistent under mis-specification and that EL can be viewed as an asymptotic form of MAP. Estimators based on other empirical discrepancies are, in general. inconsistent under misspecification.",WOS:000268604900013,ANNALS OF STATISTICS,"['ESTIMATING EQUATIONS', 'INFERENCE', 'THEOREM']",ASYMPTOTIC EQUIVALENCE OF EMPIRICAL LIKELIHOOD AND BAYESIAN MAP,2009
2062,"In the context of large-scale multiple testing, hypotheses are often accompanied with certain prior information. In this paper, we present a single-index modulated (SIM) multiple testing procedure, which maintains control of the false discovery rate while incorporating prior information, by assuming the availability of a bivariate p-value, (p(1), p(2)), for each hypothesis, where pi is a preliminary p-value from prior information and p(2) is the primary p-value for the ultimate analysis. To find the optimal rejection region for the bivariate p-value, we propose a criteria based on the ratio of probability density functions of (p(1), p(2)) under the true null and nonnull. This criteria in the bivariate normal setting further motivates us to project the bivariate p-value to a single-index, p(theta), for a wide range of directions theta. The true null distribution of p(theta) is estimated via parametric and nonparametric approaches, leading to two procedures for estimating and controlling the false discovery rate. To derive the optimal projection direction theta, we propose a new approach based on power comparison, which is further shown to be consistent under some mild conditions. Simulation evaluations indicate that the SIM multiple testing procedure improves the detection power significantly while controlling the false discovery rate. Analysis of a real dataset will be illustrated.",WOS:000342481700002,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'DNA COPY NUMBER', 'INTEGRATIVE ANALYSIS', 'PROSTATE-CANCER', 'MICROARRAY DATA', 'P-VALUES', 'RATES', 'POWER', 'EXPRESSION', 'GENES']",SINGLE-INDEX MODULATED MULTIPLE TESTING,2014
2063,"This monograph details the implementation and use of the CollocInfer package in R for smoothing-based estimation of continuous-time nonlinear dynamic systems. These routines represent an extension of the generalized profiling methods in Ramsay, Hooker, Campbell, and Cao (2007) for estimating parameters in nonlinear ordinary differential equations. An interface to the fda package is included. The package also supports discretetime systems. We describe the methodological and computational framework and the necessary steps to use the software. Equivalent functionality is available in MATLAB.",WOS:000392704000001,JOURNAL OF STATISTICAL SOFTWARE,"['PARAMETER-ESTIMATION', 'DYNAMICS', 'SYSTEMS', 'STATES']",CollocInfer: Collocation Inference in Differential Equation Models,2016
2064,"Any limiting point process for the time normalized exceedances of high levels by a stationary sequence is necessarily compound Poisson under appropriate long range dependence conditions. Typically exceedances appear in clusters. The underlying Poisson points represent the cluster positions and the multiplicities correspond to the cluster sizes. In the present paper we introduce estimators of the limiting Cluster size probabilities, which are constructed through a recursive algorithm. We derive estimators of the extremal index which plays a key role in determining the intensity of cluster positions. We study the asymptotic properties of the estimators and investigate their finite sample behavior on simulated data.",WOS:000263129000010,ANNALS OF STATISTICS,"['POISSON RANDOM SUMS', 'STATIONARY SEQUENCE', 'EMPIRICAL PROCESSES', 'INDEX ESTIMATION', 'DEPENDENT DATA', 'BEHAVIOR']",INFERENCE FOR THE LIMITING CLUSTER SIZE DISTRIBUTION OF EXTREME VALUES,2009
2065,We provide a new algorithm for the treatment of the deconvolution problem on the sphere which combines the traditional SVD inversion with an appropriate thresholding technique in a well chosen new basis. We establish upper bounds for the behavior of our procedure for any L-p loss. It is important to emphasize the adaptation properties of our procedures with respect to the regularity (sparsity) of the object to recover as well as to inhomogeneous smoothness. We also perform a numerical study which proves that the procedure shows very promising properties in practice as well.,WOS:000291183300013,ANNALS OF STATISTICS,"['INVERSE PROBLEMS', 'NEEDLETS']",LOCALIZED SPHERICAL DECONVOLUTION,2011
2066,"One-way layouts, i.e., a single factor with several levels and multiple observations at each level, frequently arise in various fields. Usually not only a global hypothesis is of interest but also multiple comparisons between the different treatment levels. In most practical situations, the distribution of observed data is unknown and there may exist a number of atypical measurements and outliers. Hence, use of parametric and semiparametric procedures that impose restrictive distributional assumptions on observed samples becomes questionable. This, in turn, emphasizes the demand on statistical procedures that enable us to accurately and reliably analyze one-way layouts with minimal conditions on available data. Nonparametric methods offer such a possibility and thus become of particular practical importance. In this article, we introduce a new R package nparcomp which provides an easy and user-friendly access to rank-based methods for the analysis of unbalanced one-way layouts. It provides procedures performing multiple comparisons and computing simultaneous confidence intervals for the estimated effects which can be easily visualized. The special case of two samples, the nonparametric Behrens-Fisher problem, is included. We illustrate the implemented procedures by examples from biology and medicine.",WOS:000352915100001,JOURNAL OF STATISTICAL SOFTWARE,"['BEHRENS-FISHER PROBLEM', 'FACTORIAL-DESIGNS', 'DOSE LEVELS', 'EFFECT SIZE', 'TREND TEST', 'INFERENCE', 'VARIABLES', 'LOCATION', 'MODELS', 'TESTS']",nparcomp: An R Software Package for Nonparametric Multiple Comparisons and Simultaneous Confidence Intervals,2015
2067,"Recent results in homotopy and solution paths demonstrate that certain well-designed greedy algorithms, with a range of values of the algorithmic parameter, can provide solution paths to a sequence of convex optimization problems. On the other hand, in regression many existing criteria in subset selection (including C-p, AIC, BIC, MDL, RIC, etc.) involve optimizing an objective function that contains a counting measure. The two optimization problems are formulated as (P1) and (P0) in the present paper. The latter is generally combinatoric and has been proven to be NP-hard. We study the conditions under which the two optimization problems have common solutions. Hence, in these situations a stepwise algorithm can be used to solve the seemingly unsolvable problem. Our main result is motivated by recent work in sparse representation, while two others emerge from different angles: a direct analysis of sufficiency and necessity and a condition on the mostly correlated covariates. An extreme example connected with least angle regression is of independent interest.",WOS:000248987600017,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'LEAST ANGLE REGRESSION', 'VARIABLE SELECTION', 'SPARSE REPRESENTATIONS', 'BASES', 'SHRINKAGE', 'BRANCH', 'LASSO', 'NOISE']",When do stepwise algorithms meet subset selection criteria?,2007
2068,"Data matching is a typical statistical problem in non experimental and/or observational studies or, more generally, in cross-sectional studies in which one or more data sets are to be compared. Several methods are available in the literature, most of which based on a particular metric or on statistical models, either parametric or nonparametric. In this paper we present two methods to calculate aproximity which have the property of being invariant under monotonic transformations. These methods require at most the notion of ordering. An open-source software in the form of a R package is also presented.",WOS:000255794900001,JOURNAL OF STATISTICAL SOFTWARE,"['PROPENSITY SCORE', 'TRAINING-PROGRAMS', 'REMOVE BIAS', 'CAUSAL']",Invariant and metric free proximities for data matching: An R package,2008
2069,"Artificial neural networks are applied in many situations. neuralnet is built to train multi-layer perceptrons in the context of regression analyses, i.e. to approximate functional relationships between covariates and response variables. Thus, neural networks are used as extensions of generalized linear models.
neuralnet is a very flexible package. The backpropagation algorithm and three versions of resilient backpropagation are implemented and it provides a custom-choice of activation and error function. An arbitrary number of covariates and response variables as well as of hidden layers can theoretically be included.
The paper gives a brief introduction to multilayer perceptrons and resilient backpropagation and demonstrates the application of neuralnet using the data set infert, which is contained in the R distribution.",WOS:000208589900006,R JOURNAL,,neuralnet: Training of Neural Networks,2010
2070,"We develop a method to carry out MAP estimation for a class of Bayesian regression models in which coefficients are assigned with Gaussian-based spike and slab priors. The objective function in the corresponding optimization problem has a Lagrangian form in that regression coefficients are regularized by a mixture of squared l(2) and l(0) norms. A tight approximation to the l(0) norm using majorization minimization techniques is derived, and a coordinate descent algorithm in conjunction with a soft-thresholding scheme is used in searching for the optimizer of the approximate objective. Simulation studies show that the proposed method can lead to more accurate variable selection than other benchmark methods. Theoretical results show that under regular conditions, sign consistency can be established, even when the Irrepresentable Condition is violated. Results on posterior model consistency and estimation consistency, and an extension to parameter estimation in the generalized linear models are provided.",WOS:000293716500015,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'GENERALIZED LINEAR-MODELS', 'ELASTIC-NET', 'LOGISTIC-REGRESSION', 'COORDINATE DESCENT', 'ORACLE PROPERTIES', 'DANTZIG SELECTOR', 'LASSO', 'REGULARIZATION', 'CLASSIFICATION']",A MAJORIZATION-MINIMIZATION APPROACH TO VARIABLE SELECTION USING SPIKE AND SLAB PRIORS,2011
2071,"Higher criticism is a method for detecting signals that are both sparse and weak. Although first proposed in cases where the noise variables are independent, higher criticism also has reasonable performance in settings where those variables are correlated. In this paper we show that, by exploiting the nature of the correlation, performance can be improved by using a modified approach which exploits the potential advantages that correlation has to offer. Indeed, it turns out that the case of independent noise is the most difficult of all, from a statistical viewpoint, and that more accurate signal detection (for a given level of signal sparsity and strength) can be obtained when correlation is present. We characterize the advantages of correlation by showing how to incorporate them into the definition of an optimal detection boundary. The boundary has particularly attractive properties when correlation decays at a polynomial rate or the correlation matrix is Toeplitz.",WOS:000277471000015,ANNALS OF STATISTICS,"['FALSE DISCOVERY RATE', 'INFINITE MATRICES', 'FEATURE-SELECTION', 'TEST STATISTICS', 'USEFUL FEATURES', 'MULTIPLE', 'DEPENDENCE', 'MIXTURES', 'RARE', 'WEAK']",INNOVATED HIGHER CRITICISM FOR DETECTING SPARSE SIGNALS IN CORRELATED NOISE,2010
2072,"MNP is a publicly available R package that fits the Bayesian multinomial probit model via Markov chain Monte Carlo. The multinomial probit model is often used to analyze the discrete choices made by individuals recorded in survey data. Examples where the multinomial probit model may be useful include the analysis of product choice by consumers in market research and the analysis of candidate or party choice by voters in electoral studies. The MNP software can also fit the model with different choice sets for each individual, and complete or partial individual choice orderings of the available alternatives from the choice set. The estimation is based on the efficient marginal data augmentation algorithm that is developed by Imai and van Dyk (2005).",WOS:000232890600001,JOURNAL OF STATISTICAL SOFTWARE,['BAYESIAN-ANALYSIS'],MNP: R package for fitting the multinomial probit model,2005
2073,"A new approach to structural equation modeling based on so-called extended redundancy analysis has been recently proposed in the literature, enhanced with the added characteristic of generalizing redundancy analysis and reduced-rank regression models for more than two blocks. In this approach, the relationships between the observed exogenous variables and the observed endogenous variables are moderated by the presence of unob-servable composites that were estimated as linear combinations of exogenous variables, permitting a great flexibility to specify and fit a variety of structural relationships. In this paper, we propose the SAS macro %ERA to specify and fit structural relationships in the extended redundancy analysis (ERA) framework. Two examples (simulation and real data) are provided in order to reproduce results appearing in the original article where ERA was proposed.",WOS:000392512800001,JOURNAL OF STATISTICAL SOFTWARE,"['LEAST-SQUARES', 'REGRESSION']",%ERA: A SAS Macro for Extended Redundancy Analysis,2016
2074,"We develop a new Gibbs sampler for a linear mixed model with a Dirichlet process random effect term, which is easily extended to a generalized linear mixed model with a probit link function. Our Gibbs sampler exploits the properties of the multinomial and Dirichlet distributions, and is shown to be an improvement, in terms of operator norm and efficiency, over other commonly used MCMC algorithms. We also investigate methods for the estimation of the precision parameter of the Dirichlet process, finding that maximum likelihood may not be desirable, but a posterior mode is a reasonable approach. Examples are given to show how these models perform on real data. Our results complement both the theoretical basis of the Dirichlet process nonparametric prior and the Computational work that has been done to date.",WOS:000275510800014,ANNALS OF STATISTICS,"['BAYESIAN NONPARAMETRIC-ESTIMATION', 'GENERALIZED LINEAR-MODELS', 'PRODUCT PARTITION MODELS', 'INFERENCE', 'DISTRIBUTIONS', 'ALGORITHMS', 'MIXTURES', 'BINARY', 'PRIORS']",ESTIMATION IN DIRICHLET RANDOM EFFECTS MODELS,2010
2075,"The R package gsbDesign provides functions to evaluate the operating characteristics of Bayesian group sequential clinical trial designs. More specifically, we consider clinical trials with interim analyses, which compare a treatment with a control, and where the endpoint is normally distributed. Prior information can either be specified for the difference of treatment and control, or separately for the effects in the treatment and the control groups. At each interim analysis, the decision to stop or continue the trial is based on the posterior distribution of the difference between treatment and control. The decision at the final analysis is also based on this posterior distribution. Multiple success and/or futility criteria can be specified to reflect adequately medical decision-making. We describe methods to evaluate the operating characteristics of such designs for scenarios corresponding to different true treatment and control effects. The characteristics of main interest are the probabilities of success and futility at each interim analysis, and the expected sample size. We illustrate the use of gsbDesign with a detailed case study.",WOS:000373918800001,JOURNAL OF STATISTICAL SOFTWARE,"['HISTORICAL CONTROL INFORMATION', 'CLINICAL-TRIAL DESIGNS', 'PROOF-OF-CONCEPT']",gsbDesign: An R Package for Evaluating the Operating Characteristics of a Group Sequential Bayesian Design,2016
2076,"A statistical functional, such as the mean or the median, is called elicitable if there is a scoring function or loss function such that the correct forecast of the functional is the unique minimizer of the expected score. Such scoring functions are called strictly consistent for the functional. The elicitability of a functional opens the possibility to compare competing forecasts and to rank them in terms of their realized scores. In this paper, we explore the notion of elicitability for multi-dimensional functionals and give both necessary and sufficient conditions for strictly consistent scoring functions. We cover the case of functionals with elicitable components, but we also show that one-dimensional functionals that are not elicitable can be a component of a higher order elicitable functional. In the case of the variance, this is a known result. However, an important result of this paper is that spectral risk measures with a spectral measure with finite support are jointly elicitable if one adds the ""correct"" quantiles. A direct consequence of applied interest is that the pair (Value at Risk, Expected Shortfall) is jointly elicitable under mild conditions that are usually fulfilled in risk management applications.",WOS:000379972900011,ANNALS OF STATISTICS,"['INVARIANT RISK MEASURES', 'ROBUSTNESS', 'FUNCTIONALS', 'INFORMATION']",HIGHER ORDER ELICITABILITY AND OSBAND'S PRINCIPLE,2016
2077,"Collinearity and near-collinearity of predictors cause difficulties when doing regression. In these cases, variable selection becomes untenable because of mathematical issues concerning the existence and numerical stability of the regression coefficients, and interpretation of the coefficients is ambiguous because gradients are not defined. Using a differential geometric interpretation, in which the regression coefficients are interpreted as estimates of the exterior derivative of a function, we develop a new method to do regression in the presence of collinearities. Our regularization scheme can improve estimation error, and it can be easily modified to include lasso-type regularization. These estimators also have simple extensions to the ""large p, small n"" context.",WOS:000288183800002,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'LEAST-SQUARES REGRESSION', 'PERIOD-COHORT ANALYSIS', 'NONPARAMETRIC REGRESSION', 'GEOMETRIC FRAMEWORK', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'MODEL SELECTION', 'LASSO', 'REGULARIZATION']",REGRESSION ON MANIFOLDS: ESTIMATION OF THE EXTERIOR DERIVATIVE,2011
2078,"It is common practice in statistical data analysis to perform data-driven variable selection and derive statistical inference from the resulting model. Such inference enjoys none of the guarantees that classical statistical theory provides for tests and confidence intervals when the model has been chosen a priori. We propose to produce valid ""post-selection inference"" by reducing the problem to one of simultaneous inference and hence suitably widening conventional confidence and retention intervals. Simultaneity is required for all linear functions that arise as coefficient estimates in all submodels. By purchasing ""simultaneity insurance"" for all possible submodels, the resulting post-selection inference is rendered universally valid under all possible model selection procedures. This inference is therefore generally conservative for particular selection procedures, but it is always less conservative than full Scheffe protection. Importantly it does not depend on the truth of the selected submodel, and hence it produces valid inference even in wrong models. We describe the structure of the simultaneous inference problem and give some asymptotic results.",WOS:000320488200015,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD ESTIMATORS', 'MODEL-SELECTION', 'CONFIDENCE-INTERVALS', 'GAUSSIAN REGRESSION', 'CONDITIONAL LEVEL', 'PROPERTY', 'STUDENTS', 'LASSO']",VALID POST-SELECTION INFERENCE,2013
2079,"We study the problem of variable selection in convex nonparametric regression. Under the assumption that the true regression function is convex and sparse, we develop a screening procedure to select a subset of variables that contains the relevant variables. Our approach is a two-stage quadratic programming method that estimates a sum of one-dimensional convex functions, followed by one-dimensional concave regression fits on the residuals. In contrast to previous methods for sparse additive models, the optimization is finite dimensional and requires no tuning parameters for smoothness. Under appropriate assumptions, we prove that the procedure is faithful in the population setting, yielding no false negatives. We give a finite sample statistical analysis, and introduce algorithms for efficiently carrying out the required quadratic programs. The approach leads to computational and statistical advantages over fitting a full model, and provides an effective, practical approach to variable screening in convex regression.",WOS:000389620800014,ANNALS OF STATISTICS,"['SUPPORT-LINE MEASUREMENTS', 'NONPARAMETRIC REGRESSION', 'ADDITIVE-MODELS', 'SELECTION', 'CONSISTENCY', 'SPARSITY']",FAITHFUL VARIABLE SCREENING FOR HIGH-DIMENSIONAL CONVEX REGRESSION,2016
2080,"We consider the first serial correlation coefficient under an AR(1) model where errors are not assumed to be Gaussian. In this case it is necessary to consider bootstrap approximations for tests based on the statistic since the distribution of errors is unknown. We obtain saddle-point approximations for tail probabilities of the statistic and its bootstrap version and use these to show that the bootstrap tail probabilities approximate the true values with given relative errors, thus extending the classical results of Daniels [Biometrika 43 (1956) 169-185] for the Gaussian case. The methods require conditioning on the set of odd numbered observations and suggest a conditional bootstrap which we show has similar relative error properties.",WOS:000320488200023,ANNALS OF STATISTICS,"['SADDLEPOINT APPROXIMATION', 'EDGEWORTH']",RELATIVE ERRORS FOR BOOTSTRAP APPROXIMATIONS OF THE SERIAL CORRELATION COEFFICIENT,2013
2081,"When designing a sampling survey, usually constraints are set on the desired precision levels regarding one or more target estimates (the Y's). If a sampling frame is available, containing auxiliary information related to each unit (the X's), it is possible to adopt a stratified sample design. For any given stratification of the frame, in the multivariate case it is possible to solve the problem of the best allocation of units in strata, by minimizing a cost function subject to precision constraints (or, conversely, by maximizing the precision of the estimates under a given budget). The problem is to determine the best stratification in the frame, i.e., the one that ensures the overall minimal cost of the sample necessary to satisfy precision constraints. The X's can be categorical or continuous; continuous ones can be transformed into categorical ones. The most detailed stratification is given by the Cartesian product of the X's (the atomic strata). A way to determine the best stratification is to explore exhaustively the set of all possible partitions derivable by the set of atomic strata, evaluating each one by calculating the corresponding cost in terms of the sample required to satisfy precision constraints. This is unaffordable in practical situations, where the dimension of the space of the partitions can be very high. Another possible way is to explore the space of partitions with an algorithm that is particularly suitable in such situations: the genetic algorithm. The R package SamplingStrata, based on the use of a genetic algorithm, allows to determine the best stratification for a population frame, i.e., the one that ensures the minimum sample cost necessary to satisfy precision constraints, in a multivariate and multi-domain case.",WOS:000349840700001,JOURNAL OF STATISTICAL SOFTWARE,['GENETIC ALGORITHM'],SamplingStrata: An R Package for the Optimization of Stratified Sampling,2014
2082,"After its introduction by Koenker and Basset (1978), quantile regression has become an important and popular tool to investigate the conditional response distribution in regression. The R package bayesQR contains a number of routines to estimate quantile regression parameters using a Bayesian approach based on the asymmetric Laplace distribution. The package contains functions for the typical quantile regression with continuous dependent variable, but also supports quantile regression for binary dependent variables. For both types of dependent variables, an approach to variable selection using the adaptive lasso approach is provided. For the binary quantile regression model, the package also contains a routine that calculates the fitted probabilities for each vector of predictors. In addition, functions for summarizing the results, creating traceplots, posterior histograms and drawing quantile plots are included. This paper starts with a brief overview of the theoretical background of the models used in the bayesQR package. The main part of this paper discusses the computational problems that arise in the implementation of the procedure and illustrates the usefulness of the package through selected examples.",WOS:000392706900001,JOURNAL OF STATISTICAL SOFTWARE,"['MEDIAN REGRESSION', 'ADAPTIVE LASSO', 'BINARY', 'LIKELIHOOD', 'INFERENCE', 'MODEL']",bayesQR: A Bayesian Approach to Quantile Regression,2017
2083,"The R package groc for generalized regression on orthogonal components contains functions for the prediction of q responses using a set of p predictors. The primary building block is the grid algorithm used to search for components(projections of the data) which are most dependent on the response. The package offers flexibility in the choice of the dependence measure which can be user-de fined. The components are found sequentially. A first component is obtained and a smooth fit produces residuals. Then, a second component orthogonal to the first is found which is most dependent on the residuals, and so on. The package can handle models with more than one response. A panoply of models can be achieved through package groc : robust multiple or multivariate linear regression, nonparametric regression on orthogonal components, and classical or robust partial least squares models. Functions for predictions and cross-validation are available and helpful in model selection. The merit of a fit through cross-validation can be assessed with the predicted residual error sum of squares or the predicted residual error median absolute deviation which is more appropriate in the presence of outliers.",WOS:000365973600001,JOURNAL OF STATISTICAL SOFTWARE,"['PARTIAL LEAST-SQUARES', 'PROJECTION']",The R Package groc for Generalized Regression on Orthogonal Components,2015
2084,"II is a complete R package for multiple imputation of missing data. The package implements a new expectation-maximization with bootstrapping algorithm that works faster, with larger numbers of variables, and is far easier to use, than various Markov chain Monte Carlo approaches, but gives essentially the same answers. The program also improves imputation models by allowing researchers to put Bayesian priors on individual cell values, thereby including a great deal of potentially valuable and extensive information. It also includes features to accurately impute cross-sectional datasets, individual time series, or sets of time series for different cross-sections. A full set of graphical diagnostics are also available. The program is easy to use, and the simplicity of the algorithm makes it far more robust; both a simple command line and extensive graphical user interface are included",WOS:000298032900001,JOURNAL OF STATISTICAL SOFTWARE,"['MULTIPLE IMPUTATION', 'BOOTSTRAP', 'ALGORITHM']",Amelia II: A Program for Missing Data,2011
2085,"XML2R is a framework that reduces the effort required to transform XML content into tables in a way that preserves parent to child relationships. pitchRx applies XML2R's grammar for XML manipulation to Major League Baseball Advanced Media (MLBAM)'s Gameday data. With pitchRx, one can easily obtain and store Gameday data in a remote database. The Gameday website hosts a wealth of XML data, but perhaps most interesting is PITCHf/x. Among other things, PITCHf/x data can be used to recreate a baseball's flight path from a pitcher's hand to home plate. With pitchRx, one can easily create animations and interactive 3D scatterplots of the baseball's flight path. PITCHf/x data is also commonly used to generate a static plot of baseball locations at the moment they cross home plate. These plots, sometimes called strike-zone plots, can also refer to a plot of event probabilities over the same region. pitchRx provides an easy and robust way to generate strike-zone plots using the ggplot2 package.",WOS:000343788100002,R JOURNAL,,Taming PITCHf/x Data with XML2R and pitchRx,2014
2086,"We study the properties of false discovery rate (FDR) thresholding, viewed as a classification procedure. The ""0""-class (null) is assumed to have a known density while the ""1""-class (alternative) is obtained from the ""0""-class either by translation or by scaling. Furthermore, the ""1""-class is assumed to have a small number of elements w.r.t. the ""0""-class (sparsity). We focus on densities of the Subbotin family, including Gaussian and Laplace models. Nonasymptotic oracle inequalities are derived for the excess risk of FDR thresholding. These inequalities lead to explicit rates of convergence of the excess risk to zero, as the number m of items to be classified tends to infinity and in a regime where the power of the Bayes rule is away from 0 and 1. Moreover, these theoretical investigations suggest an explicit choice for the target level am of FDR thresholding, as a function of m. Our oracle inequalities show theoretically that the resulting FDR thresholding adapts to the unknown sparsity regime contained in the data. This property is illustrated with numerical experiments.",WOS:000321844300008,ANNALS OF STATISTICS,"['MULTIPLE TESTING PROCEDURES', 'EMPIRICAL BAYES', 'FDR CONTROL', 'INDEPENDENCE', 'MICROARRAYS']",ON FALSE DISCOVERY RATE THRESHOLDING FOR CLASSIFICATION UNDER SPARSITY,2012
2087,"Ill-posed inverse problems arise in various scientific fields. We consider the signal detection problem for mildly, severely and extremely ill-posed inverse problems with l(q)-ellipsoids (bodies), q is an element of (0, 2], for Sobolev, analytic and generalized analytic classes of functions under the Gaussian white noise model. We study both rate and sharp asymptotics for the error probabilities in the minimax setup. By construction, the derived tests are, often, nonadaptive. Minimax rate-optimal adaptive tests of rather simple structure are also constructed.",WOS:000310650900009,ANNALS OF STATISTICS,['NOISE'],MINIMAX SIGNAL DETECTION IN ILL-POSED INVERSE PROBLEMS,2012
2088,"The PoweR package aims to help obtain or verify empirical power studies for goodness-of-fit tests for independent and identically distributed data. The current version of our package is only valid for simple null hypotheses or for pivotal test statistics for which the set of critical values does not depend on a particular choice of a null distribution (and on nuisance parameters) under the non-simple null case. We also assume that the distribution of the test statistic is continuous. As a reproducible research computational tool it can be viewed as helping to simply reproducing (or detecting errors in) simulation results already published in the literature. Using our package helps also in designing new simulation studies. The empirical levels and powers for many statistical test statistics under a wide variety of alternative distributions can be obtained quickly and accurately using a C/C++ and R environment. The parallel package can be used to parallelize computations when a multicore processor is available. The results can be displayed using LATEX tables or specialized graphs, which can be directly incorporated into a report. This article gives an overview of the main design aims and principles of our package, as well as strategies for adaptation and extension. Hands-on illustrations are presented to help new users in getting started.",WOS:000373917000001,JOURNAL OF STATISTICAL SOFTWARE,"['EMPIRICAL CHARACTERISTIC FUNCTION', 'LAPLACE DISTRIBUTION', 'OMNIBUS TEST', 'ORDER-STATISTICS', 'LIKELIHOOD-RATIO', 'VARIANCE TEST', 'P-VALUES', 'NORMALITY', 'DISTRIBUTIONS', 'ALTERNATIVES']",PoweR: A Reproducible Research Tool to Ease Monte Carlo Power Simulation Studies for Goodness-of-fit Tests in R,2016
2089,"We consider the random-design least-squares regression problem within the reproducing kernel Hilbert space (RKHS) framework. Given a stream of independent and identically distributed input/output data, we aim to learn a regression function within an RKHS H, even if the optimal predictor (i.e., the conditional expectation) is not in H. In a stochastic approximation framework where the estimator is updated after each observation, we show that the averaged unregularized least-mean-square algorithm (a form of stochastic gradient descent), given a sufficient large step-size, attains optimal rates of convergence for a variety of regimes for the smoothnesses of the optimal prediction function and the functions in H. Our results apply as well in the usual finite-dimensional setting of parametric least-squares regression, showing adaptivity of our estimator to the spectral decay of the covariance matrix of the covariates.",WOS:000379972900001,ANNALS OF STATISTICS,"['LEAST-SQUARES ALGORITHM', 'LEARNING-THEORY', 'KERNELS', 'ONLINE']",NONPARAMETRIC STOCHASTIC APPROXIMATION WITH LARGE STEP-SIZES,2016
2090,,WOS:000373913100001,JOURNAL OF STATISTICAL SOFTWARE,,"Statistical Data Analytics. Foundations for Data Mining, Informatics, and Knowledge Discovery",2016
2091,"Synergistic and antagonistic drug interactions are important to consider when developing mixtures of anticancer or other types of drugs. Boik, Newman, and Boik (2008) proposed the Mix Low method as an alternative to the Median-Effect method of Chou and Talalay (1984) for estimating drug interaction indices. One advantage of the Mix Low method is that then on linear mixed-effects model used to estimate parameters of concentration response curves can provide more accurate parameter estimates than the log linearization and least-squares analysis used in the Median-Effect method. This paper introduces the mix low package in R, an implementation of the Mix Low method. Results are reported for a small simulation study.",WOS:000276954600001,JOURNAL OF STATISTICAL SOFTWARE,['SYNERGISM'],An R Package for Assessing Drug Synergism/Antagonism,2010
2092,"Functional principal component analysis (FPCA) based on the Karhunen-Loeve decomposition has been successfully applied in many applications, mainly for one sample problems. In this paper we consider common functional principal components for two sample problems. Our research is motivated not only by the theoretical challenge of this data situation, but also by the actual question of dynamics of implied volatility (IV) functions. For different maturities the log-returns of IVs are samples of (smooth) random functions and the methods proposed here study the similarities of their stochastic behavior. First we present a new method for estimation of functional principal components from discrete noisy data. Next we present the two sample inference for FPCA and develop the two sample theory. We propose bootstrap tests for testing the equality of eigenvalues, eigenfunctions, and mean functions of two functional samples, illustrate the test-properties by simulation study and apply the method to the TV analysis.",WOS:000263129000001,ANNALS OF STATISTICS,"['LONGITUDINAL DATA', 'INFERENCE', 'CURVES']",COMMON FUNCTIONAL PRINCIPAL COMPONENTS,2009
2093,"We present new spatio-temporal geostatistical modelling and interpolation capabilities of the R package gstat. Various spatio-temporal covariance models have been implemented, such as the separable, product-sum, metric and sum-metric models. In a real-world application we compare spatio-temporal interpolations using these models with a purely spatial kriging approach. The target variable of the application is the daily mean PM10 concentration measured at rural air quality monitoring stations across Germany in 2005. R code for variogram fitting and interpolation is presented in this paper to illustrate the workflow of spatio-temporal interpolation using gstat. We conclude that the system works properly and that the extension of gstat facilitates and eases spatio-temporal geostatistical modelling and prediction for R users.",WOS:000385276100015,R JOURNAL,"['SOIL-WATER', 'PACKAGE']",Spatio-Temporal Interpolation using gstat,2016
2094,"Consider a parametrized family of general hidden Markov models, where both the observed and unobserved components take values in a complete separable metric space. We prove that the maximum likelihood estimator (MLE) of the parameter is strongly consistent under a rather minimal set of assumptions. As special cases of our main result, we obtain consistency in a large class of nonlinear state space models, as well as general results on linear Gaussian state space models and finite state models.
A novel aspect of our approach is an information-theoretic technique for proving identifiability, which does not require an explicit representation for the relative entropy rate. Our method of proof could therefore form a foundation for the investigation of MLE consistency in more general dependent and non-Markovian time series. Also of independent interest is a general concentration inequality for V-uniformly ergodic Markov chains.",WOS:000288183800016,ANNALS OF STATISTICS,"['STATE-SPACE-MODELS', 'PROBABILISTIC FUNCTIONS', 'GEOMETRIC ERGODICITY', 'CHAINS', 'INEQUALITY']",CONSISTENCY OF THE MAXIMUM LIKELIHOOD ESTIMATOR FOR GENERAL HIDDEN MARKOV MODELS,2011
2095,,WOS:000255795100001,JOURNAL OF STATISTICAL SOFTWARE,['FACTORIALS'],Sas Macros for Analysis of Unreplicated 2(K) and 2(K-P) Designs with a possible outlier,2008
2096,"Estimating the eigenvalues of a population covariance matrix from a sample covariance matrix is a problem of fundamental importance in multivariate statistics; the eigenvalues of covariance matrices play a key role in many widely used techniques, in particular in principal component analysis (PCA). In many modern data analysis problems, statisticians are faced with large datasets where the sample size, n, is of the same order of magnitude as the number of variables p. Random matrix theory predicts that in this context, the eigenvalues of the sample covariance matrix are not good estimators of the eigenvalues of the population covariance.
We propose to use a fundamental result in random matrix theory, the Marcenko-Pastur equation, to better estimate the eigenvalues of large dimensional covariance matrices. The Marcenko-Pastur equation holds in very wide generality and under weak assumptions. The estimator we obtain can be thought of as ""shrinking"" in a nonlinear fashion the eigenvalues of the sample covariance matrix to estimate the population eigenvalues. Inspired by ideas of random matrix theory, we also suggest a change of point of view when thinking about estimation of high-dimensional vectors: we do not try to estimate directly the vectors but rather a probability measure that describes them. We think this is a theoretically more fruitful way to think about these problems.
Our estimator gives fast and good or very good results in extended simulations. Our algorithmic approach is based on convex optimization. We also show that the proposed estimator is consistent.",WOS:000262731400007,ANNALS OF STATISTICS,"['LARGEST EIGENVALUE', 'EMPIRICAL DISTRIBUTION', 'LIMIT', 'NOISE']",SPECTRUM ESTIMATION FOR LARGE DIMENSIONAL COVARIANCE MATRICES USING RANDOM MATRIX THEORY,2008
2097,,WOS:000375175200016,ANNALS OF STATISTICS,,"LIMIT THEOREMS FOR EMPIRICAL PROCESSES OF CLUSTER FUNCTIONALS (vol 38, pg 2145, 2010)",2016
2098,"We consider the parameter estimation of Markov chain when the unknown transition matrix belongs to an exponential family of transition matrices. Then we show that the sample mean of the generator of the exponential family is an asymptotically efficient estimator. Further, we also define a curved exponential family of transition matrices. Using a transition matrix version of the Pythagorean theorem, we give an asymptotically efficient estimator for a curved exponential family.",WOS:000379972900005,ANNALS OF STATISTICS,"['EXPONENTIAL-FAMILIES', 'MONTE-CARLO', 'STOCHASTIC-PROCESSES', 'BOUNDS', 'ERROR', 'MCMC', 'THEOREM']",INFORMATION GEOMETRY APPROACH TO PARAMETER ESTIMATION IN MARKOV CHAINS,2016
2099,"Consider a linear model Y = X beta +z, where X = X-n,X-p and z similar to N(0, In). The vector beta is unknown but is sparse in the sense that most of its coordinates are 0. The main interest is to separate its nonzero coordinates from the zero ones (i.e., variable selection). Motivated by examples in long-memory time series (Fan and Yao [Nonlinear Time Series: Nonparametric and Parametric Methods (2003) Springer]) and the change-point problem (Bhattacharya [In Change-Point Problems (South Hadley, MA, 1992) (1994) 28-56 IMS]), we are primarily interested in the case where the Gram matrix G = X' X is non-sparse but sparsifiable by a finite order linear filter. We focus on the regime where signals are both rare and weak so that successful variable selection is very challenging but is still possible.
We approach this problem by a new procedure called the covariate assisted screening and estimation (CASE). CASE first uses a linear filtering to reduce the original setting to a new regression model where the corresponding Gram (covariance) matrix is sparse. The new covariance matrix induces a sparse graph, which guides us to conduct multivariate screening without visiting all the submodels. By interacting with the signal sparsity, the graph enables us to decompose the original problem into many separated small size subproblems (if only we know where they are!). Linear filtering also induces a so-called problem of information leakage, which can be overcome by the newly introduced patching technique. Together, these give rise to CASE, which is a two-stage screen and clean [Fan and Song Ann. Statist. 38 (2010) 3567-3604; Wasserman and Roeder Ann. Statist. 37 (2009) 2178-2201] procedure, where we first identify candidates of these submodels by patching and screening, and then re-examine each candidate to remove false positives.
For any procedure (beta) over cap for variable selection, we measure the performance by the minimax Hamming distance between the sign vectors of (beta) over cap and beta. We show that in a broad class of situations where the Gram matrix is nonsparse but sparsifiable, CASE achieves the optimal rate of convergence. The results are successfully applied to long-memory time series and the change-point model.",WOS:000345884900003,ANNALS OF STATISTICS,"['DIMENSIONAL VARIABLE SELECTION', 'LONG-RANGE DEPENDENCE', 'UNCERTAINTY PRINCIPLES', 'ORACLE PROPERTIES', 'MODEL SELECTION', 'TIME-SERIES', 'LASSO', 'REGRESSION', 'OPTIMALITY', 'PENALTY']",COVARIATE ASSISTED SCREENING AND ESTIMATION,2014
2100,"A low-degree polynomial model for a response curve is used commonly in practice. It generally incorporates a linear or quadratic function of the covariate. In this paper we suggest methods for testing the goodness of fit of a general polynomial model when there are errors in the covariates. There, the true covariates are not directly observed, and conventional bootstrap methods for testing are not applicable. We develop a new approach, in which deconvolution methods are used to estimate the distribution of the covariates under the null hypothesis, and a ""wild"" or moment-matching bootstrap argument is employed to estimate the distribution of the experimental errors (distinct from the distribution of the errors in covariates). Most of our attention is directed at the case where the distribution of the errors in covariates is known, although we also discuss methods for estimation and testing when the covariate error distribution is estimated. No assumptions are made about the distribution of experimental error, and, in particular, we depart substantially from conventional parametric models for errors-in-variables problems.",WOS:000253077800017,ANNALS OF STATISTICS,"['WILD BOOTSTRAP', 'NONPARAMETRIC REGRESSION', 'OPTIMAL RATES', 'DENSITY', 'CONVERGENCE', 'ESTIMATORS', 'SAMPLE']",Testing the suitability of polynomial models in errors-in-variables problems,2007
2101,"dglars is a publicly available R package that implements the method proposed in Augugliaro, Mineo, and Wit (2013), developed to study the sparse structure of a generalized linear model. This method, called dgLARS, is based on a differential geometrical extension of the least angle regression method proposed in Efron, Hastie, Johnstone, and Tibshirani (2004). The core of the dglars package consists of two algorithms implemented in Fortran 90 to efficiently compute the solution curve: a predictor-corrector algorithm, proposed in Augugliaro et al. (2013), and a cyclic coordinate descent algorithm, proposed in Augugliaro, Mineo, and Wit (2012). The latter algorithm, as shown here, is significantly faster than the predictor-corrector algorithm. For comparison purposes, we have implemented both algorithms.",WOS:000341806500001,JOURNAL OF STATISTICAL SOFTWARE,"['LEAST ANGLE REGRESSION', 'BREAST-CANCER RISK', 'VARIABLE SELECTION', 'DANTZIG SELECTOR', 'EXPRESSION', 'MARKER', 'HAPLOTYPES', 'LIKELIHOOD', 'SHRINKAGE', 'TISSUES']",dglars: An R Package to Estimate Sparse Generalized Linear Models,2014
2102,"This paper studies the asymptotic power of tests of sphericity against perturbations in a single unknown direction as both the dimensionality of the data and the number of observations go to infinity. We establish the convergence, under the null hypothesis and contiguous alternatives, of the log ratio of the joint densities of the sample covariance eigenvalues to a Gaussian process indexed by the norm of the perturbation. When the perturbation norm is larger than the phase transition threshold studied in Baik, Ben Arous and Peche [Ann. Probab. 33 (2005) 1643-1697] the limiting process is degenerate, and discrimination between the null and the alternative is asymptotically certain. When the norm is below the threshold, the limiting process is nondegenerate, and the joint eigenvalue densities under the null and alternative hypotheses are mutually contiguous. Using the asymptotic theory of statistical experiments, we obtain asymptotic power envelopes and derive the asymptotic power for various sphericity tests in the contiguity region. In particular, we show that the asymptotic power of the Tracy-Widom-type tests is trivial (i.e., equals the asymptotic size), whereas that of the eigenvalue-based likelihood ratio test is strictly larger than the size, and close to the power envelope.",WOS:000321847600006,ANNALS OF STATISTICS,"['SAMPLE COVARIANCE MATRICES', 'MULTIPLE HYPERGEOMETRIC-FUNCTIONS', 'EIGENVALUE BASED DETECTION', 'EMPIRICAL DISTRIBUTION', 'DISTRIBUTIONS', 'NUMBER', 'SIGNALS', 'MODELS', 'COMPONENTS', 'WISHART']",ASYMPTOTIC POWER OF SPHERICITY TESTS FOR HIGH-DIMENSIONAL DATA,2013
2103,"A simple temporal point process (SPP) is an important class of time series, where the sample realization of the process is solely composed of the times at which events occur. Particular examples of point process data are neuronal spike patterns or spike trains, and a large number of distance and similarity metrics for those data have been proposed. A marked point process (MPP) is an extension of a simple temporal point process, in which a certain vector valued mark is associated with each of the temporal points in the SPP. Analyses of MPPs are of practical importance because instances of MPPs include recordings of natural disasters such as earthquakes and tornadoes. In this paper, we introduce the R package mmpp, which implements a number of distance and similarity metrics for SPPs, and also extends those metrics for dealing with MPPs.",WOS:000368551800018,R JOURNAL,"['R PACKAGE', 'EARTHQUAKE OCCURRENCES', 'MODELS', 'PATTERNS', 'TIME']",mmpp: A Package for Calculating Similarity and Distance Metrics for Simple and Marked Temporal Point Processes,2015
2104,"Empirical best linear unbiased prediction (EBLUP) method uses a linear mixed model in combining information from different sources of information. This method is particularly useful in small area problems. The variability of an EBLUP is traditionally measured by the mean squared prediction error (MSPE), and interval estimates are generally constructed using estimates of the MSPE. Such methods have shortcomings like under-coverage or over-coverage, excessive length and lack of interpretability. We propose a parametric bootstrap approach to estimate the entire distribution of a suitably centered and scaled EBLUP. The bootstrap histogram is highly accurate, and differs from the true EBLUP distribution by only O(d(3)n(-3/2)), where d is the number of parameters and n the number of observations. This result is used to obtain highly accurate prediction intervals. Simulation results demonstrate the superiority of this method over existing techniques of constructing prediction intervals in linear mixed models.",WOS:000256504400008,ANNALS OF STATISTICS,"['BAYES CONFIDENCE-INTERVALS', 'SMALL-AREA ESTIMATORS', 'MEAN SQUARED ERROR', 'ASYMPTOTIC PROPERTIES', 'DENSITY-FUNCTIONS', 'STATISTICS', 'REGIONS', 'FIT']",Parametric bootstrap approximation to the distribution of EBLUP and related prediction intervals in linear mixed models,2008
2105,"In this paper, we study the asymptotic posterior distribution of linear functionals of the density by deriving general conditions to obtain a semiparametric version of the Bernstein-von Mises theorem. The special case of the cumulative distributive function, evaluated at a specific point, is widely considered. In particular, we show that for infinite-dimensional exponential families, under quite general assumptions, the asymptotic posterior distribution of the functional can be either Gaussian or a mixture of Gaussian distributions with different centering points. This illustrates the positive, but also the negative, phenomena that can occur in the study of Bernstein-von Mises results.",WOS:000310650900008,ANNALS OF STATISTICS,"['DIMENSIONAL EXPONENTIAL-FAMILIES', 'POSTERIOR DISTRIBUTIONS', 'CONVERGENCE-RATES', 'ASYMPTOTIC NORMALITY', 'MODEL', 'PARAMETERS']",BERNSTEIN-VON MISES THEOREM FOR LINEAR FUNCTIONALS OF THE DENSITY,2012
2106,"This paper offers a new approach to modeling and forecasting of nonstationary time series with applications to volatility modeling for financial data. The approach is based on the assumption of local homogeneity: for every time point, there exists a historical interval of homogeneity, in which the volatility parameter can be well approximated by a constant. The proposed procedure recovers this interval from the data using the local change point (LCP) analysis. Afterward, the estimate of the volatility can be simply obtained by local averaging. The approach carefully addresses the question of choosing the tuning parameters of the procedure using the so-called ""propagation"" condition. The main result claims a new ""oracle"" inequality in terms of the modeling bias which measures the quality of the local constant approximation. This result yields the optimal rate of estimation for smooth and piecewise constant volatility functions. Then, the new procedure is applied to some data sets and a comparison with a standard GARCH model is also provided. Finally, we discuss applications of the new method to the Value at Risk problem. The numerical results demonstrate a very reasonable performance of the new method.",WOS:000265619700011,ANNALS OF STATISTICS,"['AUTOREGRESSIVE CONDITIONAL HETEROSKEDASTICITY', 'STATISTICAL-INFERENCE', 'LIKELIHOOD-ESTIMATION', 'TIME', 'MODELS']",MULTISCALE LOCAL CHANGE POINT DETECTION WITH APPLICATIONS TO VALUE-AT-RISK,2009
2107,"Ecological research uses data collection techniques that are prone to substantial and unique types of measurement error to address scientific questions about species abundance and distribution. These data collection schemes include a number of survey methods in which unmarked individuals are counted, or determined to be present, at spatially-referenced sites. Examples include site occupancy sampling, repeated counts, distance sampling, removal sampling, and double observer sampling. To appropriately analyze these data, hierarchical models have been developed to separately model explanatory variables of both a latent abundance or occurrence process and a conditional detection process. Because these models have a straightforward interpretation paralleling mechanisms under which the data arose, they have recently gained immense popularity. The common hierarchical structure of these models is well-suited for a unified modeling interface. The R package unmarked provides such a unified modeling framework, including tools for data exploration, model fitting, model criticism, post-hoc analysis, and model comparison.",WOS:000294232000001,JOURNAL OF STATISTICAL SOFTWARE,"['ESTIMATING SITE OCCUPANCY', 'REPLICATED COUNTS', 'MIXTURE-MODELS', 'POPULATION', 'EXTINCTION']",Unmarked: An R Package for Fitting Hierarchical Models of Wildlife Occurrence and Abundance,2011
2108,"We study the Cox models with semiparametric relative risk, which can be partially linear with one nonparametric component, or multiple additive or nonadditive nonparametric components. A penalized partial likelihood procedure is proposed to simultaneously estimate the parameters and select variables for both the parametric and the nonparametric parts. Two penalties are applied sequentially. The first penalty, governing the smoothness of the multivariate nonlinear covariate effect function, provides a smoothing spline ANOVA framework that is exploited to derive an empirical model selection tool for the nonparametric part. The second penalty, either the smoothly-clipped-absolute-deviation (SCAD) penalty or the adaptive LASSO penalty, achieves variable selection in the parametric part. We show that the resulting estimator of the parametric part possesses the oracle property, and that the estimator of the nonparametric part achieves the optimal rate of convergence. The proposed procedures are shown to work well in simulation experiments, and then applied to a real data example on sexually transmitted diseases.",WOS:000280359400006,ANNALS OF STATISTICS,"['PROPORTIONAL HAZARDS REGRESSION', 'PARTIAL LIKELIHOOD', 'ORACLE PROPERTIES', 'LASSO']",PENALIZED VARIABLE SELECTION PROCEDURE FOR COX MODELS WITH SEMIPARAMETRIC RELATIVE RISK,2010
2109,"We find limiting distributions of the nonparametric maximum likelihood estimator (MLE) of a log-concave density, that is, a density of the form f(0) = exp phi(0) where phi(0) is a concave function on R. The pointwise limiting distributions depend on the second and third derivatives at 0 of H-k, the ""lower invelope"" of an integrated Brownian motion process minus a drift term depending on the number of vanishing derivatives of phi(0) = log f(0) at the point of interest. We also establish the limiting distribution of the resulting estimator of the mode M(f(0)) and establish a new local asymptotic minimax lower bound which shows the optimality Of Our mode estimator in terms of both rate of convergence and dependence of constants on Population values.",WOS:000265619700008,ANNALS OF STATISTICS,"['SYMMETRIC UNIMODAL DENSITY', 'ASYMPTOTIC DISTRIBUTIONS', 'SMOOTHNESS ASSUMPTIONS', 'KERNEL ESTIMATORS', 'MODE ESTIMATION', 'RESTRICTIONS', 'PROBABILITY']",LIMIT DISTRIBUTION THEORY FOR MAXIMUM LIKELIHOOD ESTIMATION OF A LOG-CONCAVE DENSITY,2009
2110,"Ecologists are concerned with the relationships between species composition and environmental factors, and with spatial structure within those relationships. A dissimilarity-based framework incorporating space explicitly is an extremely flexible tool for answering these questions. The R package ecodist brings together methods for working with dissimilarities, including some not available in other R packages. We present some of the features of ecodist, particularly simple and partial Mantel tests, and make recommendations for their effective use. Although the partial Mantel test is often used to account for the effects of space, the assumption of linearity greatly reduces its effectiveness for complex spatial patterns. We introduce a modification of the Mantel correlogram designed to overcome this restriction and allow consideration of complex nonlinear structures. This extension of the method allows the use of partial multivariate correlograms and tests of relationship between variables at different spatial scales. Some of the possibilities are demonstrated using both artificial data and data from an ongoing study of plant community composition in grazinglands of the northeastern United States.",WOS:000252430100001,JOURNAL OF STATISTICAL SOFTWARE,"['PARTIAL MANTEL TESTS', 'PERMUTATION TESTS', 'COMMUNITY ECOLOGY', 'DISTANCE MATRICES', 'REGRESSION', 'AUTOCORRELATION', 'MODEL']",The ecodist package for dissimilarity-based analysis of ecological data,2007
2111,"The web-based, Java-written SOCR (Statistical Online Computational Resource) tools have been utilized in amy undergraduate and graduate level statistics courses for seven years now (Dinov 2006; Dinov et al. 2008b). It has been proven that these resources can successfully improve students' learning (Dinov et al. 2008b). Being first published online in 2005, SOCR Analyses is a somewhat new component and it concentrate on data modeling for both parametric and non-parametric data analyses with graphical model diagnostics. One of the main purposes of SOCR Analyses is to facilitate statistical learning for high school and undergraduate students. As we have already implemented SOCR Distributions and Experiments, SOCR Analyses and Charts fulfill the rest of a standard statistics curricula. Currently, there are four core components of SOCR Analyses. Linear models included in SOCR Analyses are simple linear regression, multiple linear regression, one-way and two-way ANOVA. Tests for sample comparisons include t-test in the parametric category. Some examples of SOCR Analyses' in the non-parametric category are Wilcoxon rank sum test, Kruskal-Wallis test, Friedman's test, Kolmogorov-Smirnoff test and Fligner-Killeen test. Hypothesis testing models include contingency table, Friedan's test and Fisher's exact test. The last component of Analysis is a utility for computing sample sizes for normal distribution. In this article, we present the design framework, computational implementation and the utilization of SOCR Analyses.",WOS:000266310500001,JOURNAL OF STATISTICAL SOFTWARE,"['IMAGE REGISTRATION', 'WAVELET SHRINKAGE']",SOCR Analyses: Implementation and Demonstration of a New Graphical Statistics Educational Toolkit,2009
2112,"The receiver operating characteristic (ROC) curve, the positive predictive value (PPV) curve and the negative predictive value (NPV) curve are three measures of performance for a continuous diagnostic biomarker. The ROC, PPV and NPV curves are often estimated empirically to avoid assumptions about the distributional form of the biomarkers. Recently, there has been a push to incorporate group sequential methods into the design of diagnostic biomarker studies. A thorough understanding of the asymptotic properties of the sequential empirical ROC, PPV and NPV curves will provide more flexibility when designing group sequential diagnostic biomarker studies. In this paper, we derive asymptotic theory for the sequential empirical ROC, PPV and NPV curves under case-control sampling using sequential empirical process theory. We show that the sequential empirical ROC, PPV and NPV curves converge to the sum of independent Kiefer processes and show how these results can be used to derive asymptotic results for summaries of the sequential empirical ROC, PPV and NPV curves.",WOS:000311639700004,ANNALS OF STATISTICS,['ACCURACY'],"ASYMPTOTIC PROPERTIES OF THE SEQUENTIAL EMPIRICAL ROC, PPV AND NPV CURVES UNDER CASE-CONTROL SAMPLING",2011
2113,"A systematic study is carried out regarding universally optimal designs under the interference model, previously investigated by Kunert and Martin [Ann. Statist. 28 (2000) 1728-1742] and Kunert and Mersmann [J. Statist. Plann. Inference 141 (2011) 1623-1632]. Parallel results are also provided for the undirectional interference model, where the left and right neighbor effects are equal. It is further shown that the efficiency of any design under the latter model is at least its efficiency under the former model. Designs universally optimal for both models are also identified. Most importantly, this paper provides Kushner's type linear equations system as a necessary and sufficient condition for a design to be universally optimal. This result is novel for models with at least two sets of treatment-related nuisance parameters, which are left and right neighbor effects here. It sheds light on other models in deriving asymmetric optimal or efficient designs.",WOS:000352757100002,ANNALS OF STATISTICS,"['NEIGHBOR-BALANCED DESIGNS', 'OPTIMAL CROSSOVER DESIGNS', 'CORRELATED OBSERVATIONS', 'INTERPLOT INTERFERENCE', 'TRIALS', 'COMPETITION', 'ADJUSTMENT', 'SELF']",UNIVERSALLY OPTIMAL DESIGNS FOR TWO INTERFERENCE MODELS,2015
2114,"Principal component analysis (PCA) is a classical dimension reduction method which projects data onto the principal subspace spanned by the leading eigenvectors of the covariance matrix. However, it behaves poorly when the number of features p is comparable to, or even much larger than, the sample size n. In this paper, we propose a new iterative thresholding approach for estimating principal subspaces in the setting where the leading eigenvectors are sparse. Under a spiked covariance model, we find that the new approach recovers the principal subspace and leading eigenvectors consistently, and even optimally, in a range of high-dimensional sparse settings. Simulated examples also demonstrate its competitive performance.",WOS:000320488200014,ANNALS OF STATISTICS,"['HIGH DIMENSIONS', 'CONSISTENCY', 'PERTURBATION', 'ASYMPTOTICS']",SPARSE PRINCIPAL COMPONENT ANALYSIS AND ITERATIVE THRESHOLDING,2013
2115,"Manufacturing processes are often based on more than one quality characteristic. When these variables are correlated the process capability analysis should be performed using multivariate statistical methodologies. Although there is a growing interest in methods for evaluating the capability of multivariate processes, little attention has been given to developing user friendly software for supporting multivariate capability analysis. In this work we introduce the package MPCI for R, which allows to compute multivariate process capability indices. MPCI aims to provide a useful tool for dealing with multivariate capability assessment problems. We illustrate the use of MPCI package through both simulated and real examples.",WOS:000303804800001,JOURNAL OF STATISTICAL SOFTWARE,,MPCI: An R Package for Computing Multivariate Process Capability Indices,2012
2116,"Common high-dimensional methods for prediction rely on having either a sparse signal model, a model in which most parameters are zero and there are a small number of nonzero parameters that are large in magnitude, or a dense signal model, a model with no large parameters and very many small nonzero parameters. We consider a generalization of these two basic models, termed here a ""sparse + dense"" model, in which the signal is given by the sum of a sparse signal and a dense signal. Such a structure poses problems for traditional sparse estimators, such as the lasso, and for traditional dense estimation methods, such as ridge estimation. We propose a new penalization-based method, called lava, which is computationally efficient. With suitable choices of penalty parameters, the proposed method strictly dominates both lasso and ridge. We derive analytic expressions for the finite-sample risk function of the lava estimator in the Gaussian sequence model. We also provide a deviation bound for the prediction risk in the Gaussian regression model with fixed design. In both cases, we provide Stein's unbiased estimator for lava's prediction risk. A simulation example compares the performance of lava to lasso, ridge and elastic net in a regression example using data-dependent penalty parameters and illustrates lava's improved performance relative to these benchmarks.",WOS:000396804900002,ANNALS OF STATISTICS,"['LINEAR INVERSE PROBLEMS', 'SQUARE-ROOT LASSO', 'VARIABLE SELECTION', 'MODEL SELECTION', 'DANTZIG SELECTOR', 'M-ESTIMATORS', 'REGRESSION', 'FREEDOM', 'SMOOTHNESS', 'SHRINKAGE']",A LAVA ATTACK ON THE RECOVERY OF SUMS OF DENSE AND SPARSE SIGNALS,2017
2117,"The Rasch family of models considered in this paper includes models for polytomous items and multiple correlated latent traits, as well as for dichotomous items and a single latent variable. An R package is described that computes estimates of parameters and robust standard errors of a class of log-linear-by-linear association (LLLA) models, which are derived from a Rasch family of models. The LLLA models are special cases of log-linear models with bivariate interactions. Maximum likelihood estimation of LLLA models in this form is limited to relatively small problems; however, pseudo-likelihood estimation overcomes this limitation. Maximizing the pseudo-likelihood function is achieved by maximizing the likelihood of a single conditional multinomial logistic regression model. The parameter estimates are asymptotically normal and consistent. Based on our simulation studies, the pseudo-likelihood and maximum likelihood estimates of the parameters of LLLA models are nearly identical and the loss of efficiency is negligible. Recovery of parameters of Rasch models fit to simulated data is excellent.",WOS:000247011400001,JOURNAL OF STATISTICAL SOFTWARE,"['ITEM RESPONSE MODELS', 'ASSOCIATION MODELS', 'PSEUDOLIKELIHOOD ESTIMATION', 'CROSS-CLASSIFICATIONS', 'LOGISTIC REGRESSIONS', 'ORDERED CATEGORIES', 'SOCIAL NETWORKS']",Estimation of models in a Rasch family for polytomous items and multiple latent variables,2007
2118,"This paper addresses the estimation of locally stationary long-range dependent processes, a methodology that allows the statistical analysis of time series data exhibiting both nonstationarity and strong dependency. A time-varying parametric formulation of these models is introduced and a Whittle likelihood technique is proposed for estimating the parameters involved. Large sample properties of these Whittle estimates such as consistency, normality and efficiency are established in this work. Furthermore, the finite sample behavior of the estimators is investigated through Monte Carlo experiments. As a result from these simulations, we show that the estimates behave well even for relatively small sample sizes.",WOS:000282402800012,ANNALS OF STATISTICS,"['TIME-SERIES', 'MODELS']",AN EFFICIENT ESTIMATOR FOR LOCALLY STATIONARY GAUSSIAN LONG-MEMORY PROCESSES,2010
2119,,WOS:000208590100001,R JOURNAL,,Untitled,2011
2120,"In the common linear regression model the problem of determining optimal designs for least squares estimation is considered in the case where the observations are correlated. A necessary condition for the optimality of a given design is provided, which extends the classical equivalence theory for optimal designs in models with uncorrelated errors to the case of dependent data. If the regression functions are eigenfunctions of an integral operator defined by the covariance kernel, it is shown that the corresponding measure defines a universally optimal design. For several models universally optimal designs can be identified explicitly. In particular, it is proved that the uniform distribution is universally optimal for a class of trigonometric regression models with a broad class of covariance kernels and that the arcsine distribution is universally optimal for the polynomial regression model with correlation structure defined by the logarithmic potential. To the best knowledge of the authors these findings provide the first explicit results on optimal designs for regression models with correlated observations, which are not restricted to the location scale model.",WOS:000317451200006,ANNALS OF STATISTICS,"['REGRESSION PROBLEMS', 'ERRORS', 'TIME', 'ROBUSTNESS', 'PARAMETERS']",OPTIMAL DESIGN FOR LINEAR MODELS WITH CORRELATED OBSERVATIONS,2013
2121,"We consider experiments for comparing treatments using units that are ordered linearly over time or space within blocks. In addition to the block effect, we assume that a trend effect influences the response. The latter is modeled as a smooth component plus a random term that captures departures from the smooth trend. The model is flexible enough to cover a variety of situations; for instance, most of the effects may be either random or fixed. The information matrix for a design will be a function of several variance parameters. While data will shed light on the values of these parameters, at the design stage, they are unlikely to be known, so we suggest a maximin approach, in which a minimal information matrix is maximized. We derive maximin universally optimal designs and study their robustness. These designs are based on semibalanced arrays. Special cases correspond to results available in the literature.",WOS:000256504400003,ANNALS OF STATISTICS,"['FREE BLOCK-DESIGNS', 'CONSTRUCTION', 'TRENDS', 'EXISTENCE']",Optimal designs for mixed models in experiments based on ordered units,2008
2122,,WOS:000395669800001,R JOURNAL,,Editorial,2016
2123,"In this paper, we first develop a new family of conjugate prior distributions for the cell probability parameters of discrete graphical models Markov with respect to a set P of moral directed acyclic graphs with skeleton a given decomposable graph G. This family, which we call the P-Dirichlet, is a generalization of the hyper Dirichlet given in [Ann. Statist. 21 (1993) 12721317]: it keeps the directed strong hyper Markov property for every DAG in P but increases the flexibility in the choice of its parameters, that is, the hyper parameters.
Our second contribution is a characterization of the P-Dirichlet, which yields, as a corollary, a characterization of the hyper Dirichlet and a characterization of the Dirichlet also. Like the characterization of the Dirichlet given in [Ann. Statist. 25 (1997) 1344-1369], our characterization of the P-Dirichlet is based on local and global independence of the probability parameters and also a separability property explicitly defined here but implicitly used in that paper through the choice of two particular DAGs. Another advantage of our approach is that we need not make the assumption of the existence of a positive density function. We use the method of moments for our proofs.",WOS:000375175200005,ANNALS OF STATISTICS,"['DIRICHLET DISTRIBUTION', 'NEUTRALITIES']",A NEW PRIOR FOR DISCRETE DAG MODELS WITH A RESTRICTED SET OF DIRECTIONS,2016
2124,"Multilevel acceptance sampling for attributes is used to decide whether a lot from an incoming shipment or outgoing production is accepted or rejected when the product has multiple levels of product quality or multiple types of (mutually exclusive) possible defects. This paper describes a package which provides the tools to create, evaluate, plot, and display the acceptance sampling plans for such lots for both fixed and sequential sampling. The functions for calculating cumulative probabilities for several common multivariate distributions (which are needed in the package) are provided as well.",WOS:000293390700001,JOURNAL OF STATISTICAL SOFTWARE,['PLANS'],Multilevel Fixed and Sequential Acceptance Sampling: The R Package MFSAS,2011
2125,"The R graphics engine has new support for rendering raster images via the functions rasterImage() and grid. raster(). This leads to better scaling of raster images, faster rendering to screen, and smaller graphics files. Several examples of possible applications of these new features are described.",WOS:000208590100009,R JOURNAL,,Raster Images in R Graphics,2011
2126,,WOS:000336888400004,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'VARIABLE SELECTION', 'NP-DIMENSIONALITY', 'ORACLE PROPERTIES', 'LINEAR-MODELS']","DISCUSSION: ""A SIGNIFICANCE TEST FOR THE LASSO""",2014
2127,"High-dimensional statistical tests often ignore correlations to gain simplicity and stability leading to null distributions that depend on functionals of correlation matrices such as their Frobenius norm and other l(r) norms. Motivated by the computation of critical values of such tests, we investigate the difficulty of estimation the functionals of sparse correlation matrices. Specifically, we show that simple plug-in procedures based on thresholded estimators of correlation matrices are sparsity-adaptive and minimax optimal over a large class of correlation matrices. Akin to previous results on functional estimation, the minimax rates exhibit an elbow phenomenon. Our results are further illustrated in simulated data as well as an empirical study of data arising in financial econometrics.",WOS:000363437900014,ANNALS OF STATISTICS,"['HIGH-DIMENSIONAL DATA', 'PRINCIPAL COMPONENT ANALYSIS', 'OPTIMAL ADAPTIVE ESTIMATION', 'OPTIMAL RATES', 'QUADRATIC FUNCTIONALS', 'CONVERGENCE', 'SAMPLE', 'RISK', 'PCA', 'EQUILIBRIUM']",ESTIMATION OF FUNCTIONALS OF SPARSE COVARIANCE MATRICES,2015
2128,"We study the problem of estimability of means in undirected graphical Gaussian models with symmetry restrictions represented by a colored graph. Following on from previous studies, we partition the variables into sets of vertices whose corresponding means are restricted to being identical. We find a necessary and sufficient condition on the partition to ensure equality between the maximum likelihood and least-squares estimators of the mean.",WOS:000307608000016,ANNALS OF STATISTICS,"['COORDINATE-FREE APPROACH', 'INVARIANT NORMAL-MODELS', 'BEHRENS-FISHER PROBLEM', 'MARKOV', 'HYPOTHESES']",ESTIMATION OF MEANS IN GRAPHICAL GAUSSIAN MODELS WITH SYMMETRIES,2012
2129,"In this paper, inference for the parametric component of a semiparametric model based on sampling from the posterior profile distribution is thoroughly investigated from the frequentist viewpoint. The higher-order validity of the profile sampler obtained in Cheng and Kosorok [Ann. Statist. 36 (2008)] is extended to semiparametric models in which the infinite dimensional nuisance parameter may not have a root-n convergence rate. This is a nontrivial extension because it requires a delicate analysis of the entropy of the semiparametric models involved. We find that the accuracy of inferences based on the profile sampler improves as the convergence rate of the nuisance parameter increases. Simulation studies are used to verify this theoretical result. We also establish that an exact frequentist confidence interval obtained by inverting the profile log-likelihood ratio can be estimated with higher-order accuracy by the credible set of the same type obtained from the posterior profile distribution. Our theory is verified for several specific examples.",WOS:000258243000015,ANNALS OF STATISTICS,"['REGRESSION-MODELS', 'LIKELIHOOD', 'INFERENCE', 'SAMPLER']",General frequentist properties of the posterior profile distribution,2008
2130,"Many computer models contain unknown parameters which need to be estimated using physical observations. Tuo and Wu (2014) show that the calibration method based on Gaussian process models proposed by Kennedy and O'Hagan [J. R. Stat. Soc. Ser. B. Stat. Methodol. 63 (2001) 425-464] may lead to an unreasonable estimate for imperfect computer models. In this work, we extend their study to calibration problems with stochastic physical data. We propose a novel method, called the L-2 calibration, and show its semiparametric efficiency. The conventional method of the ordinary least squares is also studied. Theoretical analysis shows that it is consistent but not efficient. Numerical examples show that the proposed method outperforms the existing ones.",WOS:000363437900001,ANNALS OF STATISTICS,"['VALIDATION', 'METHODOLOGY', 'PREDICTION', 'OUTPUT']",EFFICIENT CALIBRATION FOR IMPERFECT COMPUTER MODELS,2015
2131,"This paper introduces the R package CDM for cognitive diagnosis models (CDMs). The package implements parameter estimation procedures for two general CDM frameworks, the generalized-deterministic input noisy-and-gate (G-DINA) and the general diagnostic model (GDM). It contains additional functions for analyzing data under these frameworks, like tools for simulating and plotting data, or for evaluating global model and item fit. The paper describes the theoretical aspects of implemented CDM frameworks and it illustrates the usage of the package with empirical data of the common fraction subtraction test by Tatsuoka (1984).",WOS:000392513100001,JOURNAL OF STATISTICAL SOFTWARE,"['ITEM RESPONSE THEORY', 'LATENT CLASS MODELS', 'DINA MODEL', 'EM ALGORITHM', 'RULE-SPACE', 'WALD TEST', 'VARIABLES', 'INDEXES', 'FAMILY', 'FIT']",The R Package CDM for Cognitive Diagnosis Models,2016
2132,"The package dcemriS4 provides a complete set of data analysis tools for quantitative assessment of dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Image processing is provided for the ANALYZE and NIfTI data formats as input with all parameter estimates being output in NIfTI format. Estimation of T1 relaxation from multiple flip-angle acquisitions, using either constant or spatially-varying flip angles, is performed via nonlinear regression. Both literature-based and data-driven arterial input functions are available and may be combined with a variety of compartmental models. Kinetic parameters are obtained from nonlinear regression, Bayesian estimation via Markov chain Monte Carlo or Bayesian maximum a posteriori estimation. A non-parametric model, using penalized splines, is also available to characterize the contrast agent concentration time curves. Estimation of the apparent diffusion coefficient (ADC) is provided for diffusion-weighted imaging. Given the size of multi-dimensional data sets commonly acquired in imaging studies, care has been taken to maximize computational efficiency and minimize memory usage. All methods are illustrated using both simulated and real-world medical imaging data available in the public domain.",WOS:000296228800001,JOURNAL OF STATISTICAL SOFTWARE,"['BRAIN-BARRIER PERMEABILITY', 'DCE-MRI', 'INPUT FUNCTION', 'DTPA', 'PERFUSION', 'MODELS']",Quantitative Analysis of Dynamic Contrast-Enhanced and Diffusion-Weighted Magnetic Resonance Imaging for Oncology in R,2011
2133,"One major goal in clinical applications of time-to-event data is the estimation of survival with censored data. The usual nonparametric estimator of the survival function is the time-honored Kaplan-Meier product-limit estimator. Though this estimator has been implemented in several R packages, the development of the condSURV R package has been motivated by recent contributions that allow the estimation of the survival function for ordered multivariate failure time data. The condSURV package provides three different approaches all based on the Kaplan-Meier estimator. In one of these approaches these quantities are estimated conditionally on current or past covariate measures. Illustration of the software usage is included using real data.",WOS:000395669800031,R JOURNAL,"['NONPARAMETRIC-ESTIMATION', 'REGRESSION', 'MODELS']",condSURV: An R Package for the Estimation of the Conditional Survival Function for Ordered Multivariate Failure Time Data,2016
2134,"In this work, we study the problem of aggregating a finite number of predictors for nonstationary sub-linear processes. We provide oracle inequalities relying essentially on three ingredients: (1) a uniform bound of the 1 norm of the time varying sub-linear coefficients, (2) a Lipschitz assumption on the predictors and (3) moment conditions on the noise appearing in the linear representation. Two kinds of aggregations are considered giving rise to different moment conditions on the noise and more or less sharp oracle inequalities. We apply this approach for deriving an adaptive predictor for locally stationary time varying autoregressive (TVAR) processes. It is obtained by aggregating a finite number of well chosen predictors, each of them enjoying an optimal minimax convergence rate under specific smoothness conditions on the TVAR coefficients. We show that the obtained aggregated predictor achieves a minimax rate while adapting to the unknown smoothness. To prove this result, a lower bound is established for the minimax rate of the prediction risk for the TVAR process. Numerical experiments complete this study. An important feature of this approach is that the aggregated predictor can be computed recursively and is thus applicable in an online prediction context.",WOS:000363437900004,ANNALS OF STATISTICS,"['LOCALLY STATIONARY-PROCESSES', 'REGRESSION', 'SERIES', 'INFERENCE', 'RATES']",AGGREGATION OF PREDICTORS FOR NONSTATIONARY SUB-LINEAR PROCESSES AND ONLINE ADAPTIVE FORECASTING OF TIME VARYING AUTOREGRESSIVE PROCESSES,2015
2135,"Many algorithms have been proposed for fitting network models with communities, but most of them do not scale well to large networks, and often fail on sparse networks. Here we propose a new fast pseudo-likelihood method for fitting the stochastic block model for networks, as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees. We show that the algorithms perform well under a range of settings, including on very sparse networks, and illustrate on the example of a network of political blogs. We also propose spectral clustering with perturbations, a method of independent interest, which works well on sparse networks where regular spectral clustering fails, and use it to provide an initial value for pseudo-likelihood. We prove that pseudo-likelihood provides consistent estimates of the communities under a mild condition on the starting value, for the case of a block model with two communities.",WOS:000326991200014,ANNALS OF STATISTICS,"['STOCHASTIC BLOCKMODELS', 'INDEPENDENT TRIALS', 'DIRECTED-GRAPHS', 'MODELS', 'DISTRIBUTIONS', 'CONSISTENCY', 'PREDICTION', 'SUCCESSES', 'NUMBER']",PSEUDO-LIKELIHOOD METHODS FOR COMMUNITY DETECTION IN LARGE SPARSE NETWORKS,2013
2136,"The space of positive definite symmetric matrices has been studied extensively as a means of understanding dependence in multivariate data along with the accompanying problems in statistical inference. Many books and papers have been written on this subject, and more recently there has been considerable interest in high-dimensional random matrices with particular emphasis on the distribution of certain eigenvalues. With the availability of modern data acquisition capabilities, smoothing or nonparametric techniques are required that go beyond those applicable only to data arising in Euclidean spaces. Accordingly, we present a Fourier method of minimax Wishart mixture density estimation on the space of positive definite symmetric matrices.",WOS:000311639700011,ANNALS OF STATISTICS,"['MULTIVARIATE STOCHASTIC VOLATILITY', 'EMPIRICAL BAYES ESTIMATION', 'NONPARAMETRIC DECONVOLUTION', 'COVARIANCE-MATRIX', 'OPTIMAL RATES', 'CONVERGENCE']",MINIMAX ESTIMATION FOR MIXTURES OF WISHART DISTRIBUTIONS,2011
2137,"The subject of stochastic approximation was founded by Robbins and Monro [Ann. Math. Statist. 22 (1951) 400-407]. After five decades of continual development, it has developed into an important area in systems control and optimization, and it has also served as a prototype for the development of adaptive algorithms for on-line estimation and control of stochastic systems. Recently, it has been used in statistics with Markov chain Monte Carlo for solving maximum likelihood estimation problems and for general simulation and optimizations. In this paper, we first show that the trajectory averaging estimator is asymptotically efficient for the stochastic approximation MCMC (SAMCMC) algorithm under mild conditions, and then apply this result to the stochastic approximation Monte Carlo algorithm [Liang, Liu and Carroll J. Amer Statist. Assoc. 102 (2007) 305-320]. The application of the trajectory averaging estimator to other stochastic approximation MCMC algorithms, for example, a stochastic approximation MLE algorithm for missing data problems, is also considered in the paper.",WOS:000282402800008,ANNALS OF STATISTICS,"['CHAIN MONTE-CARLO', 'WANG-LANDAU ALGORITHM', 'CONVERGENCE', 'COMPUTATION', 'EFFICIENCY', 'INFERENCE', 'MODEL']",TRAJECTORY AVERAGING FOR STOCHASTIC APPROXIMATION MCMC ALGORITHMS,2010
2138,"We introduce two novel procedures to test the nullity of the slope function in the functional linear model with real output. The test statistics combine multiple testing ideas and random projections of the input data through functional principal component analysis. Interestingly, the procedures are completely data-driven and do not require any prior knowledge on the smoothness of the slope nor on the smoothness of the covariate functions. The levels and powers against local alternatives are assessed in a nonasymptotic setting. This allows us to prove that these procedures are minimax adaptive (up to an unavoidable log log n multiplicative term) to the unknown regularity of the slope. As a side result, the minimax separation distances of the slope are derived for a large range of regularity classes. A numerical study illustrates these theoretical results.",WOS:000320488200016,ANNALS OF STATISTICS,"['PRINCIPAL-COMPONENTS-ANALYSIS', 'REGRESSION MODELS', 'ESTIMATORS', 'HYPOTHESES', 'RATES']",MINIMAX ADAPTIVE TESTS FOR THE FUNCTIONAL LINEAR MODEL,2013
2139,"We present the R package gMWT which is designed for the comparison of several treatments (or groups) for a large number of variables. The comparisons are made using certain probabilistic indices (PI). The PIs computed here tell how often pairs or triples of observations coming from different groups appear in a specific order of magnitude. Classical two and several sample rank test statistics such as the Mann-Whitney-Wilcoxon, Kruskal-Wllis, of Jonckheere-Terpstra test statistics are simple functions of these PI. Also new test statistics for directional alternatives are provided. The package gMWT can be used to calculate the variable-wise PI estimates, to illustrate their multivariate distribution and mutual dependence with joint scatterplot matrices, and to construct several classical and new rank tests based on the PIs. The aim of the paper is first to briefly explain the theory that is necessary to understand the behavior of the estimated PIs and the rank tests based on them. Second, the use of the package is described and illustrated with simulated and real data examples. It is stressed that the package provides a new flexible toolbox to analyze large gene or microRNA expression data sets, collected on microarrays or by other high-throughput technologies. The testing procedures can be used in an eQTL analysis, for example, as implemented in the package GeneticTools.",WOS:000365975000001,JOURNAL OF STATISTICAL SOFTWARE,,Mann-Whitney Type Tests for Microarray Experiments: The R Package gMWT,2015
2140,"There are different confirmatory techniques to compare means, like hypothesis testing and (Bayesian) model selection. However, there is no software package in which these techniques are available. A Fortran 90 program is written, which enables researchers to apply these techniques to their data. Besides traditional hypotheses, like H-0 : mu(1) = mu(2) = mu(3) and H-u : mu(1); mu(2); mu(3), order-restricted hypotheses, like mu(1) > mu(2) > mu(3) or mu(1) > mu(2) = mu(3) or mu(1) > mu(2) < mu(3), can be evaluated.",WOS:000281583900001,JOURNAL OF STATISTICAL SOFTWARE,['INEQUALITY'],A Fortran 90 Program for Confirmatory Analysis of Variance,2010
2141,"We give an overview of some of the software tools available in R, either as built-in functions or contributed packages, for the analysis of state space models. Several illustrative examples are included, covering constant and time-varying models for both univariate and multivariate time series. Maximum likelihood and Bayesian methods to obtain parameter estimates are considered.",WOS:000290526800001,JOURNAL OF STATISTICAL SOFTWARE,"['SIMULATION SMOOTHER', 'PACKAGE']",State Space Models in R,2011
2142,"In this article we present tmvtnorm, an R package implementation for the truncated multivariate normal distribution. We consider random number generation with rejection and Gibbs sampling, computation of marginal densities as well as computation of the mean and co-variance of the truncated variables. This contribution brings together latest research in this field and provides useful methods for both scholars and practitioners when working with truncated normal variables.",WOS:000208589900005,R JOURNAL,,tmvtnorm: A Package for the Truncated Multivariate Normal Distribution,2010
2143,"depmixS4 implements a general framework for defining and estimating dependent mixture models in the R programming language. This includes standard Markov models, latent/hidden Markov models, and latent class and finite mixture distribution models. The models can be fitted on mixed multivariate data with distributions from the g l m family, the (logistic) multinomial, or the multivariate normal distribution. Other distributions can be added easily, and an example is provided with the exgaus distribution. Parameters are estimated by the expectation-maximization (EM) algorithm or, when (linear) constraints are imposed on the parameters, by direct numerical optimization with the Rsolnp or Rdonlp 2 routines.",WOS:000281593600001,JOURNAL OF STATISTICAL SOFTWARE,['COMPUTATION'],depmixS4: An R Package for Hidden Markov Models,2010
2144,"We derive the maximum bias functions of the MM-estimates and the constrained M-estimates or CM-estimates of regression and compare them to the maximum bias functions of the S-estimates and the tau-estimates of regression. In these comparisons, the CM-estimates tend to exhibit the most favorable bias-robustness properties. Also, under the Gaussian model, it is shown how one can construct a CM-estimate which has a smaller maximum bias function than a given S-estimate, that is, the resulting CM-estimate dominates the S-estimate in terms of maxbias and, at the same time, is considerably more efficient.",WOS:000247498100002,ANNALS OF STATISTICS,"['HIGH BREAKDOWN-POINT', 'ROBUST REGRESSION', 'PROPERTY', 'MINIMAX']",On the maximum bias functions of MM-estimates and constrained M-estimates of regression,2007
2145,"The pcalg package for R can be used for the following two purposes: Causal structure learning and estimation of causal effects from observational data. In this document, we give a brief overview of the methodology, and demonstrate the package's functionality in both toy examples and applications.",WOS:000305065100001,JOURNAL OF STATISTICAL SOFTWARE,"['DISCOVERY', 'NETWORKS']",Causal Inference Using Graphical Models with the R Package pcalg,2012
2146,"The distribution of abundance amongst species with similar ways of life is a classical problem in ecology. The unified neutral theory of biodiversity, due to Hubbell, states that observed population dynamics may be explained on the assumption of per capita equivalence amongst individuals. One can thus dispense with differences between species, and differences between abundant and rare species: all individuals behave alike in respect of their probabilities of reproducing and death. It is a striking fact that such a parsimonious theory results in a non-trivial dominance-diversity curve ( that is, the simultaneous existence of both abundant and rare species) and even more striking that the theory predicts abundance curves that match observations across a wide range of ecologies. This paper introduces the untb package of R routines, for numerical simulation of ecological drift under the unified neutral theory. A range of visualization, analytical, and simulation tools are provided in the package and these are presented with examples in the paper.",WOS:000252430700001,JOURNAL OF STATISTICAL SOFTWARE,"['ABUNDANCE', 'BIOGEOGRAPHY', 'DIVERSITY']","Introducing untb, an R package for simulating ecological drift under the unified neutral theory of Biodiversity",2007
2147,"zoo is an R package providing an S3 class with methods for indexed totally ordered observations, such as discrete irregular time series. Its key design goals are independence of a particular index/time/date class and consistency with base R and the ""ts"" class for regular time series. This paper describes how these are achieved within zoo and provides several illustrations of the available methods for ""zoo"" objects which include plotting, merging and binding, several mathematical operations, extracting and replacing data and index, coercion and NA handling. A subclass ""zooreg"" embeds regular time series into the ""zoo"" framework and thus bridges the gap between regular and irregular time series classes in R.",WOS:000232891000001,JOURNAL OF STATISTICAL SOFTWARE,,zoo: S3 infrastructure for regular and irregular time series,2005
2148,We prove a global asymptotic equivalence of experiments in the sense of Le Cam's theory. The experiments are a continuously observed diffusion with nonparametric drift and its Euler scheme. We focus on diffusions with nonconstant-known diffusion coefficient. The asymptotic equivalence is proved by constructing explicit equivalence mappings based on random time changes. The equivalence of the discretized observation of the diffusion and the corresponding Euler scheme experiment is then derived. The impact of these equivalence results is that it justifies the use of the Euler scheme instead of the discretized diffusion process for inference purposes.,WOS:000338477800012,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'STATISTICAL EQUIVALENCE', 'DENSITY-ESTIMATION', 'ERGODIC DIFFUSIONS', 'RANDOM DESIGN', 'REGRESSION', 'MODELS', 'AUTOREGRESSION', 'NONEQUIVALENCE', 'APPROXIMATION']",ASYMPTOTIC EQUIVALENCE OF NONPARAMETRIC DIFFUSION AND EULER SCHEME EXPERIMENTS,2014
2149,"A standard tool for model selection in a Bayesian framework is the Bayes factor which compares the marginal likelihood of the data under two given different models. In this paper, we consider the class of hierarchical loglinear models for discrete data given under the form of a contingency table with multinomial sampling. We assume that the prior distribution on the loglinear parameters is the Diaconis-Ylvisaker conjugate prior, and the uniform is the prior distribution on the space of models. Under these conditions, the Bayes factor between two models is a function of the normalizing constants of the prior and posterior distribution of the loglinear parameters. These constants are functions of the hyperparameters (m, alpha) which can be interpreted, respectively, as the marginal counts and total count of a fictive contingency table.
We study the behavior of the Bayes factor when alpha tends to zero. In this study, the most important tool is the characteristic function J(C) of the interior C of the convex hull (C) over bar of the support of the multinomial distribution for a given hierarchical loglinear model. If h(C) is the support function of C, the function J(C) is the Laplace transform of exp(-h(C)). We show that, when alpha tends to 0, if the data lies on a face F-i of (C) over bar (i), i = 1, 2, of dimension k(i), the Bayes factor behaves like alpha(k1-k2). This implies in particular that when the data is in C-1 and in C-2, that is, when k(i) equals the dimension of model J(i), the sparser model is favored, thus confirming the idea of Bayesian regularization.
In order to find the faces of (C) over bar, we need to know its facets. We show that since here C is a polytope, the denominator of the rational function J(C) is the product of the equations of the facets. We also identify a category of facets common to all hierarchical models for discrete variables, not necessarily binary. Finally, we show that these facets are the only facets of (C) over bar when the model is graphical with respect to a decomposable graph.",WOS:000307608000009,ANNALS OF STATISTICS,"['LOG-LINEAR MODELS', 'GRAPHICAL MODELS']",BAYES FACTORS AND THE GEOMETRY OF DISCRETE HIERARCHICAL LOGLINEAR MODELS,2012
2150,"We consider the problem of testing multiple quantum hypotheses {rho(circle times n)(1) ,..., rho(circle times n)(r)},where an arbitrary prior distribution is given and each of the r hypotheses is n copies of a quantum state. It is known that the minimal average error probability P-e decays exponentially to zero, that is, P-e = exp{-xi n + o(n)}. However, this error exponent xi is generally unknown, except for the case that r = 2.
In this paper, we solve the long-standing open problem of identifying the above error exponent, by proving Nussbaum and Szkola's conjecture that xi= min(i not equal j) C(rho(i), rho(j)). The right-hand side of this equality is called the multiple quantum Chernoff distance, and C(rho(i), rho(j)) := max(0 <= s <= 1){-logTr rho(s)(j)rho(1-s)(j)} has been previously identified as the optimal error exponent for testing two hypotheses, rho(circle times n)(i) versus rho(circle times n)(j).
The main ingredient of our proof is a new upper bound for the average error probability, for testing an ensemble of finite-dimensional, but otherwise general, quantum states. This upper bound, up to a states-dependent factor, matches the multiple-state generalization of Nussbaum and Szkola's lower bound. Specialized to the case r = 2, we give an alternative proof to the achievability of the binary-hypothesis Chernoff distance, which was originally proved by Audenaert et al.",WOS:000379972900010,ANNALS OF STATISTICS,"['STRONG CONVERSE', 'ASYMPTOTICS', 'HYPOTHESES', 'INFORMATION', 'ENTROPY', 'THEOREM']",DISCRIMINATING QUANTUM STATES: THE MULTIPLE CHERNOFF DISTANCE,2016
2151,"Multi-State models provide a relevant framework for modelling complex event histories. Quantities of interest are the transition probabilities that can be estimated by the empirical transition matrix, that is also referred to as the Aalen-Johansen estimator. In this paper, we present the R package etm that computes and displays the transition proabilities. etm also features a Greenwood-type estimator of the covariance matrix. The use of the package is illustrated through a prominent example in bone marrow transplant for leukaemia patients.",WOS:000285981500001,JOURNAL OF STATISTICAL SOFTWARE,"['COMPETING RISKS', 'NONPARAMETRIC ESTIMATION', 'PROBABILITIES', 'INFERENCE', 'VIEW']",Empirical Transition Matrix of Multi-State Models: The etm Package,2011
2152,"Recent advances in computer technology have tremendously increased the use of functional data, whose graphical representation can be infinite-dimensional curves, images or shapes. This article describes four methods for visualizing functional time series using an R add-on package. These methods are demonstrated using age-specific Australian fertility data from 1921 to 2006 and monthly sea surface temperatures from January 1950 to December 2006.",WOS:000208590200010,R JOURNAL,,rainbow: An R Package for Visualizing Functional Time Series,2011
2153,"We estimate the Hurst parameter H of a fractional Brownian motion from discrete noisy data observed along a high frequency sampling scheme. The presence of systematic experimental noise makes recovery of H more difficult since relevant information is mostly contained in the high frequencies of the signal.
We quantify the difficulty of the statistical problem in a min-max sense: we prove that the rate n(-1/(4H+2)) is optimal for estimating H and propose rate optimal estimators based on adaptive estimation of quadratic functionals.",WOS:000251096100005,ANNALS OF STATISTICS,"['FRACTIONAL BROWNIAN-MOTION', 'WAVELET COEFFICIENTS']",Estimation of the Hurst parameter from discrete noisy data,2007
2154,,WOS:000312899000002,ANNALS OF STATISTICS,"['COVARIANCE ESTIMATION', 'LASSO']",DISCUSSION: LATENT VARIABLE GRAPHICAL MODEL SELECTION VIA CONVEX OPTIMIZATION,2012
2155,"The observation that species may be positively or negatively associated with each other is at least as old as the debate surrounding the nature of community structure which began in the early 1900's with Gleason and Clements. Since then investigating species co-occurrence patterns has taken a central role in understanding the causes and consequences of evolution, history, coexistence mechanisms, competition, and environment for community structure and assembly. This is because co-occurrence among species is a measurable metric in community datasets that, in the context of phylogeny, geography, traits, and environment, can sometimes indicate the degree of competition, displacement, and phylogenetic repulsion as weighed against biotic and environmental effects promoting correlated species distributions. Historically, a multitude of different co-occurrence metrics have been developed and most have depended on data randomization procedures to produce null distributions for significance testing. Here we improve upon and present an R implementation of a recently published model that is metric-free, distribution-free, and randomization-free. The R package, cooccur, is highly accessible, easily integrates into common analyses, and handles large datasets with high performance. In the article we develop the package's functionality and demonstrate aspects of co-occurrence analysis using three sample datasets.",WOS:000373914100001,JOURNAL OF STATISTICAL SOFTWARE,['PATTERNS'],cooccur: Probabilistic Species Co-Occurrence Analysis in R,2016
2156,"Single index models offer greater flexibility in data analysis than linear models but retain some of the desirable properties such as the interpretability of the coefficients. We consider a pseudo-profile likelihood approach to estimation and testing for single-index quantile regression models. We establish the asymptotic normality of the index coefficient estimator as well as the optimal convergence rate of the nonparametric function estimation. Moreover, we propose a score test for the index coefficient based on the gradient of the pseudo-profile likelihood, and employ a penalized procedure to perform consistent model selection and model estimation simultaneously. We also use Monte Carlo studies to support our asymptotic results, and use an empirical example to illustrate the proposed method.",WOS:000375175200012,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'VARIABLE SELECTION', 'SMOOTHING SPLINES', 'ASYMPTOTICS', 'ESTIMATORS', 'LASSO']",INFERENCE FOR SINGLE-INDEX QUANTILE REGRESSION MODELS WITH PROFILE OPTIMIZATION,2016
2157,"This paper studies a very flexible model that can be used widely to analyze the relation between a response and multiple covariates. The model is nonparametric, yet renders easy interpretation for the effects of the covariates. The model accommodates both continuous and discrete random variables for the response and covariates. It is quite flexible to cover the generalized varying coefficient models and the generalized additive models as special cases. Under a weak condition we give a general theorem that the problem of estimating the multivariate mean function is equivalent to that of estimating its univariate component functions. We discuss implications of the theorem for sieve and penalized least squares estimators, and then investigate the outcomes in full details for a kernel-type estimator. The kernel estimator is given as a solution of a system of nonlinear integral equations. We provide an iterative algorithm to solve the system of equations and discuss the theoretical properties of the estimator and the algorithm. Finally, we give simulation results.",WOS:000310650900023,ANNALS OF STATISTICS,"['LONGITUDINAL DATA', 'ADDITIVE-MODELS', 'POLYNOMIAL SPLINE', 'LINEAR-MODELS', 'TIME-SERIES', 'SELECTION', 'DYNAMICS']",FLEXIBLE GENERALIZED VARYING COEFFICIENT REGRESSION MODELS,2012
2158,"A stationary Gaussian process is said to be long-range dependent (resp., anti-persistent) if its spectral density f(lambda) can be written as f(lambda) = vertical bar lambda vertical bar(-2d) g(vertical bar lambda vertical bar), where 0 <d < 1/2 (resp., -1/2 <d < 0), and g is continuous and positive. We propose a novel Bayesian nonparametric approach for the estimation of the spectral density of such processes. We prove posterior consistency for both d and g, under appropriate conditions on the prior distribution. We establish the rate of convergence for a general class of priors and apply our results to the family of fractionally exponential priors. Our approach is based on the true likelihood and does not resort to Whittle's approximation.",WOS:000307608000013,ANNALS OF STATISTICS,"['TIME-SERIES', 'POSTERIOR DISTRIBUTIONS', 'ASYMPTOTIC EXPANSIONS', 'CONVERGENCE-RATES', 'RANGE DEPENDENCE', 'STATIONARY', 'PARAMETER', 'REGRESSION']",BAYESIAN NONPARAMETRIC ESTIMATION OF THE SPECTRAL DENSITY OF A LONG OR INTERMEDIATE MEMORY GAUSSIAN PROCESS,2012
2159,"This paper offers a visual tour throughout ViSta 6.4, a freeware statistical program based on Lisp-Stat and focused on techniques for statistical visualization ( Young 2004). This travel around ViSta is based on screen recordings that illustrate the main features of the program in action. The following aspects of ViSta 6.4 are displayed: the program's interface (ViSta's desktop, menubar and pop-up menus, help system); its data management capabilities ( data input and editing, data transformations); features associated to data analysis (data description, statistical modeling); and the options for Lisp-Stat development in ViSta. The video recordings associated to this tour (. wmv files) can be visualized at http:// www. jstatsoft.org/v13/ i08/ using the Internet Explorer navigator, or by clicking on the figures in the paper.",WOS:000232831900001,JOURNAL OF STATISTICAL SOFTWARE,,"A video tour through ViSta 6.4, a visual statistical system based on Lisp-Stat",2005
2160,"We consider a clustering problem where we observe feature vectors X-i is an element of R-P, i = 1, 2,..., n, from K possible classes. The class labels are unknown and the main interest is to estimate them. We are primarily interested in the modern regime of p >> n, where classical clustering methods face challenges.
We propose Influential Features PCA (IF-PCA) as a new clustering procedure. In IF-PCA, we select a small fraction of features with the largest Kolmogorov Smirnov (KS) scores, obtain the first (K-1) left singular vectors of the post-selection normalized data matrix, and then estimate the labels by applying the classical k-means procedure to these singular vectors. In this procedure, the only tuning parameter is the threshold in the feature selection step. We set the threshold in a data-driven fashion by adapting the recent notion of Higher Criticism. As a result, IF-PCA is a tuning-free clustering method.
We apply IF-PCA to 10 gene microarray data sets. The method has competitive performance in clustering. Especially, in three of the data sets, the error rates of IF-PCA are only 29% or less of the error rates by other methods. We have also rediscovered a phenomenon on empirical null by Efron [J. Amer. Statist. Assoc. 99 (2004) 96-104] on microarray data.
With delicate analysis, especially post-selection eigen-analysis, we derive tight probability bounds on the Kolmogorov Smirnov statistics and show that IF-PCA yields clustering consistency in a broad context. The clustering problem is connected to the problems of sparse PCA and low-rank matrix recovery, but it is different in important ways. We reveal an interesting phase transition phenomenon associated with these problems and identify the range of interest for each.",WOS:000389620800001,ANNALS OF STATISTICS,"['PRINCIPAL COMPONENT ANALYSIS', 'HIGHER CRITICISM', 'LARGE DEVIATIONS', 'SPARSE PCA', 'CLASSIFICATION', 'MODEL', 'HYPOTHESIS', 'INFERENCE', 'SELECTION', 'BOUNDARY']",INFLUENTIAL FEATURES PCA FOR HIGH DIMENSIONAL CLUSTERING,2016
2161,"Post-stratification is frequently used to improve the precision of survey estimators when categorical auxiliary information is available from sources outside the survey. In natural resource surveys, such information is often obtained from remote sensing data, classified into categories and displayed as pixel-based maps. These maps may be constructed based on classification models fitted to the sample data. Post-stratification of the sample data based on categories derived from the sample data (""endogenous post-stratification"") violates the standard post-stratification assumptions that observations are classified without error into post-strata, and post-stratum population counts are known. Properties of the endogenous post-stratification estimator are derived for the case of a sample-fitted generalized linear model, from which the post-strata are constructed by dividing the range of the model predictions into predetermined intervals. Design consistency of the endogenous post-stratification estimator is established under mild conditions. Under a super-population model, consistency and asymptotic normality of the endogenous post-stratification estimator are established, showing that it has the same asymptotic variance as the traditional post-stratified estimator with fixed strata. Simulation experiments demonstrate that the practical effect of first fitting a model to the survey data before post-stratifying is small, even for relatively small sample sizes.",WOS:000253390000016,ANNALS OF STATISTICS,,Endogenous post-stratification in surveys: Classifying with a sample-fitted model,2008
2162,"ade4 is a multivariate data analysis package for the R statistical environment, and ade4TkGUI is a Tcl/Tk graphical user interface for the most essential methods of ade4. Both packages are available on CRAN. An overview of ade4TkGUI is presented, and the pros and cons of this approach are discussed. We conclude that command line interfaces (CLI) and graphical user interfaces (GUI) are complementary. ade4TkGUI can be valuable for biologists and particularly for ecologists who are often occasional users of R. It can spare them having to acquire an in-depth knowledge of R, and it can help first time users in a first approach.",WOS:000252429900001,JOURNAL OF STATISTICAL SOFTWARE,,Interactive multivariate data analysis in R with the ade4 and ade4TkGUI packages,2007
2163,"In crossover design experiments, the proportional model, where the carryover effects are proportional to their direct treatment effects, has draw attentions in recent years. We discover that the universally optimal design under the traditional model is E-optimal design under the proportional model. Moreover, we establish equivalence theorems of Kiefer-Wolfowitz's type for four popular optimality criteria, namely A, D, E and T (trace).",WOS:000326991200019,ANNALS OF STATISTICS,['UNIVERSAL OPTIMALITY'],OPTIMAL CROSSOVER DESIGNS FOR THE PROPORTIONAL MODEL,2013
2164,"The R package ltm has been developed for the analysis of multivariate dichotomous and polytomous data using latent variable models, under the Item Response Theory approach. For dichotomous data the Rasch, the Two-Parameter Logistic, and Birnbaum's Three-Parameter models have been implemented, whereas for polytomous data Semejima's Graded Response model is available. Parameter estimates are obtained under marginal maximum likelihood using the Gauss-Hermite quadrature rule. The capabilities and features of the package are illustrated using two real data examples.",WOS:000242545700001,JOURNAL OF STATISTICAL SOFTWARE,,ltm: An R package for latent variable modeling and item response theory analyses,2006
2165,"We investigate the time-varying ARCH (tvARCH) process. It is shown that it can be used to describe the slow decay of the sample autocorrelations of the squared returns often observed in financial time series, which warrants the further study of parameter estimation methods for the model.
Since the parameters are changing over time, a successful estimator needs to perform well for small samples. We propose a kernel normalized-least-squares (kernel-NLS) estimator which has a closed form, and thus outperforms the previously proposed kernel quasi-maximum likelihood (kernel-QML) estimator for small samples. The kernel-NLS estimator is simple, works under mild moment assumptions and avoids some of the parameter space restrictions imposed by the kernel-QML estimator. Theoretical evidence shows that the kernel-NLS estimator has the same rate of convergence as the kernel-QML estimator. Due to the kernel-NLS estimator's ease of computation, computationally intensive procedures can be used. A prediction-based cross-validation method is proposed for selecting the bandwidth of the kernel-NLS estimator. Also, we use a residual-based bootstrap scheme to bootstrap the tvARCH process. The bootstrap sample is used to obtain pointwise confidence intervals for the kernel-NLS estimator. It is shown that distributions of the estimator using the bootstrap and the ""true"" tvARCH estimator asymptotically coincide.
We illustrate our estimation method on a variety of currency exchange and stock index data for which we obtain both good fits to the data and accurate forecasts.",WOS:000254502700010,ANNALS OF STATISTICS,"['STATISTICAL-INFERENCE', 'NONSTATIONARITIES', 'DEPENDENCE', 'SERIES']",Normalized least-squares estimation in time-varying ARCH models,2008
2166,"We consider the problem of estimating the fractional order of a Levy process from low frequency historical and options data. An estimation methodology is developed which allows us to treat both estimation and calibration problems in a unified way. The corresponding procedure consists of two steps: the estimation of a conditional characteristic function and the weighted least squares estimation of the fractional order in spectral domain. While the second step is identical for both calibration and estimation, the first one depends on the problem at hand. Minimax rates of convergence for the fractional order estimate are derived, the asymptotic normality is proved and a data-driven algorithm based on aggregation is proposed. The performance of the estimator in both estimation and calibration setups is illustrated by a simulation study.",WOS:000273800100010,ANNALS OF STATISTICS,"['EMPIRICAL CHARACTERISTIC FUNCTION', 'STABLE-DISTRIBUTIONS', 'PARAMETERS', 'MODELS', 'JUMPS', 'LAW']",SPECTRAL ESTIMATION OF THE FRACTIONAL ORDER OF A LEVY PROCESS,2010
2167,"The multiple testing procedure plays an important role in detecting the presence of spatial signals for large-scale imaging data. Typically, the spatial signals are sparse but clustered. This paper provides empirical evidence that for a range of commonly used control levels, the conventional FDR procedure can lack the ability to detect statistical significance, even if the p-values under the true null hypotheses are independent and uniformly distributed; more generally, ignoring the neighboring information of spatially structured data will tend to diminish the detection effectiveness of the FDR procedure. This paper first introduces a scalar quantity to characterize the extent to which the ""lack of identification phenomenon"" (LIP) of the FDR procedure occurs. Second, we propose a new multiple comparison procedure, called FDRL, to accommodate the spatial information of neighboring p-values, via a local aggregation of p-values. Theoretical properties of the FDRL procedure are investigated under weak dependence of p-values. It is shown that the FDRL procedure alleviates the LIP of the FDR procedure, thus substantially facilitating the selection of more stringent control levels. Simulation evaluations indicate that the FDRL procedure improves the detection sensitivity of the FDR procedure with little loss in detection specificity. The computational simplicity and detection effectiveness of the FDRL procedure are illustrated through a real brain fMRI dataset.",WOS:000288183800020,ANNALS OF STATISTICS,"['FALSE DISCOVERY CONTROL', 'FAMILYWISE ERROR RATE', 'RATES', 'HYPOTHESIS', 'FMRI', 'DEPENDENCE']",MULTIPLE TESTING VIA FDRL FOR LARGE-SCALE IMAGING DATA,2011
2168,"We study nonparametric isotonic confidence intervals for monotone functions. In [Ann. Statist. 29 (2001) 1699-1731], pointwise confidence intervals, based on likelihood ratio tests using the restricted and unrestricted MLE in the current status model, are introduced. We extend the method to the treatment of other models with monotone functions, and demonstrate our method with a new proof of the results of Banerjee-Wellner [Ann. Statist. 29 (2001) 1699-1731] and also by constructing confidence intervals for monotone densities, for which a theory remained be developed. For the latter model we prove that the limit distribution of the LR test under the null hypothesis is the same as in the current status model. We compare the confidence intervals, so obtained, with confidence intervals using the smoothed maximum likelihood estimator (SMLE), using bootstrap methods. The ""Lagrange-modified"" cusum diagrams, developed here, are an essential tool both for the computation of the restricted MLEs and for the development of the theory for the confidence intervals, based on the LR tests.",WOS:000362697700006,ANNALS OF STATISTICS,"['BOOTSTRAP', 'DENSITY']",NONPARAMETRIC CONFIDENCE INTERVALS FOR MONOTONE FUNCTIONS,2015
2169,"In the setting of nonparametric multivariate regression with unknown error variance sigma(2), we study asymptotic properties of a Bayesian method for estimating a regression function f and its mixed partial derivatives. We use a random series of tensor product of B-splines with normal basis coefficients as a prior for f, and sigma is either estimated using the empirical Bayes approach or is endowed with a suitable prior in a hierarchical Bayes approach. We establish pointwise, L-2 and L-infinity-posterior contraction rates for f and its mixed partial derivatives, and show that they coincide with the minimax rates. Our results cover even the anisotropic situation, where the true regression function may have different smoothness in different directions. Using the convergence bounds, we show that pointwise, L-2 and L-infinity-credible sets for f and its mixed partial derivatives have guaranteed frequentist coverage with optimal size. New results on tensor products of B-splines are also obtained in the course.",WOS:000375175200007,ANNALS OF STATISTICS,"['BERNSTEIN-VON-MISES', 'GAUSSIAN WHITE-NOISE', 'CONFIDENCE BANDS', 'ADAPTIVE ESTIMATION', 'DENSITY-ESTIMATION', 'RATES', 'PRIORS', 'CONVERGENCE', 'METRICS']",SUPREMUM NORM POSTERIOR CONTRACTION AND CREDIBLE SETS FOR NONPARAMETRIC MULTIVARIATE REGRESSION,2016
2170,"We describe bayesPop, an R package for producing probabilistic population projections for all countries. This uses probabilistic projections of total fertility and life expectancy generated by Bayesian hierarchical models. It produces a sample from the joint posterior predictive distribution of future age- and sex-specific population counts, fertility rates and mortality rates, as well as future numbers of births and deaths. It provides graphical ways of summarizing this information, including trajectory plots and various kinds of probabilistic population pyramids. An expression language is introduced which allows the user to produce the predictive distribution of a wide variety of derived population quantities, such as the median age or the old age dependency ratio. The package produces aggregated projections for sets of countries, such as UN regions or trading blocs. The methodology has been used by the United Nations to produce their most recent official population projections for all countries, published in the World Population Prospects.",WOS:000392704400001,JOURNAL OF STATISTICAL SOFTWARE,"['TOTAL FERTILITY RATE', 'LIFE EXPECTANCY', 'UNITED-STATES', 'R PACKAGE', 'COUNTRIES', 'MORTALITY', 'MODEL']",bayesPop: Probabilistic Population Projections,2016
2171,"The deterministic dynamics of populations in continuous time are traditionally described using coupled, first-order ordinary differential equations. While this approach is accurate for large systems, it is often inadequate for small systems where key species may be present in small numbers or where key reactions occur at a low rate. The Gillespie stochastic simulation algorithm (SSA) is a procedure for generating time-evolution trajectories of finite populations in continuous time and has become the standard algorithm for these types of stochastic models. This article presents a simple-to-use and flexible framework for implementing the SSA using the high-level statistical computing language R and the package GillespieSSA. Using three ecological models as examples (logistic growth, Rosenzweig-MacArthur predator-prey model, and Kermack-McKendrick SIRS metapopulation model), this paper shows how a deterministic model can be formulated as a finite-population stochastic model within the framework of SSA theory and how it can be implemented in R. Simulations of the stochastic models are performed using four different SSA Monte Carlo methods: one exact method (Gillespie's direct method); and three approximate methods (explicit, binomial, and optimized tau-leap methods). Comparison of simulation results confirms that while the time-evolution trajectories obtained from the different SSA methods are indistinguishable, the approximate methods are up to four orders of magnitude faster than the exact methods.",WOS:000255795000001,JOURNAL OF STATISTICAL SOFTWARE,"['CHEMICALLY REACTING SYSTEMS', 'KINETICS', 'CYCLES']",GillespieSSA: Implementing the stochastic simulation algorithm in R,2008
2172,"MARSS is a package for fitting multivariate autoregressive state-space models to time-series data. The MARSS package implements state-space models in a maximum likelihood framework. The core functionality of MARSS is based on likelihood maximization using the Kalman filter/smoother, combined with an EM algorithm. To make comparisons with other packages available, parameter estimation is also permitted via direct search routines available in 'optim'. The MARSS package allows data to contain missing values and allows a wide variety of model structures and constraints to be specified (such as fixed or shared parameters). In addition to model-fitting, the package provides bootstrap routines for simulating data and generating confidence intervals, and multiple options for calculating model selection criteria (such as AIC).",WOS:000313197700003,R JOURNAL,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'DYNAMIC FACTOR-ANALYSIS', 'COMMON TRENDS', 'POPULATIONS']",MARSS: Multivariate Autoregressive State-space Models for Analyzing Time-series Data,2012
2173,"We investigate structure for pairs of randomizations that do not follow each other in a chain. These are unrandomized-inclusive, independent, coincident or double randomizations. This involves taking several structures that satisfy particular relations and combining them to form the appropriate orthogonal decomposition of the data space for the experiment. We show how to establish the decomposition table giving the sources of variation, their relationships and their degrees of freedom, so that competing designs can be evaluated. This leads to recommendations for when the different types of multiple randomization should be used.",WOS:000282402800018,ANNALS OF STATISTICS,"['DESIGNS', 'VARIANCE', 'MODELS']",DECOMPOSITION TABLES FOR EXPERIMENTS. II. TWO-ONE RANDOMIZATIONS,2010
2174,"We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include l(1) (the lasso), l(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.",WOS:000275203200001,JOURNAL OF STATISTICAL SOFTWARE,"['PENALIZED LOGISTIC-REGRESSION', 'VARIABLE SELECTION', 'LASSO', 'CLASSIFICATION', 'ALGORITHMS', 'CANCER']",Regularization Paths for Generalized Linear Models via Coordinate Descent,2010
2175,"The gRbase package is intended to set the framework for computer packages for data analysis using graphical models. The gRbase package is developed for the open source language, R, and is available for several platforms. The package is intended to be widely extendible and flexible so that package developers may implement further types of graphical models using the available methods.
The gRbase package consists of a set of S version 3 classes and associated methods for representing data and models. The package is linked to the dynamicGraph package (Badsberg 2005), an interactive graphical user interface for manipulating graphs.
In this paper, we show how these building blocks can be combined and integrated with inference engines in the special cases of hierarchical log-linear models. We also illustrate how to extend the package to deal with other types of graphical models, in this case the graphical Gaussian models.",WOS:000234226800001,JOURNAL OF STATISTICAL SOFTWARE,,A common platform for graphical models in R: The gRbase package,2005
2176,"Lack-of-fit testing of a regression model with Berkson measurement error has not been discussed in the literature to date. To fill this void, we propose a class of tests based on minimized integrated square distances between nonparametric regression function estimator and the parametric model being fitted. We prove asymptotic normality of these test statistics under the null hypothesis and that of the corresponding minimum distance estimators under minimal conditions on the model being fitted. We also prove consistency of the proposed tests against a class of fixed alternatives and obtain their asymptotic power against a class of local alternatives orthogonal to the null hypothesis. These latter results are new even when there is no measurement error. A simulation that is included shows very desirable finite sample behavior of the proposed inference procedures.",WOS:000263129000005,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'IN-VARIABLES']",MINIMUM DISTANCE REGRESSION MODEL CHECKING WITH BERKSON MEASUREMENT ERRORS,2009
2177,"The solaR package allows for reproducible research both for photovoltaics (PV) systems performance and solar radiation. It includes a set of classes, methods and functions to calculate the sun geometry and the solar radiation incident on a photovoltaic generator and to simulate the performance of several applications of the photovoltaic energy. This package performs the whole calculation procedure from both daily and intradaily global horizontal irradiation to the final productivity of grid-connected PV systems and water pumping PV systems.
It is designed using a set of S4 classes whose core is a group of slots with multivariate time series. The classes share a variety of methods to access the information and several visualization methods. In addition, the package provides a tool for the visual statistical analysis of the performance of a large PV plant composed of several systems.
Although solaR is primarily designed for time series associated to a location defined by its latitude/longitude values and the temperature and irradiation conditions, it can be easily combined with spatial packages for space-time analysis.",WOS:000308909400001,JOURNAL OF STATISTICAL SOFTWARE,"['DIFFUSE', 'MODEL', 'IRRADIANCE', 'ALGORITHM']",solaR: Solar Radiation and Photovoltaic Systems with R,2012
2178,"Asymptotic equivalence theory developed in the literature so far are only for bounded loss functions. This limits the potential applications of the theory because many commonly used loss functions in statistical inference are unbounded. In this paper we develop asymptotic equivalence results for robust nonparametric regression with unbounded loss functions. The results imply that all the Gaussian nonparametric regression procedures can be robustified in a unified way. A key step in our equivalence argument is to bin the data and then take the median of each bin.
The asymptotic equivalence results have significant practical implications. To illustrate the general principles of the equivalence argument we consider two important nonparametric inference problems: robust estimation of the reagression function and the estimation of a quadratic functional. In both cases easily implementable procedures are constructed and are shown to enjoy simultaneously a high degree of robustness and adaptivity. Other problems such as construction of confidence sets and nonparametric hypothesis testing can be handled in a similar fashion.",WOS:000271673500005,ANNALS OF STATISTICS,"['GAUSSIAN WHITE-NOISE', 'WAVELET SHRINKAGE', 'QUADRATIC FUNCTIONALS', 'DENSITY-ESTIMATION', 'MINIMAX ESTIMATION', 'CONFIDENCE SETS', 'DIFFUSION', 'MODELS', 'APPROXIMATION', 'INEQUALITY']",ASYMPTOTIC EQUIVALENCE AND ADAPTIVE ESTIMATION FOR ROBUST NONPARAMETRIC REGRESSION,2009
2179,"Models of cancer progression provide insights on the order of accumulation of genetic alterations during cancer development. Algorithms to infer such models from the currently available mutational profiles collected from different cancer patients (cross-sectional data) have been defined in the literature since late the 90s. These algorithms differ in the way they extract a graphical model of the events modelling the progression, e.g., somatic mutations or copy-number alterations.
TRONCO is an R package for TRanslational ONcology which provides a series of functions to assist the user in the analysis of cross-sectional genomic data and, in particular, it implements algorithms that aim to model cancer progression by means of the notion of selective advantage. These algorithms are proved to outperform the current state-of-the-art in the inference of cancer progression models. TRONCO also provides functionalities to load input cross-sectional data, set up the execution of the algorithms, assess the statistical confidence in the results, and visualize the models.",WOS:000395669800004,R JOURNAL,,Design of the TRONCO BioConductor Package for TRanslational ONCOlogy,2016
2180,"Maximum likelihood estimation of a log-concave probability density is formulated as a convex optimization problem and shown to have an equivalent dual formulation as a constrained maximum Shannon entropy problem. Closely related maximum Renyi entropy estimators that impose weaker concavity restrictions on the fitted density are also considered, notably a minimum Hellinger discrepancy estimator that constrains the reciprocal of the square-root of the density to be concave. A limiting form of these estimators constrains solutions to the class of quasi-concave densities.",WOS:000282402800013,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'PROBABILITY DENSITY', 'DISTRIBUTIONS', 'THEOREM']",QUASI-CONCAVE DENSITY ESTIMATION,2010
2181,,WOS:000275510800024,ANNALS OF STATISTICS,,ON SOME PROBLEMS IN THE ARTICLE EFFICIENT LIKELIHOOD ESTIMATION IN STATE SPACE MODELS BY CHENG-DER FUH [Ann. Statist. 34 (2006) 2026-2068],2010
2182,"This article considers estimation of constant and time-varying coefficients in nonlinear ordinary differential equation (ODE) models where analytic closed-form solutions are not available. The numerical solution-based nonlinear least squares (NLS) estimator is investigated in this study. A numerical algorithm such as the Runge-Kutta method is used to approximate the ODE solution. The asymptotic properties are established for the proposed estimators considering both numerical error and measurement error. The B-spline is used to approximate the time-varying coefficients, and the corresponding asymptotic theories in this case are investigated under the framework of the sieve approach. Our results show that if the maximum step size of the p-order numerical algorithm goes to zero at a rate faster than n(-1)/(p boolean AND 4), the numerical error is negligible compared to the measurement error. This result provides a theoretical guidance in selection of the step size for numerical evaluations of ODEs. Moreover, we have shown that the numerical solution-based NLS estimator and the sieve NLS estimator are strongly consistent. The sieve estimator of constant parameters is asymptotically normal with the same asymptotic co-variance as that of the case where the true ODE solution is exactly known, while the estimator of the time-varying parameter has the optimal convergence rate under some regularity conditions. The theoretical results are also developed for the case when the step size of the ODE numerical solver does not go to zero fast enough or the numerical error is comparable to the measurement error. We illustrate our approach with both simulation studies and clinical data on HIV viral dynamics.",WOS:000280359400014,ANNALS OF STATISTICS,"['DYNAMICS IN-VIVO', 'PARAMETER-ESTIMATION', 'EFFICIENT ESTIMATION', 'LEAST-SQUARES', 'HIV-1 DYNAMICS', 'GLOBAL IDENTIFIABILITY', 'REGRESSION-MODELS', 'SYSTEMS', 'OPTIMIZATION', 'CONVERGENCE']",SIEVE ESTIMATION OF CONSTANT AND TIME-VARYING COEFFICIENTS IN NONLINEAR ORDINARY DIFFERENTIAL EQUATION MODELS BY CONSIDERING BOTH NUMERICAL ERROR AND MEASUREMENT ERROR,2010
2183,"Recently, new nonparametric multivariate extensions of the univariate sign methods have been proposed. Randles (2000) introduced an affine invariant multivariate sign test for the multivariate location problem. Later on, Hettmansperger and Randles (2002) considered an affine equivariant multivariate median corresponding to this test. The new methods have promising efficiency and robustness properties. In this paper, we review these developments and compare them with the classical multivariate analysis of variance model. A new SAS/IML tool for performing a spatial sign based multivariate analysis of variance is introduced.",WOS:000239139800001,JOURNAL OF STATISTICAL SOFTWARE,"['AFFINE', 'LOCATION', 'TESTS']",SAS/IML macros for a multivariate analysis of variance based on spatial signs,2006
2184,"We consider two alternative tests to the Higher Criticism test of Donoho and Jin [Ann. Statist. 32 (2004) 962-994] for high-dimensional means under the sparsity of the nonzero means for sub-Gaussian distributed data with unknown column-wise dependence. The two alternative test statistics are constructed by first thresholding L-1 and L-2 statistics based on the sample means, respectively, followed by maximizing over a range of thresholding levels to make the tests adaptive to the unknown signal strength and sparsity. The two alternative tests can attain the same detection boundary of the Higher Criticism test in [Ann. Statist. 32 (2004) 962-994] which was established for uncorrelated Gaussian data. It is demonstrated that the maximal L-2-thresholding test is at least as powerful as the maximal L-1-thresholding test, and both the maximal L-2 and L-1-thresholding tests are at least as powerful as the Higher Criticism test.",WOS:000330204900005,ANNALS OF STATISTICS,"['CENTRAL LIMIT-THEOREMS', 'MIXTURES']",TESTS ALTERNATIVE TO HIGHER CRITICISM FOR HIGH-DIMENSIONAL MEANS UNDER SPARSITY AND COLUMN-WISE DEPENDENCE,2013
2185,"There is a lack of robust statistical analyses for random effects linear models. In practice, statistical analyses, including estimation, prediction and inference, are not reliable when data are unbalanced, of small size, contain outliers, or not normally distributed. It is fortunate that rank-based regression analysis is a robust nonparametric alternative to likelihood and least squares analysis. We propose an R package that calculates rank-based statistical analyses for two-and three-level random effects nested designs. In this package, a new algorithm which recursively obtains robust predictions for both scale and random effects is used, along with three rank-based fitting methods.",WOS:000330193300008,R JOURNAL,['LINEAR-MODELS'],rlme: An R Package for Rank-Based Estimation and Prediction in Random Effects Nested Models,2013
2186,"Hierarchical Bayesian modeling of large point-referenced space-time data is increasingly becoming feasible in many environmental applications due to the recent advances in both statistical methodology and computation power. Implementation of these methods using the Markov chain Monte Carlo (MCMC) computational techniques, however, requires development of problem-specific and user-written computer code, possibly in a low-level language. This programming requirement is hindering the widespread use of the Bayesian model-based methods among practitioners and, hence there is an urgent need to develop high-level software that can analyze large data sets rich in both space and time.
This paper develops the package spTimer for hierarchical Basyesian modeling of stylized environmental space-time monitoring data as a contributed software package in the R language that is fast becoming a very popular statistical computing platform. The package is able to fit, spatially and temporally predict large amounts of space-time data using three recently developed Bayesian models. The user is given control over many options regarding covariance function selection, distance calculation, prior selection and tuning of the implemented MCMC algorithms, although suitable defaults are provided. The package has many other attractive features such as on the fly trasformation and an ability to spatially predict temporally aggregated summaries on the original scale, which saves the problem of storage when using MCMC methods for large datasets. A simulation example, with more than a million observations, and a real life data example are used to validate the underlying code and to illustrate the software capabilities.",WOS:000349846900001,JOURNAL OF STATISTICAL SOFTWARE,"['PACKAGE', 'GEOSTATISTICS']",sp Timer: Spatio-Temporal Bayesian Modeling Using R,2015
2187,"Nonlinear regression models are applied in a broad variety of scientific fields. Various R functions are already dedicated to fitting such models, among which the function nls () has a prominent position. Unlike linear regression fitting of nonlinear models relies on non-trivial assumptions and therefore users are required to carefully ensure and validate the entire modeling. Parameter estimation is carried out using some variant of the leastsquares criterion involving an iterative process that ideally leads to the determination of the optimal parameter estimates. Therefore, users need to have a clear understanding of the model and its parameterization in the context of the application and data considered, an a priori idea about plausible values for parameter estimates, knowledge of model diagnostics procedures available for checking crucial assumptions, and, finally, an understanding of the limitations in the validity of the underlying hypotheses of the fitted model and its implication for the precision of parameter estimates. Current nonlinear regression modules lack dedicated diagnostic functionality. So there is a need to provide users with an extended toolbox of functions enabling a careful evaluation of nonlinear regression fits. To this end, we introduce a unified diagnostic framework with the R package nls tools. In this paper, the various features of the package are presented and exempli fied using a worked example from pulmonary medicine.",WOS:000365977800001,JOURNAL OF STATISTICAL SOFTWARE,"['LISTERIA-MONOCYTOGENES', 'HEART-FAILURE', 'OXYGEN-UPTAKE', 'GROWTH', 'EXERCISE', 'CURVES']",A Toolbox for Nonlinear Regression in R: The Package nlstools,2015
2188,"We study the asymptotic behavior of the Maximum Likelihood and Least Squares Estimators of a k-monotone density go at a fixed point x(0) when k > 2. We find that the jib derivative of the estimators at x(0) converges at the rate n(-(k-j)/(2k+1)) for j = 0.... k - 1. The limiting distribution depends on an almost surely uniquely defined stochastic process H-k that stays above (below) the k-fold integral of Brownian motion plus a deterministic drift when k is even (odd). Both the MLE and LSE are known to be splines of degree k - I with simple knots. Establishing the order of the random gap tau(+)(n) - tau(-)(n) where tau(+/-)(n) denote two successive knots, is a key ingredient of the proof of the main results. We show that this ""gap problem"" can be solved if a conjecture about the upper bound on the error in a particular Hermite interpolation via odd-degree splines holds.",WOS:000253077800014,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'POPULATION PHARMACOKINETICS', 'REGRESSION']",Estimation of a k-monotone density: Limit distribution theory and the spline connection,2007
2189,,WOS:000208589700005,R JOURNAL,,The hwriter Package Composing HTML documents with R objects,2009
2190,"This paper studies the sparsistency and rates of convergence for estimating sparse covariance and precision matrices based on penalized likelihood with nonconvex penalty functions. Here, sparsistency refers to the property that all parameters that are zero are actually estimated as zero with probability tending to one. Depending on the case of applications, sparsity priori may occur on the covariance matrix, its inverse or its Cholesky decomposition. We study these three sparsity exploration problems under a unified framework with a general penalty function. We show that the rates of convergence for these problems under the Frobenius norm are of order (s(n) log p(n)/n)(1/2), where s(n) is the number of nonzero elements, p(n) is the size of the covariance matrix and n is the sample size. This explicitly spells out the contribution of high-dimensionality is merely of a logarithmic factor. The conditions on the rate with which the tuning parameter lambda(n) goes to 0 have been made explicit and compared under different penalties. As a result, for the L(1)-penalty, to guarantee the sparsistency and optimal rate of convergence, the number of nonzero elements should be small: s'(n) = O(p(n)) at most, among O(p(n)(2)) parameters, for estimating sparse covariance or correlation matrix, sparse precision or inverse correlation matrix or sparse Cholesky factor, where s'(n) is the number of the nonzero elements on the off-diagonal entries. On the other hand, using the SCAD or hard-thresholding penalty functions, there is no such a restriction.",WOS:000271673700019,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', 'LONGITUDINAL DATA', 'VARIABLE SELECTION', 'ORACLE PROPERTIES', 'LASSO', 'MODELS', 'REGRESSION']",SPARSISTENCY AND RATES OF CONVERGENCE IN LARGE COVARIANCE MATRIX ESTIMATION,2009
2191,"Strong consistency and asymptotic normality of the quasi-maximum likelihood estimator are given for a general class of multidimensional causal processes. For particular cases already studied in the literature [for instance univariate or multivariate ARCH(infinity) processes], the assumptions required for establishing these results are often weaker than existing conditions. The QMLE asymptotic behavior is also given for numerous new examples of univariate or multivariate processes (for instance TARCH or NLARCH processes).",WOS:000268605000006,ANNALS OF STATISTICS,"['ARCH MODELS', 'GARCH PROCESSES', 'CONSISTENCY', 'VOLATILITY']",ASYMPTOTIC NORMALITY OF THE QUASI-MAXIMUM LIKELIHOOD ESTIMATOR FOR MULTIDIMENSIONAL CAUSAL PROCESSES,2009
2192,In this paper we study the aggregation problem that can be formulated as follows. Assume that we have a family of estimators F built on the basis of available observations. The goal is to construct a new estimator whose risk is as close as possible to that of the best estimator in the family. We propose a general aggregation scheme that is universal in the following sense: it applies for families of arbitrary estimators and a wide variety of models and global risk measures. The procedure is based on comparison of empirical estimates of certain linear functionals with estimates induced by the family F. We derive oracle inequalities and show that they are unimprovable in some sense. Numerical results demonstrate good practical behavior of the procedure.,WOS:000263129000020,ANNALS OF STATISTICS,"['NONPARAMETRIC REGRESSION', 'ORACLE INEQUALITIES', 'MODEL SELECTION', 'COMPLEXITY']",A UNIVERSAL PROCEDURE FOR AGGREGATING ESTIMATORS,2009
2193,"Data frames are integral to R. They provide a standard format for passing data to model-fitting and plotting functions, and this standard makes it easier for experienced users to learn new functions that accept data as a single data frame. Still, many data sets do not easily fit into a single data frame; data sets in ecology with a so-called fourth-corner problem provide important examples. Manipulating such inherently multiple-table data using several data frames can result in long and difficult-to-read workflows. We introduce the R multitable package to provide new data storage objects called data.list objects,which extend the data.frame concept to explicitly multiple-table settings. Like data frames, data lists are lists of variables stored as vectors; what is new is that these vectors have dimension attributes that make accessing and manipulating them easier. As data.list objects can be coerced to data.frame objects, they can be used with all R functions that accept an object that is coercible to a data.frame.",WOS:000312289400001,JOURNAL OF STATISTICAL SOFTWARE,"['SPECIES TRAITS', '4TH-CORNER PROBLEM']",Multiple-Table Data in R with the multitable Package,2012
2194,,WOS:000208589700001,R JOURNAL,,Special section: The Future of R,2009
2195,"Detecting and measuring lag-dependencies is very important in time-series analysis. This study is commonly carried out by focusing on the linear lag-dependencies via the well-known autocorrelogram. However, in practice, there are many situations in which the autocorrelogram fails because of the nonlinear structure of the serial dependence.
To cope with this problem, in this paper the R package SDD is introduced. Among the available approaches to analyze the lag-dependencies in an omnibus way, the SDD package considers the autodependogram and some of its variants. The autodependogram, defined by computing the classical Pearson chi(2)-statistic at various lags, is a graphical device recently proposed in the literature to analyze lag-dependencies. The concept of reproducibility probability, and several density-based measures of divergence, are considered to define the variants of the autodependogram. An application to daily returns of the Swiss Market Index is also presented to exemplify the use of the package.",WOS:000352917500001,JOURNAL OF STATISTICAL SOFTWARE,"['TIME-SERIES', 'P-VALUES', 'AUTODEPENDOGRAM', 'INDEPENDENCE', 'STATISTICS']",SDD: An R Package for Serial Dependence Diagrams,2015
2196,"penalized is a flexible, extensible, and efficient MATLAB toolbox for penalized maximum likelihood. penalized allows you to fit a generalized linear model (gaussian, logistic, poisson, or multinomial) using any of ten provided penalties, or none. The toolbox can be extended by creating new maximum likelihood models or new penalties. The toolbox also includes routines for cross-validation and plotting.",WOS:000389072400001,JOURNAL OF STATISTICAL SOFTWARE,"['VARIABLE SELECTION', 'ORACLE PROPERTIES', 'LASSO', 'REGULARIZATION', 'REGRESSION', 'SHRINKAGE']",penalized: A MATLAB Toolbox for Fitting Generalized Linear Models with Penalties,2016
2197,"R users can often solve optimization tasks easily using the tools in the optim function in the stats package provided by default on R installations. However, there are many other optimization and nonlinear modelling tools in R or in easily installed add-on packages. These present users with a bewildering array of choices. optimx is a wrapper to consolidate many of these choices for the optimization of functions that are mostly smooth with parameters at most bounds-constrained. We attempt to provide some diagnostic information about the function, its scaling and parameter bounds, and the solution characteristics. optimx runs a battery of methods on a given problem, thus facilitating comparative studies of optimization algorithms for the problem at hand. optimx can also be a useful pedagogical tool for demonstrating the strengths and pitfalls of different classes of optimization approaches including Newton, gradient, and derivative-free methods.",WOS:000294231700001,JOURNAL OF STATISTICAL SOFTWARE,"['CONJUGATE-GRADIENT METHOD', 'UNCONSTRAINED OPTIMIZATION', 'MINIMIZATION']",Unifying Optimization Algorithms to Aid Software System Users: optimx for R,2011
2198,"This note presents the R package bayesGARCH which provides functions for the Bayesian estimation of the parsimonious and effective GARCH(1,1) model with Student-t innovations. The estimation procedure is fully automatic and thus avoids the tedious task of tuning an MCMC sampling algorithm. The usage of the package is shown in an empirical application to exchange rate log-returns.",WOS:000208590000007,R JOURNAL,,"Bayesian Estimation of the GARCH(1,1) Model with Student-t Innovations",2010
2199,"We briefly review SI units, and discuss R packages that deal with measurement units, their compatibility and conversion. Built upon udunits2 and the UNIDATA udunits library, we introduce the package units that provides a class for maintaining unit metadata. When used in expression, it automatically converts units, and simplifies units of results when possible; in case of incompatible units, errors are raised. The class flexibly allows expansion beyond predefined units. Using units may eliminate a whole class of potential scientific programming mistakes. We discuss the potential and limitations of computing with explicit units.",WOS:000395669800033,R JOURNAL,,Measurement Units in R,2016
2200,"This document provides a brief introduction to the R package gss for nonparametric statistical modeling in a variety of problem settings including regression, density estimation, and hazard estimation. Functional ANOVA (analysis of variance) decompositions are built into models on product domains, and modeling and inferential tools are provided for tasks such as interval estimates, the ""testing"" of negligible model terms, the handling of correlated data, etc. The methodological background is outlined, and data analysis is illustrated using real-data examples.",WOS:000341584400001,JOURNAL OF STATISTICAL SOFTWARE,"['NONPARAMETRIC REGRESSION', 'CROSS-VALIDATION', 'APPROXIMATION', 'DIAGNOSTICS']",Smoothing Spline ANOVA Models: R Package gss,2014
2201,"We develop a canonical framework for the study of the problem of registration of multiple point processes subjected to warping, known as the problem of separation of amplitude and phase variation. The amplitude variation of a real random function {Y(x) : x is an element of [0, 1]} corresponds to its random oscillations in the y-axis, typically encapsulated by its (co) variation around a mean level. In contrast, its phase variation refers to fluctuations in the x-axis, often caused by random time changes. We formalise similar notions for a point process, and nonparametrically separate them based on realisations of i.i.d. copies {Pi(i)} of the phase-varying point process. A key element in our approach is to demonstrate that when the classical phase variation assumptions of Functional Data Analysis (FDA) are applied to the point process case, they become equivalent to conditions interpretable through the prism of the theory of optimal transportation of measure. We demonstrate that these induce a natural Wasserstein geometry tailored to the warping problem, including a formal notion of bias expressing over-registration. Within this framework, we construct nonparametric estimators that tend to avoid over-registration in finite samples. We show that they consistently estimate the warp maps, consistently estimate the structural mean, and consistently register the warped point processes, even in a sparse sampling regime. We also establish convergence rates, and derivev root n-consistency and a central limit theorem in the Cox process case under dense sampling, showing rate optimality of our structural mean estimator in that case.",WOS:000372594300012,ANNALS OF STATISTICS,"['MAXIMUM-LIKELIHOOD-ESTIMATION', 'GAUSSIAN RANDOM-FIELDS', 'FUNCTIONAL DATA', 'EVENT DATA', 'CURVES', 'ALIGNMENT', 'SPACE', 'DEFORMATIONS', 'REGISTRATION', 'BARYCENTERS']",AMPLITUDE AND PHASE VARIATION OF POINT PROCESSES,2016
2202,"Linear models with fixed effects and many dummy variables are common in some fields. Such models are straightforward to estimate unless the factors have too many levels. The R package lfe solves this problem by implementing a generalization of the within transformation to multiple factors, tailored for large problems.",WOS:000330193300012,R JOURNAL,"['ALTERNATING PROJECTIONS', 'CONVERGENCE', 'DESIGNS', 'FIT']",lfe: Linear Group Fixed Effects,2013
2203,This paper provides an introduction to a software package called waved making available all code necessary for reproducing the figures in the recently published articles on the WaveD transform for wavelet deconvolution of noisy signals. The forward WaveD transforms and their inverses can be computed using any wavelet from the Meyer family. The WaveD coefficients can be depicted according to time and resolution in several ways for data analysis. The algorithm which implements the translation invariant WaveD transform takes full advantage of the fast Fourier transform (FFT) and runs in O(n(log n)(2)) steps only. The waved package includes functions to perform thresholding and fine resolution tuning according to methods in the literature as well as newly designed visual and statistical tools for assessing WaveD fits. We give a waved tutorial session and review benchmark examples of noisy convolutions to illustrate the non-linear adaptive properties of wavelet deconvolution.,WOS:000249734800001,JOURNAL OF STATISTICAL SOFTWARE,"['BOXCAR DECONVOLUTION', 'INVERSE PROBLEMS', 'DECOMPOSITION', 'SHRINKAGE']",The WaveD transform in R: Performs fast translation-invariant wavelet deconvolution,2007
2204,"This article describes the R package rdrobust, which provides data-driven graphical and inference procedures for RD designs. The package includes three main functions: rdrobust, rdbwselect and rdplot. The first function (rdrobust) implements conventional local-polynomial RD treatment effect point estimators and confidence intervals, as well as robust bias-corrected confidence intervals, for average treatment effects at the cutoff. This function covers sharp RD, sharp kink RD, fuzzy RD and fuzzy kink RD designs, among other possibilities. The second function (rdbwselect) implements several bandwidth selectors proposed in the RD literature. The third function (rdplot) provides data-driven optimal choices of evenly-spaced and quantile-spaced partition sizes, which are used to implement several data-driven RD plots.",WOS:000357431900005,R JOURNAL,['ESTIMATORS'],rdrobust: An R Package for Robust Nonparametric Inference in Regression-Discontinuity Designs,2015
2205,"Generalized additive models for location, scale and shape are a flexible class of regression models that allow to model multiple parameters of a distribution function, such as the mean and the standard deviation, simultaneously. With the R package gamboostLSS, we provide a boosting method to fit these models. Variable selection and model choice are naturally available within this regularized regression framework. To introduce and illustrate the R package gamboostLSS and its infrastructure, we use a data set on stunted growth in India. In addition to the specification and application of the model itself, we present a variety of convenience functions, including methods for tuning parameter selection, prediction and visualization of results. The package gamboostLSS is available from the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=gamboostLSS.",WOS:000392512300001,JOURNAL OF STATISTICAL SOFTWARE,"['CHILD GROWTH STANDARDS', 'BOOSTING ALGORITHMS', 'REGRESSION-MODELS', 'MALNUTRITION', 'PREDICTION', 'EVOLUTION']",gamboostLSS: An R Package for Model Building and Variable Selection in the GAMLSS Framework,2016
2206,The R package quantreg.nonpar implements nonparametric quantile regression methods to estimate and make inference on partially linear quantile models. quantreg.nonpar obtains point estimates of the conditional quantile function and its derivatives based on series approximations to the nonparametric part of the model. It also provides pointwise and uniform confidence intervals over a region of covariate values and/or quantile indices for the same functions using analytical and resampling methods. This paper serves as an introduction to the package and displays basic functionality of the functions contained within.,WOS:000395669800024,R JOURNAL,,quantreg. nonpar: An R Package for Performing Nonparametric Series Quantile Regression,2016
2207,"We Study the asymptotic properties of bridge estimators in sparse, high-dimensional, linear regression models when the number of covariates may increase to infinity with the sample size. We are particularly interested in the use of bridge estimators to distinguish between covariates whose coefficients are zero and covariates whose coefficients are nonzero. We show that under appropriate conditions, bridge estimators correctly select covariates with nonzero coefficients with probability converging to one and that the estimators of nonzero coefficients have the same asymptotic distribution that they would have if the zero coefficients were known in advance. Thus, bridge estimators have an oracle property in the sense of Fan and Li [J. Amer. Statist. Assoc. 96 (2001) 1348-1360] and Fan and Peng [Ann. Statist. 32 (2004) 928-961]. In general, the oracle property holds only if the number of covariates is smaller than the sample size. However, under a partial orthogonality condition in which the covariates of the zero coefficients are uncorrelated or weakly correlated with the covariates of nonzero coefficients, we show that marginal bridge estimators can correctly distinguish between covariates with nonzero and zero coefficients with probability converging to one even when the number of covariates is greater than the sample size.",WOS:000254502700004,ANNALS OF STATISTICS,"['NONCONCAVE PENALIZED LIKELIHOOD', '2-WAY SEMILINEAR MODEL', 'MICROARRAY DATA', 'VARIABLE SELECTION', 'LASSO', 'CONSISTENCY', 'PARAMETERS', 'NORMALIZATION', 'BEHAVIOR', 'P2/N']",Asymptotic properties of bridge estimators in sparse high-dimensional regression models,2008
2208,"We construct the ""expected signature matching"" estimator for differential equations driven by rough paths and we prove its consistency and asymptotic normality. We use it to estimate parameters of a diffusion and a fractional diffusions, that is, a differential equation driven by fractional Brownian motion.",WOS:000296995500008,ANNALS OF STATISTICS,"['STOCHASTIC CALCULUS', 'DIFFUSIONS']",PARAMETER ESTIMATION FOR ROUGH DIFFERENTIAL EQUATIONS,2011
2209,"Frailty models are very useful for analysing correlated survival data, when observations are clustered into groups or for recurrent events. The aim of this article is to present the new version of an R package called frailtypack. This package allows to fit Cox models and four types of frailty models (shared, nested, joint, additive) that could be useful for several issues within biomedical research. It is well adapted to the analysis of recurrent events such as cancer relapses and/or terminal events (death or lost to follow-up). The approach uses maximum penalized likelihood estimation. Right-censored or left-truncated data are considered. It also allows stratification and time-dependent covariates during analysis",WOS:000303804000001,JOURNAL OF STATISTICAL SOFTWARE,"['PROPORTIONAL HAZARDS MODEL', 'HETEROGENEITY', 'ALGORITHM']",frailtypack: An R Package for the Analysis of Correlated Survival Data with Frailty Models Using Penalized Likelihood Estimation or Parametrical Estimation,2012
2210,"Penalized splines can be viewed as BLUPs in a mixed model framework, which allows the use of mixed model software for smoothing. Thus, software originally developed for Bayesian analysis of mixed models can be used for penalized spline regression. Bayesian inference for nonparametric models enjoys the flexibility of nonparametric models and the exact inference provided by the Bayesian inferential machinery. This paper provides a simple, yet comprehensive, set of programs for the implementation of nonparametric Bayesian analysis in WinBUGS. Good mixing properties of the MCMC chains are obtained by using low-rank thin-plate splines, while simulation times per iteration are reduced employing WinBUGS specific computational tricks.",WOS:000232928800001,JOURNAL OF STATISTICAL SOFTWARE,"['LINEAR MIXED MODELS', 'VARIANCE']",Bayesian analysis for penalized spline regression using WinBUGS,2005
