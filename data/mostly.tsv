id	wos_id	accession_no	issn	issn_int	eissn	isbn	eisbn	art_no	doi	title	pubtype	doctype	pubyear	pubmonth	pubday	language	source	page_range	page_count	has_abstract	wos_id	abstract
8450	WOS:000275203800001	564HL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	grofit: Fitting Biological Growth Curves with R	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000275203800001	The grofit package was developed to fit many growth curves obtained under different conditions in order to derive a conclusive dose-response curve, for instance for a compound that potentially affects growth. grofit fits data to different parametric models and in addition provides a model free spline method to circumvent systematic errors that might occur within application of parametric methods. This amendment increases the reliability of the characteristic parameters (e. g., lag phase, maximal growth rate, stationary phase) derived from a single growth curve. By relating obtained parameters to the respective condition (e. g., concentration of a compound) a dose response curve can be derived that enables the calculation of descriptive pharma-/toxicological values like half maximum effective concentration (EC50). Bootstrap and cross-validation techniques are used for estimating confidence intervals of all derived parameters.
48550	WOS:000208590000008	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	cudaBayesreg: Bayesian Computation in CUDA	Journal	Article	2010	12	1	English	R JOURNAL	48-55	8	1	WOS:000208590000008	Graphical processing units are rapidly gaining maturity as powerful general parallel computing devices. The package cudaBayesreg uses GPU-oriented procedures to improve the performance of Bayesian computations. The paper motivates the need for devising high-performance computing strategies in the context of fMRI data analysis. Some features of the package for Bayesian analysis of brain fMRI data are illustrated. Comparative computing performance figures between sequential and parallel implementations are presented as well.
72081	WOS:000284598500001	685BH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	DTDA: An R Package to Analyze Randomly Truncated Data	Journal	Article	2010	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000284598500001	In this paper, the R package DTDA for analyzing truncated data is described. This package contains tools for performing three different but related algorithms to compute the nonparametric maximum likelihood estimator of the survival function in the presence of random truncation. More precisely, the package implements the algorithms proposed by Efron and Petrosian (1999) and Shen (2008), for analyzing randomly one-sided and two-sided (i.e., doubly) truncated data. These algorithms and some recent extensions are briefly reviewed. Two real data sets are used to show how DTDA package works in practice.
81254	WOS:000282853700005	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	simsum: Analyses of simulation studies including Monte Carlo error	Journal	Article	2010	1	1	English	STATA JOURNAL	369-385	17	1	WOS:000282853700005	A new Stata command, simsum, analyzes data from simulation studies. The data may comprise point estimates and standard errors from several analysis methods, possibly resulting from several different simulation settings. simsum can report bias, coverage, power, empirical standard error, relative precision, average model-based standard error, and the relative error of the standard error. Monte Carlo errors are available for all of these estimated quantities.
95617	WOS:000281593400001	647CM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	tolerance: An R Package for Estimating Tolerance Intervals	Journal	Article	2010	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-39	39	1	WOS:000281593400001	The tolerance package for R provides a set of functions for estimating and plotting tolerance limits. This package provides a wide-range of functions for estimating discrete and continuous tolerance intervals as well as for estimating regression tolerance intervals. An additional tool of the tolerance package is the plotting capability for the univariate and regression settings as well as for the multivariate normal setting. The tolerance package's capabilities are illustrated using simulated data sets. Formulas used for the estimation procedures are also presented.
102383	WOS:000284598400001	685BG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	US Census Spatial and Demographic Data in R: The UScensus2000 Suite of Packages	Journal	Article	2010	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000284598400001	The US Decennial Census is arguably the most important data set for social science research in the United States. The UScensus2000 suite of packages allows for convenient handling of the 2000 US Census spatial and demographic data. The goal of this article is to showcase the UScensus2000 suite of packages for R, to describe the data contained within these packages, and to demonstrate the helper functions provided for handling this data. The UScensus2000 suite is comprised of spatial and demographic data for the 50 states and Washington DC at four different geographic levels (block, block group, tract, and census designated place). The UScensus2000 suite also contains a number of functions for selecting and aggregating specific geographies or demographic information such as metropolitan statistical areas, counties, etc. These packages rely heavily on the spatial tools developed by Bivand, Pebesma, and Gomez-Rubio (2008), i.e., the sp and maptools packages. This article will provide the necessary background for working with this data set, helper functions, and finish with an applied spatial statistics example.
105054	WOS:000281588400001	647AU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	plink: An R Package for Linking Mixed-Format Tests Using IRT-Based Methods	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-33	33	1	WOS:000281588400001	The R package plink has been developed to facilitate the linking of mixed-format tests for multiple groups under a common item design using unidimensional and multidimensional IRT-based methods. This paper presents the capabilities of the package in the context of the unidimensional methods. The package supports nine unidimensional item response models (the Rasch model, 1PL, 2PL, 3PL, graded response model, partial credit and generalized partial credit model, nominal response model, and multiple-choice model) and four separate calibration linking methods (mean/sigma, mean/mean, Haebara, and Stocking-Lord). It also includes functions for importing item and/or ability parameters from common IRT software, conducting IRT true-score and observed-score equating, and plotting item response curves and parameter comparison plots.
113374	WOS:000281587700001	647AN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Controlled Optimal Design Program for the Logit Dose Response Mode	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000281587700001	The assessment of dose-response is an integral component of the drug development process. Parallel dose-response studies are conducted, customarily ,in preclinical and phase 1,2 clinical trials for this purpose. Practical constraints on dose range, dose levels and dose proportions are intrinsic issues in the design of dose response studies because of drug toxicity, efficacy, FDA regulations, protocol requirements, clinical trial logistics, and marketing issues.We provide a free on-line software package called Controlled Optimal Design 2.0 for generating controlled optimal designs that can incorporate prior information and multiple objectives, and meet multiple practical constraints at the same time. Researchers can either run the web-based design program or download its stand-alone version to construct the desired multiple-objective controlled Bayesian optimal designs. Because researchers often adopt ad-hoc design schemes such as the equal allocation rules with out knowing how efficient such designs would be for the design problem, the program also evaluates the efficiency of user-supplied design
155093	WOS:000208589900006	V27BY	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	neuralnet: Training of Neural Networks	Journal	Article	2010	6	1	English	R JOURNAL	30-38	9	1	WOS:000208589900006	Artificial neural networks are applied in many situations. neuralnet is built to train multi-layer perceptrons in the context of regression analyses, i.e. to approximate functional relationships between covariates and response variables. Thus, neural networks are used as extensions of generalized linear models. neuralnet is a very flexible package. The backpropagation algorithm and three versions of resilient backpropagation are implemented and it provides a custom-choice of activation and error function. An arbitrary number of covariates and response variables as well as of hidden layers can theoretically be included. The paper gives a brief introduction to multilayer perceptrons and resilient backpropagation and demonstrates the application of neuralnet using the data set infert, which is contained in the R distribution.
178761	WOS:000275203900001	564HM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Simple Algorithms to Calculate Asymptotic Null Distributions of Robust Tests in Case-Control Genetic Association Studies in R	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000275203900001	The case-control study is an important design for testing association between genetic markers and a disease. The Cochran-Armitage trend test (CATT) is one of the most commonly used statistics for the analysis of case-control genetic association studies. The asymptotically optimal CATT can be used when the underlying genetic model (mode of inheritance) is known. However, for most complex diseases, the underlying genetic models are unknown. Thus, tests robust to genetic model misspecification are preferable to the model-dependant CATT. Two robust tests, MAX3 and the genetic model selection (GMS), were recently proposed. Their asymptotic null distributions are often obtained by Monte-Carlo simulations, because they either have not been fully studied or involve multiple integrations. In this article, we study how components of each robust statistic are correlated, and find a linear dependence among the components. Using this new finding, we propose simple algorithms to calculate asymptotic null distributions for MAX3 and GMS, which greatly reduce the computing intensity. Furthermore, we have developed the R package Rassoc implementing the proposed algorithms to calculate the empirical and asymptotic p values for MAX3 and GMS as well as other commonly used tests in case-control association studies. For illustration, Rassoc is applied to the analysis of case-control data of 17 most significant SNPs reported in four genome-wide association studies.
233381	WOS:000286291100001	707MQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A suite of commands for fitting the skew-normal and skew-t models	Journal	Article	2010	1	1	English	STATA JOURNAL	507-539	33	1	WOS:000286291100001	Nonnormal data arise often in practice, prompting the development of flexible distributions for modeling such situations. In this article, we describe two multivariate distributions, the skew-normal and the skew-t, which can be used to model skewed and heavy-tailed continuous data. We then discuss some inferential issues that can arise when fitting these distributions to real data. We also consider the use of these distributions in a regression setting for more flexible parametric modeling of the conditional distribution given other predictors. We present commands for fitting univariate and multivariate skew-normal and skew-t regressions in Stata (skewnreg, skewtreg, mskewnreg, and mskewtreg) as well as sonic postestimation features (predict and skewrplot). We also demonstrate the use of the commands for the analysis of the famous Australian Institute of Sport data and U.S. precipitation data.
245720	WOS:000281593000001	647CI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Zero-Coupon Yield Curve Estimation with the Package terms	Journal	Article	2010	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000281593000001	Since zero-coupon rates are rarely directly observable, they have to be estimated from market data. In this paper we review several widely-used parametric term structure estimation methods. We propose a weighted constrained optimization procedure with analytical gradients and a globally optimal start parameter search algorithm. Moreover, we introduce the R package termstrc, which offers a wide range of functions for term structure estimation based on static and dynamic coupon bond and yield data sets. It provides extensive summary statistics and plots to compare the results of the different estimation methods. We illustrate the application of the package through practical examples using market data from European government bonds and yields.
282255	WOS:000281588300001	647AT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Introducing COZIGAM: An R Package for Unconstrained and Constrained Zero-Inflated Generalized Additive Model Analysis	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000281588300001	Zero-inflation problem is very common in ecological studies as well as other areas. Nonparametric regression with zero-inflated data may be studied via the zero-inflated generalized additive model (ZIGAM), which assumes that the zero-inflated responses come from a probabilistic mixture of zero and a regular component whose distribution belongs to the 1-parameter exponential family. With the further assumption that the probability of non-zero-inflation is some monotonic function of the mean of the regular component, we propose the constrained zero-inflated generalized additive model (COZIGAM) for analyzing zero-inflated data. When the hypothesized constraint obtains, the new approach provides a unified framework for modeling zero-inflated data, which is more parsimonious and efficient than the unconstrained ZIGAM. We have developed an R package COZIGAM which contains functions that implement an iterative algorithm for fitting ZIGAMs and COZIGAMs to zero-inflated data based on the penalized likelihood approach. Other functions included in the package are useful for model prediction and model selection. We demonstrate the use of the COZIGAM package via some simulation studies and a real application.
357839	WOS:000208590000012	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	What's New?	Journal	Article	2010	12	1	English	R JOURNAL	74-76	3	1	WOS:000208590000012	We discuss how news in R and addon packages can be disseminated. R 2.10.0 has added facilities for computing on news in a common plain text format. In R 2.12.0, the Rd format has been further enhanced so that news can be very conveniently recorded in Rd, allowing for both improved rendering and the development of new news services.
369381	WOS:000284004600001	677PP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An R Package for Dynamic Linear Models	Journal	Article	2010	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000284004600001	We describe an R package focused on Bayesian analysis of dynamic linear models. The main features of the package are its flexibility to deal with a variety of constant or time-varying, univariate or multivariate models, and the numerically stable singular value decomposition-based algorithms used for filtering and smoothing. In addition to the examples of "out-of-the-box" use, we illustrate how the package can be used in advanced applications to implement a Gibbs sampler for a user-specified model.
381044	WOS:000208589900008	V27BY	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Online Reproducible Research: An Application to Multivariate Analysis of Bacterial DNA Fingerprint Data	Journal	Article	2010	6	1	English	R JOURNAL	44-52	9	1	WOS:000208589900008	This paper presents an example of online reproducible multivariate data analysis. This example is based on a web page providing an online computing facility on a server. HTML forms contain editable R code snippets that can be executed in any web browser thanks to the Rweb software. The example is based on the multivariate analysis of DNA fingerprints of the internal bacterial flora of the poultry red mite Dermanyssus gallinae. Several multivariate data analysis methods from the ade4 package are used to compare the fingerprints of mite pools coming from various poultry farms. All the computations and graphical displays can be redone interactively and further explored online, using only a web browser. Statistical methods are detailed in the duality diagram framework, and a discussion about online reproducibility is initiated.
383387	WOS:000208590000006	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	stringr: modern, consistent string processing	Journal	Article	2010	12	1	English	R JOURNAL	38-40	3	1	WOS:000208590000006	String processing is not glamorous, but it is frequently used in data cleaning and preparation. The existing string functions in R are powerful, but not friendly. To remedy this, the stringr package provides string functions that are simpler and more consistent, and also fixes some functionality that R is missing compared to other programming languages.
409185	WOS:000276954600001	586ZH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An R Package for Assessing Drug Synergism/Antagonism	Journal	Article	2010	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000276954600001	Synergistic and antagonistic drug interactions are important to consider when developing mixtures of anticancer or other types of drugs. Boik, Newman, and Boik (2008) proposed the Mix Low method as an alternative to the Median-Effect method of Chou and Talalay (1984) for estimating drug interaction indices. One advantage of the Mix Low method is that then on linear mixed-effects model used to estimate parameters of concentration response curves can provide more accurate parameter estimates than the log linearization and least-squares analysis used in the Median-Effect method. This paper introduces the mix low package in R, an implementation of the Mix Low method. Results are reported for a small simulation study.
435192	WOS:000281593500001	647CN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An R Package for the Adaptive Management of Epidemiological Interventions	Journal	Article	2010	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000281593500001	The amei package for R is a tool that provides a flexible statistical framework for generating optimal epidemiological interventions that are designed to minimize the total expected cost of an emerging epidemic. Uncertainty regarding the underlying disease parameters is propagated through to the decision process via Bayesian posterior inference. The strategies produced through this framework are adaptive: vaccination schedules are iteratively adjusted to reflect the anticipated trajectory of the epidemic given the current population state and updated parameter estimates. This document briefly covers the background and methodology underpinning the implementation provided by the package and contains extensive examples showing the functions and methods in action.
454979	WOS:000284004900001	677PS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Interactive Teaching Tools for Spatial Sampling	Journal	Article	2010	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000284004900001	The statistical analysis of data which is measured over a spatial region is well established as a scientific tool which makes considerable contributions to a wide variety of application areas. Further development of these tools also remains a central part of the research scene in statistics. However, understanding of the concepts involved often bene fits from an intuitive and experimental approach, as well as a formal description of models and methods. This paper describes software which is intended to assist in this understanding. The role of simulation is advocated, in order to explain the meaning of spatial correlation and to interpret the parameters involved in standard models. Realistic scenarios where decisions on the locations of sampling points in a spatial setting are required are also described. Students are provided with a variety of sampling strategies and invited to select the most appropriate one in two different settings. One involves water sampling in the lagoon of the Mururoa Atoll while the other involves sea bed sampling in a Scottish firth. Once a student has decided on a sampling strategy, simulated data are provided for further analysis. This extends the range of teaching activity from the analysis of data collected by others to involvement in data collection and the need to grapple with issues of design. It is argued that this approach has significant bene fits in learning. The software which implements these tools is built on existing R packages, using rpanel controls for the geoR geostatistical simulation and modelling tools. The operation and construction of the software are described in detail. The software is made available as additional functions rp.geosim, rp.mururoa and rp.firth in the rpanel package.
463498	WOS:000281588200001	647AS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	R Package distrMod: S4 Classes and Methods for Probability Models	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000281588200001	Package distrMod provides an object oriented (more specifically S4-style) implementation of probability models. Moreover, it contains functions and methods to compute minimum criterion estimators - in particular, maximum likelihood and minimum distance estimators.
523250	WOS:000282057200001	653BI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Feature Selection with the Boruta Package	Journal	Article	2010	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000282057200001	This article describes a R package Boruta, implementing a novel feature selection algorithm for finding all relevant variables. The algorithm is designed as a wrapper around a Random Forest classification algorithm. It iteratively removes the features which are proved by a statistical test to be less relevant than random probes. The Boruta package provides a convenient interface to the algorithm. The short description of the algorithm and examples of its application are presented.
525658	WOS:000280076600003	627XD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Model fit assessment via marginal model plots	Journal	Article	2010	1	1	English	STATA JOURNAL	215-225	11	1	WOS:000280076600003	We present a new Stata command, mmp, that generates marginal model plots (Cook and Weisberg, 1.997, Journal of the American Statistical Association 92: 490-499) for a regression model. These plots allow for the comparison of the fitted model with a nonparametric or semiparametric model fit. The user may precisely specify how the alternative fit is computed. Demonstrations are given for logistic and linear regressions, using the lowess smoother to generate the alternate fit. Guidelines for the use of map under different models (through glm and other commands) and different smoothers (such as lpoly) are also presented.
525872	WOS:000282853700010	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Translation from narrative text to standard codes variables with Stata	Journal	Article	2010	1	1	English	STATA JOURNAL	458-481	24	1	WOS:000282853700010	In this article, we describe screening, a new Stata command for data management that can be used to examine the content of complex narrative-text variables to identify one or more user-defined keywords. The command is useful when dealing with string data contaminated with abbreviations, typos, or mistakes. A rich set of options allows a direct translation from the original narrative string to a user-defined standard coding scheme. Moreover, screening is flexible enough to facilitate the merging of information from different sources and to extract or reorganize the content of string variables.
527223	WOS:000286291100006	707MQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A simple feasible procedure to fit models with high-dimensional fixed effects	Journal	Article	2010	1	1	English	STATA JOURNAL	628-649	22	1	WOS:000286291100006	In this article, we describe an iterative approach for the estimation of linear regression models with high-dimensional fixed effects. This approach is computationally intensive but imposes minimum memory requirements. We also show that the approach can be extended to nonlinear models and to more than two high-dimensional fixed effects.
531534	WOS:000284597100001	685AT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	DAKS: An R Package for Data Analysis Methods in Knowledge Space Theory	Journal	Article	2010	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	31	1	WOS:000284597100001	Knowledge space theory is part of psychometrics and provides a theoretical framework for the modeling, assessment, and training of knowledge. It utilizes the idea that some pieces of knowledge may imply others, and is based on order and set theory. We introduce the R package D A K S for performing basic and advanced operations in knowledge space theory. This package implements three inductive item tree analysis algorithms for deriving quasi orders from binary data, the original, corrected, and minimized corrected algorithms, in sample as well as population quantities. It provides functions for computing population and estimated asymptotic variances of and one and two sample Z tests for the diff fit measures, and for switching between test item and knowledge state representations. Other features are a function for computing response pattern and knowledge state frequencies, a data (based on a finite mixture latent variable model) and quasi order simulation tool, and a Hasse diagram drawing device. We describe the functions of the package and demonstrate their usage by real and simulated data examples.
549017	WOS:000277918600011	599MZ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: The statsby strategy	Journal	Article	2010	1	1	English	STATA JOURNAL	143-151	9	1	WOS:000277918600011	The statsby command collects statistics from a command yielding r-class or e-class results across groups of observations and yields a new reduced dataset. statsby is commonly used to graph such data in comparisons of groups; the subsets and total options of statsby are particularly useful in this regard. In this article; I give examples of using this approach to produce box plots and plots of confidence intervals.
554415	WOS:000276954700001	586ZI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	rpartOrdinal: An R Package for Deriving a Classification Tree for Predicting an Ordinal Response	Journal	Article	2010	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000276954700001	This paper describes an R package, rpartOrdinal, that implements alternative splitting functions for fitting a classification tree when interest lies in predicting an ordinal response. This includes the generalized Gini impurity function, which was introduced as a method for predicting an ordinal response by including costs of misclassification into the impurity function, as well as an alternative ordinal impurity function due to Piccarreta (2008) that does not require the assignment of misclassification costs. The ordered twoing splitting method, which is not defined as a decrease in node impurity, is also included in the package. Since, in the ordinal response setting, misclassifying observations to adjacent categories is a less egregious error than misclassifying observations to distant categories, this pack a geal so includes a function for estimating an ordinal measure of association, the gamma statistic.
595424	WOS:000286291100002	707MQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Fitting heterogeneous choice models with oglm	Journal	Article	2010	1	1	English	STATA JOURNAL	540-567	28	1	WOS:000286291100002	When a binary or ordinal regression model incorrectly assumes that error variances are the same for all cases, the standard errors are wrong and (unlike ordinary least squares regression) the parameter estimates are biased. Heterogeneous choice models (also known as location scale models or heteroskedastic ordered models) explicitly specify the determinants of heteroskedasticity in an attempt to correct for it. Such models are also useful when the variance itself is of substantive interest. This article illustrates how the author's Stata program oglm (ordinal generalized linear models) can be used to fit heterogeneous choice and related models. It shows that two other models that have appeared in the literature (Allison's model for group comparisons and Hauser and Andrew's logistic response model with proportionality constraints) are special cases of a heterogeneous choice model and alternative parameterizations of it. The article further argues that heterogeneous choice models may sometimes be an attractive alternative to other ordinal regression models, such as the generalized ordered logit model fit by gologit2. Finally, the article offers guidelines on how to interpret, test, and modify heterogeneous choice models.
657251	WOS:000208590000005	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	dclone: Data Cloning in R	Journal	Article	2010	12	1	English	R JOURNAL	29-37	9	1	WOS:000208590000005	The dclone R package contains low level functions for implementing maximum likelihood estimating procedures for complex models using data cloning and Bayesian Markov Chain Monte Carlo methods with support for JAGS, WinBUGS and OpenBUGS.
672588	WOS:000208590000004	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	hglm: A Package for Fitting Hierarchical Generalized Linear Models	Journal	Article	2010	12	1	English	R JOURNAL	20-28	9	1	WOS:000208590000004	We present the hglm package for fitting hierarchical generalized linear models. It can be used for linear mixed models and generalized linear mixed models with random effects for a variety of links and a variety of distributions for both the outcomes and the random effects. Fixed effects can also be fitted in the dispersion part of the model.
687986	WOS:000282853700004	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Using Stata with PHASE and Haploview: Commands for importing and exporting data	Journal	Article	2010	1	1	English	STATA JOURNAL	359-368	10	1	WOS:000282853700004	Modern genetics studies require the use of many specialty software programs for various aspects of the statistical analysis. PHASE is a program often used to reconstruct haplotypes from genotype data, and Haploview is a program often used to visualize and analyze single nucleotide polymorphism data. Three new commands are described for performing these three steps: 1) exporting genotype data stored in Stata to PHASE, 2) importing the resulting inferred haplotypes back into Stata, and 3) exporting the haplotype/single nucleotide polymorphism data from Stata to Haploview.
718782	WOS:000277918600006	599MZ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	riskplot: A graphical aid to investigate the effect of multiple categorical risk factors	Journal	Article	2010	1	1	English	STATA JOURNAL	61-68	8	1	WOS:000277918600006	In this article, we describe a command, riskplot, aiming to provide a visual aid to assess the strength, importance, and consistency of risk factor effects. The plotted form is a dendrogram that branches out as it moves from left to right. It displays the mean of some score or the absolute risk of some outcome for a sample that is progressively disaggregated by a sequence of categorical risk factors. Examples of the application of the new command are drawn from the analysis of depression and fluid intelligence in a sample of elderly men and women.
749084	WOS:000282853700007	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	metaan: Random-effects meta-analysis	Journal	Article	2010	1	1	English	STATA JOURNAL	395-407	13	1	WOS:000282853700007	This article describes the new meta-analysis command metaan, which can be used to perform fixed- or random-effects meta-analysis. Besides the standard DerSimonian and Laird approach, metaan offers a wide choice of available models: maximum likelihood, profile likelihood, restricted maximum likelihood, and a permutation model. The command reports a variety of heterogeneity measures, including Cochran's Q, I(2), H(M)(2), and the between-studies variance estimate (T) over cap (2). A forest plot and a graph of the maximum likelihood function can also be generated.
754696	WOS:000275204000001	564HN	1548-7660	1548766	NULL	NULL	NULL	ARTN 9	NULL	Solving Differential Equations in R: Package deSolve	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000275204000001	In this paper we present the R package deSolve to solve initial value problems (IVP) written as ordinary differential equations (ODE), differential algebraic equations (DAE) of index 0 or 1 and partial differential equations (PDE), the latter solved using the method of lines approach. The differential equations can be represented in R code or as compiled code. In the latter case, R is used as a tool to trigger the integration and post-process the results, which facilitates model development and application, whilst the compiled code significantly increases simulation speed. The methods implemented are efficient, robust, and well documented public-domain Fortran routines. They include four integrators from the ODEPACK package (LSODE, LSODES, LSODA, LSODAR), DVODE and DASPK2.0. In addition, a suite of Runge-Kutta integrators and special-purpose solvers to efficiently integrate 1-, 2- and 3-dimensional partial differential equations are available. The routines solve both stiff and non-stiff systems, and include many options, e. g., to deal in an efficient way with the sparsity of the Jacobian matrix, or finding the root of equations. In this article, our objectives are threefold: (1) to demonstrate the potential of using R for dynamic modeling, (2) to highlight typical uses of the different methods implemented and (3) to compare the performance of models specified in R code and in compiled code for a number of test cases. These comparisons demonstrate that, if the use of loops is avoided, R code can efficiently integrate problems comprising several thousands of state variables. Nevertheless, the same problem may be solved from 2 to more than 50 times faster by using compiled code compared to an implementation using only R code. Still, amongst the bene fits of R are a more flexible and interactive implementation, better readability of the code, and access to R's high-level procedures. deSolve is the successor of packageo desolve which will be deprecated in the future; it is free software and distributed under the GNU General Public License, as part of the R software project.
790457	WOS:000275203300001	564HG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MCMC Methods for Multi-Response Generalized Linear Mixed Models: The MCMCglmm R Package	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000275203300001	Generalized linear mixed models provide a flexible framework for modeling a range of data, although with non-Gaussian response variables the likelihood cannot be obtained in closed form. Markov chain Monte Carlo methods solve this problem by sampling from a series of simpler conditional distributions that can be evaluated. The R package M C M C g l m m implements such an algorithm for a range of model fitting problems. More than one response variable can be analyzed simultaneously, and these variables are allowed to follow Gaussian, Poisson, multi(bi) nominal, exponential, zero-inflated and censored distributions. A range of variance structures are permitted for the random effects, including interactions with categorical or continuous variables (i.e., random regression), and more complicated variance structures that arise through shared ancestry, either through a pedigree or through a phylogeny. Missing values are permitted in the response variable(s) and data can be known up to some level of measurement error as in meta-analysis. All simulation is done in C/C++ using the CSparse library for sparse linear systems.
790948	WOS:000281593600001	647CO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	depmixS4: An R Package for Hidden Markov Models	Journal	Article	2010	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000281593600001	depmixS4 implements a general framework for defining and estimating dependent mixture models in the R programming language. This includes standard Markov models, latent/hidden Markov models, and latent class and finite mixture distribution models. The models can be fitted on mixed multivariate data with distributions from the g l m family, the (logistic) multinomial, or the multivariate normal distribution. Other distributions can be added easily, and an example is provided with the exgaus distribution. Parameters are estimated by the expectation-maximization (EM) algorithm or, when (linear) constraints are imposed on the parameters, by direct numerical optimization with the Rsolnp or Rdonlp 2 routines.
801137	WOS:000281583900001	646ZD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Fortran 90 Program for Confirmatory Analysis of Variance	Journal	Article	2010	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000281583900001	There are different confirmatory techniques to compare means, like hypothesis testing and (Bayesian) model selection. However, there is no software package in which these techniques are available. A Fortran 90 program is written, which enables researchers to apply these techniques to their data. Besides traditional hypotheses, like H-0 : mu(1) = mu(2) = mu(3) and H-u : mu(1); mu(2); mu(3), order-restricted hypotheses, like mu(1) > mu(2) > mu(3) or mu(1) > mu(2) = mu(3) or mu(1) > mu(2) < mu(3), can be evaluated.
814316	WOS:000281587500001	647AL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	PyMC: Bayesian Stochastic Modelling in Python	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-81	81	1	WOS:000281587500001	This user guide describes a Python package, PyMC, that allows users to efficiently code a probabilistic model and draw samples from its posterior distribution using Markov chain Monte Carlo techniques.
833887	WOS:000277918600001	599MZ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A conversation with Kit Baum	Journal	Editorial Material	2010	1	1	English	STATA JOURNAL	3-8	6	1	WOS:000277918600001	As explained in the Editorial announcements at the start of this issue, the Stata Journal will be featuring various interviews with selected members of the Stata community in 2010, Stata's 25th anniversary year. In this issue, we start with an online interview, dated October 14, 2009, of Christopher F. "Kit" Baum, an economics professor at Boston College. Kit has been highly active as a Stata user since the 1990s, contributing as an author and associate editor to this journal and its predecessor, the Stata Technical Bulletin; as a much-downloaded Stata. program author; as an author of two notable Stata-based texts; as a frequent participant on Statalist and at Users Group meetings in both the United States and several European countries; and as founder and maintainer for more than a decade of the SSC archive, which now contains many hundred user-written Stata packages. In this interview; he comments on how he got into Stata and adds his own speculations for the future.
855714	WOS:000281584500001	646ZJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	glmulti: An R Package for Easy Automated Model Selection with (Generalized) Linear Models	Journal	Article	2010	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000281584500001	We introduce glmulti, an R package for automated model selection and multi-model inference with glm and related functions. From a list of explanatory variables, the provided function glmulti builds all possible unique models involving these variables and, optionally, their pairwise interactions. Restrictions can be specified for candidate models, by excluding specific terms, enforcing marginality, or controlling model complexity. Models are fitted with standard R functions like glm. The n best models and their support (e.g., (Q)AIC, (Q)AICc, or BIC) are returned, allowing model selection and multi-model inference through standard R functions. The package is optimized for large candidate sets by avoiding memory limitation, facilitating parallelization and providing, in addition to exhaustive screening, a compiled genetic algorithm method. This article briefly presents the statistical framework and introduces the package, with applications to simulated and real data.
876234	WOS:000275204100001	564HO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Inference with Linear Equality and Inequality Constraints Using R: The Package ic.infer	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000275204100001	In linear models and multivariate normal situations, prior information in linear inequality form may be encountered, or linear inequality hypotheses may be subjected to statistical tests. R package ic.infer has been developed to support inequality-constrained estimation and testing for such situations. This article gives an overview of the principles underlying inequality-constrained inference that are far less well-known than methods for unconstrained or equality-constrained models, and describes their implementation in the package.
916007	WOS:000284597400001	685AW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An Object-Oriented Framework for Statistical Simulation: The R Package simFrame	Journal	Article	2010	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-36	36	1	WOS:000284597400001	Simulation studies are widely used by statisticians to gain insight into the quality of developed methods. Usually some guidelines regarding, e. g., simulation designs, contamination, missing data models or evaluation criteria are necessary in order to draw meaningful conclusions. The R package simFrame is an object-oriented framework for statistical simulation, which allows researchers to make use of a wide range of simulation designs with a minimal effort of programming. Its object-oriented implementation provides clear interfaces for extensions by the user. Since statistical simulation is an embarrassingly parallel process, the framework supports parallel computing to increase computational performance. Furthermore, an appropriate plot method is selected automatically depending on the structure of the simulation results. In this paper, the implementation of si m Frame is discussed in great detail and the functionality of the framework is demonstrated in examples for different simulation designs.
945501	WOS:000208589900005	V27BY	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	tmvtnorm: A Package for the Truncated Multivariate Normal Distribution	Journal	Article	2010	6	1	English	R JOURNAL	25-29	5	1	WOS:000208589900005	In this article we present tmvtnorm, an R package implementation for the truncated multivariate normal distribution. We consider random number generation with rejection and Gibbs sampling, computation of marginal densities as well as computation of the mean and co-variance of the truncated variables. This contribution brings together latest research in this field and provides useful methods for both scholars and practitioners when working with truncated normal variables.
958026	WOS:000208589900003	V27BY	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	MCMC for Generalized Linear Mixed Models with glmmBUGS	Journal	Article	2010	6	1	English	R JOURNAL	13-17	5	1	WOS:000208589900003	The glmmBUGS package is a bridging tool between Generalized Linear Mixed Models (GLMMs) in R and the BUGS language. It provides a simple way of performing Bayesian inference using Markov Chain Monte Carlo (MCMC) methods, taking a model formula and data frame in R and writing a BUGS model file, data file, and initial values files. Functions are provided to reformat and summarize the BUGS results. A key aim of the package is to provide files and objects that can be modified prior to calling BUGS, giving users a platform for customizing and extending the models to accommodate a wide variety of analyses.
959530	WOS:000281587200001	647AI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Learning Bayesian Networks with the bnlearn R Package	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000281587200001	bnlearn is an R package (R Development Core Team 2010) which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables. Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the snow package (Tierney et al. 2008) to improve their performance via parallel computing. Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use. Advanced plotting options are provided by the Rgraphviz package (Gentry et al. 2010).
969729	WOS:000284596800001	685AQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	High-Dimensional Graphical Model Search with the gRapHD R Package	Journal	Article	2010	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000284596800001	This paper presents the R package gRapHD for efficient selection of high-dimensional undirected graphical models. The package provides tools for selecting trees, forests, and decomposable models minimizing information criteria such as AIC or BIC, and for displaying the independence graphs of the models. It has also some useful tools for analysing graphical structures. It supports the use of discrete, continuous, or both types of variables.
996867	WOS:000286291100005	707MQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Age-period-cohort modeling	Journal	Article	2010	1	1	English	STATA JOURNAL	606-627	22	1	WOS:000286291100005	Age-period-cohort models provide a useful method for modeling incidence and mortality rates. It is well known that age-period-cohort models suffer from an identifiability problem due to the exact relationship between the variables (cohort = period - age). In 2007, Carstensen published an article advocating the use of an analysis that models age, period, and cohort as continuous variables through the use of spline functions (Carstensen, 2007, Statistics in Medicine 26: 3018-3045). Carstensen implemented his method for age-period-cohort models in the Epi package for R. In this article, a new command is introduced, apcfit, that performs the methods in Stata. The identifiability problem is overcome by forcing constraints on either the period or cohort effects. The use of the command is illustrated through an example relating to the incidence of colon cancer in Finland. The example shows how to include covariates in the analysis.
1016898	WOS:000273882100001	547IZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Analyzing Temperature Effects on Mortality Within the R Environment: The Constrained Segmented Distributed Lag Parameterization	Journal	Article	2010	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000273882100001	Here we present and discuss the R package modTempEff including a set of functions aimed at modelling temperature effects on mortality with time series data. The functions fit a particular log linear model which allows to capture the two main features of mortality-temperature relationships: nonlinearity and distributed lag effect. Penalized splines and segmented regression constitute the core of the modelling framework. We briefly review the model and illustrate the functions throughout a simulated dataset.
1033030	WOS:000277918600004	599MZ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Using the world development indicators database for statistical analysis in Stata	Journal	Article	2010	1	1	English	STATA JOURNAL	30-45	16	1	WOS:000277918600004	The World Bank's world development indicators (WDI) compilation is a rich and widely used database about the development of most economies in the world. However, after insheeting a WDI dataset, some data management is required prior to performing statistical analysis. In this article, I propose a new Stata command, wdireshape, for automating this data management. While reshaping a WDI dataset into structures amenable to panel data, seeming unrelated regression, or cross-sectional modeling, wdireshape renames the series and places the series descriptors into variable labels.
1040801	WOS:000276706700001	583UZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Extended Model Formulas in R: Multiple Parts and Multiple Responses	Journal	Article	2010	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000276706700001	Model formulas are the standard approach for specifying the variables in statistical models in the S language. Although being eminently useful in an extremely wide class of applications, they have certain limitations including being confined to single responses and not providing convenient support for processing formulas with multiple parts. The latter is relevant for models with two or more sets of variables, e. g., different equations for different model parameters (such as mean and dispersion), regressors and instruments in instrumental variable regressions, two-part models such as hurdle models, or alternative-specific and individual-specific variables in choice models among many others. The R package Formula addresses these two problems by providing a new class "Formula" (inheriting from "formula") that accepts an additional formula operator vertical bar separating multiple parts and by allowing all formula operators (including the new vertical bar) on the left-hand side to support multiple responses.
1042992	WOS:000275203400001	564HH	1548-7660	1548766	NULL	NULL	NULL	ARTN 3	NULL	Inverse Modelling, Sensitivity and Monte Carlo Analysis in R Using Package FME	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	28	1	WOS:000275203400001	Mathematical simulation models are commonly applied to analyze experimental or environmental data and eventually to acquire predictive capabilities. Typically these models depend on poorly defined, unmeasurable parameters that need to be given a value. Fitting a model to data, so-called inverse modelling, is often the sole way of finding reasonable values for these parameters. There are many challenges involved in inverse model applications, e. g., the existence of non-identifiable parameters, the estimation of parameter uncertainties and the quantification of the implications of these uncertainties on model predictions. The R package F M E is a modeling package designed to confront a mathematical model with data. It includes algorithms for sensitivity and Monte Carlo analysis, parameter identifiability, model fitting and provides a Markov-chain based method to estimate parameter confidence intervals. Although its main focus is on mathematical systems that consist of differential equations, F M E can deal with other types of models. In this paper, F M E is applied to a model describing the dynamics of the HIV virus.
1043333	WOS:000281587800001	647AO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Mateda-2.0: Estimation of Distribution Algorithms in MATLAB	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000281587800001	This paper describes Mateda-2.0, a MATLAB package for estimation of distribution algorithms (EDAs). This package can be used to solve single and multi-objective discrete and continuous optimization problems using EDAs based on undirected and directed probabilistic graphical models. The implementation contains several methods commonly employed by EDAs. It is also conceived as an open package to allow users to incorporate different combinations of selection, learning, sampling, and local search procedures. Additionally, it includes methods to extract, process and visualize the structures learned by the probabilistic models. This way, it can unveil previously unknown information about the optimization problem domain. Mateda-2.0 also incorporates a module for creating and validating function models based on the probabilistic models learned by EDAs.
1060459	WOS:000276707000001	583VC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Beta Regression in R	Journal	Article	2010	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000276707000001	The class of beta regression models is commonly used by practitioners to model variables that assume values in the standard unit interval (0, 1). It is based on the assumption that the dependent variable is beta-distributed and that its mean is related to a set of regressors through a linear predictor with unknown coefficients and a link function. The model also includes a precision parameter which may be constant or depend on a (potentially different) set of regressors through a link function as well. This approach naturally incorporates features such as heteroskedasticity or skewness which are commonly observed in data taking values in the standard unit interval, such as rates or proportions. This paper describes the betareg package which provides the class of beta regressions in the R system for statistical computing. The underlying theory is briefly outlined, the implementation discussed and illustrated in various replication exercises.
1065088	WOS:000281587900001	647AP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	PtProcess: An R Package for Modelling Marked Point Process Indexed by Time	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000281587900001	This paper describes the package PtProcess which uses the R statistical language. The package provides a unified approach to fitting and simulating a wide variety of temporal point process or temporal marked point process models. The models are specified by an intensity function which is conditional on the history of the process. The user needs to provide routines for calculating the conditional intensity function. Then the package enables one to carry out maximum likelihood fitting, goodness of fit testing, simulation and comparison of models. The package includes the routines for the conditional intensity functions for a variety of standard point process models. The package is intended to simplify the fitting of point process models indexed by time in much the same way as generalized linear model programs have simplified the fitting of various linear models. The primary examples used in this paper are earthquake sequances but the package is intended to have a much wider applicability.
1069114	WOS:000273371500001	540WO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A General Framework for Multivariate Analysis with Optimal Scaling: The R Package aspect	Journal	Article	2010	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000273371500001	In a series of papers De Leeuw developed a general framework for multivariate analysis with optimal scaling. The basic idea of optimal scaling is to transform the observed variables (categories) in terms of quantifications. In the approach presented here the multivariate data are collected into a multivariable. An aspect of a multivariable is a function that is used to measure how well the multivariable satisfies some criterion. Basically we can think of two different families of aspects which unify many well-known multivariate methods: Correlational aspects based on sums of correlations, eigenvalues and determinants which unify multiple regression, path analysis, correspondence analysis, nonlinear PCA, etc. Non-correlational aspects which linearize bivariate regressions and can be used for SEM preprocessing with categorical data. Additionally, other aspects can be established that do not correspond to classical techniques at all. By means of the R package aspect we provide a unified majorization-based implementation of this methodology. Using various data examples we will show the flexibility of this approach and how the optimally scaled results can be represented using graphical tools provided by the package.
1072732	WOS:000286291100003	707MQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Frequentist q-values for multiple-test procedures	Journal	Article	2010	1	1	English	STATA JOURNAL	568-584	17	1	WOS:000286291100003	Multiple-test procedures are increasingly important as technology increases scientists' ability to make large numbers of multiple measurements, as they do in genome scans. Multiple-test procedures were originally defined to input a vector of input p-values and an uncorrected critical p-value, interpreted as a familywise error rate or a false discovery rate, and to output a corrected critical p-value and a discovery set, defined as the subset of input p-values that are at or below the corrected critical p-value. A range of multiple-test procedures is implemented using the smileplot package in Stata (Newson and the ALSPAC Study Team 2003, Stata Journal 3: 109-132; 2010, Stata Journal 10: 691-692). The qqvalue command uses an alternative formulation of multiple-test procedures, which is also used by the R function p. adjust. qqvalue inputs a variable of p-values and outputs a variable of q-values that are equal in each observation to the minimum familywise error rate or false discovery rate that would result in the inclusion of the corresponding p-value in the discovery set if the specified multiple-test procedure was applied to the full set of input p-values. Formulas and examples are presented.
1074462	WOS:000208589900007	V27BY	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	glmperm: A Permutation of Regressor Residuals Test for Inference in Generalized Linear Models	Journal	Article	2010	6	1	English	R JOURNAL	39-43	5	1	WOS:000208589900007	We introduce a new R package called glmperm for inference in generalized linear models especially for small and moderate-sized data sets. The inference is based on the permutation of regressor residuals test introduced by Potter (2005). The implementation of glmperm outperforms currently available permutation test software as glmperm can be applied in situations where more than one covariate is involved.
1105502	WOS:000208590000003	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Source References	Journal	Article	2010	12	1	English	R JOURNAL	16-19	4	1	WOS:000208590000003	Since version 2.10.0, R includes expanded support for source references in R code and '.Rd' files. This paper describes the origin and purposes of source references, and current and future support for them.
1120948	WOS:000208590000002	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Solving Differential Equations in R	Journal	Article	2010	12	1	English	R JOURNAL	5-15	11	1	WOS:000208590000002	Although R is still predominantly applied for statistical analysis and graphical representation, it is rapidly becoming more suitable for mathematical computing. One of the fields where considerable progress has been made recently is the solution of differential equations. Here we give a brief overview of differential equations that can now be solved by R.
1130281	WOS:000285847300001	701TW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	RGtk2: A Graphical User Interface Tool kit for R	Journal	Article	2010	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-52	52	1	WOS:000285847300001	Graphical user interfaces (GUIs) are growing in popularity as a complement or alternative to the traditional command line interfaces to R. RGtk2 is an R package for creating GUIs in R. The package provides programmatic access to GTK+ 2.0, an open-source GUI tool kit written in C. To construct a GUI, the R programmer calls RGtk2 functions that map to functions in the underlying GTK+ library. This paper introduces the basic concepts underlying GTK+ and explains how to use RGtk2 to construct GUIs from R. The tutorial is based on simple and pratical programming examples. We also provide more complex examples illustrating the advanced features of the package. The design of the RGtk2 API and the low-level interface from R to GTK+ are discussed at length. We compare RGtk2 to alternative GUI tool kits for R.
1134620	WOS:000208590000007	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Bayesian Estimation of the GARCH(1,1) Model with Student-t Innovations	Journal	Article	2010	12	1	English	R JOURNAL	41-47	7	1	WOS:000208590000007	This note presents the R package bayesGARCH which provides functions for the Bayesian estimation of the parsimonious and effective GARCH(1,1) model with Student-t innovations. The estimation procedure is fully automatic and thus avoids the tedious task of tuning an MCMC sampling algorithm. The usage of the package is shown in an empirical application to exchange rate log-returns.
1135959	WOS:000273372100001	540WU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian Functional Data Analysis Using WinBUGS	Journal	Article	2010	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-33	33	1	WOS:000273372100001	We provide user friendly software for Bayesian analysis of functional data models using WinBUGS 1.4. The excellent properties of Bayesian analysis in this context are due to: (1) dimensionality reduction, which leads to low dimensional projection bases; (2) mixed model representation of functional models, which provides a modular approach to model extension; and (3) orthogonality of the principal component bases, which contributes to excellent chain convergence and mixing properties. Our paper provides one more, essential, reason for using Bayesian analysis for functional models: the existence of software.
1141730	WOS:000280076600006	627XD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multivariate outlier detection in Stata	Journal	Article	2010	1	1	English	STATA JOURNAL	259-266	8	1	WOS:000280076600006	Before implementing any multivariate statistical analysis based on empirical covariance matrices, it is important to check whether outliers are present because their existence could induce significant biases. In this article, we present the minimum covariance determinant estimator, which is commonly used in robust statistics to estimate location parameters and multivariate scales. These estimators can be used to robustify Mahalanobis distances and to identify outliers. Verardi and Croux (1999, Stata Journal 9: 439-453; 2010, Stata Journal 10: 313) programmed this estimator in Stata and made it available with the mcd command. The implemented algorithm is relatively fast and, as we show in the simulation example section, outperforms the methods already available in Stata, such as the Hadi method.
1142116	WOS:000208590000010	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The RecordLinkage Package: Detecting Errors in Data	Journal	Article	2010	12	1	English	R JOURNAL	61-67	7	1	WOS:000208590000010	Record linkage deals with detecting homonyms and mainly synonyms in data. The package RecordLinkage provides means to perform and evaluate different record linkage methods. A stochastic framework is implemented which calculates weights through an EM algorithm. The determination of the necessary thresholds in this model can be achieved by tools of extreme value theory. Furthermore, machine learning methods are utilized, including decision trees (rpart), bootstrap aggregating (bagging), ada boost (ada), neural nets (nnet) and support vector machines (svm). The generation of record pairs and comparison patterns from single data items are provided as well. Comparison patterns can be chosen to be binary or based on some string metrics. In order to reduce computation time and memory usage, blocking can be used. Future development will concentrate on additional and refined methods, performance improvements and input/output facilities needed for real-world application.
1148468	WOS:000277918600003	599MZ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Direct and indirect effects in a logit model	Journal	Article	2010	1	1	English	STATA JOURNAL	11-29	19	1	WOS:000277918600003	In this article, I discuss a method by Erikson et al. (2005, Proceedings of the National Academy of Science 102: 9730-9733) for decomposing a total effect in a logit model into direct and indirect effects. Moreover, I extend this method in three ways. First, in the original method the variable through which the indirect effect occurs is assumed to be normally distributed. In this article, the method is generalized by allowing this variable to have any distribution. Second, the original method did not provide standard errors for the estimates. In this article, the bootstrap is proposed as a method of providing those. Third, I show how to include control variables in this decomposition, which was not allowed in the original method. The original method and these extensions are implemented in the ldecomp command.
1154424	WOS:000280076600001	627XD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Resampling variance estimation for complex survey data	Journal	Article	2010	1	1	English	STATA JOURNAL	165-199	35	1	WOS:000280076600001	In this article, I discuss the main approaches to resampling variance estimation in complex survey data: balanced repeated replication, the jackknife, and the bootstrap. Balanced repeated replication and the jackknife are implemented in the State. svy suite. The bootstrap for complex survey data is implemented by the bsweights command. I describe this command and provide working examples.
1193616	WOS:000208589900009	V27BY	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Two-sided Exact Tests and Matching Confidence Intervals for Discrete Data	Journal	Article	2010	6	1	English	R JOURNAL	53-58	6	1	WOS:000208589900009	There is an inherent relationship between two-sided hypothesis tests and confidence intervals. A series of two-sided hypothesis tests may be inverted to obtain the matching 100(1-alpha)% confidence interval defined as the smallest interval that contains all point null parameter values that would not be rejected at the alpha level. Unfortunately, for discrete data there are several different ways of defining two-sided exact tests and the most commonly used two-sided exact tests are defined one way, while the most commonly used exact confidence intervals are inversions of tests defined another way. This can lead to inconsistencies where the exact test rejects but the exact confidence interval contains the null parameter value. The packages exactci and exact2x2 provide several exact tests with the matching confidence intervals avoiding these inconsistencies as much as possible. Examples are given for binomial and Poisson parameters and both paired and unpaired 2 x 2 tables.
1195897	WOS:000276707400001	583VG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	On the Numerical Accuracy of Spreadsheets	Journal	Article	2010	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000276707400001	This paper discusses the numerical precision of five spreadsheets (Calc, Excel, Gnumeric, NeoOffice and Oleo) running on two hardware platforms (i386 and amd64) and on three operating systems (Windows Vista, Ubuntu Intrepid and Mac OS Leopard). The methodology consists of checking the number of correct significant digits returned by each spreadsheet when computing the sample mean, standard deviation, first-order autocorrelation, F statistic in ANOVA tests, linear and nonlinear regression and distribution functions. A discussion about the algorithms for pseudorandom number generation provided by these platforms is also conducted. We conclude that there is no safe choice among the spreadsheets here assessed: they all fail in nonlinear regression and they are not suited for Monte Carlo experiments.
1206759	WOS:000281588100001	647AR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	JM: An R Package for the Joint Modelling of Longitudinal and Time-to-Event Data	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-33	33	1	WOS:000281588100001	In longitudinal studies measurements are often collected on different types of outcomes for each subject. These may include several longitudinally measured responses (such as blood values relevant to the medical condition under study) and the time at which an event of particular interest occurs (e. g., death, development of a disease or dropout from the study). These outcomes are often separately analyzed; however, in many instances, a joint modeling approach is either required or may produce a better insight into the mechanisms that underlie the phenomenon under study. In this paper we present the R package JM that fits joint models for longitudinal and time-to-event data.
1223164	WOS:000281587600001	647AM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	rEMM: Extensible Markov Model for Data Stream Clustering in R	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000281587600001	Clustering streams of continuously arriving data has become an important application of data mining in recent years and efficient algorithms have been proposed by several researchers. However, clustering alone neglects the fact that data in a data stream is not only characterized by the proximity of data points which is used by clustering, but also by a temporal component. The extensible Markov model (EMM) adds the temporal component to data stream clustering by superimposing a dynamically adapting Markov chain. In this paper we introduce the implementation of the R extension package rEMM which implements EMM and we discuss some examples and applications.
1236745	WOS:000284597800001	685BA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	HE Plots for Repeated Measures Designs	Journal	Article	2010	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-40	40	1	WOS:000284597800001	Hypothesis error (HE) plots, introduced in Friendly (2007), provide graphical methods to visualize hypothesis tests in multivariate linear models, by displaying hypothesis and error covariation as ellipsoids and providing visual representations of effect size and significance. These methods are implemented in the heplots for R (Fox, Friendly, and Monette 2009a) and SAS (Friendly 2006), and apply generally to designs with fixed-effect factors (MANOVA), quantitative regressors (multivariate multiple regression) and combined cases (MANCOVA). This paper describes the extension of these methods to repeated measures designs in which the multivariate responses represent the outcomes on one or more "within-subject" factors. This extension is illustrated using the heplots for R. Examples describe one-sample profile analysis,designs with multiple between-S and within-S factors,and doubly multivariate designs,with multivariate responses observed on multiple occasion
1240455	WOS:000277918600007	599MZ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Power transformation via multivariate Box-Cox	Journal	Article	2010	1	1	English	STATA JOURNAL	69-81	13	1	WOS:000277918600007	We present a new Stata estimation program, mboxcox, that computes the normalizing scaled power transformations for a set of variables. The multivariate Box Cox method (defined in Velilla, 1993, Statistics and Probability Letters 17: 259-263; used in Weisberg, 2005, Applied Linear Regression [Wiley]) is used to determine the transformations. We demonstrate using a generated example and a real dataset.
1257468	WOS:000282853700002	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	bacon: An effective way to detect outliers in multivariate data using Stata (and Mata)	Journal	Article	2010	1	1	English	STATA JOURNAL	331-338	8	1	WOS:000282853700002	Identifying outliers in multivariate data is computationally intensive. The bacon command, presented in this article, allows one to quickly identify outliers, even on large datasets of tens of thousands of observations, bacon constitutes an attractive alternative to hadimvo, the only other command available in Stata for the detection of outliers.
1269444	WOS:000280076600002	627XD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Optimal power transformation via inverse response plots	Journal	Article	2010	1	1	English	STATA JOURNAL	200-214	15	1	WOS:000280076600002	We present a new Stata command, irp, that generates the inverse response plot (Cook and Weisberg, 1994, Biometrika 81: 731-737) of a response on its predictors. Using the inverse response plot, an appropriate scaled power transformation for the positive response variable can be found so that the transformed response mean is linear in the predictors. The optimal transformation is displayed in the plot, as are user-specified guesses. By using the graphical display, the user may determine whether an appropriate transformation exists as well as determine its value. We demonstrate the irp command using both a generated and a real example.
1273956	WOS:000208589900004	V27BY	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Mapping and Measuring Country Shapes The cshapes Package	Journal	Article	2010	6	1	English	R JOURNAL	18-24	7	1	WOS:000208589900004	The article introduces the cshapes R package, which includes our CShapes dataset of contemporary and historical country boundaries, as well as computational tools for computing geographical measures from these maps. We provide an overview of the need for considering spatial dependence in comparative research, how this requires appropriate historical maps, and detail how the cshapes associated R package cshapes can contribute to these ends. We illustrate the use of the package for drawing maps, computing spatial variables for countries, and generating weights matrices for spatial statistics.
1291397	WOS:000286291100007	707MQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Variable selection in linear regression	Journal	Article	2010	1	1	English	STATA JOURNAL	650-669	20	1	WOS:000286291100007	We present a new Stata program, vselect, that helps users perform variable selection after performing a linear regression. Options for stepwise methods such as forward selection and backward elimination are provided. The user may specify Mallows's C-p, Akaike's information criterion, Akaike's corrected information criterion, Bayesian information criterion, or R-2 adjusted as the information criterion for the selection. When the user specifies the best subset option, the leaps-and-bounds algorithm (Furnival and Wilson, Technometrics 16: 499-511) is used to determine the best subsets of each predictor size. All the previously mentioned information criteria are reported for each of these subsets. We also provide options for doing variable selection only on certain predictors (as in [R] nestreg) and support for weighted linear regression. All options are demonstrated on real datasets with varying numbers of predictors.
1349989	WOS:000281593100001	647CJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Exact and Asymptotic Weighted Logrank Tests for Interval Censored Data: The interval R Package	Journal	Article	2010	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000281593100001	For right-censored data perhaps the most commonly used tests are weighted logrank tests, such as the logrank and Wilcoxon-type tests. In this paper we review several generalizations of those weighted logrank tests to interval-censored data and present an R package, interval, to implement many of them. The interval package depends on the perm package, also presented here, which performs exact and asymptotic linear permutation tests. The perm package performs many of the tests included in the already available coin package, and provides an independent validation of coin. We review analysis methods for interval-censored data, and we describe and show how to use the interval and perm packages.
1409904	WOS:000275203500001	564HI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	clues: An R Package for Nonparametric Clustering Based on Local Shrinking	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000275203500001	Determining the optimal number of clusters appears to be a persistant and controversial issue in cluster analysis. Most existing R packages targeting clustering require the user to specify the number of clusters in advance. However, if this subjectively chosen number is far from optimal, clustering may produce seriously misleading results. In order to address this vexing problem, we develop the R package clues to automate and evaluate the selection of an optimal number of clusers, which is widely applicable in the field of clustering analysis. Package clues uses two main procedures, shrinking and partitioning, to estimate an optimal number of clusters by maximizing an index function, either the CH index or the Silhouette index, rather than relying on guessing a pre-specified number. Five agreement indices (Rand index, Hubert and Arabie's adjusted Rand index, Morey and Agresti's adjusted Rand index, Fowlkes and Mallows index and Jaccard index), which measure the degree of agreement between any two partitions, are also provided in clues. In addition to numerical evidence, clues also supplies a deeper insight into the partitioning process with trajectory plots.
1413608	WOS:000286291100008	707MQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Graphing subsets	Journal	Article	2010	1	1	English	STATA JOURNAL	670-681	12	1	WOS:000286291100008	Graphical comparison of results for two or more groups or subsets can be accomplished by way of subdivision, superimposition, or juxtaposition. The choice between superimposition (several groups in one panel) and juxtaposition (several groups in several panels) can require fine discrimination: while juxtaposition increases clarity, it requires mental superimposition to be most effective. Discussion of this dilemma leads to exploration of a compromise design in which each subset is plotted in a separate panel, with the rest of the data as a backdrop. Univariate and bivariate examples are given, and associated Stata coding tips and tricks are commented on in detail.
1424268	WOS:000281584000001	646ZE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Modeling Multivariate Distributions with Continuous Margins Using the copula R Package	Journal	Article	2010	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	20	1	WOS:000281584000001	The copula-based modeling of multivariate distributions with continuous margins is presented as a succession of rank-based tests: a multivariate test of randomness followed by a test of mutual independence and a series of goodness-of-fit tests. All the tests under consideration are based on the empirical copula, which is a nonparametric rank-based estimator of the true unknown copula. The principles of the tests are recalled and their implementation in the copula R package is briefly described. Their use in the construction of a copula model from data is thoroughly illustrated on real insurance and financial data.
1430875	WOS:000280076600005	627XD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Computing Murphy-Topel-corrected variances in a heckprobit model with endogeneity	Journal	Article	2010	1	1	English	STATA JOURNAL	252-258	7	1	WOS:000280076600005	We outline a fairly simple method to obtain in Stata Murphy-Topel-corrected variances for a two-step estimation of a heckprobit model with endogeneity in the main equation. The procedure uses predict's score option and the powerful matrix tool accum in Stata and builds on previous works by Hardin (2002, Stata journal 2: 253-266) and Hole (2006, Stata journal 6: 521-529).
1436972	WOS:000280076600009	627XD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Multivariable Model-building: A Pragmatic Approach to Regression Analysis Based on Fractional Polynomials for Modeling Continuous Variables, by Royston and Sauerbrei	Journal	Review	2010	1	1	English	STATA JOURNAL	297-302	6	1	WOS:000280076600009	This article reviews Multivariable Model-building: A Pragmatic Approach to Regression Analysis Based on Fractional Polynomials for Modeling Continuous Variables, by Patrick Royston and Willi Sauerbrei.
1437099	WOS:000280076600008	627XD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Finding variables	Journal	Article	2010	1	1	English	STATA JOURNAL	281-296	16	1	WOS:000280076600008	A standard task in data management is producing a list of variable names showing which variables have specific properties, such as being of string type, or having value labels attached, or having a date format. A first-principles strategy is to loop over variables, checking one by one which have the property or properties concerned. I discuss this strategy in detail with a variety of examples. A canned alternative is offered by the official command ds or by a new command, findname, published formally with this column.
1471828	WOS:000282853700003	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Comparing the predictive powers of survival models using Harrell's C or Somers' D	Journal	Article	2010	1	1	English	STATA JOURNAL	339-358	20	1	WOS:000282853700003	Medical researchers frequently make statements that one model predicts survival better than another, and they are frequently challenged to provide rigorous statistical justification for those statements. Stata provides the estat concordance command to calculate the rank parameters Harrell's C and Somers' D as measures of the ordinal predictive power of a model. However, no confidence limits or p-values are provided to compare the predictive power of distinct models. The somersd package, downloadable from Statistical Software Components, can provide such confidence intervals, but they should not be taken seriously if they are calculated in the dataset in which the model was fit. Methods are demonstrated for fitting alternative models to a training set of data, and then measuring and comparing their predictive powers by using out-of-sample prediction and somersd in a test set to produce statistically sensible confidence intervals and p-values for the differences between the predictive powers of different models.
1477858	WOS:000275203600001	564HJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Measures of Analysis of Time Series (MATS): A MATLAB Toolkit for Computation of Multiple Measures on Time Series Data Bases	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000275203600001	In many applications, such as physiology and finance, large time series data bases are to be analyzed requiring the computation of linear, nonlinear and other measures. Such measures have been developed and implemented in commercial and freeware softwares rather selectively and independently. The Measures of Analysis of Time Series (MATS) MATLAB toolkit is designed to handle an arbitrary large set of scalar time series and compute a large variety of measures on them, allowing for the specification of varying measure parameters as well. The variety of options with added facilities for visualization of the results support different settings of time series analysis, such as the detection of dynamics changes in long data records, resampling (surrogate or bootstrap) tests for independence and linearity with various test statistics, and discrimination power of different measures and for different combinations of their parameters. The basic features of MATS are presented and the implemented measures are briefly described. The usefulness of MATS is illustrated on some empirical examples along with screenshots.
1477929	WOS:000276707300001	583VF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian Estimation of MIRT Models with General and Specific Latent Traits in MATLAB	Journal	Article	2010	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000276707300001	Multidimensional item response models have been developed to incorporate a general trait and several specific trait dimensions. Depending on the structure of these latent traits, different models can be considered. This paper provides the requisite information and description of software that implement the Gibbs sampling procedures for three such models with a normal ogive form. The software developed is written in the MATLAB package IRTm2noHA. The package is flexible enough to allow a user the choice to simulate binary response data with a latent structure involving general and specific traits, specify prior distributions for model parameters, check convergence of the MCMC chain, and obtain Bayesian fit statistics. Illustrative examples are provided to demonstrate and validate the use of the software package.
1493853	WOS:000286291100004	707MQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Making spatial analysis operational: Commands for generating spatial-effect variables in monadic and dyadic data	Journal	Article	2010	1	1	English	STATA JOURNAL	585-605	21	1	WOS:000286291100004	Spatial dependence exists whenever the expected utility of one unit of analysis is affected by the decisions or behavior made by other units of analysis. Spatial dependence is ubiquitous in social relations and interactions. Yet, there are surprisingly few social science studies accounting for spatial dependence. This holds true for settings in which researchers use monadic data, where the unit of analysis is the individual unit, agent, or actor, and even more true for dyadic data settings, where the unit of analysis is the pair or dyad representing an interaction or a relation between two individual units, agents, or actors. Dyadic data offer more complex ways of modeling spatial-effect variables than do monadic data. The commands described in this article facilitate spatial analysis by providing an easy tool for generating, with one command line, spatial-effect variables for monadic contagion as well as for all possible forms of contagion in dyadic data.
1495118	WOS:000281584200001	646ZG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Spatio-Temporal Multiway Decompositions Using Principal Tensor Analysis on k-Modes: The R Package PTAk	Journal	Article	2010	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000281584200001	The purpose of this paper is to describe the R package PTAk and how the spatio-temporal context can be taken into account in the analyses. Essentially PTAk() is a multiway multidimensional method to decompose a multi-entries data-array, seen mathematically as a tensor of any order. This PTAk-modes method proposes a way of generalizing SVD (singular value decomposition), as well as some other well known methods included in the R package, such as PARAFAC or CANDECOMP and the PCAn-modes or Tucker-n model. The example datasets cover different domains with various spatio-temporal characteristics and issues: (i) medical imaging in neuropsychology with a functional MRI (magnetic resonance imaging) study, (ii) pharmaceutical research with a pharmacodynamic study with EEG (electro-encephaloegraphic) data for a central nervous system (CNS) drug, and (iii) geographical information system (GIS) with a climatic dataset that characterizes arid and semi-arid variations. All the methods implemented in the R package PTAk also support non-identity metrics, as well as penalizations during the optimization process. As a result of these flexibilities, together with pre-processing facilities, PTAk constitutes a framework for devising extensions of multidimensional methods such as correspondence analysis, discriminant analysis, and multidimensional scaling, also enabling spatio-temporal constraints.
1506566	WOS:000275203700001	564HK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Categorical Inputs, Sensitivity Analysis, Optimization and Importance Tempering with tgp Version 2, an R Package for Treed Gaussian Process Models	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-48	48	1	WOS:000275203700001	This document describes the new features in version 2.x of the tgp package for R, implementing treed Gaussian process (GP) models. The topics covered include methods for dealing with categorical inputs and excluding inputs from the tree or GP part of the model; fully Bayesian sensitivity analysis for inputs/covariates; sequential optimization of black-box functions; and a new Monte Carlo method for inference in multi-modal posterior distributions that combines simulated tempering and importance sampling. These additions extend the functionality of tgp across all models in the hierarchy: from Bayesian linear models, to classification and regression trees (CART), to treed Gaussian processes with jumps to the limiting linear model. It is assumed that the reader is familiar with the baseline functionality of the package, outlined in the first vignette (Gramacy 2007).
1514153	WOS:000208590000011	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	spikeslab: Prediction and Variable Selection Using Spike and Slab Regression	Journal	Article	2010	12	1	English	R JOURNAL	68-73	6	1	WOS:000208590000011	Weighted generalized ridge regression offers unique advantages in correlated high-dimensional problems. Such estimators can be efficiently computed using Bayesian spike and slab models and are effective for prediction. For sparse variable selection, a generalization of the elastic net can be used in tandem with these Bayesian estimates. In this article, we describe the R-software package spikeslab for implementing this new spike and slab prediction and variable selection methodology.
1526377	WOS:000273372000001	540WT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The VGAM Package for Categorical Data Analysis	Journal	Article	2010	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000273372000001	Classical categorical regression models such as the multinomial logit and proportional odds models are shown to be readily handled by the vector generalized linear and additive model (VGLM/VGAM) framework. Additionally, there are natural extensions, such as reduced-rank VGLMs for dimension reduction, and allowing covariates that have values specific to each linear/additive predictor, e.g., for consumer choice modeling. This article describes some of the framework behind the VGAM R package, its usage and implementation details.
1541509	WOS:000282853700008	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Regression analysis of censored data using pseudo-observations	Journal	Article	2010	1	1	English	STATA JOURNAL	408-422	15	1	WOS:000282853700008	We draw upon a series of articles in which a method based on pseudovalues is proposed for direct regression modeling of the survival function, the restricted mean, and the cumulative incidence function in competing risks with right-censored data. The models, once the pseudovalues have been computed, can be fit using standard generalized estimating equation software. Here we present Stata procedures for computing these pseudo-observations. An example from a bone marrow transplantation study is used to illustrate the method.
1551970	WOS:000282853700006	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Projection of power and events in clinical trials with a time-to-event outcome	Journal	Article	2010	1	1	English	STATA JOURNAL	386-394	9	1	WOS:000282853700006	In 2005, Barthel, Royston, and Babiker presented a menu-driven Stata program under the generic name of ART (assessment of resources for trials) to calculate sample size and power for complex clinical trial designs with a time-to-event or binary outcome. In this article, we describe a Stata tool called ARTPEP, which is intended to project the power and events of a. trial with a time-to-event outcome into the future given patient accrual figures so far and assumptions about event rates and other defining parameters. ARTPEP has been designed to work closely with the ART program and has an associated dialog box. We illustrate the use of ARTPEP with data from a phase III trial in esophageal cancer.
1556868	WOS:000277918600010	599MZ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Mata Matters: Stata in Mata	Journal	Article	2010	1	1	English	STATA JOURNAL	125-142	18	1	WOS:000277918600010	Mata is Sta.ta's matrix language. In the Mata Matters column, we show how Mata can be used interactively to solve problems and as a programming language to add new features to Stata. The subject of this column is using Mata to solve data analysis problems with the new Stata commands putmata and getmata, which were added to official Stata 11 in the update of 11 February 2010.
1633745	WOS:000284598000001	685BC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Non-Standard Semiparametric Regression via BRugs	Journal	Article	2010	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000284598000001	We provide several illustrations of Bayesian semiparametric regression analyses in the BRugs package. BRugs facilitates use of the BUGS inference engine from the R computing environment and allows analyses to be managed using scripts. The examples are chosen to represent an array of non-standard situations, for which mixed model software is not viable. The situations include: the response variable being outside of the one-parameter exponential family,data subject to missingness, data subject to measurement error and parameters entering the model via an index.
1645229	WOS:000281587000001	647AG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	%QLS SAS Macro: A SAS Macro for Analysis of Correlated Data Using Quasi-Least Squares	Journal	Article	2010	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000281587000001	Quasi-least squares (QLS) is an alternative computational approach for estimation of the correlation parameter in the framework of generalized estimating equations (GEE). QLS overcomes some limitations of GEE that were discussed in Crowder (1995). In addition, it allows for easier implementation of some correlation structures that are not available for GEE. We describe a user written SAS macro called %QLS, and demonstrate application of our macro using a clinical trial example for the comparison of two treatments for a common toenail infection. %QLS also computes the lower and upper boundaries of the correlation parameter for analysis of longitudinal binary data that were described by Prentice (1988). Furthermore, it displays a warning message if the Prentice constraints are violated. This warning is not provided in existing GEE software packages and other packages that were recently developed for application of QLS (in Stata, MATLAB, and R). %QLS allows for analysis of continuous, binary, or count data with one of the following working correlation structures: the first-order autoregressive, equicorrelated, Markov, or tri-diagonal structures.
1665483	WOS:000281593300001	647CL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	On Simulation of Manifold Indexed Fractional Gaussian Fields	Journal	Article	2010	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000281593300001	To simulate fractional Brownian motion indexed by a manifold poses serious numerical problems: storage, computing time and choice of an appropriate grid. We propose an effective and fast method, valid not only for fractional Brownian fields indexed by a manifold, but for any Gaussian fields indexed by a manifold. The performance of our method is illustrated with different manifolds (sphere, hyperboloid).
1686872	WOS:000277918600008	599MZ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Centering and reference groups for estimates of fixed effects: Modifications to felsdvreg	Journal	Article	2010	1	1	English	STATA JOURNAL	82-103	22	1	WOS:000277918600008	Availability of large, multilevel longitudinal databases in various fields including labor economics (with workers and firms observed over time) and education research (with students and teachers observed over time) has increased the application of panel-data models with multiple levels of fixed-effects. Existing software routines for fitting fixed-effects models were not designed for applications in which the primary interest is obtaining estimates of any of the fixed-effects parameters. Such routines typically report estimates of fixed effects relative to arbitrary holdout units. Contrasts to holdout units are not ideal in cases where the fixed-effects parameters are of interest because they can change capriciously, they do not correspond to the structural parameters that are typically of interest, and they are inappropriate for empirical Bayes (shrinkage) estimation. We develop an improved parameterization of fixed-effects models using sum-to-zero constraints that provides estimates of fixed effects relative to mean effects within well-defined reference groups (e.g., all firms of a given type or all teachers of a given grade) and provides standard errors for those estimates that are appropriate for shrinkage estimation. We implement our parameterization in a Stata routine called felsdvregdm by modifying the felsdvreg routine designed for fitting high-dimensional fixed-effects models. We demonstrate our routine with an example dataset from the Florida Education Data Warehouse.
1687138	WOS:000281593200001	647CK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Conducting Meta-Analyses in R with the metafor Package	Journal	Article	2010	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-48	48	1	WOS:000281593200001	The metafor package provides functions for conducting meta-analyses in R. The package includes functions for fitting the meta-analytic fixed- and random-effects models and allows for the inclusion of moderators variables (study-level covariates) in these models. Meta-regression analyses with continuous and categorical moderators can be conducted in this way. Functions for the Mantel-Haenszel and Peto's one-step method for meta-analyses of 2 x 2 table data are also available. Finally, the package provides various plot functions (for example, for forest, funnel, and radial plots) and functions for assessing the model fit, for obtaining case diagnostics, and for tests of publication bias.
1694267	WOS:000281593800001	647CQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	LinkCluE: A MATLAB Package for Link-Based Cluster Ensembles	Journal	Article	2010	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-36	36	1	WOS:000281593800001	Cluster ensembles have emerged as a powerful meta-learning paradigm that provides improved accuracy and robustness by aggregating several input data clusterings. In particular,link-based similarity method shave recently been introduced with superior performance to the conventional co-association approach. This paper presents a MATLAB package, LinkCluE, that implements the link-based cluster ensemble framework. A variety of functional methods for evaluating clustering results, based on both internal and external criteria,are also provided. Additionally,the underlying algorithms together with the sample uses of the package with interesting real and synthetic datasets are demonstrated herein.
1707605	WOS:000281586900001	647AF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	sphet: Spatial Models with Heteroskedastic Innovations in R	Journal	Article	2010	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000281586900001	sphet is a package for estimating and testing spatial models with heteroskedastic innovations. We implement recent generalized moments estimators and semiparametric methods for the estimation of the coefficients variance-covariance matrix. This paper is a general description of sphet and all functionalities are illustrated by application to the popular Boston housing dataset. The package in its current version is limited to the estimators based on Arraiz, Drukker, Kelejian, and Prucha (2010); Kelejian and Prucha (2007, 2010). The estimation functions implemented in sphet are able to deal with virtually any sample size.
1743566	WOS:000282056300001	653BC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	spam: A Sparse Matrix R Package with Emphasis on MCMC Methods for Gaussian Markov Random Fields	Journal	Article	2010	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000282056300001	spam is an R package for sparse matrix algebra with emphasis on a Cholesky factorization of sparse positive definite matrices. The implemantation of spam is based on the competing philosophical maxims to be competitively fast compared to existing tools and to be easy to use, modify and extend. The first is addressed by using fast Fortran routines and the second by assuring S3 and S4 compatibility. One of the features of spam is to exploit the algorithmic steps of the Cholesky factorization and hence to perform only a fraction of the workload when factorizing matrices with the same sparsity structure. Simulations show that exploiting this break-down of the factorization results in a speed-up of about a factor 5 and memory savings of about a factor 10 for large matrices and slightly smaller factors for huge matrices. The article is motivated with Markov chain Monte Carlo methods for Gaussian Markov random fields, but many other statistical applications are mentioned that profit from an efficient Cholesky factorization as well.
1762859	WOS:000275203200001	564HF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Regularization Paths for Generalized Linear Models via Coordinate Descent	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000275203200001	We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include l(1) (the lasso), l(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.
1773016	WOS:000277918600009	599MZ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Creating synthetic discrete-response regression models	Journal	Article	2010	1	1	English	STATA JOURNAL	104-124	21	1	WOS:000277918600009	The development and use of synthetic regression models has proven to assist statisticians in better understanding bias in data, as well as how to best interpret various statistics associated with a modeling situation. In this article, I present code that can be easily amended for the creation of synthetic binomial, count, and categorical response models. Parameters may be assigned to any number of predictors (which are shown as continuous, binary, or categorical), negative binomial heterogeneity parameters may be assigned, and the number of levels or cut points and values may be specified for ordered and unordered categorical response models. I also demonstrate how to introduce an offset into synthetic data and how to test synthetic models using Monte Carlo simulation. Finally, I introduce code for constructing a synthetic NB2-logit hurdle model.
1781987	WOS:000281593700001	647CP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	vrmlgen: An R Package for 3D Data Visualization on the Web	Journal	Article	2010	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000281593700001	The 3-dimensional representation and inspection of complex data is a frequently used strategy in many data analysis domains. Existing data mining software often lacks functionality that would enable users to explore 3D data interactively, especially if one wishes to make dynamic graphical representations directly viewable on the web. In this paper we present vrmlgen, a software package for the statistical programming language R to create 3D data visualizations in web formats like the Virtual Reality Markup Language (VRML) and Live Graphics 3D .vrmlgen can be used to generate 3D charts and bar plots, scatter plots with density estimation contour surfaces, and visualizations of height maps, 3D object models and parametric functions. For greater fexibility, the user can also access low-level plotting methods through a unified interface and freely group different function calls to gether to create new higher-level plotting methods. Additionally, we present a web tool allowing users to visualize 3D data online and test some of vrmlgen's features without the need to install any software on their computer.
1803236	WOS:000276954400001	586ZG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Generalizing the Convex Hull of a Sample: The R Package alphahull	Journal	Article	2010	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000276954400001	This paper presents the R package alphahull which implements the alpha-convex hull and the alpha-shape of a finite set of points in the plane. These geometric structures provide an informative overview of the shape and properties of the point set. Unlike the convex hull, the alpha-convex hull and the alpha-shape are able to reconstruct non-convex sets. This flexibility make them specially useful in set estimation. Since the implementation is based on the intimate relation of theses constructs with Delaunay triangulations, the R package alphahull also includes functions to compute Voronoi and Delaunay tesselations. The usefulness of the package is illustrated with two small simulation studies on boundary length estimation.
1811894	WOS:000277918600005	599MZ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Tabulating SPost results using estout and esttab	Journal	Article	2010	1	1	English	STATA JOURNAL	46-60	15	1	WOS:000277918600005	The SPost user package (Long and Freese, 2006, Regression Models for Categorical Dependent Variables Using Stata [Stata Press]) is a suite of post-estimation commands to compute additional tests and effects representations for a variety of regression models. To facilitate and automate the task of tabulating results from SPost commands for inclusion in reports, publications, and presentations, we introduce tools to integrate SPost with the estout user package (Jann, 2005, Stata journal 5: 288-308; 2007, Stata Journal 7: 227-244). The estadd command can retrieve results computed by the SPost commands brant, fitstat; listcoef, mlogtest, prchange, prvalue, and asprvalue. These results can then be tabulated by esttab or estout.
1817645	WOS:000208589900002	V27BY	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	IsoGene: An R Package for Analyzing Dose-response Studies in Microarray Experiments	Journal	Article	2010	6	1	English	R JOURNAL	5-12	8	1	WOS:000208589900002	IsoGene is an R package for the analysis of dose-response microarray experiments to identify gene or subsets of genes with a monotone relationship between the gene expression and the doses. Several testing procedures (i.e., the likelihood ratio test, Williams, Marcus, the M, and Modified M), that take into account the order restriction of the means with respect to the increasing doses are implemented in the package. The inference is based on resampling methods, both permutations and the Significance Analysis of Microarrays (SAM).
1831546	WOS:000275204200001	564HP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Generalization of the Dirichlet Distribution	Journal	Article	2010	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000275204200001	This paper discusses a generalization of the Dirichlet distribution, the 'hyperdirichlet', in which various types of incomplete observations may be incorporated. It is conjugate to the multinomial distribution when some observations are censored or grouped. The hyperdirichlet R package is introduced and examples given. A number of statistical tests are performed on the example datasets, which are drawn from diverse disciplines including sports statistics, the sociology of climate change, and psephology.
1834284	WOS:000280076600004	627XD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Analyzing longitudinal data in the presence of informative drop-out: The jmre1 command	Journal	Article	2010	1	1	English	STATA JOURNAL	226-251	26	1	WOS:000280076600004	Many studies in various research areas have designs that involve repeated measurements over time of a continuous variable across a group of subjects. A frequent and serious problem in such studies is the occurrence of missing data. In many cases, missing data are caused by an event that leads to a premature termination of the series of repeated measurements on some subjects. When the probability of the occurrence of this event is related to the subject-specific underlying trend of the variable of interest, this missingness process is called informative censoring or informative drop-out. Standard likelihood-based methods (for example, linear mixed models) fail to give consistent estimates. In such cases, one needs to apply methods that simultaneously model the observed data and the missingness process. In this article, we review a method proposed by Touloumi et al. (1999, Statistics in Medicine 18: 1215-1233) to adjust for informative drop-out in longitudinal data analysis. We also present the jmre1 command, which can be used to fit the proposed model. The estimation method combines the restricted iterative generalized least-squares method with a nested expectation-maximization algorithm. The method is implemented mainly using Stata's matrix programming language, Mata. Our example is derived from the epidemiology of the HIV infection.
1843048	WOS:000280076600007	627XD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Data envelopment analysis	Journal	Article	2010	1	1	English	STATA JOURNAL	267-280	14	1	WOS:000280076600007	In this article, we introduce a user-written data envelopment analysis command for Stata. Data envelopment analysis is a linear programming method for assessing the efficiency and productivity of units called decision-making units. Over the last decades, data envelopment analysis has gained considerable attention as a managerial tool for measuring performance of organizations, and it has been used widely for assessing the efficiency of public and private sectors such as banks, airlines, hospitals, universities, defense firms, and manufacturers. The dea command in Stata will allow users to conduct the standard optimization procedure and extended managerial analysis. The dea command developed in this article selects the chosen variables from a Stata data file and constructs a linear programming model based on the selected dea options. Examples are given to illustrate how one could use the code to measure the efficiency of decision-making units.
1846188	WOS:000282853700011	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: The limits of sample skewness and kurtosis	Journal	Article	2010	1	1	English	STATA JOURNAL	482-495	14	1	WOS:000282853700011	Sample skewness and kurtosis are limited by functions of sample size. The limits, or approximations to them, have repeatedly been rediscovered over the last several decades, but nevertheless seem to remain only poorly known. The limits impart bias to estimation and, in extreme cases, imply that no sample could bear exact witness to its parent distribution. The main results are explained in a tutorial review, and it is shown how Stata and Mata may be used to confirm and explore their consequences.
1903430	WOS:000281584400001	646ZI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Computing Generalized Method of Moments and Generalized Empirical Likelihood with R	Journal	Article	2010	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-35	35	1	WOS:000281584400001	This paper shows how to estimate models by the generalized method of moments and the generalized empirical likelihood using the R package gmm. A brief discussion is offered on the theoretical aspects of both methods and the functionality of the package is presented through several examples in economics and finance.
1935656	WOS:000282853700009	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation of quantile treatment effects with Stata	Journal	Article	2010	1	1	English	STATA JOURNAL	423-457	35	1	WOS:000282853700009	In this article, we discuss the implementation of various estimators proposed to estimate quantile treatment effects. We distinguish four cases involving conditional and unconditional quantile treatment effects with either exogenous or endogenous treatment variables. The introduced ivqte command covers four different estimators: the classical quantile regression estimator of Koenker and Bassett (1978, Econometrica 46: 33-50) extended to heteroskedasticity consistent standard errors; the instrumental-variable quantile regression estimator of Abadie, Angrist, and Imbens (2002, Econometrica 70: 91-117); the estimator for unconditional quantile treatment effects proposed by Firpo (2007, Econometrica, 75: 259-276); and the instrumental-variable estimator for unconditional quantile treatment effects proposed by Frolich and Melly (2008, IZA discussion paper 3288). The implemented instrumental-variable procedures estimate the causal effects for the subpopulation of compliers and are only well suited for binary instruments. ivqte also provides analytical standard errors and various options for nonparametric estimation. As a by-product, the locreg command implements local linear and local logit estimators for mixed data (continuous, ordered discrete, unordered discrete, and binary regressors).
1949517	WOS:000282853700001	662ZV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	An introduction to maximum entropy and minimum cross-entropy estimation using Stata	Journal	Article	2010	1	1	English	STATA JOURNAL	315-330	16	1	WOS:000282853700001	Maximum entropy and minimum cross-entropy estimation are applicable when faced with ill-posed estimation problems. T. introduce a Stata command that estimates a probability distribution using a maximum entropy or minimum cross-entropy criterion. I show how this command can be used to calibrate survey data to various population totals.
1957844	WOS:000208590000009	V27BZ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	binGroup: A Package for Group Testing	Journal	Article	2010	12	1	English	R JOURNAL	56-60	5	1	WOS:000208590000009	When the prevalence of a disease or of some other binary characteristic is small, group testing (also known as pooled testing) is frequently used to estimate the prevalence and/or to identify individuals as positive or negative. We have developed the binGroup package as the first package designed to address the estimation problem in group testing. We present functions to estimate an overall prevalence for a homogeneous population. Also, for this setting, we have functions to aid in the very important choice of the group size. When individuals come from a heterogeneous population, our group testing regression functions can be used to estimate an individual probability of disease positivity by using the group observations only. We illustrate our functions with data from a multiple vector transfer design experiment and a human infectious disease prevalence study.
1983605	WOS:000352916400001	CF9VX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SAVE: An R Package for the Statistical Analysis of Computer Models	Journal	Article	2015	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000352916400001	This paper introduces the R package SAVE which implements statistical methodology for the analysis of computer models. Namely, the package includes routines that perform emulation, calibration and validation of this type of models. The methodology is Bayesian and is essentially that of Bayarri, Berger, Paulo, Sacks, Cafeo, Cavendish, Lin, and Tu (2007). The package is available through the Comprehensive R Archive Network. We illustrate its use with a real data example and in the context of a simulated example.
1999686	WOS:000364359100011	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Bandwidth selection in kernel distribution function estimation	Journal	Article	2015	1	1	English	Stata Journal	784-795	12	1	WOS:000364359100011	I present a new command, kcdf, for bandwidth selection in kernel estimation of the cumulative distribution function. I briefly review plug-in and cross-validation bandwidth selectors, both of which are implemented in kcdf. I then describe the command syntax and illustrate its use with an application to artificial data.
2007514	WOS:000353664600017	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Generating nonnegatively correlated binary random variates	Journal	Article	2015	1	1	English	STATA JOURNAL	301-308	8	1	WOS:000353664600017	In this article, I discuss the methods for generating nonnegatively correlated binary random variates. I provide a new command, rbinary, with examples showing how the command can be used.
2013611	WOS:000357139500001	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Global search regression: A new automatic model-selection technique for cross-section, time-series, and panel-data regressions	Journal	Article	2015	1	1	English	STATA JOURNAL	325-349	25	1	WOS:000357139500001	In this article, we present gsreg, a new automatic model-selection technique for cross-section, time-series, and panel-data regressions. Like other exhaustive search algorithms (for example, vselect), gsreg avoids characteristic path-dependence traps of standard approaches as well as backward- and forward-looking approaches (like Pc Gets or relevant transformation of the inputs network approach). However, gsreg is the first code that 1) guarantees optimality with out-of-sample selection criteria; 2) allows residual testing for each alternative; and 3) provides (depending on user specifications) a full-information dataset with outcome statistics for every alternative model.
2032740	WOS:000357431900003	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	fanplot: An R Package for Visualising Sequential Distributions	Journal	Article	2015	6	1	English	R JOURNAL	15-23	9	1	WOS:000357431900003	Fan charts, first developed by the Bank of England in 1996, have become a standard method for visualising forecasts with uncertainty. Using shading fan charts focus the attention towards the whole distribution away from a single central measure. This article describes the basics of plotting fan charts using an R add-on package alongside some additional methods for displaying sequential distributions. Examples are based on distributions of both estimated parameters from a time series model and future values with uncertainty.
2043127	WOS:000365978900001	CX8UE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	dawai: An R Package for Discriminant Analysis with Additional Information	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	19	1	WOS:000365978900001	The incorporation of additional information into discriminant rules is receiving increasing attention as the rules including this information perform better than the usual rules. In this paper we introduce an R package called dawai, which provides the functions that allow to define the rules that take into account this additional information expressed in terms of restrictions on the means, to classify the samples and to evaluate the accuracy of the results. Moreover, in this paper we extend the results and definitions given in previous papers (Fernandez, Rueda, and Salvador 2006, Conde, Fernandez, Rueda, and Salvador 2012, Conde, Salvador, Rueda, and Fernandez 2013) to the case of unequal co-variances among the populations, and consequently define the corresponding restricted quadratic discriminant rules. We also define estimators of the accuracy of the rules for the general more than two populations case. The wide range of applications of these procedures is illustrated with two data sets from two different fields, i.e., biology and pattern recognition.
2045338	WOS:000357139500007	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Modeling heaped count data	Journal	Article	2015	1	1	English	STATA JOURNAL	457-479	23	1	WOS:000357139500007	We present motivation and new commands for modeling heaped count data. These data may appear when subjects report counts that are rounded or favor multiples (digit preference) of a certain outcome, such as the number of cigarettes reported. The new commands for fitting count regression models (Poisson, generalized Poisson, negative binomial) are also accompanied by real-world examples comparing the heaped regression model with the usual regression model as well as the heaped zero-inflated model with the usual zero-inflated model.
2085079	WOS:000357139500010	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Generalized maximum entropy estimation of discrete choice models	Journal	Article	2015	1	1	English	STATA JOURNAL	512-522	11	1	WOS:000357139500010	In this article, we describe the gmentropylogit command, which implements the generalized maximum entropy estimation methodology for discrete choice models. This information theoretic procedure is preferred over its maximum likelihood counterparts because it is more efficient, avoids strong parametric assumptions, works well when the sample size is small, performs well when the covariates are highly correlated, and functions well when the matrix is ill conditioned. Here we introduce the generalized maximum entropy procedure and provide an example using the gmentropylogit command.
2131837	WOS:000352917200001	CF9WF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Building a Nomogram for Survey-Weighted Cox Models Using R	Journal	Article	2015	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000352917200001	Nomograms have become very useful tools among clinicians as they provide individualized predictions based on the characteristics of the patient. For complex design survey data with survival outcome, Binder (1992) proposed methods for fitting survey-weighted Cox models, but to the best of our knowledge there is no available software to build a nomogram based on such models. This paper introduces an R package, SvyNom, to accomplish this goal and illustrates its use on a gastric cancer dataset. Validation and calibration routines are also included.
2148841	WOS:000353664600013	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Regression models for count data from truncated distributions	Journal	Article	2015	1	1	English	STATA JOURNAL	226-246	21	1	WOS:000353664600013	We present new commands for analyzing count-data regression models for truncated distributions. The trncregress command allows specification of a regression model for the mean of the truncated distribution through options. In addition to support for truncated Poisson and negative binomial, trncregress fits models based on truncated versions of distributions including generalized Poisson, Poisson-inverse Gaussian, three-parameter negative binomial power, three-parameter Waring negative binomial, and three-parameter Famoye negative binomial.
2154973	WOS:000366015100001	CX9HH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Genetic Algorithm for Selection of Fixed-Size Subsets with Application to Design Problems	Journal	Article	2015	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	18	1	WOS:000366015100001	The R function kofnGA conducts a genetic algorithm search for the best subset of k items from a set of n alternatives, given an objective function that measures the quality of a subset. The function fills a gap in the presently available subset selection software, which typically searches over a range of subset sizes, restricts the types of objective functions considered, or does not include freely available code. The new function is demonstrated on two types of problem where a fixed-size subset search is desirable: design of environmental monitoring networks, and D-optimal design of experiments. Additionally, the performance is evaluated on a class of constructed test problems with a novel design that is interesting in its own right.
2159374	WOS:000357431900010	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Correspondence Analysis on Generalised Aggregated Lexical Tables (CA-GALT) in the FactoMineR Package	Journal	Article	2015	6	1	English	R JOURNAL	109-117	9	1	WOS:000357431900010	Correspondence analysis on generalised aggregated lexical tables (CA-GALT) is a method that generalizes classical CA-ALT to the case of several quantitative, categorical and mixed variables. It aims to establish a typology of the external variables and a typology of the events from their mutual relationships. In order to do so, the influence of external variables on the lexical choices is untangled cancelling the associations among them, and to avoid the instability issued from multicollinearity, they are substituted by their principal components. The CaGalt function, implemented in the FactoMineR package, provides numerous numerical and graphical outputs. Confidence ellipses are also provided to validate and improve the representation of words and variables. Although this methodology was developed mainly to give an answer to the problem of analyzing open-ended questions, it can be applied to any kind of frequency/contingency table with external variables.
2169087	WOS:000365983000001	CX8VS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The glarma Package for Observation-Driven Time Series Regression of Counts	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-36	36	1	WOS:000365983000001	We review the theory and application of generalized linear autoregressive moving average observation-driven models for time series of counts with explanatory variables and describe the estimation of these models using the R package glarma. Forecasting, diagnostic and graphical methods are also illustrated by several examples.
2189450	WOS:000365976300001	CX8TE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	RARtool: A MATLAB Software Package for Designing Response-Adaptive Randomized Clinical Trials with Time-to-Event Outcomes	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	22	1	WOS:000365976300001	Response-adaptive randomization designs are becoming increasingly popular in clinical trial practice. In this paper, we present RARtool, a user interface software developed in MATLAB for designing response-adaptive randomized comparative clinical trials with censored time-to-event outcomes. The RARtool software can compute different types of optimal treatment allocation designs, and it can simulate response-adaptive randomization procedures targeting selected optimal allocations. Through simulations, an investigator can assess design characteristics under a variety of experimental scenarios and select the best procedure for practical implementation. We illustrate the utility of our RARtool software by redesigning a survival trial from the literature.
2215062	WOS:000349847600001	CB7ZM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian Spatial Modelling with R-INLA	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000349847600001	The principles behind the interface to continuous domain spatial models in the R INLA software package for R are described. The integrated nested Laplace approximation (INLA) approach proposed by Rue, Martino, and Chopin (2009) is a computationally effective alternative to MCMC for Bayesian inference. INLA is designed for latent Gaussian models, a very wide and flexible class of models ranging from (generalized) linear mixed to spatial and spatio-temporal models. Combined with the stochastic partial differential equation approach (SPDE, Lindgren, Rue, and Lindstrom 2011), one can accommodate all kinds of geographically referenced data, including areal and geostatistical ones, as well as spatial point process data. The implementation interface covers stationary spatial models, non-stationary spatial models, and also spatio-temporal models, and is applicable in epidemiology, ecology, environmental risk assessment, as well as general geostatistics.
2225336	WOS:000357431900004	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	sparkTable: Generating Graphical Tables for Websites and Documents with R	Journal	Article	2015	6	1	English	R JOURNAL	24-37	14	1	WOS:000357431900004	Visual analysis of data is important to understand the main characteristics, main trends and relationships in data sets and it can be used to assess the data quality. Using the R package sparkTable, statistical tables holding quantitative information can be enhanced by including spark-type graphs such as sparklines [GRAPHICS] and sparkbars [GRAPHICS] . These kind of graphics are well-known in literature and are considered as simple, intense and illustrative graphs that are small enough to fit in a single line. Thus, they can easily enrich tables and texts with additional information in a comprehensive visual way. The R package sparkTable uses a clean S4 class design and provides methods to create different types of sparkgraphs that can be used in websites, presentations and documents. We also implemented an easy way for non-experts to create highly complex tables. In this case, graphical parameters can be interactively changed, variables can be sorted, graphs can be added and removed in an interactive manner. Thereby it is possible to produce custom-tailored graphical tables - standard tables that are enriched with graphs - that can be displayed in a browser and exported to various formats.
2226210	WOS:000365986000001	CX8WU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The Forward Search for Very Large Datasets	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	20	1	WOS:000365986000001	The identification of atypical observations and the immunization of data analysis against both outliers and failures of modeling are important aspects of modern statistics. The forward search is a graphics rich approach that leads to the formal detection of outliers and to the detection of model inadequacy combined with suggestions for model enhancement. The key idea is to monitor quantities of interest, such as parameter estimates and test statistics, as the model is fitted to data subsets of increasing size. In this paper we propose some computational improvements of the forward search algorithm and we provide a recursive implementation of the procedure which exploits the information of the previous step. The output is a set of efficient routines for fast updating of the model parameter estimates, which do not require any data sorting, and fast computation of likelihood contributions, which do not require matrix inversion or qr decomposition. It is shown that the new algorithms enable a reduction of the computation time by more than 80%. Furthemore, the running time now increases almost linearly with the sample size. All the routines described in this paper are included in the FSDA toolbox for MATLAB which is freely downloadable from the internet.
2229221	WOS:000357139500013	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Transition matrix for a bivariate normal distribution in Stata	Journal	Article	2015	1	1	English	STATA JOURNAL	547-553	7	1	WOS:000357139500013	trabinor calculates the population transition matrix between two discretized variables when the original continuous variables follow a bivariate normal distribution. The user can specify the five parameters of bivariate normal and how to discretize the two variables by choosing either a given number of quantiles or a set of absolute boundaries.
2244501	WOS:000365979800001	CX8UN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Testing Goodness-of-Fit with the Kernel Density Estimator: GoFKernel	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000365979800001	To assess the goodness-of-fit of a sample to a continuous random distribution, the most popular approach has been based on measuring, using either L-infinity- or L-2-norms, the distance between the null hypothesis cumulative distribution function and the empirical cumulative distribution function. Indeed, as far as I know, almost all the tests currently available in R related to this issue (ks. test in package stats, ad.test in package AD-GofTest, and ad.test, ad2.test, ks.test, v.test and w2.test in package truncgof) use one of these two distances on cumulative distribution functions. This paper (i) proposes dgeometric.test, a new implementation of the test that measures the discrepancy between a sample kernel estimate of the density function and the null hypothesis density function on the L-1-norm, (ii) introduces the GoFKernel package, and (iii) performs a large simulation exercise to assess the calibration and sensitivity of the above listed tests as well as the Fan's test (Fan 1994), fan. test, also implemented in the GoFKernel package. In addition to dgeometric. test and f an. test, the GoFKernel package adds a couple of functions that R users might also find of interest: density. reflected extends density, allowing the computation of consistent kernel density estimates for bounded random variables, and random. function offers an ad-hoc and universal (although computational expensive and potentially inaccurate for long tail distributions) sampling method. In light of the simulation results, we can conclude that (i) the tests implemented in the truncgof package should not be used to assess goodness-of-fit (at least for non-truncated distributions), (ii) the test fan.test shows an over-tendency to not reject the null hypothesis, being visibly miscalibrated (at least in its default option, where the bandwidth parameter is estimated using dpik from package KernSmooth), (iii) the tests ks.test and ad. test show similar power, with ad. test being slightly preferable in large samples, and (iv) dgeometric.test represents a good alternative given its satisfactory calibration and its, in general, superior power in samples of medium and large sizes. As a counterpart it entails more computational burden when the random generator of the null hypothesis density function is not available in R and random. function must be used.
2251214	WOS:000353664600015	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Tools for checking calibration of a Cox model in external validation: Prediction of population-averaged survival curves based on risk groups	Journal	Article	2015	1	1	English	STATA JOURNAL	275-291	17	1	WOS:000353664600015	Royston (2014, Stata Journal 14: 738-755) explained how a popular application of the Cox proportional hazards model "is to develop a multivariable prediction model, often a prognostic model to predict the future clinical outcome of patients with a particular disorder from 'baseline' factors measured at some initial time point. For such a model to be useful in practice, it must be 'validated'; that is, it must perform satisfactorily in an external sample of patients independent of the one on which the model was originally developed. One key aspect of performance is calibration, which is the accuracy of prediction, particularly of survival (or equivalently, failure or event) probabilities at any time after the time origin". In this article, I suggest an approach to assess calibration by comparing observed (Kaplan-Meier) and predicted survival probabilities in several prognostic groups derived by placing cutpoints on the prognostic index. I distinguish between full validation, where all relevant quantities are estimated on the derivation dataset and predicted on the validation dataset, and partial validation, where the prognostic index and prognostic groups are derived from published information and the baseline distribution function is estimated in the validation dataset. Partial validation is more feasible in practice because it is uncommon to have access to individual patient values in both datasets. I exemplify the method by detailed analysis of two datasets in the disease primary biliary cirrhosis; the datasets comprise a derivation and a validation dataset. I describe a new ado-file, stcoxgrp, that performs the necessary calculations. Results for stcoxgrp are displayed graphically, which makes it easier for users to picture calibration (or lack thereof) according to follow-up time.
2260436	WOS:000366013900001	CX9GV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	sms: Microdata for Geographical Analysis in R	Journal	Article	2015	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000366013900001	Spatial microsimulation is a methodology aiming to simulate entities such as households, individuals or businesses in the finest possible scale. This process requires the use of individual based microdatasets. The package presented in this work facilitates the production of small area population microdata by combining various datasets such as census data and individual based datasets. This package includes a parallel implementation of random selection with optimization to select a group of individual records that match a macro description. This methodological approach has been used in a number of topics ranging from measuring inequalities in educational attainment (Kavroudakis, Ballas, and Birkin 2012) to estimating poverty at small area levels (Tanton, McNamara, Harding, and Morrison 2007). The development of the method over recent years is driving computational complexity to the edge as it uses modern computational approaches for the combination of data. The R package sms presented in this work uses parallel processing approaches for the efficient production of small area population microdata, which can be subsequently used for geographical analysis. Finally, a complete case study of fitting geographical data with the R package is presented and discussed.
2264459	WOS:000349846900001	CB7ZG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	sp Timer: Spatio-Temporal Bayesian Modeling Using R	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000349846900001	Hierarchical Bayesian modeling of large point-referenced space-time data is increasingly becoming feasible in many environmental applications due to the recent advances in both statistical methodology and computation power. Implementation of these methods using the Markov chain Monte Carlo (MCMC) computational techniques, however, requires development of problem-specific and user-written computer code, possibly in a low-level language. This programming requirement is hindering the widespread use of the Bayesian model-based methods among practitioners and, hence there is an urgent need to develop high-level software that can analyze large data sets rich in both space and time. This paper develops the package spTimer for hierarchical Basyesian modeling of stylized environmental space-time monitoring data as a contributed software package in the R language that is fast becoming a very popular statistical computing platform. The package is able to fit, spatially and temporally predict large amounts of space-time data using three recently developed Bayesian models. The user is given control over many options regarding covariance function selection, distance calculation, prior selection and tuning of the implemented MCMC algorithms, although suitable defaults are provided. The package has many other attractive features such as on the fly trasformation and an ability to spatially predict temporally aggregated summaries on the original scale, which saves the problem of storage when using MCMC methods for large datasets. A simulation example, with more than a million observations, and a real life data example are used to validate the underlying code and to illustrate the software capabilities.
2266907	WOS:000357431900005	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	rdrobust: An R Package for Robust Nonparametric Inference in Regression-Discontinuity Designs	Journal	Article	2015	6	1	English	R JOURNAL	38-51	14	1	WOS:000357431900005	This article describes the R package rdrobust, which provides data-driven graphical and inference procedures for RD designs. The package includes three main functions: rdrobust, rdbwselect and rdplot. The first function (rdrobust) implements conventional local-polynomial RD treatment effect point estimators and confidence intervals, as well as robust bias-corrected confidence intervals, for average treatment effects at the cutoff. This function covers sharp RD, sharp kink RD, fuzzy RD and fuzzy kink RD designs, among other possibilities. The second function (rdbwselect) implements several bandwidth selectors proposed in the RD literature. The third function (rdplot) provides data-driven optimal choices of evenly-spaced and quantile-spaced partition sizes, which are used to implement several data-driven RD plots.
2277808	WOS:000366015200001	CX9HI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	OrdNor: An R Package for Concurrent Generation of Correlated Ordinal and Normal Data	Journal	Article	2015	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	14	1	WOS:000366015200001	In this article, operational details of the R package OrdNor that is designed for the concurrent generation of correlated ordinal and normal data are described, and examples of some important functions are given. The package provides needed tools that have been lacking for generating multivariate data with a mixture of ordinal and normal components.
2320546	WOS:000349844800001	CB7YM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	micromap: A Package for Linked Micromaps	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000349844800001	The R package micromap is used to create linked micromaps, which display statistical summaries associated with areal units, or polygons. Linked micromaps provide a mesns to simultaneously summarize and display both statistical and geographic distributions by linking statistical summaries to a series of small maps. The package contains functions dependent on the ggplot2 package to produce a row-oriented graph composed of different panels, or columns, of information. These panels at a minimum typically contain maps, a legend, and statistical summaries, with the color-coded legend linking the maps and statistical summaries. We first describe the layout of linked with the statistical dataset. Highly detailed polygons are not appropriate for display in linked micromaps so we describe how polygon boundaries can be simplified, decreasing the time required so we describe how polygon boundaries can be simplified, describing the time required to draw the graphs, while retaining adequate detail for detection of spatial patterns. Our worked examples of linked micromaps use public health data as well as environment data collected from spatially balanced probabilistic surveys.
2340194	WOS:000352915700001	CF9VQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	BatchJobs and BatchExperiments: Abstraction Mechanisms for Using R in Batch Environments	Journal	Article	2015	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000352915700001	Empirical analysis of statistical algorithms often demands time-consuming experiments. We present two R packages which greatly simplify working in batch computing environments. The package Batch Jobs implements the basic objects and procedures to control any batch cluster from within R. It is structured around cluster versions of the well-known higher order functions Map, Reduce and Filter from functional programming. Computations are performed asynchronously and all job states are persistently stored in a database, which can be queried at any point in time. The second package, BatchExperiments, is tailored for the still very general scenario of analyzing arbitrary algorithms on problem instances. It extends package BatchJobs by letting the user define an array of jobs of the kind "apply algorithm A to problem instance P and store results". It is possible to associate statistical designs with parameters of problems and algorithms and therefore to systematically study their influence on the results. The packages' main features are: (a) Convenient usage : All relevant batch system operations are either handled internally or mapped to simple R functions. (b) Portability : Both packages use a clear and well-de fined interface to the batch system which makes them applicable in most high-performance computing environments. (c) Reproducibility : Every computational part has an associated seed to ensure reproducibility even when the underlying batch system changes. (d) Abstraction an d good software design : The code layers for algorithms, experiment definitions and execution are cleanly separated and enable the writing of readable and maintainable code.
2348093	WOS:000365982600001	CX8VP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	pdc: An R Package for Complexity-Based Clustering of Time Series	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000365982600001	Permutation distribution clustering is a complexity-based approach to clustering time series. The dissimilarity of time series is formalized as the squared Hellinger distance between the permutation distribution of embedded time series. The resulting distance measure has linear time complexity, is invariant to phase and monotonic transformations, and robust to outliers. A probabilistic interpretation allows the determination of the number of significantly different clusters. An entropy-based heuristic relieves the user of the need to choose the parameters of the underlying time-delayed embedding manually and, thus, makes it possible to regard the approach as parameter-free. This approach is illustrated with examples on empirical data.
2349055	WOS:000365975200001	CX8ST	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	DiceDesign and DiceEval: Two R Packages for Design and Analysis of Computer Experiments	Journal	Article	2015	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	38	1	WOS:000365975200001	This paper introduces two R packages available on the Comprehensive R Archive network. The main application concerns the study of computer code output. Package DiceDesign is dedicated to numerical design of experiments, fromt he construction to the study of the design properties. Package DiceEval deals with the fit, the validation and the comparison of metamodels. After a brief presentation of the context, we focus on the architecture of these two packages. A two-dimensional test function will be a running example to illustrate the main functionalities of these packages and an industrial case study in five dimensions will also be detailed.
2370848	WOS:000357139500015	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Species of origin	Journal	Article	2015	1	1	English	STATA JOURNAL	574-587	14	1	WOS:000357139500015	Counting and measurement scales often have natural origins that deserve respect in analysis. In other problems, we should feel free to shift or translate the origin or to rotate a periodic scale. Examples with environmental data focus on trends in time and seasonal cycles. Several small tips are bundled together on working with noncalendar years in Stata.
2406508	WOS:000352911400001	CF9UD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Constructing and Modifying Sequence Statistics for relevent Using informR in R	Journal	Article	2015	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-36	36	1	WOS:000352911400001	The informR package greatly simplifies the analysis of complex event histories in R by providing user friendly tools to build sufficient statistics for the relevent package. Historically, building sufficient statistics to model event sequences (of the form a -> b) using the egocentric generalization of Butts' (2008) relational event framework for modeling social action has been cumbersome. The informR package simplifies the construction of the complex list of arrays needed by the rem() model fitting for a variety of cases involving egocentric event data, multiple event types, and/or support constraints. This paper introduces these tools using examples from real data extracted from the American Time Use Survey.
2436122	WOS:000357139500008	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Conducting interrupted time-series analysis for single- and multiple-group comparisons	Journal	Article	2015	1	1	English	STATA JOURNAL	480-500	21	1	WOS:000357139500008	In this article, I introduce the it sa command, which performs interrupted time-series analysis for single- and multiple-group comparisons. In an interrupted time-series analysis, an outcome variable is observed over multiple, equally spaced time periods before and after the introduction of an intervention that is expected to interrupt its level or trend. The itsa command estimates the effect of an intervention on an outcome variable either for a single treatment group or when compared with one or more control groups. Additionally, its options allow the user to control for autocorrelated disturbances and to estimate treatment effects over multiple periods.
2450728	WOS:000365975400001	CX8SV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ergm.graphlets: A Package for ERG Modeling Based on Graphlet Statistics	Journal	Article	2015	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000365975400001	Exponential-family random graph models are probabilistic network models that are parametrized by sufficient statistics based on structural (i.e., graph-theoretic) properties. The ergm package for the R statistical computing environment is a collection of tools for the analysis of network data within an exponential-family random graph model framework. Many different network properties can be employed as sufficient statistics for exponentialfamily random graph models by using the model terms defined in the ergm package; this functionality can be expanded by the creation of packages that code for additional network statistics. Here, our focus is on the addition of statistics based on graphlets. Graphlets are classes of small, connected, induced subgraphs that can be used to describe the topological structure of a network. We introduce an R package called ergm.graphlets that enables the use of graphlet properties of a network within the ergm package of R. The ergm.graphlets package provides a complete list of model terms that allows to incorporate statistics of any 2-, 3-, 4- and 5-node graphlets into exponential-family random graph models. The new model terms of the ergm.graphlets package enable both exponential-family random graph modeling of global structural properties and investigation of relationships between node attributes (i.e., covariates) and local topologies around nodes.
2455231	WOS:000365975100001	CX8SS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	remote: Empirical Orthogonal Teleconnections in R	Journal	Article	2015	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000365975100001	In climate science, teleconnection analysis has a long standing history as a means for describing regions that exhibit above average capability of explaining variance over time within a certain spatial domain (e.g., global). The most prominent example of a global coupled ocean-atmosphere teleconnection is the El Nino Southern Oscillation. There are numerous signal decomposition methods for identifying such regions, the most widely used of which are (rotated) empirical orthogonal functions. First introduced by van den Dool, Saha, and Johansson (2000), empirical orthogonal teleconnections (EOT) denote a regression based approach that allows for straight-forward interpretation of the extracted modes. In this paper we present the R implementation of the original algorithm in the remote package. To highlight its usefulness, we provide three examples of potential use-case scenarios for the method including the replication of one of the original examples from van den Dool et al. (2000). Furthermore, we highlight the algorithm's use for cross-correlations between two different geographic fields (identifying sea surface temperature drivers for precipitation), as well as statistical downscaling from coarse to fine grids (using Normalized Difference Vegetation Index fields).
2455234	WOS:000365976600001	CX8TH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Parametric and Nonparametric Sequential Change Detection in R: The cpm Package	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	20	1	WOS:000365976600001	The change point model framework introduced in Hawkins, Qiu, and Kang (2003) and Hawkins and Zamba (2005a) provides an effective and computationally efficient method for detecting multiple mean or variance change points in sequences of Gaussian random variables, when no prior information is available regarding the parameters of the distribution in the various segments. It has since been extended in various ways by Hawkins and Deng (2010), Ross, Tasoulis, and Adams (2011), Ross and Adams (2012) to allow for fully nonparametric change detection in non-Gaussian sequences, when no knowledge is available regarding even the distributional form of the sequence. Another extension comes from Ross and Adams (2011) and Ross (2014) which allows change detection in streams of Bernoulli and Exponential random variables respectively, again when the values of the parameters are unknown. This paper describes the R package cpm, which provides a fast implementation of all the above change point models in both batch (Phase I) and sequential (Phase II) settings, where the sequences may contain either a single or multiple change points.
2485741	WOS:000365973800001	CX8SF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	simPH: An R Package for Illustrating Estimates from Cox Proportional Hazard Models Including for Interactive and Nonlinear Effects	Journal	Article	2015	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000365973800001	The R package simPH provides tools for effectively communicating results from Cox proportional hazard (PH) models, including models with interactive and nonlinear effects. The Cox (PH) model is a popular tool for examining event data. However, previously available computational tools have not made it easy to explore and communicate quantities of interest and associated uncertainty estimated from them. This is especially true when the effects are interactions or nonlinear transformations of continuous variables. These transformations are especially useful with Cox PH models because they can be employed to correctly specifying models that would otherwise violate the nonproportional hazards assumption. Package simPH makes it easy to simulate and then plot quantities of interest for a variety of effects estimated from Cox PH models including interactive effects, nonlinear effects, as well as standard linear effects. Package simPH employs visual weighting in order to effectively communicate estimation uncertainty. There are options to show either the standard central interval of the simulation's distribution or the shortest probability interval - which can be useful for asymmetrically distributed estimates. This paper uses hypothetical and empirical examples to illustrate package simPH's syntax and capabilities.
2539078	WOS:000349845400001	CB7YS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ads Package for R: A Fast Unbiased Implementation of the K-function Family for Studying Spatial Point Patterns in Irregular-Shaped Sampling Windows	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000349845400001	ads is an R package that performs multi-scale spatial point pattern analyses through methods derived from Ripley's K-function. These methods apply to univariate, multivariate or marked point data mapped in a rectangular, circular or irregular-shaped sampling window. Specific tests of statistical significance based on Monte Carlo simulations are associated to these methods. The main features of ads is to call fast C subroutines for computing Ripley's unbiased local correction of edge effects for various sampling window con figurations and for performing Monte Carlo simulations. It thus allows one to analyze large datasets and to compute robust confidence envelopes. This paper is an introduction to ads version 1.5, focusing on its complementarity with the other R packages for spatial point pattern analysis, and on recent original developments towards the introduction of multivariate functions for analyzing spatial pattern of species diversity.
2564008	WOS:000349847500001	CB7ZL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Comparing Implementations of Estimation Methods for Spatial Econometrics	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-36	36	1	WOS:000349847500001	Recent advances in the implementation of spatial econometrics model estimation techniques have made it desirable to compare results, which should correspond between implementations across software applications for the same data. These model estimation techniques are associated with methods for estimating impacts (emanating effects), which are also presented and compared. This review constitutes an up-to-date comparison of generalized method of moments and maximum likelihood implementations now available. The comparison uses the cross-sectional US county data set provided by Drukker, Prucha, and Raciborski (2013d). The comparisons will be cast in the context of alternatives using the MATLAB Spatial Econometrics toolbox, Stata's user-written sppack commands, Python with PySAL and R packages including spdep, sphet and McSpatial.
2564178	WOS:000357139500009	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Top 10 Stata "gotchas"	Journal	Article	2015	1	1	English	STATA JOURNAL	501-511	11	1	WOS:000357139500009	Stata is a powerful and user-friendly package for setting up data and performing statistical analysis. Nevertheless, some features often cause unexpected errors that users either fail to notice or spend hours trying to correct. In this article, I list my top 10 Stata "gotchas" and suggest ways to combat them. Awareness of these "gotchas" will hopefully help newer users particularly those making their first forays into Stata programming avoid the most common pitfalls.
2578438	WOS:000364359100017	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	The Berry-Levinsohn-Pakes estimator of the random-coefficients logit demand model	Journal	Article	2015	1	1	English	Stata Journal	854-880	27	1	WOS:000364359100017	In this article, I describe the algorithm proposed by Berry, Levinsohn, and Fakes (1995, Econometrica 63: 841-890) to fit the random-parameters logit demand model from product market shares. I present a new command, blp, for this estimator.
2580455	WOS:000357139500017	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	graphlog: Creating log files with embedded graphics	Journal	Editorial Material	2015	1	1	English	STATA JOURNAL	594-596	3	1	WOS:000357139500017	In this note, I describe the user-written command graphlog. Stata's built-in log command lacks the ability to include graphics in log files. graphlog embeds graphics in a Stata log file and saves the result to a PDF file. This improves the documentation of the work done.
2626687	WOS:000349846800001	CB7ZF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	spate: An R Package for Spatio-Temporal Modeling with a Stochastic Advection-Diffusion Process	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000349846800001	The R package spate implements methodology for modeling of large space-time data sets. A spatio-temporal Gaussian process is defined through a stochastic partial differential equation (SPDE) which is solved using spectral methods. In contrast to the traditional geostatistical way of relying on the covariance function, the spectral SPDE approach is computationally tractable and provides a realistic space-time parametrization. This package aims at providing tools for simulating and modeling of spatio-temporal processes using an SPDE based approach. The package contains functions for obtaining parametrizations, such as propagator or innovation covariance matrices, of the spatio-temporal model. This allows for building customized hierarchical Bayesian models using the SPDE based model at the process stage. The functions of the package then provide computationally efficient algorithms needed for doing inference with the hierarchical model. Furthermore, an adaptive Markov chain Monte Carlo (MCMC) algorithm implemented in the package can be used as an algorithm for doing inference without any additional modeling. This function is flexible and allows for application specific customizing. The MCMC algorithm supports data that follow a Gaussian or a censored distribution with point mass at zero. Spatio-temporal covariates can be included in the model through a regression term.
2641493	WOS:000365981600001	CX8VF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multivariate and 2D Extensions of Singular Spectrum Analysis with the Rssa Package	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-78	78	1	WOS:000365981600001	Implementation of multivariate and 2D extensions of singular spectrum analysis (SSA) by means of the R package Rssa is considered. The extensions include MSSA for simultaneous analysis and forecasting of several time series and 2D-SSA for analysis of digital images. A new extension of 2D-SSA analysis called shaped 2D-SSA is introduced for analysis of images of arbitrary shape, not necessary rectangular. It is shown that implementation of shaped 2D-SSA can serve as a basis for implementation of MSSA and other generalizations. Efficient implementation of operations with Hankel and Hankel-block-Hankel matrices through the fast Fourier transform is suggested. Examples with code fragments in R, which explain the methodology and demonstrate the proper use of Rssa, are presented.
2644087	WOS:000366013800001	CX9GU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	CovSel: An R Package for Covariate Selection When Estimating Average Causal Effects	Journal	Article	2015	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	20	1	WOS:000366013800001	We describe the R package CovSel, which reduces the dimension of the covariate vector for the purpose of estimating an average causal effect under the unconfoundedness assumption. Covariate selection algorithms developed in De Luna, Waernbaum, and Richardson (2011) are implemented using model- free backward elimination. We show how to use the package to select minimal sets of covariates. The package can be used with continuous and discrete covariates and the user can choose between marginal co- ordinate hypothesis tests and kernel- based smoothing as model- free dimension reduction techniques.
2647919	WOS:000357139500004	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Person-centered treatment (PeT) effects: Individualized treatment effects using instrumental variables	Journal	Article	2015	1	1	English	STATA JOURNAL	397-410	14	1	WOS:000357139500004	I describe a command, petiv, that uses a local instrumental-variables (LIV) approach to estimate person-centered treatment effects for a variety of specifications for the LIV estimand as outlined in Basu (2014, Journal of Applied Econometrics 29: 671-691). The petiv command creates a new variable in the dataset that contains the person-centered treatment effects for each individual in the dataset. However, the command takes the validity of the instrumental variables and the specification of the LIV estimand as given. Appropriateness of these features of an LIV analysis should be determined before running the petiv command. The individual effects can be used to answer distributional questions and can also be easily aggregated to obtain mean treatment-effects estimates.
2671992	WOS:000357431900007	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The Complex Multivariate Gaussian Distribution	Journal	Article	2015	6	1	English	R JOURNAL	73-80	8	1	WOS:000357431900007	Here I introduce package cmvnorm, a complex generalization of the mvtnorm package. A complex generalization of the Gaussian process is suggested and numerical results presented using the package. An application in the context of approximating theWeierstrass sigma-function using a complex Gaussian process is given.
2686346	WOS:000357431900016	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Manipulation of Discrete Random Variables with discreteRV	Journal	Article	2015	6	1	English	R JOURNAL	185-194	10	1	WOS:000357431900016	A prominent issue in statistics education is the sometimes large disparity between the theoretical and the computational coursework. discreteRV is an R package for manipulation of discrete random variables which uses clean and familiar syntax similar to the mathematical notation in introductory probability courses. The package offers functions that are simple enough for users with little experience with statistical programming, but has more advanced features which are suitable for a large number of more complex applications. In this paper, we introduce and motivate discreteRV, describe its functionality, and provide reproducible examples illustrating its use.
2702549	WOS:000353664600009	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	newspell: Easy management of complex spell data	Journal	Article	2015	1	1	English	STATA JOURNAL	155-172	18	1	WOS:000353664600009	Biographical data gathered in surveys are often stored in spell format, allowing for overlaps between spell states. On the one hand, these kind of data provide useful information for researchers. On the other hand, the data structure is often complex and not easy to handle. The newspell program offers a solution to the problem of spell-data management with three important features. First, it can rank spells and cut off overlaps according to the rank order. Second, newspell can combine overlapping parts of spells into new categories of spells, generating entirely new states. Third, it can detect gaps in the spell data that are not yet coded. It also includes subcommands for the management of complex spell data. Spell states can be merged and filled in with information from adjacent spells, and the data can be transformed to long or wide format. The command can be used to clean data, to combine two spell-data sources that have information on different kinds of states, or to deal with spell data that are complex by survey design. newspell is useful for users who are not familiar with complex spell data and have little experience in Stata programming or data management. For experienced users, newspell saves a lot of time and coding work.
2740226	WOS:000352915500001	CF9VO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	gems: An R Package for Simulating from Disease Progression Models	Journal	Article	2015	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000352915500001	Mathematical models of disease progression predict disease outcomes and are useful epidemiological tools for planners and evaluators of health interventions. The R package gems is a tool that simulates disease progression in patients and predicts the effect of different interventions on patient outcome. Disease progression is represented by a series of events (e.g., diagnosis, treatment and death), displayed in a directed acyclic graph. The vertices correspond to disease states and the directed edges represent events. The package gems allows simulations based on a generalized multistate model that can be described by a directed acyclic graph with continuous transition-specific hazard functions. The user can specify an arbitrary hazard function and its parameters. The model includes parameter uncertainty, does not need to be a Markov model, and may take the history of previous events into account. Applications are not limited to the medical field and extend to other areas where multistate simulation is of interest. We provide a technical explanation of the multistate models used by gems, explain the functions of gems and their arguments, and show a sample application.
2741125	WOS:000349845700001	CB7YV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Analysis, Simulation and Prediction of Multivariate Random Fields with Package Random Fields	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000349845700001	Modeling of and inference on multivariate data that have been measured in space, such as temperature and pressure, are challenging tasks in environmental sciences, physics and materials science. We give an overview over and some background on modeling with cross-covariance models. The R package RandomFields supports the simulation, the parameter estimation and the prediction in particular for the linear model of coregionalization, the multivariate Matern models, the delay model, and a spectrum of physically motivated vector valued models. An example on weather data is considered, illustrating the use of RandomFields for parameter estimation and prediction.
2768900	WOS:000349846200001	CB7YZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Parallelizing Gaussian Process Calculations in R	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000349846200001	We consider parallel computation for Gaussian process calculations to overcome computational and memory constraints on the size of datasets that can be analyzed. Using a hybrid parallelization approach that uses both threading (shared memory) and message-passing (distributed memory), we implement the core linear algebra operations used in spatial statistics and Gaussian process regression in an R package called bigGP that relies on C and MPI. The approach divides the covariance matrix into blocks such that the computational load is balanced across processes while communication between processes is limited. The package provides an API enabling R programmers to implement Gaussian process-based methods by using the distributed linear algebra operations without any C or MPI coding. We illustrate the approach and software by analyzing an astrophysics dataset with n = 67; 275 observations.
2782978	WOS:000352915100001	CF9VK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	nparcomp: An R Software Package for Nonparametric Multiple Comparisons and Simultaneous Confidence Intervals	Journal	Article	2015	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	17	1	WOS:000352915100001	One-way layouts, i.e., a single factor with several levels and multiple observations at each level, frequently arise in various fields. Usually not only a global hypothesis is of interest but also multiple comparisons between the different treatment levels. In most practical situations, the distribution of observed data is unknown and there may exist a number of atypical measurements and outliers. Hence, use of parametric and semiparametric procedures that impose restrictive distributional assumptions on observed samples becomes questionable. This, in turn, emphasizes the demand on statistical procedures that enable us to accurately and reliably analyze one-way layouts with minimal conditions on available data. Nonparametric methods offer such a possibility and thus become of particular practical importance. In this article, we introduce a new R package nparcomp which provides an easy and user-friendly access to rank-based methods for the analysis of unbalanced one-way layouts. It provides procedures performing multiple comparisons and computing simultaneous confidence intervals for the estimated effects which can be easily visualized. The special case of two samples, the nonparametric Behrens-Fisher problem, is included. We illustrate the implemented procedures by examples from biology and medicine.
2822379	WOS:000357431900002	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Peptides: A Package for Data Mining of Antimicrobial Peptides	Journal	Article	2015	6	1	English	R JOURNAL	4-14	11	1	WOS:000357431900002	Antimicrobial peptides (AMP) are a promising source of antibiotics with a broad spectrum activity against bacteria and low incidence of developing resistance. The mechanism by which an AMP executes its function depends on a set of computable physicochemical properties from the amino acid sequence. The Peptides package was designed to allow the quick and easy computation of ten structural characteristics own of the antimicrobial peptides, with the aim of generating data to increase the accuracy in classification and design of new amino acid sequences. Moreover, the options to read and plot XVG output files from GROMACS molecular dynamics package are included.
2833206	WOS:000365973700001	CX8SE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	CompPD: A MATLAB Package for Computing Projection Depth	Journal	Article	2015	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000365973700001	Since the seminal work of Tukey (1975), depth functions have proved extremely useful in robust data analysis and inference for multivariate data. Many notions of depth have been developed in the last decades. Among others, projection depth appears to be very favorable. It turns out that (Zuo 2003; Zuo, Cui, and He 2004; Zuo 2006), with appropriate choices of univariate location and scale estimators, the projection depth induced estimators usually possess very high breakdown point robustness and finite sample relative efficiency. However, the computation of the projection depth seems hopeless and intimidating if not impossible. This hinders the further inference procedures development in practice. Sporadically algorithms exist in individual papers, though an unified computation package for projection depth has not been documented. To fill the gap, a MATLAB package entitled CompPD is presented in this paper, which is in fact an implementation of the latest developments (Liu, Zuo, and Wang 2013; Liu and Zuo 2014). Illustrative examples are also provided to guide readers through step-by-step usage of package CompPD to demonstrate its utility.
2833338	WOS:000364359100013	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Regression analysis of censored data using pseudo-observations: An update	Journal	Article	2015	1	1	English	Stata Journal	809-821	13	1	WOS:000364359100013	We present updated versions of the stpsurv, stpci, and stpmean commands, which were introduced in Parner and Andersen (2010, Stata Journal 10: 408-422), along with a new command, stplost. The commands generate pseudo-observations of the survival function, the cumulative incidence function under competing risks, the restricted mean survival-time function, and the cause-specific lost-lifetime function. The pseudo-observations can be used to assess the effects of covariates on their respective functions at different times by fitting generalized linear models to the pseudo-observations. The updated commands feature new options, an increase in computational speed, and the ability to handle survival data with delayed entry.
2834217	WOS:000364359100008	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A note on adding objects to an existing twoway graph	Journal	Article	2015	1	1	English	Stata Journal	751-755	5	1	WOS:000364359100008	In Stata, graphs are usually generated by one call to the graph command. Sometimes, however, it would be convenient to be able to add objects to a graph after the graph has been created. In this article, I provide a command called addplot that offers such functionality for twoway graphs, capitalizing on an undocumented feature of Stata's graphics system.
2837524	WOS:000365975800001	CX8SZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An Improved Evaluation of Kolmogorov's Distribution	Journal	Article	2015	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-8	8	1	WOS:000365975800001	We propose a new algorithm for computing extreme probabilities of Kolmogorov's goodness-of-fit measure, D-n. This algorithm is an improved version of the method originally proposed by Wang, Tsang, and Marsaglia (2003) based on a result from Durbin (1973). The new algorithm keeps the same numerical precision of the Wang et al. (2003) method, but is more efficient: it features linear instead of quadratic space complexity and has better time complexity for a common range of input parameters of practical importance. The proposed method is implemented in the R package kolmim, which also includes an improved routine to perform one-sample two-sided exact Kolmogorv-Smirnov tests.
2838490	WOS:000353664600018	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Alan Acock's Discovering Structural Equation Modeling Using Stata, Revised Edition	Journal	Review	2015	1	1	English	STATA JOURNAL	309-315	7	1	WOS:000353664600018	In this article, I review Discovering Structural Equation Modeling Using Stata, Revised Edition, by Alan Acock (2013 [Stata Press]).
2862888	WOS:000365984000001	CX8WB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian State-Space Modelling on High-Performance Hardware Using LibBi	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-36	36	1	WOS:000365984000001	LibBi is a software package for state space modelling and Bayesian inference on modern computer hardware, including multi-core central processing units, many-core graphics processing units, and distributed-memory clusters of such devices. The software parses a domain-specific language for model specification, then optimizes, generates, compiles and runs code for the given model, inference method and hardware platform. In presenting the software, this work serves as an introduction to state space models and the specialized methods developed for Bayesian inference with them. The focus is on sequential Monte Carlo (SMC) methods such as the particle filter for state estimation, and the particle Markov chain Monte Carlo and SMC2 methods for parameter estimation. All are well-suited to current computer hardware. Two examples are given and developed throughout, one a linear three-element windkessel model of the human arterial system, the other a nonlinear Lorenz '96 model. These are specified in the prescribed modelling language, and LibBi demonstrated by performing inference with them. Empirical results are presented, including a performance comparison of the software with different hardware configurations.
2863609	WOS:000365973900001	CX8SG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	kml and kml3d: R Packages to Cluster Longitudinal Data	Journal	Article	2015	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000365973900001	Longitudinal studies are essential tools in medical research. In these studies, variables are not restricted to single measurements but can be seen as variable-trajectories, either single or joint. Thus, an important question concerns the identification of homogeneous patient trajectories. kml and kml3d are R packages providing an implementation of k-means designed to work specifically on trajectories (kml) or on joint trajectories (kml3d). They provide various tools to work on longitudinal data: imputation methods for trajectories (nine classic and one original), methods to de fine starting conditions in k-means (four classic and three original) and quality criteria to choose the best number of clusters (four classic and one original). In addition, they off er graphic facilities to "visualize" the trajectories, either in 2D (single trajectory) or 3D (joint-trajectories). The 3D graph representing the mean joint-trajectories of each cluster can be exported through LATEX in a 3D dynamic rotating PDF graph (Figures 1 and 9).
2888045	WOS:000352910400001	CF9TT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	iqLearn: Interactive Q-Learning in R	Journal	Article	2015	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	25	1	WOS:000352910400001	Chronic illness treatment strategies must adapt to the evolving health status of the patient receiving treatment. Data-driven dynamic treatment regimes can offer guidance for clinicians and intervention scientists on how to treat patients over time in order to bring about the most favorable clinical outcome on average. Methods for estimating optimal dynamic treatment regimes, such as Q-learning, typically require modeling nonsmooth, nonmonotone transformations of data. Thus, building well-fitting models can be challenging and in some cases may result in a poor estimate of the optimal treatment regime. Interactive Q-learning (IQ-learning) is an alternative to Q-learning that only requires modeling smooth, monotone transformations of the data. The R package iqLearn provides functions for implementing both the IQ-learning and Q-learning algorithms. We demonstrate how to estimate a two-stage optimal treatment policy with iqLearn using a generated data set bmiData which mimics a two-stage randomized body mass index reduction trial with binary treatments at each stage.
2889843	WOS:000353664600007	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Fixed-effect panel threshold model using Stata	Journal	Article	2015	1	1	English	STATA JOURNAL	121-134	14	1	WOS:000353664600007	Threshold models are widely used in macroeconomics and financial analysis for their simple and obvious economic implications. With these models, however, estimation and inference is complicated by the existence of nuisance parameters. To combat this issue, Hansen (1999, Journal of Econometrics 93: 345 368) proposed the fixed-effect panel threshold model. In this article, I introduce a new command (xthreg) for implementing this model. I also use Monte Carlo simulations to show that, although the size distortion of the threshold-effect test is small, the coverage rate of the confidence interval estimator is unsatisfactory. I include an example on financial constraints (originally from Hansen [1999, Journal of Econometrics 93: 345-368]) to further demonstrate the use of xthreg.
2894052	WOS:000353664600003	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	More power through symbolic computation: Extending Stata by using the Maxima computer algebra system	Journal	Article	2015	1	1	English	STATA JOURNAL	45-76	32	1	WOS:000353664600003	Maxima is a free and open-source computer algebra system that can perform symbolic computations such as solving equations, determining derivatives of functions, obtaining Taylor series, and manipulating algebraic expressions. In this article, I present the Maxima Bridge System, which is a collection of software programs that allows Stata to interface with Maxima so that Maxima can be used for symbolic computation to transfer data from Stata to Maxima and to retrieve results from Maxima. The cooperation between Stata and Maxima provides an environment for statistical analysis in which symbolic computation can be easily used together with all the facilities supplied by Stata. In this environment, symbolic computation algorithms can be used to manage the complexity of algebra and calculus, whereas numerical computation can be used when speed matters. I also discuss the software architecture of the Maxima Bridge System, and I present several examples to illustrate how to develop new Stata commands that exploit the capabilities of Maxima.
2902116	WOS:000365975600001	CX8SX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MF Calculator: A Web-Based Application for Analyzing Similarity	Journal	Article	2015	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-10	10	1	WOS:000365975600001	This paper presents the metric-frequency calculator (MF Calculator), an online application to analyze similarity. The MF Calculator implements a metric-frequency similarity algorithm for the quantitative assessment of similarity in ill-structured data sets. It is widely applicable as it can be used with nominal, ordinal, or interval data when there is little prior control over the variables to be observed regarding number or content. The MF Calculator generates a proximity matrix in CSV, XML or DOC format that can be used as input to traditional statistical techniques such as hierarchical clustering, additive trees, or multidimensional scaling. The MF Calculator also displays a graphical representation of outputs using additive similarity trees. A simulated example illustrates the implementation of the MF calculator. An additional example with real data is presented, in order to illustrate the potential of combining the MF Calculator with cluster analysis. The MF Calculator is a user-friendly tool available free of charge. It can be accessed from http : //mfcalculator.celiasales.org/Calculator.aspx, and it can be used by non-experts from a wide range of social sciences.
2909293	WOS:000349847700001	CB7ZN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Spatial Data Analysis with R-INLA with Some Extensions	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000349847700001	The integrated nested Laplace approximation (INLA) provides an interesting way of approximating the posterior marginals of a wide range of Bayesian hierarchical models. This approximation is based on conducting a Laplace approximation of certain functions and numerical integration is extensively used to integrate some of the models parameters out. The R-INLA package offers an interface to INLA, providing a suitable framework for data analysis. Although the INLA methodology can deal with a large number of models, only the most relevant have been implemented within R-INLA. However, many other important models are not available for R-INLA yet. In this paper we show how to fit a number of spatial models with R-INLA, including its interaction with other R packages for data analysis. Secondly, we describe a novel method to extend the number of latent models available for the model parameters. Our approach is based on conditioning on one or several model parameters and fit these conditioned models with R-INLA. Then these models are combined using Bayesian model averaging to provide the final approximations to the posterior marginals of the model. Finally, we show some examples of the application of this technique in spatial statistics. It is worth noting that our approach can be extended to a number of other fields, and not only spatial statistics.
2936579	WOS:000349847100001	CB7ZI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SPODT: An R Package to Perform Spatial Partitioning	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000349847100001	Spatial cluster detection is a classical question in epidemiology: Are cases located near other cases? In order to classify a study area into zones of different risks and determine their boundaries, we have developed a spatial partitioning method based on oblique de- cision trees, which is called spatial oblique decision tree (SpODT). This non-parametric method is based on the classification and regression tree (CART) approach introduced by Leo Breiman. Applied to epidemiological spatial data, the algorithm recursively searches among the coordinates for a threshold or a boundary between zones, so that the risks estimated in these zones are as different as possible. While the CART algorithm leads to rectangular zones, providing perpendicular splits of longitudes and latitudes, the SpODT algorithm provides oblique splitting of the study area, which is more appropriate and accu- rate for spatial epidemiology. Oblique decision trees can be considered as non-parametric regression models. Beyond the basic function, we have developed a set of functions that enable extended analyses of spatial data, providing: inference, graphical representations, spatio-temporal analysis, adjustments on covariates, spatial weighted partition, and the gathering of similar adjacent final classes. In this paper, we propose a new R package, SPODT, which provides an extensible set of functions for partitioning spatial and spatio- temporal data. The implementation and extensions of the algorithm are described. Func- tion usage examples are proposed, looking for clustering malaria episodes in Bandiagara, Mali, and samples showing three different cluster shapes.
2939491	WOS:000357431900013	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The gridGraphics Package	Journal	Article	2015	6	1	English	R JOURNAL	151-162	12	1	WOS:000357431900013	The gridGraphics package provides a function, grid. echo(), that can be used to convert a plot drawn with the graphics package to a visually identical plot drawn using grid. This conversion provides access to a variety of grid tools for making customisations and additions to the plot that are not possible with the graphics package.
2941179	WOS:000353664600014	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	dynemp: A routine for distributed microdata analysis of business dynamics	Journal	Article	2015	1	1	English	STATA JOURNAL	247-274	28	1	WOS:000353664600014	In this article, we introduce a new command, dynemp, that implements a distributed microdata analysis of business and employment dynamics and firm demographics. As its data source, dynemp requires business registers or comparable firm- or establishment-level longitudinal databases that cover the (near-)universe of companies in all business sectors. Access to such confidential data is usually restricted, and the microlevel data cannot be brought together to a single platform for cross-country analysis. To solve this confidentiality problem while also maintaining a high level of harmonization of the key economic concepts, dynemp can be distributed in a network of researchers who have access to the national confidential microdata. This way, the rich firm-level employment dynamics can be analyzed from new angles (such as firm age and size), significantly expanding the scope of analyses relying only on more aggregated data.
2947870	WOS:000364359100007	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Fitting adjusted limited dependent variable mixture models to EQ-5D	Journal	Article	2015	1	1	English	Stata Journal	737-750	14	1	WOS:000364359100007	In this article, we describe the aldvmm command for fitting adjusted limited dependent variable mixture models to either UK or U.S. tariff EQ-5D data. We present and explain the command and postestimation command through examples. The aldvmm command requires use of Stas Kolenikov's simulated annealing package (simann()), which can be easily installed by typing net install simann pkg from(http://web.missouri.edu/-kolenikovs/stata).
2952842	WOS:000365982900001	CX8VR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Rmixmod: The R Package of the Model-Based Unsupervised, Supervised, and Semi-Supervised Classification Mixmod Library	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000365982900001	Mixmod is a well-established software package for fitting mixture models of multivariate Gaussian or multinomial probability distribution functions to a given dataset with either a clustering, a density estimation or a discriminant analysis purpose. The Rmixmod S4 package provides an interface from the R statistical computing environment to the C++ core library of Mixmod (mixmodLib). In this article, we give an overview of the model-based clustering and classification methods implemented, and we show how the R package Rmixmod can be used for clustering and discriminant analysis.
2961881	WOS:000357431900008	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	sae: An R Package for Small Area Estimation	Journal	Article	2015	6	1	English	R JOURNAL	81-98	18	1	WOS:000357431900008	We describe the R package sae for small area estimation. This package can be used to obtain model-based estimates for small areas based on a variety of models at the area and unit levels, along with basic direct and indirect estimates. Mean squared errors are estimated by analytical approximations in simple models and applying bootstrap procedures in more complex models. We describe the package functions and show how to use them through examples.
2970839	WOS:000352914400001	CF9VD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	NHPoisson: An R Package for Fitting and Validating Nonhomogeneous Poisson Processes	Journal	Article	2015	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000352914400001	NHPoisson is an R package for the modeling of nonhomogeneous Poisson processes in one dimension. It includes functions for data preparation, maximum likelihood estimation, covariate selection and inference based on asymptotic distributions and simulation methods. It also provides specific methods for the estimation of Poisson processes resulting from a peak over threshold approach. In addition, the package supports a wide range of model validation tools and functions for generating nonhomogenous Poisson process trajectories. This paper is a description of the package and aims to help those interested in modeling data using nonhomogeneous Poisson processes.
2980147	WOS:000365973600001	CX8SD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The R Package groc for Generalized Regression on Orthogonal Components	Journal	Article	2015	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000365973600001	The R package groc for generalized regression on orthogonal components contains functions for the prediction of q responses using a set of p predictors. The primary building block is the grid algorithm used to search for components(projections of the data) which are most dependent on the response. The package offers flexibility in the choice of the dependence measure which can be user-de fined. The components are found sequentially. A first component is obtained and a smooth fit produces residuals. Then, a second component orthogonal to the first is found which is most dependent on the residuals, and so on. The package can handle models with more than one response. A panoply of models can be achieved through package groc : robust multiple or multivariate linear regression, nonparametric regression on orthogonal components, and classical or robust partial least squares models. Functions for predictions and cross-validation are available and helpful in model selection. The merit of a fit through cross-validation can be assessed with the predicted residual error sum of squares or the predicted residual error median absolute deviation which is more appropriate in the presence of outliers.
2993032	WOS:000357431900017	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Estimability Tools for Package Developers	Journal	Article	2015	6	1	English	R JOURNAL	195-199	5	1	WOS:000357431900017	When a linear model is rank-deficient, then predictions based on that model become questionable because not all predictions are uniquely estimable. However, some of them are, and the estimability package provides tools that package developers can use to tell which is which. With the use of these tools, a model object's predict method could return estimable predictions as-is while flagging non-estimable ones in some way, so that the user can know which predictions to believe. The estimability package also provides, as a demonstration, an estimability-enhanced epredict method to use in place of predict for models fitted using the stats package.
2995273	WOS:000364359100015	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Model specification and bootstrapping for multiply imputed data: An application to count models for the frequency of alcohol use	Journal	Article	2015	1	1	English	Stata Journal	833-844	12	1	WOS:000364359100015	Stata's mi commands provide powerful tools to conduct multiple imputation in the presence of ignorable missing data. In this article, I present Stata code to extend the capabilities of the mi commands to address two areas of statistical inference where results are not easily aggregated across imputed datasets. First, mi commands are restricted to covariate selection. I show how to address model fit to correctly specify a model. Second, the mi commands readily aggregate model-based standard errors. I show how standard errors can be bootstrapped for situations where model assumptions may not be met. I illustrate model specification and bootstrapping on frequency counts for the number of times that alcohol was consumed in data with missing observations from a behavioral intervention.
2998469	WOS:000364359100009	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating treatment effects for ordered outcomes using maximum simulated likelihood	Journal	Article	2015	1	1	English	Stata Journal	756-774	19	1	WOS:000364359100009	I present four new commands to estimate the effect of a binary endogenous treatment on an ordered outcome. Such models conventionally rely upon joint normality of the unobservables in treatment and outcome processes, as do treatoprobit and switchoprobit. In this article, I highlight the capabilities of treatoprobitsim and switchoprobitsim, which both use a latent-factor structure to model the joint distribution of the treatment and outcome and allow the researcher to relax the assumption of joint normality.
3000743	WOS:000357139500016	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Alan Acock's A Gentle Introduction to Stata, Fourth Edition	Journal	Review	2015	1	1	English	STATA JOURNAL	588-593	6	1	WOS:000357139500016	In this article, I review A Gentle Introduction to Stata, Fourth Edition, by Alan Acock (2014 [Stata Press]).
3017794	WOS:000357431900011	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Implementing Persistent O(1) Stacks and Queues in R	Journal	Article	2015	6	1	English	R JOURNAL	118-126	9	1	WOS:000357431900011	True to their functional roots, most R functions are side-effect-free, and users expect datatypes to be persistent. However, these semantics complicate the creation of efficient and dynamic data structures. Here, we describe the implementation of stack and queue data structures satisfying these conditions in R, available in the CRAN package rstackdeque. Guided by important work in purely functional languages, we look at both partially-and fully-persistent versions of queues, comparing their performance characteristics. Finally, we illustrate the usefulness of such dynamic structures with examples of generating and solving mazes.
3021683	WOS:000353664600012	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A robust test for weak instruments in Stata	Journal	Article	2015	1	1	English	STATA JOURNAL	216-225	10	1	WOS:000353664600012	We introduce a routine, weakivtest, that implements the test for weak instruments by Montiel Olea and Pflueger (2013, Journal of Business and Economic Statistics 31: 358-369). weakivtest allows for errors that are not conditionally homoskedastic and serially uncorrelated. It extends the Stock and Yogo (2005, Testing for weak instruments in linear IV regression. In Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg, ed. D. W. K. Andrews and J. J. Stock, 80-108. [Cambridge University Press]) weak-instrument tests available in ivreg2 and in the ivregress postestimation command estat firststage. weakivtest tests the null hypothesis that instruments are weak or that the estimator's Nagar (1959, Econometrica 27: 575-595) bias is large relative to a benchmark for both two-stage least-squares estimation and limited-information maximum likelihood with one endogenous regressor. The routine can accommodate Eicker-Huber-White heteroskedasticity robust estimates, Newey and West (1987, Econometrica 55: 703-708) heteroskedasticity-and autocorrelation-consistent estimates, and clustered variance estimates.
3031833	WOS:000365980200001	CX8UR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MLGA: A SAS Macro to Compute Maximum Likelihood Estimators via Genetic Algorithms	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	11	1	WOS:000365980200001	Nonlinear regression is usually implemented in SAS either by using PROC NLIN or PROC NLMIXED. Apart from the model structure, initial values need to be specified for each parameter. And after some convergence criteria are fulfilled, the second order conditions need to be analyzed. But numerical problems are expected to appear in case the likelihood is nearly discontinuous, has plateaus, multiple maxima, or the initial values are distant from the true parameter estimates. The usual solution consists of using a grid, and then choosing the set of parameters reporting the highest log-likelihood. However, if the amount of parameters or grid points is large, the computational burden will be excessive. Furthermore, there is no guarantee that, as the number of grid points increases, an equal or better set of points will be found. Genetic algorithms can overcome these problems by replicating how nature optimizes its processes. The MLGA macro is presented; it solves a maximum likelihood estimation problem under normality through PROC GA, and the resulting values can later be used as the starting values in SAS nonlinear procedures. As will be demonstrated, this macro can avoid the usual trial and error approach that is needed when convergence problems arise. Finally, it will be shown how this macro can deal with complicated restrictions involving multiple parameters.
3038640	WOS:000365975500001	CX8SW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	LazySorted: A Lazily, Partially Sorted Python List	Journal	Article	2015	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-10	10	1	WOS:000365975500001	LazySorted is a Python C extension implementing a partially and lazily sorted list data structure. It solves a common problem faced by programmers, in which they need just part of a sorted list, like its middle element (the median), but sort the entire list to get it. LazySorted presents them with the abstraction that they are working with a fully sorted list, while actually only sorting the list partially with quicksort partitions to return the requested sub-elements. This enables programmers to use naive "sort first" algorithms but nonetheless attain linear run-times when possible. LazySorted may serve as a drop-in replacement for the built-in sorted function in most cases, and can sometimes achieve run-times more than 7 times faster.
3040887	WOS:000353664600005	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Generating univariate and multivariate nonnormal data	Journal	Article	2015	1	1	English	STATA JOURNAL	95-109	15	1	WOS:000353664600005	Because the assumption of normality is common in statistics, the robustness of statistical procedures to the violation of the normality assumption is often of interest. When one examines the impact of the violation of the normality assumption, it is important to simulate data from a nonnormal distribution with varying degrees of skewness and kurtosis. Fleishman (1978, Psychometrika 43: 521-532) developed a method to simulate data from a univariate distribution with specific values for the skewness and kurtosis. Vale and Maurelli (1983, Psychometrika 48: 465-471) extended Fleishman's method to simulate data from a multivariate nonnormal distribution. In this article, I briefly introduce these two methods and present two new commands, rnonnormal and rmvnonnormal, for simulating data from the univariate and multivariate nonnormal distributions.
3042001	WOS:000365981400001	CX8VD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fitting Linear Mixed-Effects Models Using lme4	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-48	48	1	WOS:000365981400001	Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.
3051129	WOS:000365983900001	CX8WA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	sanon: An R Package for Stratified Analysis with Nonparametric Covariable Adjustment	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-37	37	1	WOS:000365983900001	Kawaguchi, Koch, and Wang (2011) provide methodology and applications for a stratified Mann-Whitney estimator that addresses the same comparison between two randomized groups for a strictly ordinal response variable as the van Elteren test statistic for randomized clinical trials with strata. The sanon package provides the implementation of the method within the R programming environment. The usage of sanon is illustrated with five examples. The first example is a randomized clinical trial with eight strata and a univariate ordinal response variable. The second example is a randomized clinical trial with four strata, two covariables, and four ordinal response variables. The third example is a crossover design randomized clinical trial with two strata, one covariable, and two ordinal response variables. The fourth example is a randomized clinical trial with seven strata (which are managed as a categorical covariable), three ordinal covariables with missing values, and three ordinal response variables with missing values. The fifth example is a randomized clinical trial with six strata, a categorical covariable with three levels, and three ordinal response variables with missing values.
3058163	WOS:000353664600010	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating net survival using a life-table approach	Journal	Article	2015	1	1	English	STATA JOURNAL	173-185	13	1	WOS:000353664600010	Cancer registries are often interested in estimating net survival (NS), the probability of survival if the cancer under study is the only possible cause of death. Pohar Perme, Stare, and Esteve (2012, Biometrics 68: 113-120) proposed a new estimator of NS based on inverse probability weighting. They demonstrated that existing estimators of NS based on relative survival were biased, whereas the new estimator was unbiased. The new estimator was developed for continuous survival times, yet cancer registries often have only discrete survival times (for example, survival time in completed months or years). Therefore, we propose an approach to estimation for when survival times are discrete. In this article, we describe the stnet command for life-table estimation of NS, adapting the Pohar Perme estimation approach to life-table estimation. Estimates can be made using a period or hybrid approach in addition to the traditional cohort (or complete) approach, and age-standardized survival estimates are available.
3067415	WOS:000357139500011	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	gpsbound: A command for importing and verifying geographical information from a user-provided shapefile	Journal	Article	2015	1	1	English	STATA JOURNAL	523-536	14	1	WOS:000357139500011	Geographical coordinates such as Global Positioning System (GPS) latitude and longitude estimates form the foundation of many spatial statistical methods. gpsbound allows users to 1) import geographical information from the attribute table of a polygon shapefile based on the identified location of GPS coordinates in a Stata dataset, and 2) check whether the GPS coordinates lie within the bounds of a polygon demarcated in the shapefile (for example, enumeration areas or primary sampling units). gpsbound also allows users to work with spatial data in Stata without the use of Geographical Information System software.
3068810	WOS:000365974200001	CX8SJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The VGAM Package for Capture-Recapture Data Using the Conditional Likelihood	Journal	Article	2015	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-33	33	1	WOS:000365974200001	It is well known that using individual covariate information (such as body weight or gender) to model heterogeneity in capture-recapture (CR) experiments can greatly enhance inferences on the size of a closed population. Since individual covariates are only observable for captured individuals, complex conditional likelihood methods are usually required and these do not constitute a standard generalized linear model (GLM) family. Modern statistical techniques such as generalized additive models (GAMs), which allow a relaxing of the linearity assumptions on the covariates, are readily available for many standard GLM families. Fortunately, a natural statistical framework for maximizing conditional likelihoods is available in the Vector GLM and Vector GAM classes of models. We present several new R functions (implemented within the VGAM package) specifically developed to allow the incorporation of individual covariates in the analysis of closed population CR data using a GLM/GAM-like approach and the conditional likelihood. As a result, a wide variety of practical tools are now readily available in the VGAM object oriented framework. We discuss and demonstrate their advantages, features and flexibility using the new VGAM CR functions on several examples.
3075348	WOS:000352916200001	CF9VV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	GPfit: An R Package for Fitting a Gaussian Process Model to Deterministic Simulator Outputs	Journal	Article	2015	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000352916200001	Gaussian process (GP) models are commonly used statistical metamodels for emulating expensive computer simulators. Fitting a GP model can be numerically unstable if any pair of design points in the input space are close together. Hanjan, Haynes, and Karsten (2011) proposed a computationally stable approach for fitting GP models to deterministic computer simulators. They used a genetic algorithm based approach that is robust but computationally intensive for maximizing the likelihood. This paper implements a slightly modified version of the model proposed by Ranjan el al. (2011) in the R package GPfit. A novel parameterization of the spatial correlation function and a clustering based multistart gradient based optimization algorithm yield robust optimization that is typically faster than the genetic algorithm based approach. We present two examples with R codes to illustrate the usage of the main functions in GPfit. Several test functions are used for performance comparison with the popular R package mlegp. We also use GPfit for a real application, i.e., for emulating the tidal kinetic energy model for the Bay of Fundy, Nova Scotia, Canada. GPfit is free software and distributed under the General Public License and available from the Comprehensive R Archive Network.
3094446	WOS:000357139500002	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A menu-driven facility for sample-size calculation in multiarm, multistage randomized controlled trials with time-to-event outcomes: Update	Journal	Article	2015	1	1	English	STATA JOURNAL	350-368	19	1	WOS:000357139500002	Barthel, Royston, and Parmar (2009, Stata Journal 9: 505-523) presented a menu-driven Stata program for calculating sample size and other design characteristics for a class of multiarm, multistage trials with time-to-event outcomes. In this article, we present several new features for the package. First, we allow hazard ratios greater than 1 to be targeted under the alternative hypothesis to make the design more widely applicable to other outcomes and disease areas. Second, we introduce new subroutines that use simulation to more accurately estimate the correlation between treatment effects at different stages and that calculate the familywise error rate, and we apply them to an example using the original design of the multiarm, multistage trial in prostate cancer. Finally,. we present a new design of the dialog menu for nstage that improves its usability and incorporates options for calling the new subroutines.
3117082	WOS:000357431900009	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	showtext: Using System Fonts in R Graphics	Journal	Article	2015	6	1	English	R JOURNAL	99-108	10	1	WOS:000357431900009	This article introduces the showtext package that makes it easy to use system fonts in R graphics. Unlike other methods to embed fonts into graphics, showtext converts text into raster images or polygons, and then adds them to the plot canvas. This method produces platform-independent image files that do not rely on the fonts that create them. It supports a large number of font formats and R graphics devices, and meanwhile provides convenient features such as using web fonts and integrating with knitr. This article provides an elaborate introduction to the showtext package, including its design, usage, and examples.
3129381	WOS:000365974700001	CX8SO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	DTR: An R Package for Estimation and Comparison of Survival Outcomes of Dynamic Treatment Regimes	Journal	Article	2015	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000365974700001	Sequentially randomized designs, more recently known as sequential multiple assignment randomized trial (SMART) designs, are widely used in biomedical research, particlularly in clinically trials, to assess and compare the effects of various treatment sequences. In such designs, patients are initially randomized to one of the first-stage therapies. Then patients meeting some criteria (e.g., no relapse of disease) participate in the second-stage randomized to one of the second-stage therapies. The advantage of such a design is that it allows the investiator to study various treatment sequences where the patients second-stage therapies can be adjusted based on their responses to the first-stage therapies. In the past few years, substantial improvement has been made in the statitical methods for analysing the data from SMARTs. Much of the proposed statitical approaches focus on estimating and comparing the survival outcomes of treatment sequences embedded in the SMART designs. In this article, we introduce the R package DTR, which provides a set of functions that can be used to estimate and compare the effects of different treatment sequences on survival outcomes using the newly proposed statistical approaches. The proposed package is also illustrated using simulated data from SMARTs.
3141561	WOS:000352911200001	CF9UB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	fitdistrplus: An R Package for Fitting Distributions	Journal	Article	2015	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000352911200001	The package fitdistrplus provides functions for fitting univariate distributions to different types of data (continuous censored or non-censored data and discrete data) and allowing different estimation methods (maximum likelihood, moment matching, quantile matching and maximum goodness-of-fit estimation). Outputs of fitdist and fitdistcens functions are S3 objects, for which kind generic methods are provided, including summary, plot and quantile. This package also provides various functions to compare the fit of several distributions to a same data set and can handle bootstrap of parameter estimates. Detailed examples are given in food risk assessment, ecotoxicology and insurance contexts.
3149503	WOS:000349846400001	CB7ZB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	geoCount: An R Package for the Analysis of Geostatistical Count Data	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-33	33	1	WOS:000349846400001	We describe the R package geo Count for the analysis of geostatistical count data. The package performs Bayesian analysis for the Poisson-lognormal and binomial- logitnormal spatial models, which are subclasses of the class of generalized linear spatial models proposed by Diggle, Tawn, and Moyeed (1998). The package implements the computational intensive tasks in C++ using an R/C++ interface, and has parallel computation capabilities to speed up the computations. geoCount also implements group updating, LangevinHastings algorithms and a data-based parameterization, algorithmic approaches proposed by Christensen, Roberts, and Skold (2006) to improve the efficiency of the Markov chain Monte Carlo algorithms. In addition, the package includes functions for simulation and visualization, as well as three geostatistical count datasets taken from the literature. One of those is used to illustrate the package capabilities. Finally, we provide a side-by-side comparison between geoCount and the R packages geoRglm and INLA
3151030	WOS:000353664600011	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating and modeling relative survival	Journal	Article	2015	1	1	English	STATA JOURNAL	186-215	30	1	WOS:000353664600011	When estimating patient survival using data collected by population-based cancer registries, it is common to estimate net survival in a relative-survival framework. Net survival can be estimated using the relative-survival ratio, which is the ratio of the observed survival of the patients (where all deaths are considered events) to the expected survival of a comparable group from the general population. In this article, we describe a command, strs, for life-table estimation of relative survival. We discuss three methods for estimating expected survival, as well as the cohort, period, and hybrid approaches for estimating relative survival. We also implement a life-table version of the Pohar Perme (2012, Biometrics 68: 113-120) estimator of net survival, and we describe two methods for age standardization. We also explain how, in addition to net probabilities of death, crude probabilities of death due to cancer and due to other causes can be estimated using the method of Cronin and Feuer (2000, Statistics in Medicine 19: 1729-1740). We conclude this article with discussion and examples of modeling excess mortality using various approaches, including the full-likelihood approach (using the ml command) and Poisson regression (using the gin command with a user-specified link function).
3152862	WOS:000352917500001	CF9WI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SDD: An R Package for Serial Dependence Diagrams	Journal	Article	2015	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000352917500001	Detecting and measuring lag-dependencies is very important in time-series analysis. This study is commonly carried out by focusing on the linear lag-dependencies via the well-known autocorrelogram. However, in practice, there are many situations in which the autocorrelogram fails because of the nonlinear structure of the serial dependence. To cope with this problem, in this paper the R package SDD is introduced. Among the available approaches to analyze the lag-dependencies in an omnibus way, the SDD package considers the autodependogram and some of its variants. The autodependogram, defined by computing the classical Pearson chi(2)-statistic at various lags, is a graphical device recently proposed in the literature to analyze lag-dependencies. The concept of reproducibility probability, and several density-based measures of divergence, are considered to define the variants of the autodependogram. An application to daily returns of the Swiss Market Index is also presented to exemplify the use of the package.
3155057	WOS:000349848100001	CB7ZR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Pitfalls in the Implementation of Bayesian Hierarchical Modeling of Areal Count Data: An Illustration Using BYM and Leroux Models	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000349848100001	Several different hierarchical Bayesian models can be used for the estimation of spatial risk patterns based on spatially aggregated count data. Typically, the resulting posterior distributions of the model parameters cannot be expressed in closed forms, and MCMC approaches are required for inference. However, implementations of hierarchical Bayesian models for such areal data are error-prone. Also, different implementation methods exist, and a surprisingly large variability may develop between the methods as well as between the different MCMC runs of one method. This paper has four main goals: (1) to present a point by point annotated code of two commonly used models for areal count data, namely the BYM and the Leroux models (2) to discuss technical variations in the implementation of a formula-driven sampler and to assess the variability in the posterior results from various alternative implementations (3) to give graphical tools to compare sample(r)s which complement existing convergence diagnostics and (4) to give various practical tips for implementing samplers.
3174669	WOS:000365980300001	CX8US	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MixMAP: An R Package for Mixed Modeling of Meta-Analysis p Values in Genetic Association Studies	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	11	1	WOS:000365980300001	Genetic association studies are commonly conducted to identify genes that explain the variability in a measured trait (e.g., disease status or disease progression). Often, results of these studies are summarized in the form of a p value corresponding to a test of association between each single nucleotide polymorphisms (SNPs) and the trait under study. As genes are comprised of multiple SNPs, post hoc approaches are generally applied to determine gene-level association. For example, if any SNP within a gene is significantly associated with the trait at a genome-wide significance level (p < 5 x 10-8), then the corresponding gene is considered significant. A complementary strategy, termed mixed modeling of meta-analysis p values (MixMAP) was proposed recently to characterize formally the associations between genes (or gene regions) and a trait based on multiple SNP-level p values. Here, the MixMAP package is presented as a means for implementing the MixMAP procedure in R.
3199319	WOS:000365977900001	CX8TU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SemiMarkov: An R Package for Parametric Estimation in Multi-State Semi-Markov Models	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000365977900001	Multi-state models provide a relevant tool for studying the observations of a continuous-time process at arbitrary times. Markov models are often considered even if semi-Markov are better adapted in various situations. Such models are still not frequently applied mainly due to lack of available software. We have developed the R package SemiMarkov to fit homogeneous semi-Markov models to longitudinal data. The package performs maximum likelihood estimation in a parametric framework where the distributions of the sojourn times can be chosen between exponential, Weibull or exponentiated Weibull. The package computes and displays the hazard rates of sojourn times and the hazard rates of the semi-Markov process. The effects of covariates can be studied with a Cox proportional hazards model for the sojourn times distributions. The number of covariates and the distribution of sojourn times can be specified for each possible transition providing a great flexibility in a model's definition. This article presents parametric semi-Markov models and gives a detailed description of the package together with an application to asthma control.
3226124	WOS:000365974400001	CX8SL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	frbs: Fuzzy Rule-Based Systems for Classification and Regression in R	Journal	Article	2015	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000365974400001	Fuzzy rule-based systems (FRBSs) are a well-known method family within soft computing. They are based on fuzzy concepts to address complex real-world problems. We present the R package frbs which implements the most widely used FRBS models, namely, Mamdani and Takagi Sugeno Kang (TSK) ones, as well as some common variants. In addition a host of learning methods for FRBSs, where the models are constructed from data, are implemented. In this way, accurate and interpretable systems can be built for data analysis and modeling tasks. In this paper, we also provide some examples on the usage of the package and a comparison with other common classification and regression methods available in R.
3236428	WOS:000357431900015	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Identifying Complex Causal Dependencies in Configurational Data with Coincidence Analysis	Journal	Article	2015	6	1	English	R JOURNAL	176-184	9	1	WOS:000357431900015	We present cna, a package for performing Coincidence Analysis (CNA). CNA is a configurational comparative method for the identification of complex causal dependencies-in particular, causal chains and common cause structures-in configurational data. After a brief introduction to the method's theoretical background and main algorithmic ideas, we demonstrate the use of the package by means of an artificial and a real-life data set. Moreover, we outline planned enhancements of the package that will further increase its applicability.
3257353	WOS:000357431900014	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	fslr: Connecting the FSL Software with R	Journal	Article	2015	6	1	English	R JOURNAL	163-175	13	1	WOS:000357431900014	We present the package fslr, a set of R functions that interface with FSL (FMRIB Software Library), a commonly-used open-source software package for processing and analyzing neuroimaging data. The fslr package performs operations on 'nifti' image objects in R using command-line functions from FSL, and returns R objects back to the user. fslr allows users to develop image processing and analysis pipelines based on FSL functionality while interfacing with the functionality provided by R. We present an example of the analysis of structural magnetic resonance images, which demonstrates how R users can leverage the functionality of FSL without switching to shell commands. [GRAPHICS]
3272982	WOS:000364359100005	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation of mean health care costs and incremental cost-effectiveness ratios with possibly censored data	Journal	Article	2015	1	1	English	Stata Journal	698-711	14	1	WOS:000364359100005	In this article, we describe the hcost program for estimating mean health care costs and incremental cost-effectiveness ratios with possibly censored data. hcost estimates the mean survival time and the mean costs, as well as their variances and covariance, for a given time horizon. If the group variable is specified, hcost will report the differences between two groups, as well as the incremental cost-effectiveness ratio and its confidence interval (optional). hcost can estimate the mean costs using two methods corresponding to different types of data: the Bang and Tsiatis (2000, Biometrika 87: 329-343) estimator using only the total costs or the Zhao and Tian (2001, Biometrics 57: 1002-1008) estimator when cost-history data are available. The hcost program can also be used to specify the annual discounting rates for survival time and costs.
3314345	WOS:000349845200001	CB7YQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	plotKML: Scientific Visualization of Spatio-Temporal Data	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000349845200001	plotKML is an R package that provides methods for writing the most common R spatial classes into KML files. It builds up on the existing XML parsing functionality (X M L package), and provides similar plotting functionality as the lattice package. Its main objective is to provide a simple interface to generate KML files with a small number of arguments, and allows users to visually explore spatio-temporal data available in R : points, polygons, gridded maps, trajectory-type data, vertical pro files, ground photographs, time series vector objects or raster images, along with the results of spatial analysis such as geostatistical mapping, spatial simulations of vector and gridded objects, optimized sampling designs, species distribution models and similar. A generic plotKML ( ) function automatically determines the parsing order and visualizes data directly from R; lower level functions can be combined to allow for new user-created visualization templates. In comparison to other packages writing KML, plotKML seems to be more object oriented, it links more closely to the existing R classes for spatio-temporal data (sp, spacetime and raster packages) than the alternatives, and provides users with the possibility to create their own templates.
3315253	WOS:000357139500006	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multiple imputation of covariates by substantive-model compatible fully conditional specification	Journal	Article	2015	1	1	English	STATA JOURNAL	437-456	20	1	WOS:000357139500006	Multiple imputation is a practical, principled approach to handling missing data. When used to impute missing values in covariates of regression models, imputation models may be misspecified if they are not compatible with the substantive model of interest for the outcome. In this article, we introduce the smcf cs command, which imputes covariates by substantive-model compatible fully conditional specification. This modifies the popular fully conditional specification or chained-equations approach to multiple imputation by imputing each covariate compatibly with a user-specified substantive model. We compare the smcf cs command with standard fully conditional specification imputation using mi impute chained in a simulation study and illustrative analysis of data from a study investigating time to tumor recurrence in breast cancer.
3328605	WOS:000357139500012	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A general-purpose nomogram generator for predictive logistic regression models	Journal	Article	2015	1	1	English	STATA JOURNAL	537-546	10	1	WOS:000357139500012	Multivariate logistic regression is a statistical method commonly used in several fields to build predictive models. A nomogram is a tool that provides graphical depictions of all variables in the model and enables the user to easily compute output probabilities. Our objective was to build a flexible and easy-to-use nomogram generator in Stata. The script works after arbitrary logit or logistic commands.
3342891	WOS:000365986400001	CX8WX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Tools to Characterize Point Patterns: dbmss for R	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	15	1	WOS:000365986400001	The dbmss package for R provides an easy-to-use toolbox to characterize the spatial structure of point patterns. Our contribution presents the state of the art of distance-based methods employed in economic geography and which are also used in ecology. Topographic functions such as Ripley's K, absolute functions such as Duranton and Overman's K-d and relative functions such as Marcon and Puech's M are implemented. Their confidence envelopes (including global ones) and tests against counterfactuals are included in the package.
3370083	WOS:000365976500001	CX8TG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	OptGS: An R Package for Finding Near-Optimal Group-Sequential Designs	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	13	1	WOS:000365976500001	A group-sequential clinical trial design is one in which interim analyses of the data are conducted after groups of patients are recruited. After each interim analysis, the trial may stop early if the evidence so far shows the new treatment is particularly effective or ineffective. Such designs are ethical and cost-effective, and so are of great interest in practice. An optimal group-sequential design is one which controls the type-I error rate and power at a specified level, but minimizes the expected sample size of the trial when the true treatment effect is equal to some specified value. Searching for an optimal group-sequential design is a significant computational challenge because of the high number of parameters. In this paper the R package OptGS is described. Package OptGS searches for near-optimal and balanced (i.e., one which balances more than one optimality criterion) group-sequential designs for randomized controlled trials with normally distributed outcomes. Package OptGS uses a two-parameter family of functions to determine the stopping boundaries, which improves the speed of the search process whilst still allowing flexibility in the possible shape of stopping boundaries. The resulting package allows optimal designs to be found in a matter of seconds - much faster than a previous approach.
3372848	WOS:000365974900001	CX8SQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	PCovR: An R Package for Principal Covariates Regression	Journal	Article	2015	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000365974900001	In this article, we present PCovR, an R package for performing principal covariates regression (PCovR; De Jong and Kiers 1992). PCovR was developed for analyzing regression data with many and/or highly collinear predictor variables. The method simultaneously reduces the predictor variables to a limited number of components and regresses the criterion variables on these components. The flexibility, interpretational advantages, and computational simplicity of PCovR make the method stand out between many other regression methods. The PCovR package offers data preprocessing options, new model selection procedures, and several component rotation strategies, some of which were not available in R up till now. The use and usefulness of the package is illustrated with a real dataset, called psychiatrists
3380807	WOS:000365981900001	CX8VI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Threshold Value Estimation Using Adaptive Two-Stage Plans in R	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	19	1	WOS:000365981900001	This paper introduces the R package twostageTE for estimation of an inverse regression function at a given point when one can sample an explanatory covariate at different values and measure the corresponding responses. The package implements a number of nonparametric methods for budget constrained threshold value estimation. Specifically, it contains methods for classical one-stage designs and also adaptive two-stage designs, which have been shown to yield more efficient and accurate results. A major advantage of the methods in package twostageTE is that threshold value estimation is performed without penalization or kernel smoothing, and hence, avoids the well-known problems of choosing the corresponding tuning parameter (regularization, bandwidth). The user can easily perform a two-stage analysis with twostageTE by (i) identifying the second stage sampling region from an initial sample, and (ii) computing various types of confidence intervals to ensure a robust analysis. The package twostageTE is illustrated through simulated examples.
3385089	WOS:000364359100002	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Prediction in linear index models with endogenous regressors	Journal	Article	2015	1	1	English	Stata Journal	627-644	18	1	WOS:000364359100002	In this article, we examine prediction in the context of linear index models when one or more of the regressors are endogenous. To facilitate both within-sample and out-of-sample predictions, Stata offers the postestimation command predict (see [R] predict). We believe that the usefulness of the predictions provided by this command is limited, especially if one is interested in out-of-sample predictions. We demonstrate our point using a probit model with continuous endogenous regressors, although it clearly generalizes readily to other linear index models. We subsequently provide a program that offers one possible implementation of a new command, ivpredict, that can be used to address this shortcoming of predict, and we then illustrate its use with an empirical example.
3432037	WOS:000365982100001	CX8VK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Statistical Disclosure Control for Micro-Data Using the R Package sdcMicro	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	36	1	WOS:000365982100001	The demand for data from surveys, censuses or registers containing sensible information on people or enterprises has increased significantly over the last years. However, before data can be provided to the public or to researchers, confidentiality has to be respected for any data set possibly containing sensible information about individual units. Confidentiality can be achieved by applying statistical disclosure control (SDC) methods to the data in order to decrease the disclosure risk of data. The R package sdcMicro serves as an easy-to-handle, object-oriented S4 class implementation of SDC methods to evaluate and anonymize confidential micro-data sets. It includes all popular disclosure risk and perturbation methods. The package performs automated recalculation of frequency counts, individual and global risk measures, information loss and data utility statistics after each anonymization step. All methods are highly optimized in terms of computational costs to be able to work with large data sets. Reporting facilities that summarize the anonymization process can also be easily used by practitioners. We describe the package and demonstrate its functionality with a complex household survey test data set that has been distributed by the International Household Survey Network.
3438449	WOS:000349847900001	CB7ZP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Structured Additive Regression Models: An R Interface to BayesX	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-46	46	1	WOS:000349847900001	Structured additive regression (STAR) models provide a flexible framework for modeling possible nonlinear e ff ects of covariates: They contain the well established frameworks of generalized linear models and generalized additive models as special cases but also allow a wider class of effects, e.g., for geographical or spatio-temporal data, allowing for speci fi cation of complex and realistic models. BayesX is standalone software package providing software for fi tting general class of STAR models. Based on a comprehensive open-source regression toolbox written in C++, BayesX uses Bayesian inference for estimating STAR models based on Markov chain Monte Carlo simulation techniques, a mixed model representation of STAR models, or stepwise regression techniques combining penalized least squares estimation with model selection. BayesX not only covers models for responses from univariate exponential families, but also models from less-standard regression situations such as models for multi-categorical responses with either ordered or unordered categories, continuous time survival data, or continuous time multi-state models. This paper presents a new fully interactive R interface to BayesX : the R package R2BayesX. With the new package, STAR models can be conveniently speci fi ed using R's formula language (with some extended terms), fitted using the BayesX binary, represented in R with objects of suitable classes, and fi nally printed/ summarized/ plotted. This makes BayesX much more accessible to users familiar with R and adds extensive graphics capabilities for visualizing fi tted STAR models. Furthermore, R2BayesX complements the already impressive capabilities for semiparametric regression in R by a comprehensive toolbox comprising in particular more complex response types and alternative inferential procedures such as simulation-based Bayesian inference.
3449673	WOS:000353664600008	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Frailty models and frailty-mixture models for recurrent event times	Journal	Article	2015	1	1	English	STATA JOURNAL	135-154	20	1	WOS:000353664600008	The analysis of recurrent event times faces three challenges: between-subject heterogeneity (frailty), within-subject event dependence, and the possibility of a cured fraction. Frailty can be handled by including a latent random-effects term in a Cox-type model. Event dependence may be considered as contributing to the intervention effect, or it may be considered as a source of nuisance, depending on the analysts' specific research questions. If it is seen as a nuisance, the analysis can stratify the recurrent event times according to event order. If it is seen as contributing to the intervention effect, stratification should not be used. Models with and without stratification for event order estimate two types of treatment effects. They are analogous to per-protocol analysis and intention-to-treat analysis, respectively. In the context of chronic disease treatment, we want to estimate whether there is a cured fraction; for infectious disease prevention, this is called a nonsusceptible fraction. In infectious disease prevention, we want to understand whether an intervention protects each of its recipients to some extent ("leaky" model) or whether it totally protects some recipients but offers no protection to the rest ("all-or-none" model). The truth may be a mixture of the two modes of protection. We describe a class of regression models that can handle all three issues in the analysis of recurrent event times. The model parameters are estimated by the expectation-maximization algorithm, and their variances are estimated by Louis's formula. We provide a new command, strmcure, for implementing these models.
3452758	WOS:000366014300001	CX9GZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian Model Averaging and Jointness Measures for gretl	Journal	Article	2015	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000366014300001	This paper presents a software package that implements Bayesian model averaging for gretl, the GNU regression, econometrics and time-series library. Bayesian model averaging is a model-building strategy that takes account of model uncertainty in conclusions about estimated parameters. It is an efficient tool for discovering the most probable models and obtaining estimates of their posterior characteristics. In recent years we have observed an increasing number of software packages devoted to Bayesian model averaging for different statistical and econometric software. In this paper, we propose the BMA package for gretl, which is an increasingly popular free, open-source software for econometric analysis with an easy-to-use graphical user interface. We introduce the BMA package for linear regression models with jointness measures proposed by Ley and Steel (2007) and Doppelhofer and Weeks (2009).
3454916	WOS:000352914600001	CF9VF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	PReMiuM: An R Package for Profile Regression Mixture Models Using Dirichlet Processes	Journal	Article	2015	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000352914600001	PReMiuM is a recently developed R package for Bayesian clustering using a Dirichlet process mixture model. This model is an alternative to regression models, non-parametrically linking a response vector to covariate data through cluster membership (Molitor, Papathomas, Jerrett, and Richardson 2010). The package allows binary, categorical, count and continuous response, as well as continuous and discrete covariates. Additionally, predictions may be made for the response, and missing values for the covariates are handled. Several samplers and label switching moves are implemented along with diagnostic tools to assess convergence. A number of R functions for post-processing of the output are also provided. In addition to fitting mixtures, it may additionally be of interest to determine which covariates actively drive the mixture components. This is implemented in the package as variable selection.
3455514	WOS:000366014100001	CX9GX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian Model Averaging Employing Fixed and Flexible Priors: The BMS Package for R	Journal	Article	2015	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-37	37	1	WOS:000366014100001	This article describes the BMS (Bayesian model sampling) package for R that implements Bayesian model averaging for linear regression models. The package excels in allowing for a variety of prior structures, among them the "binomial-beta" prior on the model space and the so-called "hyper-g" specifications for Zellner's g prior. Furthermore, the BMS package allows the user to specify her own model priors and offers a possibility of subjective inference by setting "prior inclusion probabilities" according to the researcher's beliefs. Furthermore, graphical analysis of results is provided by numerous built-in plot functions of posterior densities, predictive densities and graphical illustrations to compare results under different prior settings. Finally, the package provides full enumeration of the model space for small scale problems as well as two efficient MCMC (Markov chain Monte Carlo) samplers that sort through the model space when the number of potential covariates is large.
3462047	WOS:000365978800001	CX8UD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SSMMATLAB: A Set of MATLAB Programs for the Statistical Analysis of State Space Models	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-37	37	1	WOS:000365978800001	This article discusses and describes SSMMATLAB, a set of programs written by the author in MATLAB for the statistical analysis of state space models. The state space model considered is very general. It may have univariate or multivariate observations, time-varying system matrices, exogenous inputs, regression effects, incompletely specified initial conditions, such as those that arise with cointegrated VARMA models, and missing values. There are functions to put frequently used models, such as multiplicative VARMA models, VARMAX models in echelon form, cointegrated VARMA models, and univariate structural or ARIMA model-based unobserved components models, into state space form. There are also functions to implement the Hillmer-Tiao canonical decomposition and the smooth trend and cycle estimation proposed by Gomez (2001). Once the model is in state space form, other functions can be used for likelihood evaluation, model estimation, forecasting and smoothing. A set of examples is presented in the SSMMATLAB manual to illustrate the use of these functions.
3462727	WOS:000352910600001	CF9TV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fitting Heavy Tailed Distributions: The poweRlaw Package	Journal	Article	2015	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000352910600001	Over the last few years, the power law distribution has been used as the data generating mechanism in many disparate fields. However, at times the techniques used to fit the power law distribution have been inappropriate. This paper describes the poweRlaw R package, which makes fitting power laws and other heavy-tailed distributions straight forward. This package contains R functions for fitting, comparing and visualizing heavy tailed distributions. Overall, it provides a principled approach to power law fitting.
3467589	WOS:000349845600001	CB7YU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian Inference and Data Augmentation Schemes for Spatial, Spatiotemporal and Multivariate Log-Gaussian Cox Processes in R	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-48	48	1	WOS:000349845600001	Log-Gaussian Cox processes are an important class of models for spatial and spatiotemporal point-pattern data. Delivering robust Bayesian inference for this class of models presents a substantial challenge, since Markov chain Monte Carlo (MCMC) algorithms require careful tuning in order to work well. To address this issue, we describe recent advances in MCMC methods for these models and their implementation in the R package lgcp. Our suite of R functions provides an extensible framework for inferring covariate effects as well as the parameters of the latent field. We also present methods for Bayesian inference in two further classes of model based on the log-Gaussian Cox process. The first of these concerns the case where we wish to fit a point process model to data consisting of event-counts aggregated to a set of spatial regions: we demonstrate how this can be achieved using data-augmentation. The second concerns Bayesian inference for a class of marked-point processes specified via a multivariate log-Gaussian Cox process model. For both of these extensions, we give details of their implementation in R.
3479321	WOS:000364359100001	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	precombine: A command to examine n >= 2 datasets before combining	Journal	Article	2015	1	1	English	Stata Journal	607-626	20	1	WOS:000364359100001	In this article, I present a new command, precombine, that alerts the user to, and provides assurance concerning, some problems that can occur when multiple Stata datasets are merged and appended. It describes variables that are common to multiple datasets as well as variables that are unique to one dataset. Where value labels are attached to variables, it checks whether code sets are identical across datasets. Summary statistics for values of all variables can also be listed and left in memory along with the descriptions of each variable of each dataset.
3486107	WOS:000349844700001	CB7YL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Software for Spatial Statistics	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-8	8	1	WOS:000349844700001	We give an overview of the papers published in this special issue on spatial statistics, of the Journal of Statistical Software. 21 papers address issues covering visualization (micromaps, links to Google Maps or Google Earth), point pattern analysis, geostatistics, analysis of areal aggregated or lattice data, spatio-temporal statistics, Bayesian spatial statistics, and Laplace approximations. We also point to earlier publications in this journal on the same topic.
3496481	WOS:000364359100006	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Approximate Bayesian logistic regression via penalized likelihood by data augmentation	Journal	Article	2015	1	1	English	Stata Journal	712-736	25	1	WOS:000364359100006	We present a command, penlogit, for approximate Bayesian logistic regression using penalized likelihood estimation via data augmentation. This command automatically adds specific prior-data records to a dataset. These records are computed so that they generate a penalty function for the log likelihood of a logistic model, which equals (up to an additive constant) a set of independent log prior distributions on the model parameters. This command overcomes the necessity of relying on specialiZed software and statistical tools (such as Markov chain Monte Carlo) for fitting Bayesian models, and allows one to assess the information content of a prior in terms of the data that would be required to generate the prior as a likelihood function. The command produces data equivalent to normal and generalized log-F priors for the model parameters, providing flexible translation of background information into prior data, which allows calculation of approximate posterior medians and intervals from ordinary maximum likelihood programs. We illustrate the command through an example using data from an observational study of neonatal mortality.
3522228	WOS:000353664600006	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Bayesian optimal interval design for phase I oncology clinical trials	Journal	Article	2015	1	1	English	STATA JOURNAL	110-120	11	1	WOS:000353664600006	The Bayesian optimal interval (BOIN) design is a novel phase I trial design for finding the maximum tolerated dose (MTD). With the BOIN design, phase I trials are conducted as a sequence of decision-making steps for assigning an appropriate dose for each enrolled patient. The design optimizes the assignment of doses to patients by minimizing incorrect decisions of dose escalation or deescalation; that is, it decreases the chance of erroneously escalating or de-escalating the dose when the current dose is higher or lower than the MTD. This feature of the BOIN design strongly ensures adherence to ethical standards. The most prominent advantage of the BOIN design is that it simultaneously achieves design simplicity and superior performance in comparison with similar methods. The BOIN design can be implemented in a simple way that is similar to the 3 3 design, but it yields substantially better operating characteristics. Compared with the well-known continual reassessment method, the BOIN design yields average performance when selecting the MTD, but it has a substantially lower risk of assigning patients to subtherapeutic or overly toxic doses. In this article, we present a command (optinterval) for implementing the BOIN design in a phase I clinical trial setting.
3538231	WOS:000353664600001	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	twopm: Two-part models	Journal	Article	2015	1	1	English	STATA JOURNAL	3-20	18	1	WOS:000353664600001	In this article, we describe twopm, a command for fitting two-part models for mixed discrete-continuous outcomes. In the two-part model, a binary choice model is fit for the probability of observing a positive-versus-zero outcome. Then, conditional on a positive outcome, an appropriate regression model is fit for the positive outcome. The twopm command allows the user to leverage the capabilities of predict and margins to calculate predictions and marginal effects and their standard errors from the combined first- and second-part models.
3538904	WOS:000353664600002	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Implementing intersection bounds in Stata	Journal	Article	2015	1	1	English	STATA JOURNAL	21-44	24	1	WOS:000353664600002	We present the clrbound, clr2bound, clr3bound, and clrtest commands for estimation and inference on intersection bounds as developed by Chernozhukov, Lee, and Rosen (2013, Econometrics 81: 667-737). The intersection bounds framework encompasses situations where a population parameter of interest is partially identified by a collection of consistently estimable upper and lower bounds. The identified set for the parameter is the intersection of regions defined by this collection of bounds. More generally, the methodology can be applied to settings where an estimable function of a vector-valued parameter is bounded from above and below, as is the case when the identified set is characterized by conditional moment inequalities. The commands clrbound, clr2bound, and clr3bound provide bound estimates that can be used directly for estimation or to construct asymptotically valid confidence sets. clrtest performs an intersection bound test of the hypothesis that a collection of lower intersection bounds is no greater than zero. The command clrbound provides bound estimates for one-sided lower or upper intersection bounds on a parameter, while clr2bound and clr3bound provide two-sided bound estimates using both lower and upper intersection bounds. clr2bound uses Bonferroni's inequality to construct two-sided bounds that can be used to perform asymptotically valid inference on the identified set or the parameter of interest, whereas clr3bound provides a generally tighter confidence interval for the parameter by inverting the hypothesis test performed by clrt est. More broadly, inversion of this test can also be used to construct confidence sets based on conditional moment inequalities as described in Chernozhukov, Lee, and Rosen (2013). The commands include parametric, series, and local linear estimation procedures.
3571297	WOS:000349846500001	CB7ZC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Model-Based Geostatistics the Easy Way	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000349846500001	This paper briefly describes geostatistical models for Gaussian and non-Gaussian data and demonstrates the geostatsp and diease mapping packages for performing inference using these models. Making use of R's spatial data types, and raster objects in particular, makes spatial analyses using geostatistical models simple and convenient. Examples using real data are shown for Gaussian spatial data, binomially distributed spatial data, a log-Gaussian Cox process, and an area-level model for case counts.
3578567	WOS:000365978400001	CX8TZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Consistent and Clear Reporting of Results from Diverse Modeling Techniques: The A3 Method	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000365978400001	The measurement and reporting of model error is of basic importance when constructing models. Here, a general method and an R package, A 3, are presented to support the assessment and communication of the quality of a model fit along with metrics of variable importance. The presented method is accurate, robust, and adaptable to a wide range of predictive modeling algorithms. The method is described along with case studies and a usage guide. It is shown how the method can be used to obtain more accurate models for prediction and how this may simultaneously lead to altered inferences and conclusions about the impact of potential drivers within a system.
3590597	WOS:000353664600004	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Time-efficient algorithms for robust estimators of location, scale, symmetry, and tail heaviness	Journal	Article	2015	1	1	English	STATA JOURNAL	77-94	18	1	WOS:000353664600004	The analysis of the empirical distribution of univariate data often includes the computation of location, scale, skewness, and tail-heaviness measures, which are estimates of specific parameters of the underlying population distribution. Several measures are available, but they differ by Gaussian efficiency, robustness regarding outliers, and meaning in the case of asymmetric distributions. In this article, we briefly compare, for each type of parameter (location, scale, skewness, and tail heaviness), the "classical" estimator based on (centered) moments of the empirical distribution, an estimator based on specific quantiles of the distribution, and an estimator based on pairwise comparisons of the observations. This last one always performs better than the other estimators, particularly in terms of robustness, but it requires a heavy computation time of an order of n(2). Fortunately, as explained in Croux and Rousseeuw (1992, Computational Statistics 1: 411-428), the algorithm of Johnson and Mizoguchi (1978, SIAM Journal of Scientific Computing 7: 147-153) allows one to substantially reduce the computation time to an order of n log n and, hence, allows the use of robust estimators based on pairwise comparisons, even in very large datasets. This has motivated us to program this algorithm for Stata. In this article, we describe the algorithm and the associated commands. We also illustrate the computation of these robust estimators by involving them in a normality test of Jarque-Bera form (Jarque and Bera 1980, Economics Letters 6: 255-259; Brys, Hubert, and Struyf, 2008, Computational Statistics 23: 429-442) using real data.
3593293	WOS:000365986100001	CX8WV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	%lrasch_mml: A SAS Macro for Marginal Maximum Likelihood Estimation in Longitudinal Polytomous Rasch Models	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000365986100001	Item response theory models are often applied when a number items are used to measure a unidimensional latent variable. Originally proposed and used within educational research, they are also used when focus is on physical functioning or psychological wellbeing. Modern applications often need more general models, typically models for multidimensional latent variables or longitudinal models for repeated measurements. This paper describes a SAS macro that fits two-dimensional polytomous Rasch models using a specification of the model that is sufficiently flexible to accommodate longitudinal Rasch models. The macro estimates item parameters using marginal maximum likelihood estimation. A graphical presentation of item characteristic curves is included.
3597770	WOS:000349847400001	CB7ZK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-50	50	1	WOS:000349847400001	Spatial statistics is a growing discipline providing important analytical techniques in a wide range of disciplines in the natural and social sciences. In the R package GWmodel, we present techniques from a particular branch of spatial statistics, termed geographically weighted (GW) models. GW models suit situations when data are not described well by some global model, but where there are spatial regions where a suitably localized calibration provides a better description. The approach uses a moving window weighting technique, where localized models are found at target locations. Outputs are mapped to provide a useful exploratory tool into the nature of the data spatial heterogeneity. Currently, GWmodel includes functions for: GW summary statistics, GW principal components analysis, GW regression, and GW discriminant analysis; some of which are provided in basic and robust forms.
3619351	WOS:000364359100003	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Fitting fixed- and random-effects meta-analysis models using structural equation modeling with the sem and gsem commands	Journal	Article	2015	1	1	English	Stata Journal	645-671	27	1	WOS:000364359100003	In this article, we demonstrate how to fit fixed- and random-effects meta-analysis, meta-regression, and multivariate outcome meta-analysis models under the structural equation modeling framework using the sem and gsem commands. While all of these models can be fit using existing user-written commands, formulating the models in the structural equation modeling framework provides deeper insight into how they work. Further, the heterogeneity test for meta-regression and multivariate meta-analysis is readily available from this output.
3624715	WOS:000364359100012	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	didq: A command for treatment-effect estimation under alternative assumptions	Journal	Article	2015	1	1	English	Stata Journal	796-808	13	1	WOS:000364359100012	When several pretreatment periods are available, identification of the treatment effect in a difference-in-differences framework requires an assumption relating dynamics for controls and treated in absence of treatment. Mora and Reggio (2012, Working Paper 12-33, Universidad Carlos III de Madrid) define a family of alternative identifying assumptions and propose a model that, contrary to the usual econometric specifications, allows one to identify the treatment effect for any given assumption in the family. In this article, we introduce a command, didq, that implements the model presented in Mora and Reggio, reports the estimated effect under alternative assumptions, and performs tests for the equivalence of the estimates. We also explain how to use the command to obtain the standard difference-in-differences estimator with or without polynomial trends.
3653719	WOS:000365977700001	CX8TS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fitting Diffusion Item Response Theory Models for Responses and Response Times Using the R Package diffIRT	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000365977700001	In the psychometric literature, item response theory models have been proposed that explicitly take the decision process underlying the responses of subjects to psychometric test items into account. Application of these models is however hampered by the absence of general and flexible software to fit these models. In this paper, we present diffIRT, an R package that can be used to fit item response theory models that are based on a diffusion process. We discuss parameter estimation and model fit assessment, show the viability of the package in a simulation study, and illustrate the use of the package with two datasets pertaining to extraversion and mental rotation. In addition, we illustrate how the package can be used to fit the traditional diffusion model (as it has been originally developed in experimental psychology) to data.
3654616	WOS:000357139500005	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Bounding treatment effects: A command for the partial identification of the average treatment effect with endogenous and misreported treatment assignment	Journal	Article	2015	1	1	English	STATA JOURNAL	411-436	26	1	WOS:000357139500005	We present a new command, tebounds, that implements a variety of techniques to bound the average treatment effect of a binary treatment on a binary outcome in light of endogenous and misreported treatment assignment. To tighten the worst case bounds, the monotone treatment selection, monotone treatment response, and monotone instrumental-variable assumptions of Manski and Pepper (2000, Econometrica 68: 997-1010), Kreider and Pepper (2007, Journal of the American Statistical Association 102: 432-441), Kreider et al. (2012, Journal of the American Statistical Association 107: 958-975), and Gundersen, Kreider, and Pepper (2012, Journal of Econometrics 166: 79-91) may be imposed. Imbens-Manski confidence intervals are provided.
3668181	WOS:000364359100010	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Creating summary tables using the sumtable command	Journal	Article	2015	1	1	English	Stata Journal	775-783	9	1	WOS:000364359100010	In many fields of statistics, summary tables are used to describe characteristics within a study population and are often used to compare characteristics of two or more groups. The sumtable command can be used to quickly and easily produce such summary tables, allowing for different types of data to be summarized within one table. In this article, we provide examples and advice for its use.
3670177	WOS:000364359100004	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Record linkage using Stata: Preprocessing, linking, and reviewing utilities	Journal	Review	2015	1	1	English	Stata Journal	672-697	26	1	WOS:000364359100004	In this article, we describe Stata utilities that facilitate probabilistic record linkage-the technique typically used for merging two datasets with no common record identifier. While the preprocessing tools are developed specifically for linking two company databases, the other tools can be used for many different types of linkage. Specifically, the stnd_compname and stnd_address commands parse and standardize company names and addresses to improve the match quality when linking. The reclink2 command is a generalized version of Blasnik's reclink (2010, Statistical Software Components S456876, Department of Economics, Boston College) that allows for many-to-one matching. Finally, clrevmatch is an interactive tool that allows the user to review matched results in an efficient and seamless manner. Rather than exporting results to another file format (for example, Excel), inputting clerical reviews, and importing back into Stata, one can use the clrevmatch tool to conduct all of these steps within Stata. This helps improve the speed and flexibility of matching, which often involves multiple runs.
3679993	WOS:000349846700001	CB7ZE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	spBayes for Large Univariate and Multivariate Point-Referenced Spatio-Temporal Data Models	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000349846700001	In this paper we detail the reformulation and rewrite of core functions in the spBayes R package. These efforts have focused on improving computational efficiency, flexibility, and usability for point-referenced data models. Attention is given to algorithm and computing developments that result in improved sampler convergence rate and efficiency by reducing parameter space; decreased sampler run-time by avoiding expensive matrix computations, and; increased scalability to large datasets by implementing a class of predictive process models that attempt to overcome computational hurdles by representing spatial processes in terms of lower-dimensional realizations. Beyond these general computational improvements for existing model functions, we detail new functions for modeling data indexed in both space and time. These new functions implement a class of dynamic spatio-temporal models for settings where space is viewed as continuous and time is taken as discrete.
3687265	WOS:000357139500014	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating almost-ideal demand systems with endogenous regressors	Journal	Article	2015	1	1	English	STATA JOURNAL	554-573	20	1	WOS:000357139500014	In this article, we present the new aidsills command for estimating almost-ideal demand systems and their quadratic extensions. In contrast with Poi's (2012, Stata Journal 12: 433-446) quaids command, which is based on the nonlinear nlsur command, aidsills uses the computationally attractive iterated linear least-squares estimator developed by Blundell and Robin (1999, Journal of Applied Econometrics 14: 209-232). The new command further allows one to account for endogenous prices and total expenditure by using instrumental-variable techniques. Elasticities and their standard errors can be obtained using the aidsills_elas postestimation command.
3689212	WOS:000352914800001	CF9VH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	R Package multgee: A Generalized Estimating Equations Solver for Multinomial Response	Journal	Article	2015	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000352914800001	The R package multgee implements the local odds ratios generalized estimating equations (GEE) approach proposed by Touloumis et al. (2013), a GEE approach for correlated multinomial responses that circumvents theoretical and practical limitations of the GEE method. A main strength of multgee is that it provides GEE routines for both ordinal (ordLORgee) and nominal (nomLORgee) responses, while relevant softwares in R and SAS are restricted to ordinal responses under a marginal cumulative link model specification. In addition, multgee offers a marginal adjacent categories logit model for ordinal responses and a marginal baseline category logit model for nominal. Further, utility functions are available to ease the local odds ratios structure selection (intrinsic.pars) and to perform a Wald type goodness-of-fit test between two nested GEE models (waldts). We demonstrate the application of multgee through a clinical trial with clustered ordinal multinomial responses.
3718785	WOS:000353664600016	CG9UC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Nonparametric pairwise multiple comparisons in independent groups using Dunn's test	Journal	Article	2015	1	1	English	STATA JOURNAL	292-300	9	1	WOS:000353664600016	Dunn's test is the appropriate nonparametric pairwise multiple-comparison procedure when a Kruskal-Wallis test is rejected, and it is now implemented for Stata in the dunntest command. dunntest produces multiple comparisons following a Kruskal-Wallis k-way test by using Stata's built-in kwallis command. It includes options to control the familywise error rate by using Dunn's proposed Bonferroni adjustment, the Sidak adjustment, the Holm stepwise adjustment, or the Holm-Sidak stepwise adjustment. There is also an option to control the false discovery rate using the Benjamini-Hochberg stepwise adjustment.
3731120	WOS:000364359100014	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Tests for normality in linear panel-data models	Journal	Article	2015	1	1	English	Stata Journal	822-832	11	1	WOS:000364359100014	We propose a new command, xtsktest, for explaining nonnormalities in linear panel-data models. The command performs tests to explore skewness and excess kurtosis, allowing researchers to identify departures from Gaussianity in both error components of a standard panel regression, separately or jointly. The tests are based on recent results by Galvao et al. (2013, Journal of Multivariate Analysis 122: 35-52) and extend the classical Jarque-Bera normality test for the case of panel data.
3755312	WOS:000364359100018	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Feasible fitting of linear models with N fixed effects	Journal	Article	2015	1	1	English	Stata Journal	881-898	18	1	WOS:000364359100018	In this article, I describe an alternative approach for fitting linear models with multiple high-order fixed effects. The strategy relies on transforming the data before fitting the model. While the approach is computationally intensive, the hardware requirements for the fitting are minimal, allowing for estimation in models with multiple high-order fixed effects for large datasets. I illustrate implementing this approach using the U.S. Census Bureau Current Population Survey data with four fixed effects. I also present a new Stata command, regxfe, for implementing this strategy.
3756760	WOS:000357139500003	CL7GC	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Two-stage individual participant data meta-analysis and generalized forest plots	Journal	Article	2015	1	1	English	STATA JOURNAL	369-396	28	1	WOS:000357139500003	In this article, I describe a command, ipdmetan, that facilitates two-stage individual participant data meta-analysis of any measure of effect and its standard error by fitting a specified model to data from each study.. The command can estimate random effects and heterogeneity statistics and include additional covariates and interactions. If individual participant data are available for certain studies and aggregate data for others, ipdmetan allows them to be combined in one analysis. This command can produce detailed and flexible forest plots, including ones outside the context of formal meta-analysis.
3761253	WOS:000365975000001	CX8SR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Mann-Whitney Type Tests for Microarray Experiments: The R Package gMWT	Journal	Article	2015	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000365975000001	We present the R package gMWT which is designed for the comparison of several treatments (or groups) for a large number of variables. The comparisons are made using certain probabilistic indices (PI). The PIs computed here tell how often pairs or triples of observations coming from different groups appear in a specific order of magnitude. Classical two and several sample rank test statistics such as the Mann-Whitney-Wilcoxon, Kruskal-Wllis, of Jonckheere-Terpstra test statistics are simple functions of these PI. Also new test statistics for directional alternatives are provided. The package gMWT can be used to calculate the variable-wise PI estimates, to illustrate their multivariate distribution and mutual dependence with joint scatterplot matrices, and to construct several classical and new rank tests based on the PIs. The aim of the paper is first to briefly explain the theory that is necessary to understand the behavior of the estimated PIs and the rank tests based on them. Second, the use of the package is described and illustrated with simulated and real data examples. It is stressed that the package provides a new flexible toolbox to analyze large gene or microRNA expression data sets, collected on microarrays or by other high-throughput technologies. The testing procedures can be used in an eQTL analysis, for example, as implemented in the package GeneticTools.
3769417	WOS:000349844900001	CB7YN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	micromapST: Exploring and Communicating Geospatial Patterns in US State Data	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000349844900001	The linked micromap graphical design uses color to link each geograhic unit's name with its statistical graphic elements and map location across columns in a single row. Perceptual grouping of these rows into smaller chunks of data facilitates local focus and visual queries. Sorting the geographic units (the rows) in different ways can reveal patterns in the statistics. in the maps, and in the association between them. This design supports both exploration and communication in a multivariate geospatial context. This paper describes micromapST, an R package that implements the linked micromap graphical design specificially formatted for US state data, a common geographic unit used to display geographic patterns of health and other factors within the US. This package creates a graphic for the 51 geographic units (50 states plus DC) that fits on a single page, with states comprising the rows and state names, graphs and maps the columns. The graphical element for each state/column combination may represent a single statistical value, e.g., by a dot or horizontal bar, with or without an uncertanity measure. The distibution of values within each state, e.g., for counties, may be displayed by a boxplot. Two values per state may be represented by an arrow indicating the change in values, e.g., between two time points, or a scatter plot of the paired data. Categorical counts may be displayed as horizontal stacked bars, with optional standardization to percents or centering of the bars. Layout options include specification of the sort order for the rows, the graph/map linking colors, a vertical refernce line and others. Output may be directed to the screen but is best displayed on a pre-defined linked micromap layout specifically for the 51 US states with graphical displays of single values, data distributions, change between two values, scatter plots of paired values, time series data catagorical data, facilitates quick exploration and communication of US state data for most common data types.
3778107	WOS:000365978600001	CX8UB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The R Package threg to Implement Threshold Regression Models	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000365978600001	This paper introduces the R package threg, which implements the estimation procedure of a threshold regression model, which is based on the first-hitting-time of a boundary by the sample path of a Wiener diffusion process. The threshold regression methodology is well suited to applications involving survival and time-to-event data, and serves as an important alternative to the Cox proportional hazards model. This new package includes four functions: threg, and the methods hr, predict and plot for `threg' objects returned by threg. The threg function is the model-fitting function which is used to calculate regression coefficient estimates, asymptotic standard errors and p values. The hr method for `threg' objects is the hazard-ratio calculation function which provides the estimates of hazard ratios at selected time points for specified scenarios (based on given categories or value settings of covariates). The predict method for `threg objects is used for prediction. And the plot method for `threg' objects provides plots for curves of estimated hazard functions, survival functions and probability density functions of the first-hitting-time; function curves corresponding to different scenarios can be overlaid in the same plot for comparison to give additional research insights.
3780213	WOS:000364359100016	CV6CU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	mqtime: A Stata tool for calculating travel time and distance using MapQuest web services	Journal	Article	2015	1	1	English	Stata Journal	845-853	9	1	WOS:000364359100016	In this article, I describe mqtime, a new Stata library that provides functionality to perform a variety of mapping tasks, including calculating travel time, distance (driving, biking, or walking), and estimated fuel use. ragtime uses an overlooked free and open-source mapping service provided by Map Quest. This service has significantly more attractive terms of use than widely used alternatives (for example, Google Maps), which limit use to a few thousand queries per day. Hence, mqtime makes analysis with even very large datasets practical. I also provide a convenient function for geocoding character addresses to geographic coordinates
3788396	WOS:000352911000001	CF9TZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Exploring Diallelic Genetic Markers: The HardyWeinberg Package	Journal	Article	2015	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000352911000001	Testing genetic markers for Hardy-Weinberg equilibrium is an important issue in genetic association studies. The HardyWeinberg package offers the classical tests for equilibrium, functions for power computation and for the simulation of marker data under equilibrium and disequilibrium. Functions for testing equilibrium in the presence of missing data by using multiple imputation are provided. The package also supplies various graphical tools such as ternary plots with acceptance regions, log-ratio plots and Q-Q plots for exploring the equilibrium status of a large set of diallelic markers. Classical tests for equilibrium and graphical representations for diallelic marker data are reviewed. Several data sets illustrate the use of the package.
3788669	WOS:000365983300001	CX8VV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	entropart: An R Package to Measure and Partition Diversity	Journal	Article	2015	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000365983300001	entropart is a package for R designed to estimate diversity based on HCDT entropy or similarity-based entropy. It allows calculating species-neutral, phylogenetic and functional entropy and diversity, partitioning them and correcting them for estimation bias.
3799729	WOS:000357431900012	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	R as an Environment for Reproducible Analysis of DNA Amplification Experiments	Journal	Article	2015	6	1	English	R JOURNAL	127-150	24	1	WOS:000357431900012	There is an ever-increasing number of applications, which use quantitative PCR (qPCR) or digital PCR (dPCR) to elicit fundamentals of biological processes. Moreover, quantitative isothermal amplification (qIA) methods have become more prominent in life sciences and point-of-care-diagnostics. Additionally, the analysis of melting data is essential during many experiments. Several software packages have been developed for the analysis of such datasets. In most cases, the software is either distributed as closed source software or as monolithic block with little freedom to perform highly customized analysis procedures. We argue, among others, that R is an excellent foundation for reproducible and transparent data analysis in a highly customizable cross-platform environment. However, for novices it is often challenging to master R or learn capabilities of the vast number of packages available. In the paper, we describe exemplary workflows for the analysis of qPCR, qIA or dPCR experiments including the analysis of melting curve data. Our analysis relies entirely on R packages available from public repositories. Additionally, we provide information related to standardized and reproducible research.
3804741	WOS:000357431900006	CM1HM	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Frames2: A Package for Estimation in Dual Frame Surveys	Journal	Article	2015	6	1	English	R JOURNAL	52-72	21	1	WOS:000357431900006	Data from complex survey designs require special consideration with regard to estimation of finite population parameters and corresponding variance estimation procedures, as a consequence of significant departures from the simple random sampling assumption. In the past decade a number of statistical software packages have been developed to facilitate the analysis of complex survey data. All these statistical software packages are able to treat samples selected from one sampling frame containing all population units. Dual frame surveys are very useful when it is not possible to guarantee a complete coverage of the target population and may result in considerable cost savings over a single frame design with comparable precision. There are several estimators available in the statistical literature but no existing software covers dual frame estimation procedures. This gap is now filled by package Frames2. In this paper we highlight the main features of the package. The package includes the main estimators in dual frame surveys and also provides interval confidence estimation.
3828262	WOS:000349845800001	CB7YW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Analysis of Random Fields Using CompRandFld	Journal	Article	2015	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000349845800001	Statistical analysis based on random fields has become a widely used approach in order to better understand real processes in many fields such as engineering, environmental sciences, etc. Data analysis based on random fields can be sometimes problematic to carry out from the inferential prospective. Examples are when dealing with: large dataset, counts or binary responses and extreme values data. This article explains how to perform, with the R package CompRandFld, challenging statistical analysis based on Gaussian, binary and max-stable random fields. The software provides tools for performing the statistical inference based on the composite likelihood in complex problems where standard likelihood methods are difficult to apply. The principal features are illustrated by means of simulation examples and an application of Irish daily wind speeds.
3829054	WOS:000366014000001	CX9GW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Parallel Sequential Monte Carlo for Efficient Density Combination: The DeCo MATLAB Toolbox	Journal	Article	2015	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000366014000001	This paper presents the MATLAB package DeCo (density combination) which is based on the paper by Billio, Casarin, Ravazzolo, and van Dijk (2013) where a constructive Bayesian approach is presented for combining predictive densities originating from different models or other sources of information. The combination weights are time-varying and may depend on past predictive forecasting performances and other learning mechanisms. The core algorithm is the function DeCo which applies banks of parallel sequential Monte Carlo algorithms to filter the time-varying combination weights. The DeCo procedure has been implemented both for standard CPU computing and for graphical process unit (GPU) parallel computing. For the GPU implementation we use the MATLAB parallel computing toolbox and show how to use general purpose GPU computing almost effortlessly. This GPU implementation provides a speed-up of the execution time of up to seventy times on a standard CPU MATLAB implementation on a multicore CPU. We show the use of the package and the computational gain of the GPU version through some simulation experiments and empirical applications.
3841290	WOS:000365977800001	CX8TT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Toolbox for Nonlinear Regression in R: The Package nlstools	Journal	Article	2015	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000365977800001	Nonlinear regression models are applied in a broad variety of scientific fields. Various R functions are already dedicated to fitting such models, among which the function nls () has a prominent position. Unlike linear regression fitting of nonlinear models relies on non-trivial assumptions and therefore users are required to carefully ensure and validate the entire modeling. Parameter estimation is carried out using some variant of the leastsquares criterion involving an iterative process that ideally leads to the determination of the optimal parameter estimates. Therefore, users need to have a clear understanding of the model and its parameterization in the context of the application and data considered, an a priori idea about plausible values for parameter estimates, knowledge of model diagnostics procedures available for checking crucial assumptions, and, finally, an understanding of the limitations in the validity of the underlying hypotheses of the fitted model and its implication for the precision of parameter estimates. Current nonlinear regression modules lack dedicated diagnostic functionality. So there is a need to provide users with an extended toolbox of functions enabling a careful evaluation of nonlinear regression fits. To this end, we introduce a unified diagnostic framework with the R package nls tools. In this paper, the various features of the package are presented and exempli fied using a worked example from pulmonary medicine.
3882928	WOS:000341808100001	AP1EB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	deltaPlotR: An R Package for Differential Item Functioning Analysis with Angoff's Delta Plot	Journal	Article	2014	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000341808100001	Angoff's delta plot is a straightforward and not computationally intensive method to identify differential item functioning (DIF) among dichotomously scored items. This approach was recently improved by proposing an optimal threshold selection and by considering several item purification processes. Moreover, to support practical DIF analyses with the delta plot and these improvements, the R package deltaPlotR was also developed. The purpose of this paper is twofold: to outline the delta plot by describing the original method and its recent improvements in a user-oriented way, and to illustrate the structure and performances of the deltaPlotR package. A real data set about language skill assessment is being analyzed as an illustrative example.
3895126	WOS:000349844000001	CB7YF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	vSMC: Parallel Sequenced Monte Carlo in C plus	Journal	Article	2014	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-49	49	1	WOS:000349844000001	Sequential Monte Carlo is a family of algorithms for sampling from a sequence of distributions. Some of these algorithms, such as particle filters, are widely used in the physics and signal processing researches. More recent developments have established their application in more general inference problems such as Bayesian modeling. These algorithms have attracted considerable attentions in recent years as they admit natural and scalable parallelizations. However, these algorithms are perceived to be difficult to implement. In addition, parallel programming is often unfamiliar to many researchers though conceptually appealing, especially for sequential Monte Carlo related fields. A C++ template library is presented for the purpose of implementing general sequential Monte Carlo algorithms on parallel hardware. Two examples are presented: a simple particle filter and a classic Bayesian modeling problem.
3939453	WOS:000341793000001	AP0YQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	General Purpose Convolution Algorithm in S4 Classes by Means of FFT	Journal	Article	2014	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	25	1	WOS:000341793000001	Object orientation provides a flexible framework for the implementation of the convolution of arbitrary distributions of real-valued random variables. We discuss an algorithm which is based on the fast Fourier transform. It directly applies to lattice-supported distributions. In the case of continuous distributions an additional discretization to a linear lattice is necessary and the resulting lattice-supported distributions are suitably smoothed after convolution. We compare our algorithm to other approaches aiming at a similar generality as to accuracy and speed. In situations where the exact results are known, several checks confirm a high accuracy of the proposed algorithm which is also illustrated for approximations of non-central chi(2) distributions. By means of object orientation this default algorithm is overloaded by more specific algorithms where possible, in particular where explicit convolution formulae are available. Our focus is on R package distr which implements this approach, overloading operator + for convolution; based on this convolution, we define a whole arithmetics of mathematical operations acting on distribution objects, comprising operators +, -, *, /, and <^>.
3947063	WOS:000348651700013	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	MVN: An R Package for Assessing Multivariate Normality	Journal	Article	2014	12	1	English	R JOURNAL	151-162	12	1	WOS:000348651700013	Assessing the assumption of multivariate normality is required by many parametric multivariate statistical methods, such as MANOVA, linear discriminant analysis, principal component analysis, canonical correlation, etc. It is important to assess multivariate normality in order to proceed with such statistical methods. There are many analytical methods proposed for checking multivariate normality. However, deciding which method to use is a challenging process, since each method may give different results under certain conditions. Hence, we may say that there is no best method, which is valid under any condition, for normality checking. In addition to numerical results, it is very useful to use graphical methods to decide on multivariate normality. Combining the numerical results from several methods with graphical approaches can be useful and provide more reliable decisions. Here, we present an R package, MVN, to assess multivariate normality. It contains the three most widely used multivariate normality tests, including Mardia's, Henze-Zirkler's and Royston's, and graphical approaches, including chi-square Q-Q, perspective and contour plots. It also includes two multivariate outlier detection methods, which are based on robust Mahalanobis distances. Moreover, this package offers functions to check the univariate normality of marginal distributions through both tests and plots. Furthermore, especially for non-R users, we provide a user-friendly web application of the package. This application is available at http://www.biosoft.hacettepe.edu.tr/MVN/.
3955850	WOS:000349843200001	CB7XX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Learning Continuous Time Bayesian Network Classifiers Using MapReduce	Journal	Article	2014	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000349843200001	Parameter and structural learning on continuous time Bayesian network classifiers are challenging tasks when you are dealing with big data. This paper describes an efficient scalable parallel algorithm for parameter and structural learning in the case of complete data using the MapReduce framework. Two popular instances of classifiers are analyzed, namely the continuous time naive Bayes and the continuous time tree augmented naive Bayes. Details of the proposed algorithm are presented using Hadoop, an open-source implementation of a distributed file system and the MapReduce framework for distributed data processing. Performance evaluation of the designed algorithm shows a robust parallel scaling.
3967294	WOS:000342606300011	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	The bmte command: Methods for the estimation of treatment effects when exclusion restrictions are unavailable	Journal	Article	2014	1	1	English	STATA JOURNAL	670-683	14	1	WOS:000342606300011	We present a new Stata command, bate (bias-minimizing treatment effects), that implements two new estimators proposed in Millimet and Tchernis (2013, Journal of Applied Econometrics 28: 982-1017) and designed to estimate the effect of treatment when selection on unobserved variables exists and appropriate exclusion restrictions are unavailable. In addition, the bate command estimates treatment effects from several alternative estimators that also do not rely on exclusion restrictions for identification of the causal effects of the treatment, including the following: 1) Heckman's two-step estimator (1976, Annals of Economic and Social Measurement 5: 475-492; 1979, Econometrica 47: 153-161); 2) a control function approach outlined in Heckman, LaLonde, and Smith (1999, Handbook of Labor Economics 3: 1865-2097) and Navarro (2008, The New Palgrave Dictionary of Economics [Palgrave Macmillan]); and 3) a more recent estimator proposed by Klein and Vella (2009, Journal of Applied Econometrics 24: 735-762) that exploits heteroskedasticity for identification. By implementing two new estimators alongside preexisting estimators, the bate command provides a picture of the average causal effects of the treatment across a variety of assumptions. We present an example application of the command following Millimet and Tchernis (2013, Journal of Applied Econometrics 28: 982-1017).
4014622	WOS:000341806300001	AP1DJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	phtt: Panel Data Analysis with Heterogeneous Time Trends in R	Journal	Article	2014	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-33	33	1	WOS:000341806300001	The R package phtt provides estimation procedures for panel data with large dimensions n, T, and general forms of unobservable heterogeneous effects. Particularly, the estimation procedures are those of Bai (2009) and Kneip, Sickles, and Song (2012), which complement one another very well: both models assume the unobservable heterogeneous effects to have a factor structure. Kneip et al. (2012) considers the case in which the time-varying common factors have relatively smooth patterns including strongly positively auto-correlated stationary as well as non-stationary factors, whereas the method of Bai (2009) focuses on stochastic bounded factors such as ARMA processes. Additionally, the phtt package provides a wide range of dimensionality criteria in order to estimate the number of the unobserved factors simultaneously with the remaining model parameters.
4023614	WOS:000348651700003	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Automatic Conversion of Tables to LongForm Dataframes	Journal	Article	2014	12	1	English	R JOURNAL	16-26	11	1	WOS:000348651700003	TableToLongForm automatically converts hierarchical Tables intended for a human reader into a simple LongForm dataframe that is machine readable, making it easier to access and use the data for analysis. It does this by recognising positional cues present in the hierarchical Table (which would normally be interpreted visually by the human brain) to decompose, then reconstruct the data into a LongForm dataframe. The article motivates the benefit of such a conversion with an example Table, followed by a short user manual, which includes a comparison between the simple one argument call to TableToLongForm, with code for an equivalent manual conversion. The article then explores the types of Tables the package can convert by providing a gallery of all recognised patterns. It finishes with a discussion of available diagnostic methods and future work.
4026318	WOS:000343788100003	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	A Multiscale Test of Spatial Stationarity for Textured Images in R	Journal	Article	2014	6	1	English	R JOURNAL	20-30	11	1	WOS:000343788100003	The ability to automatically identify areas of homogeneous texture present within a greyscale image is an important feature of image processing algorithms. This article describes the R package LS2Wstat which employs a recent wavelet-based test of stationarity for locally stationary random fields to assess such spatial homogeneity. By embedding this test within a quadtree image segmentation procedure we are also able to identify texture regions within an image.
4034342	WOS:000334020400001	AE5IG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Design of Diverging Stacked Bar Charts for Likert Scales and Other Applications	Journal	Article	2014	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	32	1	WOS:000334020400001	Rating scales,such as Likert scales, are very common in marketing research, customer satisfaction studies, psychometrics, opinion surveys, population studies, and numerous other fields. We recommend diverging stacked bar charts as the primary graphical display technique for Likert and related scales. We also show other applications where diverging stacked bar charts are useful. Many examples of plots of Likert scales are given. We discuss the perceptual and programming issues in constructing these graphs. We present two implementations for diverging stacked bar charts. Most examples in this paper were drawn with the likert function included in the HH package in R. We also have a dashboard in Tableau.
4056934	WOS:000332109200001	AB9JP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MissMech: An R Package for Testing Homoscedasticity, Multivariate Normality, and Missing Completely at Random (MCAR)	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000332109200001	Researchers are often faced with analyzing data sets that are not complete. To properly analyze such data sets requires the knowledge of the missing data mechanism. If data are missing completely at random (MCAR), then many missing data analysis techniques lead to valid inference. Thus, tests of MCAR are desirable. The package MissMech implements two tests developed by Jamshidian and Jalal (2010) for this purpose. These tests can be run using a function called TestMCARNormality. One of the tests is valid if data are normally distributed, and another test does not require any distributional assumptions for the data. In addition to testing MCAR, in some special cases, the function TestMCARNormality is also able to test whether data have a multivariate normal distribution. As a bonus, the functions in MissMech can also be used for the following additional tasks: (i) test of homoscedasticity for several groups when data are completely observed, (ii) perform the k-sample test of Anderson-Darling to determine whether k groups of univariate data come from the same distribution, (iii) impute incomplete data sets using two methods, one where normality is assumed and one where no specific distributional assumptions are made, (iv) obtain normal-theory maximum likelihood estimates for mean and covariance matrix when data are incomplete, along with their standard errors, and finally (v) perform the Neyman's test of uniformity. All of these features are explained in the paper, including examples.
4073295	WOS:000348651700011	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	SMR: An R Package for Computing the Externally Studentized Normal Midrange Distribution	Journal	Article	2014	12	1	English	R JOURNAL	123-136	14	1	WOS:000348651700011	The main purpose of this paper is to present the main algorithms underlining the construction and implementation of the SMR package, whose aim is to compute studentized normal midrange distribution. Details on the externally studentized normal midrange and standardized normal midrange distributions are also given. The package follows the same structure as the probability functions implemented in R. That is: the probability density function (dSMR), the cumulative distribution function (pSMR), the quantile function (qSMR) and the random number generating function (rSMR). Pseudocode and illustrative examples of how to use the package are presented.
4084332	WOS:000342606300013	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Stata and Dropbox	Journal	Article	2014	1	1	English	STATA JOURNAL	693-696	4	1	WOS:000342606300013	Dropbox makes scholarly collaboration much easier because it allows scholars to share files across different computers. However, because the Dropbox directories have different pathnames for different users, sharing do-files can be complicated. In this article, I offer some tips on how to navigate pathnames in do-files when using Dropbox, and I present a command that automatically finds and changes to a user's Dropbox directory.
4098084	WOS:000347512800006	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	The chi-squared goodness-of-fit test for count-data models	Journal	Article	2014	1	1	English	STATA JOURNAL	798-816	19	1	WOS:000347512800006	In this article, we discuss the implementation of Andrews's (1988a, Journal of Econometrics 37: 135-156; 1988b, Econometrica 56: 1419-1453) chisquared goodness-of-fit test as a postestimation command. The new command chi2gof reports the test statistic, its degrees of freedom, and its p-value. chi2gof can be used after the poisson, nbreg, zip, and zinb commands.
4103942	WOS:000334150900005	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Net survival estimation with stns	Journal	Article	2014	1	1	English	STATA JOURNAL	87-102	16	1	WOS:000334150900005	In this article, we introduce a new Stata command to estimate the net survival function and the net cumulative hazard. The command includes graphic facilities. It was designed to have a similar syntax to sts, the Stata command dedicated to estimate survival and related functions. We present two examples to illustrate the use of this new command.
4107777	WOS:000341583700001	AO8DO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	R Marries NetLogo: Introduction to the RNetLogo Package	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-41	41	1	WOS:000341583700001	The RNetLogo package delivers an interface to embed the agent-based modeling platform NetLogo into the R environment with headless (no graphical user interface) or interactive GUI mode. It provides functions to load models, execute commands, push values, and to get values from NetLogo reporters. Such a seamless integration of a widely used agent-based modeling platform with a well-known statistical computing and graphics environment opens up various possibilities. For example, it enables the modeler to design simulation experiments, store simulation results, and analyze simulation output in a more systematic way. It can therefore help close the gaps in agent-based modeling regarding standards of description and analysis. After a short overview of the agent-based modeling approach and the software used here, the paper delivers a step-by-step introduction to the usage of the RNetLogo package by examples.
4113164	WOS:000345288500001	AU0BP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	On Best Practice Optimization Methods in R	Journal	Article	2014	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000345288500001	R (R Core Team 2014) provides a powerful and flexible system for statistical computations. It has a default-install set of functionality that can be expanded by the use of several thousand add-in packages as well as user-written scripts. While R is itself a programming language, it has proven relatively easy to incorporate programs in other languages, particularly Fortran and C. Success, however, can lead to its own costs: Users face a confusion of choice when trying to select packages in approaching a problem. A need to maintain workable examples using early methods may mean some tools offered as a default may be dated. In an open-source project like R, how to decide what tools offer "best practice" choices, and how to implement such a policy, present a serious challenge. We discuss these issues with reference to the tools in R for nonlinear parameter estimation (NLPE) and optimization, though for the present article 'optimization' will be limited to function minimization of essentially smooth functions with at most bounds constraints on the parameters. We will abbreviate this class of problems as NLPE. We believe that the concepts proposed are transferable to other classes of problems seen by R users.
4130696	WOS:000341583900001	AO8DQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Hierarchical Archimedean Copulae: The HAC Package	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000341583900001	This paper presents the R package H A C, which provides user friendly methods for dealing with hierarchical Archimedean copulae (HAC). Computationally efficient estimation procedures allow to recover the structure and the parameters of HAC from data. In addition, arbitrary HAC can be constructed to sample random vectors and to compute the values of the corresponding cumulative distribution plus density functions. Accurate graphics of the HAC structure can be produced by the plot method implemented for these objects.
4134947	WOS:000332112600001	AB9KR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Introducing localgauss, an R Package for Estimating and Visualizing Local Gaussian Correlation	Journal	Article	2014	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000332112600001	Quantifying non-linear dependence structures between two random variables is a challenging task. There exist several bona-fide dependence measures able to capture the strength of the non-linear association, but they typically give little information about how the variables are associated. This problem has been recognized by several authors and has given rise to the concept of local measures of dependence. A local measure of dependence is able to capture the "local" dependence structure in a particular region. The idea is that the global dependence structure is better described by a portfolio of local measures of dependence computed in different regions than a one-number measure of dependence. This paper introduces the R package localgauss which estimates and visualizes a measure of local dependence called local Gaussian correlation. The package provides a function for estimation, a function for local independence testing and corresponding functions for visualization purposes, which are all demonstrated with examples.
4156446	WOS:000349840800001	CB7XB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Natter: A Python Natural Image Statistics Toolbox	Journal	Article	2014	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000349840800001	The statistical analysis and modeling of natural images is an important branch of statistics with applications in image signaling, image compression, computer vision, and human perception. Because the space of all possible images is too large to be sampled exhaustively, natural image models must inevitably make assumptions in order to stay tractable. Subsequent model comparison can then filter out those models that best capture the statistical regularities in natural images. Proper model comparison, however, often requires that the models and the preprocessing of the data match down to the implementation details. Here we present the Natter, a statistical software toolbox for natural images models, that can provide such consistency. The Natter includes powerful but tractable baseline model as well as standardized data preprocessing steps. It has an extensive test suite to ensure correctness of its algorithms, it interfaces to the modular toolkit for data processing toolbox MDP, and provides simple ways to log the results of numerical experiments. Most importantly, its modular structure can be extended by new models with minimal coding effort, thereby providing a platform for the development and comparison of probabilistic models for natural image data.
4170643	WOS:000338091300003	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Regression models for count data based on the negative binomial(p) distribution	Journal	Article	2014	1	1	English	STATA JOURNAL	280-291	12	1	WOS:000338091300003	We present new Stata commands for estimating several regression models suitable for analyzing overdispersed count outcomes. The nbregp command nests the dispersion (constant) and dispersion (mean) versions of Stata's nbreg command in a model for negative binomial(p) regression. The zignbreg command extends Stata's gnbreg command for zero inflation, and the zinbregp command fits a negative binomial(p) regression model with zero inflation. The new commands for zero-inflated models allow specification of links within the glm command's collection for the Bernoulli model of zero inflation. These commands will optionally calculate a Vuong test, which compares the zero-inflated model with the nonzero-inflated model.
4178968	WOS:000345289800001	AU0CD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Continuous Global Optimization in R	Journal	Article	2014	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	45	1	WOS:000345289800001	This article surveys currently available implementations in R for continuous global optimization problems. A new R package globalOptTests is presented that provides a set of standard test problems for continuous global optimization based on C functions by Ali, Khompatraporn, and Zabinsky (2005). 48 of the objective functions contained in the package are used in empirical comparison of 18 R implementations in terms of the quality of the solutions found and speed.
4185340	WOS:000349843500001	CB7YA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	D-STEM: A Software for the Analysis and Mapping of Environmental Space-Time Variables	Journal	Article	2014	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000349843500001	This paper discusses the software D-STEM as a statistical tool for the analysis and mapping of environmental space-time variables. The software is based on a flexible hierarchical space-time model which is able to deal with multiple variables, heterogeneous spatial supports, heterogeneous sampling networks and missing data. Model estimation is based on the expectation maximization algorithm and it can be performed using a distributed computing environment to reduce computing time when dealing with large data sets. The estimated model is eventually used to dynamically map the variables over the geographic region of interest. Three examples of increasing complexity illustrate usage and capabilities of D-STEM, both in terms of modeling and implementation, starting from a univariate model and arriving at a multivariate data fusion with tapering.
4211999	WOS:000334020200001	AE5IE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian Semi- and Non-Parametric Models for Longitudinal Data with Multiple Membership Effects in R	Journal	Article	2014	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-35	35	1	WOS:000334020200001	We introduce growcurves for R that performs analysis of repeated measures multiple membership (MM) data. This data structure arises in studies under which an intervention is delivered to each subject through the subject's participation in a set of multiple elements that characterize the intervention. In our motivating study design under which subjects receive a group cognitive behavioral therapy (CBT) treatment, an element is a group CBT session and each subject attends multiple sessions that, together, comprise the treatment. The sets of elements, or group CBT sessions, attended by subjects will partly overlap with some of those from other subjects to induce a dependence in their responses. The growcurves package offers two alternative sets of hierarchical models: 1. Separate terms are specified for multivariate subject and MM element random effects, where the subject effects are modeled under a Dirichlet process prior to produce a semi-parametric construction; 2. A single term is employed to model joint subject-by-MM effects. A fully non-parametric dependent Dirichlet process formulation allows exploration of differences in subject responses across different MM elements. This model allows for borrowing information among subjects who express similar longitudinal trajectories for flexible estimation. growcurves deploys "estimation" functions to perform posterior sampling under a suite of prior options. An accompanying set of "plot" functions allows the user to readily extract by-subject growth curves. The design approach intends to anticipate inferential goals with tools that fully extract information from repeated measures data. Computational efficiency is achieved by performing the sampling for estimation functions using compiled C++ code.
4273651	WOS:000347512800008	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	iop: Estimating ex-ante inequality of opportunity	Journal	Article	2014	1	1	English	STATA JOURNAL	830-846	17	1	WOS:000347512800008	This article describes the user-written command lop to estimate exante inequality of opportunity for different types of variables. Inequality of opportunity is the part of inequality that is due to circumstances beyond the control of the individual. Therefore, it is the ethically offensive part of inequality. Several estimation procedures have been proposed over the past years, and lop is a comprehensive and easy-to-use command that implements many of them. It handles continuous, dichotomous, and ordered variables. In addition to the point estimates, lop also provides bootstrap standard errors and two decomposition methods.
4288417	WOS:000343788100007	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	PivotalR: A Package for Machine Learning on Big Data	Journal	Article	2014	6	1	English	R JOURNAL	57-67	11	1	WOS:000343788100007	PivotalR is an R package that provides a front-end to PostgreSQL and all PostgreSQLlike databases such as Pivotal Inc.'s Greenplum Database (GPDB), HAWQ. When running on the products of Pivotal Inc., PivotalR utilizes the full power of parallel computation and distributive storage, and thus gives the normal R user access to big data. PivotalR also provides an R wrapper for MADlib. MADlib is an open-source library for scalable in-database analytics. It provides data-parallel implementations of mathematical, statistical and machine-learning algorithms for structured and unstructured data. Thus PivotalR also enables the user to apply machine learning algorithms on big data.
4320409	WOS:000331696000001	AB3MX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	R Package FrF2 for Creating and Analyzing Fractional Factorial 2-Level Designs	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-56	56	1	WOS:000331696000001	This article describes the R package FrF2 for design and analysis of experiments with 2-level factors. The package offers both regular and non-regular fractional factorial 2-level designs, in the regular case with blocking and split plot facilities and algorithms for ensuring estimability of certain two-factor interactions. Furthermore, simple analysis facilities are on offer, first and foremost plotting functions for half normal plots, main effects and interaction plots, and cube plots. Package FrF2 receives infrastructure support from package DoE.base and heavily builds on a subgraph isomorphism algorithm from package igraph for one of its estimability algorithms.
4360110	WOS:000343788100013	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	RStorm: Developing and Testing Streaming Algorithms in R	Journal	Article	2014	6	1	English	R JOURNAL	123-132	10	1	WOS:000343788100013	Streaming data, consisting of indefinitely evolving sequences, are becoming ubiquitous in many branches of science and in various applications. Computer scientists have developed streaming applications such as Storm and the S4 distributed stream computing platform to deal with data streams. However, in current production packages testing and evaluating streaming algorithms is cumbersome. This paper presents RStorm for the development and evaluation of streaming algorithms analogous to these production packages, but implemented fully in R. RStorm allows developers of streaming algorithms to quickly test, iterate, and evaluate various implementations of streaming algorithms. The paper provides both a canonical computer science example, the streaming word count, and examples of several statistical applications of RStorm.
4376559	WOS:000343788100010	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	investr: An R Package for Inverse Estimation	Journal	Article	2014	6	1	English	R JOURNAL	90-100	11	1	WOS:000343788100010	Inverse estimation is a classical and well-known problem in regression. In simple terms, it involves the use of an observed value of the response to make inference on the corresponding unknown value of the explanatory variable. To our knowledge, however, statistical software is somewhat lacking the capabilities for analyzing these types of problems. In this paper, we introduce investr (which stands for inverse estimation in R), a package for solving inverse estimation problems in both linear and nonlinear regression models.
4382888	WOS:000342606300001	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	ivtreatreg: A command for fitting binary treatment models with heterogeneous response to treatment and unobservable selection	Journal	Article	2014	1	1	English	STATA JOURNAL	453-480	28	1	WOS:000342606300001	In this article, I present ivtreatreg, a command for fitting four different binary treatment models with and without heterogeneous average treatment effects under selection-on-unobservables (that is, treatment endogeneity). Depending on the model specified by the user, ivtreatreg provides consistent estimation of average treatment effects by using instrumental-variables estimators and a generalized two-step Heckman selection model. The added value of this new command is that it allows for generalization of the regression approach typically used in standard program evaluation by assuming heterogeneous response to treatment. It also serves as a sort of toolbox for conducting joint comparisons of different treatment methods, thus readily permitting checks on the robustness of results.
4390415	WOS:000349843300001	CB7XY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	TPmsm: Estimation of the Transition Probabilities in 3-State Models	Journal	Article	2014	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000349843300001	One major goal in clinical applications of multi-state models is the estimation of transition probabilities. The usual nonparametric estimator of the transition matrix for non-homogeneous Markov processes is the Aalen-Johansen estimator (Aalen and Johansen 1978). However, two problems may arise from using this estimator: first, its standard error may be large in heavy censored scenarios; second, the estimator may be inconsistent if the process is non-Markovian. The development of the R package TPmsm has been motivated by several recent contributions that account for these estimation problems. Estimation and statistical inference for transition probabilities can be performed using TPmsm. The TPmsm package provides seven different approaches to three-state illness-death modeling. In two of these approaches the transition probabilities are estimated conditionally on current or past covariate measures. Two real data examples are included for illustration of software usage.
4393089	WOS:000338091300008	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A menu-driven facility for power and detectable-difference calculations in stepped-wedge cluster-randomized trials	Journal	Article	2014	1	1	English	STATA JOURNAL	363-380	18	1	WOS:000338091300008	This article introduces the Stata menu-driven program steppedwedge, which calculates detectable differences and power for stepped-wedge randomized trials. The command permits continuous, binary, and rate outcomes (with normal approximations) for comparisons using two-sided tests. The command allows specification of the number of clusters randomized at each step, the number of steps and the average cluster (cell) size, or an incomplete design in which the user specifies the design pattern (a matrix with one row per cluster, one column per time point, and entries indicating exposure and observable data). Cluster heterogeneity can be parameterized using either the intracluster correlation or the coefficient of variation (of the outcome). The command is illustrated via examples.
4394924	WOS:000334150900004	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Indirect treatment comparison	Journal	Article	2014	1	1	English	STATA JOURNAL	76-86	11	1	WOS:000334150900004	This article presents a command, indirect, for the estimation of effects of multiple treatments in the absence of randomized controlled trials for direct comparisons of interventions.
4400419	WOS:000341583800001	AO8DP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	changepoint: An R Package for Changepoint Analysis	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000341583800001	One of the key challenges in changepoint analysis is the ability to detect multiple changes within a given time series or sequence. The changepoint package has been developed to provide users with a choice of multiple changepoint search methods to use in conjunction with a given changepoint method and in particular provides an implementation of the recently proposed PELT algorithm. This article describes the search methods which are implemented in the package as well as some of the available test statistics whilst highlighting their application with simulated and practical examples. Particular emphasis is placed on the PELT algorithm and how results differ from the binary segmentation approach.
4402726	WOS:000349843600001	CB7YB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ecp: An R Package for Nonparametric Multiple Change Point Analysis of Multivariate Data	Journal	Article	2014	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000349843600001	There are many different ways in which change point analysis can be performed, from purely parametric methods to those that are distribution free. The ecp package is designed to perform multiple change point analysis while making as few assumptions as possible. While many other change point methods are applicable only for univariate data, this R package is suitable for both univariate and multivariate observations. Hierarchical estimation can be based upon either a divisive or agglomerative algorithm. Divisive estimation sequentially identifiers change points via a bisection algorithm. The agglomerative algorithm estimates change point location by determining an optimal segmentation. Both apporaches are able to detect any type of distributional change within the data. This provides an advantage over many existing change point algorithm which are only able to detect changes within the marginal distributions.
4406707	WOS:000343788100002	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Taming PITCHf/x Data with XML2R and pitchRx	Journal	Article	2014	6	1	English	R JOURNAL	5-19	15	1	WOS:000343788100002	XML2R is a framework that reduces the effort required to transform XML content into tables in a way that preserves parent to child relationships. pitchRx applies XML2R's grammar for XML manipulation to Major League Baseball Advanced Media (MLBAM)'s Gameday data. With pitchRx, one can easily obtain and store Gameday data in a remote database. The Gameday website hosts a wealth of XML data, but perhaps most interesting is PITCHf/x. Among other things, PITCHf/x data can be used to recreate a baseball's flight path from a pitcher's hand to home plate. With pitchRx, one can easily create animations and interactive 3D scatterplots of the baseball's flight path. PITCHf/x data is also commonly used to generate a static plot of baseball locations at the moment they cross home plate. These plots, sometimes called strike-zone plots, can also refer to a plot of event probabilities over the same region. pitchRx provides an easy and robust way to generate strike-zone plots using the ggplot2 package.
4413737	WOS:000334019900001	AE5IB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	lavaan.survey: An R Package for Complex Survey Analysis of Structural Equation Models	Journal	Article	2014	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000334019900001	This paper introduces the R package lavaan. survey, a user-friendly interface to designbased complex survey analysis of structural equation models (SEMs). By leveraging existing code in the lavaan and survey packages, the lavaan. survey package allows for SEM analyses of stratified, clustered, and weighted data, as well as multiply imputed complex survey data. lavaan. survey provides several features such as SEMs with replicate weights, a variety of resampling techniques for complex samples, and finite population corrections, features that should prove useful for SEM practitioners faced with the common situation of a sample that is not iid.
4415934	WOS:000348651700004	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Prinsimp	Journal	Article	2014	12	1	English	R JOURNAL	27-42	16	1	WOS:000348651700004	Principal Components Analysis (PCA) is a common way to study the sources of variation in a high-dimensional data set. Typically, the leading principal components are used to understand the variation in the data or to reduce the dimension of the data for subsequent analysis. The remaining principal components are ignored since they explain little of the variation in the data. However, the space spanned by the low variation principal components may contain interesting structure, structure that PCA cannot find. Prinsimp is an R package that looks for interesting structure of low variability. "Interesting" is defined in terms of a simplicity measure. Looking for interpretable structure in a low variability space has particular importance in evolutionary biology, where such structure can signify the existence of a genetic constraint.
4416906	WOS:000341642300001	AO8YQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	copulaedas: An R Package for Estimation of Distribution Algorithms Based on Copulas	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000341642300001	The use of copula-based models in EDAs (estimation of distribution algorithms) is currently an active area of research. In this context, the copulaedas package for R provides a platform where EDAs based on copulas can be implemented and studied. The package offers complete implementations of various EDAs based on copulas and vines, a group of well-known optimization problems, and utility functions to study the performance of the algorithms. Newly developed EDAs can be easily integrated into the package by extending an S4 class with generic functions for their main components. This paper presents copulaedas by providing an overview of EDAs based on copulas, a description of the implementation of the package, and an illustration of its use through examples. The examples include running the EDAs defined in the package, implementing new algorithms, and performing an empirical study to compare the behavior of different algorithms on benchmark functions and a real-world problem.
4431016	WOS:000349841300001	CB7XF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	OptimalCutpoints: An R Package for Selecting Optimal Cutpoints in Diagnostic Tests	Journal	Article	2014	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-36	36	1	WOS:000349841300001	Continuous diagnostic tests are often used for discriminating between healthy and diseased populations. For the clinical application of such tests, it is useful to select a cutpoint or discrimination value c that defines positive and negative test results. In general, individuals with a diagnostic test value of c or higher are classified as diseased. Several search strategies have been proposed for choosing optimal cutpoints in diagnostic tests, depending on the underlying reason for this choice. This paper introduces an R package, known as OptimalCutpoints, for selecting optimal cutpoints in diagnostic tests. It incorporates criteria that take the costs of the different diagnostic decisions into account, as well as the prevalence of the target disease and several methods based on measures of diagnostic test accuracy. Moreover, it enables optimal levels to be calculated according to levels of given (categorical) covariates. While the numerical output includes the optimal cutpoint values and associated accuracy measures with their confidence intervals, the graphical output includes the receiver operating characteristic (ROC) and predictive ROC curves. An illustration of the use of OptimalCutpoints is provided, using a real biomedical dataset.
4432615	WOS:000349840200001	CB7WV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ectree: Evolutionary Learning of Globally Optimal Classification and Regression Trees in R	Journal	Article	2014	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000349840200001	Commonly used classification and regression tree methods like the CART algorithm are recursive partitioning methods that build the model in a forward stepwise search. Although this approach is known to be an efficient heuristic, the results of recursive tree methods are only locally optimal, as splits are chosen to maximize homogeneity at the next step only. An alternative way to search over the parameter space of trees is to use global optimization methods like evolutionary algorithms. This paper describes the evtree package, which implements an evolutionary algorithm for learning globally optimal classification and regression trees in R. Computationally intensive tasks are fully computed in C++ while the partykit package is leveraged for representing the resulting trees in R, providing unified infrastructure for summaries, visualizations, and predictions. evtree is compared to the open-source CART implementation rpart, conditional inference trees (ctree), and the open-source C4.5 implementation J48. A benchmark study of predictive accuracy and complexity is carried out in which evtree achieved at least similar and most of the time better results compared to rpart, ctree, and J48. Furthermore, the usefulness of evtree in practice is illustrated in a textbook customer classication task.
4436847	WOS:000348651700010	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	bshazard: A Flexible Tool for Nonparametric Smoothing of the Hazard Function	Journal	Article	2014	12	1	English	R JOURNAL	114-122	9	1	WOS:000348651700010	The hazard function is a key component in the inferential process in survival analysis and relevant for describing the pattern of failures. However, it is rarely shown in research papers due to the difficulties in nonparametric estimation. We developed the bshazard package to facilitate the computation of a nonparametric estimate of the hazard function, with data-driven smoothing. The method accounts for left truncation, right censoring and possible covariates. B-splines are used to estimate the shape of the hazard within the generalized linear mixed models framework. Smoothness is controlled by imposing an autoregressive structure on the baseline hazard coefficients. This perspective allows an 'automatic' smoothing by avoiding the need to choose the smoothing parameter, which is estimated from the data as a dispersion parameter. A simulation study demonstrates the capability of our software and an application to estimate the hazard of Non-Hodgkin's lymphoma in Swedish population data shows its potential.
4441314	WOS:000332111200001	AB9KG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	mmeta: An R Package for Multivariate Meta-Analysis	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000332111200001	This paper describes the core features of the R package mmeta, which implements the exact posterior inference of odds ratio, relative risk, and risk difference given either a single 2x2 table or multiple 2x2 tables when the risks within the same study are independent or correlated.
4443466	WOS:000347512800002	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Plotting regression coefficients and other estimates	Journal	Article	2014	1	1	English	STATA JOURNAL	708-737	30	1	WOS:000347512800002	Graphical display of regression results has become increasingly popular in presentations and in scientific literature because graphs are often much easier to read than tables. Such plots can be produced in Stata by the marginsplot command (see [R] marginsplot). However, while marginsplot is versatile and flexible, it has two major limitations: it can only process results left behind by margins (see [R] margins), and it can handle only one set of results at a time. In this article, I introduce a new command called coefplot that overcomes these limitations. It plots results from any estimation command and combines results from several models into one graph. The default behavior of coefplot is to plot markers for coefficients and horizontal spikes for confidence intervals. However, coef plot can also produce other types of graphs. I illustrate the capabilities of coefplot by using a series of examples.
4456132	WOS:000347512800013	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Robust data-driven inference in the regression-discontinuity design	Journal	Article	2014	1	1	English	STATA JOURNAL	909-946	38	1	WOS:000347512800013	In this article, we introduce three commands to conduct robust data-driven statistical inference in regression-discontinuity (RD) designs. First, we present rdrobust, a command that implements the robust bias-corrected confidence intervals proposed in Calonico, Cattaneo, and Titiunik (2014d, Econometrica 82: 2295-2326) for average treatment effects at the cutoff in sharp RD, sharp kink RD, fuzzy RD, and fuzzy kink RD designs. This command also implements other conventional nonparametric RD treatment-effect point estimators and confidence intervals. Second, we describe the companion command rdbwselect, which implements several bandwidth selectors proposed in the RD literature. Following the results in Calonico, Cattaneo, and Titiunik (2014a, Working paper, University of Michigan), we also introduce rdplot, a command that implements several data-driven choices of the number of bins in evenly spaced and quantile-spaced partitions that are used to construct the RD plots usually encountered in empirical applications. A companion R package is described in Calonico, Cattaneo, and Titiunik (2014b, Working paper, University of Michigan).
4459044	WOS:000338091300006	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A smooth covariate rank transformation for use in regression models with a sigmoid dose-response function	Journal	Article	2014	1	1	English	STATA JOURNAL	329-341	13	1	WOS:000338091300006	We consider how to represent sigmoid-type regression relationships in a practical and parsimonious way. A pure sigmoid relationship has an asymptote at both ends of the range of a continuous covariate. Curves with a single asymptote are also important in practice. Many smoothers, such as fractional polynomials and restricted cubic regression splines, cannot accurately represent doubly asymptotic curves. Such smoothers may struggle even with singly asymptotic curves. Our approach to modeling sigmoid relationships involves applying a preliminary scaled rank transformation to compress the tails of the observed distribution of a continuous covariate. We include a step that provides a smooth approximation to the empirical cumulative distribution function of the covariate via the scaled ranks. The procedure defines the approximate cumulative distribution transformation of the covariate. To fit the substantive model, we apply fractional polynomial regression to the outcome with the smoothed, scaled ranks as the covariate. When the resulting fractional polynomial function is monotone, we have a sigmoid function. We demonstrate several practical applications of the approximate cumulative distribution transformation while also illustrating its ability to model some unusual functional forms. We describe a command, acd, that implements it.
4470947	WOS:000343788100004	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Stratified Weibull Regression Model for Interval-Censored Data	Journal	Article	2014	6	1	English	R JOURNAL	31-40	10	1	WOS:000343788100004	Interval censored outcomes arise when a silent event of interest is known to have occurred within a specific time period determined by the times of the last negative and first positive diagnostic tests. There is a rich literature on parametric and non-parametric approaches for the analysis of interval-censored outcomes. A commonly used strategy is to use a proportional hazards (PH) model with the baseline hazard function parameterized. The proportional hazards assumption can be relaxed in stratified models by allowing the baseline hazard function to vary across strata defined by a subset of explanatory variables. In this paper, we describe and implement a new R package straweib, for fitting a stratified Weibull model appropriate for interval censored outcomes. We illustrate the R package straweib by analyzing data from a longitudinal oral health study on the timing of the emergence of permanent teeth in 4430 children.
4476831	WOS:000342606300003	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A command for significance and power to test for the existence of a unique most probable category	Journal	Article	2014	1	1	English	STATA JOURNAL	499-510	12	1	WOS:000342606300003	The analysis of multinomial data often includes the following question of interest: Is a particular category the most populous (that is, does it have the largest probability)? Berry (2001, Journal of Statistical Planning and Inference 99: 175-182) developed a likelihood-ratio test for assessing the evidence for the existence of a unique most probable category. Nettleton (2009, Journal of the American Statistical Association 104: 1052-1059) developed a likelihood-ratio test for testing whether a particular category was most probable, showed that the test was an example of an intersection-union test, and proposed other intersection-union tests for testing whether a particular category was most probable. He extended his likelihood-ratio test to the existence of a unique most probable category and showed that his test was equivalent to the test developed by Berry (2001, Journal of Statistical Planning and Inference 99: 175-182). Nettleton (2009, Journal of the American Statistical Association 104: 1052-1059) showed that the likelihood ratio for identifying a unique most probable cell could be viewed as a union-intersection test. The purpose of this article is to survey different methods and present a command, cellsupremacy, for the analysis of multinomial data as it pertains to identifying the significantly most probable category; the article also presents a command for sample-size calculations and power analyses, power_cellsupremacy, that is useful for planning multinomial data studies.
4501658	WOS:000338091300010	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Power analyses for detecting effects for multiple coefficients in regression	Journal	Article	2014	1	1	English	STATA JOURNAL	389-397	9	1	WOS:000338091300010	I present applications of Stata to conduct power analysis for multiple regression. Simple power analyses are provided through the Stata command power and the user-written command powerreg. However, tools for power analysis for multiple regression coefficients are limited. I present strategies for detecting significant regression coefficients in a single study (for example, power estimates for two or more coefficients). I also address the power for detecting multiple effects (that is, the probability that all coefficients are statistically significant) in the same sample through the Monte Carlo approaches afforded by the simulate command.
4515854	WOS:000334150900008	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating the dose-response function through a generalized linear model approach	Journal	Article	2014	1	1	English	STATA JOURNAL	141-158	18	1	WOS:000334150900008	In this article, we revise the estimation of the dose response function described in Hirano and Imbens (2004, Applied Bayesian Modeling and Causal Inference from Incomplete-Data Perspectives, 73-84) by proposing a flexible way to estimate the generalized propensity score when the treatment variable is not necessarily normally distributed. We also provide a set of programs that accomplish this task. To do this, in the existing doseresponse program (Bin and Mattei, 2008, Stata Journal 8: 354-373), we substitute the maximum likelihood estimator in the first step of the computation with the more flexible generalized linear model.
4528517	WOS:000332109600001	AB9JT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	lmdme: Linear Models on Designed Multivariate Experiments in R	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000332109600001	The lmdme package decomposes analysis of variance (ANOVA) through linear models on designed multivariate experiments, allowing ANOVA-principal component analysis (APCA) and ANOVA-simultaneous component analysis (ASCA) in R. It also extends both methods with the application of partial least squares (PLS) through the specification of a desired output matrix. The package is freely available from Bioconductor and licensed under the GNU General Public License. ANOVA decomposition methods for designed multivariate experiments are becoming popular in "omics" experiments (transcriptomics, metabolomics, etc.), where measurements are performed according to a predefined experimental design, with several experimental factors or including subject-specific clinical covariates, such as those present in current clinical genomic studies. ANOVA-PCA and ASCA are well-suited methods for studying interaction patterns on multidimensional datasets. However, currently an R implementation of APCA is only available for Spectra data in the ChemoSpec package, whereas ASCA is based on average calculations on the indices of up to three design matrices. Thus, no statistical inference on estimated effects is provided. Moreover, ASCA is not available in an R package. Here, we present an R implementation for ANOVA decomposition with PCA/PLS analysis that allows the user to specify (through a flexible formula interface), almost any linear model with the associated inference on the estimated effects, as well as to display functions to explore results both of PCA and PLS. We describe the model, its implementation and two high-throughput microarray examples: one applied to interaction pattern analysis and the other to quality assessment.
4541799	WOS:000338091300004	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation and testing of binomial and beta-binomial regression models with and without zero inflation	Journal	Article	2014	1	1	English	STATA JOURNAL	292-303	12	1	WOS:000338091300004	We present new Stata commands for carrying out several regression commands suitable for binomial outcomes. The zib command extends Stata's binreg command to allow zero inflation. The betabin command fits binomial regression models allowing for beta overdispersion, and the zibbin command fits a beta-binomial regression model with zero inflation. All the new commands allow the specification of links within the gin command's collection for both outcome and zero inflation. The zero-inflated commands optionally calculate a Vuong test comparing the zero-inflated model with the nonzero-inflated model, and the zibbin command optionally includes a likelihood-ratio test of the overdispersion parameter.
4548924	WOS:000347512800007	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	txttool: Utilities for text analysis in Stata	Journal	Article	2014	1	1	English	STATA JOURNAL	817-829	13	1	WOS:000347512800007	This article describes txttool, a command that provides a set of tools for managing free-form text. The command integrates several built-in Stata functions with new text capabilities. These latter functions include a utility to create a bag-of-words representation of text and an implementation of Porter's (1980, Program: Electronic library and information systems 14: 130-137) word-stemming algorithm. Collectively, these utilities provide a text-processing suite for text mining and other text-based applications in Stata.
4613546	WOS:000347512800012	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	General-to-specific modeling in Stata	Journal	Article	2014	1	1	English	STATA JOURNAL	895-908	14	1	WOS:000347512800012	Empirical researchers are frequently confronted with issues regarding which explanatory variables to include in their models. This article describes the application of a well-known model-selection algorithm to Stata: general-to-specific (GETS) modeling. This process provides a prescriptive and defendable way of. selecting a few relevant variables from a large list of potentially important variables when fitting a regression model. Several empirical issues in GETS modeling are then discussed, specifically, how such an algorithm can be applied to estimations based upon cross-sectional, time-series, and panel data. A command is presented, written in Stata and Mata, that implements this algorithm for various data types in a flexible way. This command is based on Stata's regress or xtreg command, so it is suitable for researchers in the broad range of fields where regression analysis is used. Finally, the genspec command is illustrated using data from applied studies of GETS modeling with Monte Carlo simulation. It is shown to perform as empirically predicted and to have good size and power (or gauge and potency) properties under simulation.
4618061	WOS:000349842400001	CB7XQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Tutorial Survival Estimation for Cox Regression Models with Time-Varying coefficients	Journal	Article	2014	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000349842400001	Survival estimates are an essential compliment to multivariable regression models for time-to-event data, both for prediction and illustration of covariate effects. They are easily obtained under the Cox Proportional-hazards model. In populations defined by an intial, acute event, like myocardial infarction, or in studies with long-term follow-up, the proportional hazards assumption of constant hazard ratios is frequently violated. One alternative is to fit an interaction between covariates and a prespecified function of time, implemented as a time-dependent covariate. This effectively creates a time-varying coefficient that is easily estimated in software such as SAS and R. However, the usual programming statements for survival estimation are not directly applicable. Unique data manipulation and syntax is required, but is not well documented for either software. This paper offers a tutorial in surviva; estimation for time-varying coefficient model, implemented in SAS and R. We provide a macro coxtve to facilitate estimation in SAS where the current functionality is more limited. The macro is validated in simulated data and illustrated in an application.
4621151	WOS:000341806400001	AP1DK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SNSeqyate: Standard and Nonstandard Statistical Models and Methods for Test Equating	Journal	Article	2014	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	30	1	WOS:000341806400001	Equating is a family of statistical models and methods that are used to adjust scores on two or more versions of a test, so that the scores from different tests may be used interchangeably. In this paper we present the R package SNSequate which implements both standard and nonstandard statistical models and methods for test equating. The package construction was motivated by the need of having a modular, simple, yet comprehensive, and general software that carries out traditional and new equating methods. SNSequate currently implements the traditional mean, linear and equipercentile equating methods, as well as the mean-mean, mean-sigma, Haebara and Stocking-Lord item response theory linking methods. It also supports the newest methods such as local equating, kernel equating, and item response theory parameter linking methods based on asymmetric item characteristic functions. Practical examples are given to illustrate the capabilities of the software. A list of other programs for equating is presented, highlighting the main differences between them. Future directions for the package are also discussed.
4629919	WOS:000347512800003	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Tools for checking calibration of a Cox model in external validation: Approach based on individual event probabilities	Journal	Article	2014	1	1	English	STATA JOURNAL	738-755	18	1	WOS:000347512800003	The Cox proportional hazards model has been used extensively in medicine over the last 40 years. A popular application is to develop a multivariable prediction model, often a prognostic model to predict the clinical outcome of patients with a particular disorder from "baseline" factors measured at some initial time point. For such a model to be useful in practice, it must be "validated"; that is, it must perform satisfactorily in an external sample of patients independent of the sample on which the model was originally developed. One key aspect of performance is calibration, which is the accuracy of prediction, particularly of survival (or equivalently, failure or event) probabilities at any time after the time origin. We believe systematic evaluation of the calibration of a Cox model has been largely ignored in the literature. In this article, we suggest an approach to assessing calibration using individual event probabilities estimated at different time points. We exemplify the method by detailed analysis of two datasets in the disease primary biliary cirrhosis; the datasets comprise a derivation and a validation dataset. We describe a new command, stcoxcal, that performs the necessary calculations. Results for stcoxcal can be displayed graphically, which makes it easier for users to picture calibration (or lack thereof) according to follow-up time.
4635684	WOS:000334019700001	AE5HZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Criteria to Select a Working Correlation Structure for the Generalized Estimating Equations Method in SAS	Journal	Article	2014	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-10	10	1	WOS:000334019700001	The generalized estimating equations (GEE) method is popular for analyzing clustered and longitudinal data. It is important to determine a proper working correlation matrix when applying the GEE method since an improper selection sometimes results in inefficient parameter estimates. In this paper, we provide the CriteriaWorkCorr macro in SAS to calculate the criteria proposed by Pan (2001), Hin, Carey, andWang (2007), Hin andWang (2009), and Gosho, Hamada, and Yoshimura (2011) for selecting the working correlation structure when the GEE method is applied. We illustrate the implementation and an example of the macro.
4653761	WOS:000341021100001	AO0SC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Linear Quantile Mixed Models: The lqmm Package for Laplace Quantile Regression	Journal	Article	2014	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000341021100001	Inference in quantile analysis has received considerable attention in the recent years. Linear quantile mixed models (Geraci and Bottai 2014) represent a flexible statistical tool to analyze data from sampling designs such as multilevel, spatial, panel or longitudinal, which induce some form of clustering. In this paper, I will show how to estimate conditional quantile functions with random effects using the R package lqmm. Modeling, estimation and inference are discussed in detail using a real data example. A thorough description of the optimization algorithms is also provided.
4659968	WOS:000349843100001	CB7XW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Seasonal Adjustment with the R Packages x12 and x12GUI	Journal	Article	2014	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000349843100001	The X-12-ARIMA seasonal adjustment program of the US Census Bureau extracts the different components (mainly: seasonal component, trend component, outlier component and irregular component) of a monthly of quarterly time series. It is the state-of-the-art technology for seasonal adjustment used in many statisfical offices. It is possible to include a moving holiday effect, a trading day effect and user-defined regressors, and additionally incorporates automatic and creates an output data set containing the adjusted time series and intermediate calculation. The original output from X-12-ARIMA is somehow static and it not always an easy task for users to extract the required information for further processing. The R package x12 provides wrapper functions and an abstraction layer for batch processing of X-12-ARIMA. It allows summarizing, modifying and storing the output from X-12-ARIMA within a well-defined class-oriented implementation. On top of the class-oriented (command line) implementation the graphical user interface allows access to the R package x12 without requiring too much R knowledge. User can interactively select additive outlines, level shifts and temporary changes and see the impact immediately. The provision of the powerful X-12-ARIMA seasonal adjustment program available directly from within R, as well as of the new facilities for marking outliers, batch processing and change tracking, makes the package a potent and functional tool.
4704145	WOS:000341808500001	AP1EE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Nestedness for Dummies (NeD): A User-Friendly Web Interface for Exploratory Nestedness Analysis	Journal	Article	2014	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-9	9	1	WOS:000341808500001	Recent theoretical advances in nestedness analysis have led to the introduction of several alternative metrics to overcome most of the problems biasing the use of matrix 'temperature' calculated by Atmar's Nestedness Temperature Calculator. However, all of the currently available programs for nestedness analysis lack the user friendly appeal that has made the Nestedness Temperature Calculator one of the most popular community ecology programs. The software package NeD is an intuitive open source application for nestedness analysis that can be used online or locally under different operating systems. NeD is able to automatically handle different matrix formats, has batch functionalities and produces an output that can be easily copied and pasted to a spreadsheet. In addition to numerical results, NeD provides a graphic representation of the matrix under examination and of the corresponding maximally packed matrix. NeD allows users to select among the most used nestedness metrics, and to combine them with different null models. Integrating easiness of use with the recent theoretical advances in the field, NeD provides researchers not directly involved in theoretical debates with a simple yet robust statistical tool for a more conscious performance of nestedness analysis. NeD can be accessed at http//purl.ocic.org/ned.
4720148	WOS:000341807500001	AP1DV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	structSSI: Simultaneous and Selective Inference for Grouped or Hierarchically Structured Data	Journal	Article	2014	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	21	1	WOS:000341807500001	The R package structSSI provides an accessible implementation of two recently developed simultaneous and selective inference techniques: the group Benjamini-Hochberg and hierarchical false discovery rate procedures. Unlike many multiple testing schemes, these methods specifically incorporate existing information about the grouped or hierarchical dependence between hypotheses under consideration while controlling the false discovery rate. Doing so increases statistical power and interpretability. Furthermore, these procedures provide novel approaches to the central problem of encoding complex dependency between hypotheses. We briefly describe the group Benjamini-Hochberg and hierarchical false discovery rate procedures and then illustrate them using two examples, one a measure of ecological microbial abundances and the other a global temperature time series. For both procedures, we detail the steps associated with the analysis of these particular data sets, including establishing the dependence structures, performing the test, and interpreting the results. These steps are encapsulated by R functions, and we explain their applicability to general data sets.
4724517	WOS:000349841500001	CB7XH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	NPCirc: An R Package for Nonarametric Circular Methods	Journal	Article	2014	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000349841500001	Nonparametric density and regression estimation methods for circular data are included in the R package NPCirc. Specifically, a circular kernel density estimation procedure is provided, jointly with different alternatives for choosing the smoothing parameter. In the regression setting, nonparametric estimation for circular-linear, circular-circular and linear-circular data is also possible via the adaptation of the classical Nadaraya-Watson and local linear estimators. In order to assess the significance of the features observed in the smooth curves, both for density and regression with a circular covariate and a linear response, a SiZer technique is developed for circular data, namely CircSiZer. Some data examples are also included in the package, jointly with a routine that allows generating mixtures of different circular distributions.
4726420	WOS:000331695000001	AB3MN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A MATLAB Package for Computing Two-Level Search Design Performance Criteria	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-8	8	1	WOS:000331695000001	In a 2(m) factorial design, search designs are consider for searching and estimating some non-zero interactions based on search linear models. There are some criteria for comparing search designs. Computing these criteria is a heavy task. In this paper, we provide the SD package for the numerical computing environment MATLAB to compute these criteria and also to check Srivastava's condition for a given design. These package is illustrated by an example.
4726882	WOS:000342606300012	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Panel cointegration analysis with xtpedroni	Journal	Article	2014	1	1	English	STATA JOURNAL	684-692	9	1	WOS:000342606300012	In this article, I introduce the new command xtpedroni, which implements the Pedroni (1999, Oxford Bulletin of Economics and Statistics 61: 653-670; 2004, Econometric Theory 20: 597-625) panel cointegration test and the Pedroni (2001, Review of Economics and Statistics 83: 727-731) group-mean panel-dynamic ordinary least-squares estimator. For nonstationary heterogeneous panels that are long (large T) and wide (large N), xtpedroni tests for cointegration among one or more regressors by using seven test statistics under the null of no cointegration, and it also estimates the cointegrating equation for each individual as well as the group mean of the panel. The test can include common time dummies and unbalanced panels.
4733662	WOS:000348651700002	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Coordinate-Based Meta-Analysis of fMRI Studies with R	Journal	Article	2014	12	1	English	R JOURNAL	5-15	11	1	WOS:000348651700002	This paper outlines how to conduct a simple meta-analysis of neuroimaging foci of activation in R. In particular, the first part of this paper reviews the nature of fMRI data, and presents a brief overview of the existing packages that can be used to analyze fMRI data in R. The second part illustrates how to handle fMRI data by showing how to visualize the results of different neuroimaging studies in a so-called orthographic view, where the spatial distribution of the foci of activation from different fMRI studies can be inspected visually. Functional MRI (fMRI) is one of the most important and powerful tools of neuroscientific research. Although not as commonly used for fMRI analysis as some specific applications such as SPM (Friston et al., 2006), AFNI (Cox and Hyde, 1997), or FSL (Smith et al., 2004), R does provide several packages that can be employed in neuroimaging research. These packages deal with a variety of topics, ranging from reading and manipulating fMRI datasets, to implementing sophisticated statistical models. The goal of this paper is to provide a brief introduction to fMRI analysis, and the various R packages that can be used to carry it out. As an example, it will show how to use simple R commands to read fMRI images and plot results from previous studies, which can then be visually compared. This is a special form of meta-analysis, and a common way to compare results from the existing literature.
4738208	WOS:000340586600001	AN4VL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	R Package multiPIM: A Causal Inference Approach to Variable Importance Analysis	Journal	Article	2014	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000340586600001	We describe the R package multiPIM, including statistical background, functionality and user options. The package is for variable importance analysis, and is meant primarily for analyzing data from exploratory epidemiological studies, though it could certainly be applied in other areas as well. The approach taken to variable importance comes from the causal inference field, and is different from approaches taken in other R packages. By default, multiPIM uses a double robust targeted maximum likelihood estimator (TMLE) of a parameter akin to the attributable risk. Several regression methods/machine learning algorithms are available for estimating the nuisance parameters of the models, including super learner, a meta-learner which combines several different algorithms into one. We describe a simulation in which the double robust TMLE is compared to the graphical computation estimator. We also provide example analyses using two data sets which are included with the package.
4742686	WOS:000343788100016	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Archiving Reproducible Research with R and Dataverse	Journal	Article	2014	6	1	English	R JOURNAL	151-158	8	1	WOS:000343788100016	Reproducible research and data archiving are increasingly important issues in research involving statistical analyses of quantitative data. This article introduces the dvn package, which allows R users to publicly archive datasets, analysis files, codebooks, and associated metadata in Dataverse Network online repositories, an open-source data archiving project sponsored by Harvard University. In this article I review the importance of data archiving in the context of reproducible research, introduce the Dataverse Network, explain the implementation of the dvn package, and provide example code for archiving and releasing data using the package.
4748626	WOS:000331695400001	AB3MR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SNP_NLMM: A SAS Macro to Implement a Flexible Random Effects Density for Generalized Linear and Nonlinear Mixed Models	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000331695400001	Generalized linear and nonlinear mixed models (GLMMs and NLMMs) are commonly used to represent non-Gaussian or nonlinear longitudinal or clustered data. A common assumption is that the random effects are Gaussian. However, this assumption may be unrealistic in some applications, and misspecification of the random effects density may lead to maximum likelihood parameter estimators that are inconsistent, biased, and inefficient. Because testing if the random effects are Gaussian is difficult, previous research has recommended using a flexible random effects density. However, computational limitations have precluded widespread use of flexible random effects densities for GLMMs and NLMMs. We develop a SAS macro, SNP_NLMM, that overcomes the computational challenges to fit GLMMs and NLMMs where the random effects are assumed to follow a smooth density that can be represented by the seminonparametric formulation proposed by Gallant and Nychka (1987). The macro is flexible enough to allow for any density of the response conditional on the random effects and any nonlinear mean trajectory. We demonstrate the SNP_NLMM macro on a GLMM of the disease progression of toenail infection and on a NLMM of intravenous drug concentration over time.
4750536	WOS:000341584500001	AO8DV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	KernSmoothIRT: An R Package for Kernel Smoothing in Item Response Theory	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000341584500001	Item response theory (IRT) models are a class of statistical models used to describe the response behaviors of individuals to a set of items having a certain number of options. They are adopted by researchers in social science, particularly in the analysis of performance or attitudinal data, in psychology, education, medicine, marketing and other fields where the aim is to measure latent constructs. Most IRT analyses use parametric models that rely on assumptions that often are not satisfied. In such cases, a nonparametric approach might be preferable; nevertheless, there are not many software implementations allowing to use that. To address this gap, this paper presents the R package Kern SmoothIRT. It implements kernel smoothing for the estimation of option characteristic curves, and adds several plotting and analytical tools to evaluate the whole test/questionnaire, the items, and the subjects. In order to show the package's capabilities, two real datasets are used, one employing multiple-choice responses, and the other scaled responses.
4750609	WOS:000334150900009	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Sample size and power calculations for trials and quasi-experimental studies with clustering	Journal	Article	2014	1	1	English	STATA JOURNAL	159-175	17	1	WOS:000334150900009	This article considers the estimation of power and sample size in experimental and quasi-experimental intervention studies, where there is clustering of subjects within one or both intervention arms, for both continuous and binary outcomes. A new command, clsampsi, which has a wide range of options, calculates the power and sample size needed (that is, the number of clusters and cluster size) by using the noncentral F distribution as described by Moser, Stevens, and Watts (1989, Communications in Statistics-Theory and Methods 18: 3963-3975). For comparative purposes, this command can also produce power and sample-size estimates on the basis of existing methods that use a normal approximation.
4817519	WOS:000349843400001	CB7XZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	c060: Extended Inference with Lasso and Elastic-Net Regularized Cox and Generalized Linear Models	Journal	Article	2014	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000349843400001	We have developed the R package c060 with the aim of improving R software functionality for high-dimensional risk prediction modeling, e.g., for prognostic modeling of survival data using high-throughput genomic data. Penalized regression models provide a statistically appealing way of building risk prediction models from high-dimensional data. The popular CRAN package glmnet implements an efficient algorithm for fitting penalized Cox and generalized linear models. However, in practical applications the data analysis will typically not stop at the point where the model has been fitted. One is for example often interested in the stability of selected features and in assessing the prediction performance of a model and we provide functions to deal with both of these tasks. Our R functions are computationally efficient and off er the possibility of speeding up computing time through parallel computing. Another feature which can drastically reduce computing time is an e fficient interval-search algorithm, which we have implemented for selecting the optimal parameter combination for elastic net penalties. These functions have been useful in our daily work at the Biostatistics department (C060) of the German Cancer Research Center where prognostic modeling of patient survival data is of particular interest. Although we focus on a survival data application of penalized Cox models in this article, the functions in our R package are in general applicable to all types of regression models implemented in the glmnet package, with the exception of prediction error curves, which are specific to time-to-event data.
4819199	WOS:000341020800001	AO0RZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Building Bivariate Tables: The compareGroups Package for R	Journal	Article	2014	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000341020800001	The R package compareGroups provides functions meant to facilitate the construction of bivariate tables (descriptives of several variables for comparison between groups) and generates reports in several formats (LATEX, HTML or plain text CSV). Moreover, bivariate tables can be viewed directly on the R console in a nice format. A graphical user interface (GUI) has been implemented to build the bivariate tables more easily for those users who are not familiar with the R software. Some new functions and methods have been incorporated in the newest version of the compareGroups package (version 1.x) to deal with time-to-event variables, stratifying tables, merging several tables, and revising the statistical methods used. The GUI interface also has been improved, making it much easier and more intuitive to set the inputs for building the bivariate tables. The first version (version 0.x) and this version were presented at the 2010 useR! conference (Sanz, Subirana, and Vila 2010) and the 2011 useR! conference (Sanz, Subirana, and Vila 2011), respectively.
4832970	WOS:000341642400001	AO8YR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	movMF: An R Package for Fitting Mixtures of von Mises-Fisher Distributions	Journal	Article	2014	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000341642400001	Finite mixtures of von Mises-Fisher distributions allow to apply model-based clustering methods to data which is of standardized length, i.e., all data points lie on the unit sphere. The R package movMF contains functionality to draw samples from finite mixtures of von Mises-Fisher distributions and to fit these models using the expectation-maximization algorithm for maximum likelihood estimation. Special features are the possibility to use sparse matrix representations for the input data, different variants of the expectation-maximization algorithm, different methods for determining the concentration parameters in the M-step and to impose constraints on the concentration parameters over the components. In this paper we describe the main fitting function of the package and illustrate its application. In addition we compare the clustering performance of finite mixtures of von Mises-Fisher distributions to spherical k-means. We also discuss the resolution of several numerical issues which occur for estimating the concentration parameters and for determining the normalizing constant of the von Mises-Fisher distribution.
4839790	WOS:000342606300002	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Obtaining critical values for test of Markov regime switching	Journal	Article	2014	1	1	English	STATA JOURNAL	481-498	18	1	WOS:000342606300002	For Markov regime-switching models, a nonstandard test statistic must be used to test for the possible presence of multiple regimes. Carter and Steigerwald (2013, Journal of Econometric Methods 2: 25-34) derive the analytic steps needed to implement the Markov regime-switching test proposed by Cho and White (2007, Econometrica 75: 1671-1720). We summarize the implementation steps and address the computational issues that arise. We then introduce a new command to compute regime-switching critical values, rscv, and present it in the context of empirical research.
4846188	WOS:000343788100017	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	oligoMask: A Framework for Assessing and Removing the Effect of Genetic Variants on Microarray Probes	Journal	Article	2014	6	1	English	R JOURNAL	159-163	5	1	WOS:000343788100017	As expression microarrays are typically designed relative to a reference genome, any individual genetic variant that overlaps a probe's genomic position can possibly cause a reduction in hybridization due to the probe no longer being a perfect match to a given sample's mRNA at that locus. If the samples or groups used in a microarray study differ in terms of genetic variants, the results of the microarray experiment can be negatively impacted. The oligoMask package is an R/SQLite framework which can utilize publicly available genetic variants and works in conjunction with the oligo package to read in the expression data and remove microarray probes which are likely to impact a given microarray experiment prior to analysis. Tools are provided for creating an SQLite database containing the probe and variant annotations and for performing the commonly used RMA preprocessing procedure for Affymetrix microarrays. The oligoMask package is freely available at https://github.com/dbottomly/oligoMask.
4853018	WOS:000343788100012	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The stringdist Package for Approximate String Matching	Journal	Article	2014	6	1	English	R JOURNAL	111-122	12	1	WOS:000343788100012	Comparing text strings in terms of distance functions is a common and fundamental task in many statistical text-processing applications. Thus far, string distance functionality has been somewhat scattered around R and its extension packages, leaving users with inconistent interfaces and encoding handling. The stringdist package was designed to offer a low-level interface to several popular string distance algorithms which have been re-implemented in C for this purpose. The package offers distances based on counting q-grams, edit-based distances, and some lesser known heuristic distance functions. Based on this functionality, the package also offers inexact matching equivalents of R's native exact matching functions match and % in%.
4865811	WOS:000348651700005	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	phaseR: An R Package for Phase Plane Analysis of Autonomous ODE Systems	Journal	Article	2014	12	1	English	R JOURNAL	43-51	9	1	WOS:000348651700005	When modelling physical systems, analysts will frequently be confronted by differential equations which cannot be solved analytically. In this instance, numerical integration will usually be the only way forward. However, for autonomous systems of ordinary differential equations (ODEs) in one or two dimensions, it is possible to employ an instructive qualitative analysis foregoing this requirement, using so-called phase plane methods. Moreover, this qualitative analysis can even prove to be highly useful for systems that can be solved analytically, or will be solved numerically anyway. The package phaseR allows the user to perform such phase plane analyses: determining the stability of any equilibrium points easily, and producing informative plots.
4897187	WOS:000347512800004	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation of multiprocess survival models with cmp	Journal	Article	2014	1	1	English	STATA JOURNAL	756-777	22	1	WOS:000347512800004	Multilevel multiprocess hazard models are routinely used by demographers to control for endogeneity and selection effects. These models consist of multilevel proportional hazards equations, and possibly probit equations, with correlated random effects. Although Stata currently lacks a specialized command for fitting systems of multilevel proportional hazards models, systems of seemingly unrelated lognormal survival models can be fit with the user-written cup command (Roodman 2011, Stata Journal 11: 159-206). In this article, we describe multiprocess survival models and demonstrate theoretical and practical aspects of estimation. We also illustrate the application of the cmp command using examples related to demographic research. The examples use a dataset shipped with the statistical software aML.
4897574	WOS:000349840700001	CB7XA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SamplingStrata: An R Package for the Optimization of Stratified Sampling	Journal	Article	2014	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000349840700001	When designing a sampling survey, usually constraints are set on the desired precision levels regarding one or more target estimates (the Y's). If a sampling frame is available, containing auxiliary information related to each unit (the X's), it is possible to adopt a stratified sample design. For any given stratification of the frame, in the multivariate case it is possible to solve the problem of the best allocation of units in strata, by minimizing a cost function subject to precision constraints (or, conversely, by maximizing the precision of the estimates under a given budget). The problem is to determine the best stratification in the frame, i.e., the one that ensures the overall minimal cost of the sample necessary to satisfy precision constraints. The X's can be categorical or continuous; continuous ones can be transformed into categorical ones. The most detailed stratification is given by the Cartesian product of the X's (the atomic strata). A way to determine the best stratification is to explore exhaustively the set of all possible partitions derivable by the set of atomic strata, evaluating each one by calculating the corresponding cost in terms of the sample required to satisfy precision constraints. This is unaffordable in practical situations, where the dimension of the space of the partitions can be very high. Another possible way is to explore the space of partitions with an algorithm that is particularly suitable in such situations: the genetic algorithm. The R package SamplingStrata, based on the use of a genetic algorithm, allows to determine the best stratification for a population frame, i.e., the one that ensures the minimum sample cost necessary to satisfy precision constraints, in a multivariate and multi-domain case.
4906541	WOS:000334150900002	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Calibrating survey data using iterative proportional fitting (raking)	Journal	Article	2014	1	1	English	STATA JOURNAL	22-59	38	1	WOS:000334150900002	In this article, I introduce the ipfraking package, which implements weight-calibration procedures known as iterative proportional fitting, or raking, of complex survey weights. The package can handle a large number of control variables and trim the weights in various ways. It also provides diagnostic tools for the weights it creates. I provide examples of its use and a suggested workflow for creating raked replicate weights.
4910464	WOS:000338091300002	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Simulated multivariate random-effects probit models for unbalanced panels	Journal	Article	2014	1	1	English	STATA JOURNAL	259-279	21	1	WOS:000338091300002	This article develops a method for implementing a simulated multivariate random-effects probit model for unbalanced panels (with gaps) and illustrates the model by using artificial data. Halton draws generated by mdraws are used to simulate multivariate normal probabilities with the mvnp() egen function. The estimator can be easily adjusted, for example, to allow for autocorrelated errors. The advantages of this simulated estimation, when compared with existing commands such as redpace, are high accuracy and improved stability.
4916855	WOS:000341020200001	AO0RU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A SAS Package for Logistic Two-Phase Studies	Journal	Article	2014	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000341020200001	Two-phase designs, in which for a large study a dichotomous outcome and partial or proxy information on risk factors is available, whereas precise or complete measurements on covariates have been obtained only in a stratified sub-sample, extend the standard case-control design and have been proven useful in practice. The application of two-phase designs, however, seems to be hampered by the lack of appropriate, easy-to-use software. This paper introduces sas-twophase-package, a collection of SAS-macros, to fulfill this task. sas-twophase-package implements weighted likelihood, pseudo likelihood and semi-parametric maximum likelihood estimation via the EM algorithm and via pro file likelihood in two-phase settings with dichotomous outcome and a given stratification.
4922453	WOS:000342606300007	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A Stata package for the application of semiparametric estimators of dose-response functions	Journal	Article	2014	1	1	English	STATA JOURNAL	580-604	25	1	WOS:000342606300007	In many observational studies, the treatment may not be binary or categorical but rather continuous, so the focus is on estimating a continuous dose- response function. In this article, we propose a set of programs that semiparametrically estimate the dose response function of a continuous treatment under the unconfoundedness assumption. We focus on kernel methods and penalized spline models and use generalized propensity-score methods under continuous treatment regimes for covariate adjustment. Our programs use generalized linear models to estimate the generalized propensity score, allowing users to choose between alternative parametric assumptions. They also allow users to impose a common support condition and evaluate the balance of the covariates using various approaches. We illustrate our routines by estimating the effect of the prize amount on subsequent labor earnings for Massachusetts lottery winners, using data collected by Imbens, Rubin, and Sacerdote (2001, American Economic Review, 778-794).
4928597	WOS:000341020300001	AO0RV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	PaCAL: A Python Package for Arithmetic Computations with Random Variables	Journal	Article	2014	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000341020300001	In this paper we present PaCAL, a Python package for arithmetical computations on random variables. The package is capable of performing the four arithmetic operations: addition, subtraction, multiplication and division, as well as computing many standard functions of random variables. Summary statistics, random number generation, plots, and histograms of the resulting distributions can easily be obtained and distribution parameter fitting is also available. The operations are performed numerically and their results interpolated allowing for arbitrary arithmetic operations on random variables following practically any probability distribution encountered in practice. The package is easy to use, as operations on random variables are performed just as they are on standard Python variables. Independence of random variables is, by default, assumed on each step but some computations on dependent random variables are also possible. We demonstrate on several examples that the results are very accurate, often close to machine precision. Practical applications include statistics, physical measurements or estimation of error distributions in scientific computations.
4948614	WOS:000341642900001	AO8YW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Regularization Paths for Conditional Logistic Regression: The clogitL1 Package	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000341642900001	We apply the cyclic coordinate descent algorithm of Friedman, Hastie, and Tibshirani (2010) to the fitting of a conditional logistic regression model with lasso (l(1)) and elastic net penalties. The sequential strong rules of Tibshirani, Bien, Hastie, Friedman, Taylor, Simon, and Tibshirani (2012) are also used in the algorithm and it is shown that these offer a considerable speed up over the standard coordinate descent algorithm with warm starts. Once implemented, the algorithm is used in simulation studies to compare the variable selection and prediction performance of the conditional logistic regression model against that of its unconditional (standard) counterpart. We find that the conditional model performs admirably on datasets drawn from a suitable conditional distribution, outperforming its unconditional counterpart at variable selection. The conditional model is also fit to a small real world dataset, demonstrating how we obtain regularization paths for the parameters of the model and how we apply cross validation for this method where natural unconditional prediction rules are hard to come by.
4954334	WOS:000332108900001	AB9JM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	HLMdiag: A Suite of Diagnostics for Hierarchical Linear Models in R	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	28	1	WOS:000332108900001	Over the last twenty years there have been numerous developments in diagnostic procedures for hierarchical linear models; however, these procedures are not widely implemented in statistical software packages, and those packages that do contain a complete framework for model assessment are not open source. The lack of availability of diagnostic procedures for hierarchical linear models has limited their adoption in statistical practice. The R package HLMdiag provides diagnostic tools targeting all aspects and levels of continuous response hierarchical linear models with strictly nested dependence structures fit using the lmer() function in the lme4 package. In this paper we discuss the tools implemented in HLMdiag for both residual and influence analysis.
4974712	WOS:000338091300001	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Self-consistent density estimation	Journal	Article	2014	1	1	English	STATA JOURNAL	237-258	22	1	WOS:000338091300001	Estimating a continuous density function from a finite set of data points is an important tool in many scientific disciplines. Popular nonparametric density estimators include histograms and kernel density methods. These methods require the researcher to control the degree of smoothing inherent in an estimated function. In a recent approach, a new method for nonparametric density estimation was proposed that finds the estimate self-consistently, that is without requiring the researcher to choose a smoothing parameter a priori. In this article, we outline the basic ideas of the self-consistent density estimator, and we present a Stata implementation of the method. In addition, we present results of Monte Carlo simulations that show that the self-consistent estimator performs better than other methods, especially for larger data samples.
4976810	WOS:000342606300004	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Merger simulation with nested logit demand	Journal	Article	2014	1	1	English	STATA JOURNAL	511-540	30	1	WOS:000342606300004	In this article, we show how to implement merger simulation in Stata as a postestimation command, that is, after estimating an aggregate nested logit demand system with a linear regression model. We also show how to implement merger simulation when the demand parameters are not estimated but instead calibrated to be consistent with outside information on average price elasticities and profit margins. We allow for a variety of extensions, including the role of (marginal) cost savings, remedies (divestiture), and conduct different from Bertrand-Nash behavior.
4976944	WOS:000343788100015	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	MRCV: A Package for Analyzing Categorical Variables with Multiple Response Options	Journal	Article	2014	6	1	English	R JOURNAL	144-150	7	1	WOS:000343788100015	Multiple response categorical variables (MRCVs), also known as "pick any" or "choose all that apply" variables, summarize survey questions for which respondents are allowed to select more than one category response option. Traditional methods for analyzing the association between categorical variables are not appropriate with MRCVs due to the within-subject dependence among responses. We have developed the MRCV package as the first R package available to correctly analyze MRCV data. Statistical methods offered by our package include counterparts to traditional Pearson chi-square tests for independence and loglinear models, where bootstrap methods and Rao-Scott adjustments are relied on to obtain valid inferences. We demonstrate the primary functions within the package by analyzing data from a survey assessing the swine waste management practices of Kansas farmers.
4981681	WOS:000347512800010	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Analysis of partially observed clustered data using generalized estimating equations and multiple imputation	Journal	Article	2014	1	1	English	STATA JOURNAL	863-883	21	1	WOS:000347512800010	Clustered data arise in many settings, particularly within the social and biomedical sciences. For example, multiple-source reports are commonly collected in child and adolescent psychiatric epidemiologic studies where researchers use various informants (for instance, parents and adolescents) to provide a holistic view of a subject's symptoms. Fitzmaurice et al. (1995, American Journal of Epidemiology 142: 1194-1203) have described estimation of multiple-source models using a standard generalized estimating equation (GEE) framework. However, these studies often have missing data because additional stages of consent and assent are required. The usual GEE is unbiased when data are missing completely at random in the context of Little and Rubin (2002, Statistical Analysis with Missing Data [Wiley]). This is a strong assumption that may not be tenable. Other options, such as the weighted GEE, are computationally challenging when missingness is nonmonotone. Multiple imputation is an attractive method to fit incomplete data models while requiring only the less restrictive missing-at-random assumption. Previously, estimation of partially observed clustered data was computationally challenging. However, recent developments in Stata have facilitated using them in practice. We demonstrate how to use multiple imputation in conjunction with a GEE to investigate the prevalence of eating disorder symptoms in adolescents as reported by parents and adolescents and to determine the factors associated with concordance and prevalence. The methods are motivated by the Avon Longitudinal Study of Parents and their Children, a cohort study that enrolled more than 14,000 pregnant mothers in 1991-92 and has followed the health and development of their children at regular intervals. While point estimates for the missing-at-random model were fairly similar to those for the GEE under missing completely at random, the missing-at-random model had smaller standard errors and required less stringent assumptions regarding missingness.
5001750	WOS:000345288900001	AU0BT	1548-7660	1548766	NULL	NULL	NULL	NULL	10.18637/jss.v060.i03	Spectral Projected Gradient Methods: Review and Perspectives	Journal	Review	2014	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	21	1	WOS:000345288900001	Over the last two decades, it has been observed that using the gradient vector as a search direction in large-scale optimization may lead to efficient algorithms. The effectiveness relies on choosing the step lengths according to novel ideas that are related to the spectrum of the underlying local Hessian rather than related to the standard decrease in the objective function. A review of these so-called spectral projected gradient methods for convex constrained optimization is presented. To illustrate the performance of these low-cost schemes, an optimization problem on the set of positive definite matrices is described.
5009701	WOS:000340587300001	AN4VS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	DBKGrad: An R Package for Mortality Rates Graduation by Discrete Beta Kernel Techniques	Journal	Article	2014	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000340587300001	We introduce the R package DBKGrad, conceived to facilitate the use of kernel smoothing in graduating mortality rates. The package implements univariate and bivariate adaptive discrete beta kernel estimators. Discrete kernels have been preferred because, in this context, variables such as age, calendar year and duration, are pragmatically considered as discrete and the use of beta kernels is motivated since it reduces boundary bias. Furthermore, when data on exposures to the risk of death are available, the use of adaptive bandwidth, that may be selected by cross-validation, can provide additional benefits. To exemplify the use of the package, an application to Italian mortality rates, for different ages and calendar years, is presented.
5034716	WOS:000341808300001	AP1EC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	runmixregls: A Program to Run the MIXREGLS Mixed-Effects Location Scale Software from within Stata	Journal	Article	2014	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-41	41	1	WOS:000341808300001	Hedeker and Nordgren (2013) present the stand-alone MIXREGLS program for fitting the mixed-effects location scale model to continuous longitudinal and other clustered data. This model can be used when interest lies in joint modeling the mean and dispersion of subjects' responses over time. The model extends the standard two-level random-intercept mixed model by allowing both the within- and between-subject variances to be influenced by the covariates and for the within-subject variance to additionally depend on a subject random-scale effect. In this article we present the runmixregls command to run MIXREGLS seamlessly from within Stata. We illustrate the notable advantages of using runmixregls by replicating and extending the two example analyses presented in Hedeker and Nordgren (2013). We then use runmixregls to demonstrate a new and important research finding. Namely, that ignoring the random-scale effect in the within-subject variance function will lead to the regression coefficients in this function to be estimated with spurious precision, especially the regression coefficients of subject-level covariates.
5038210	WOS:000334150900006	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Reports and other PDF documents	Journal	Article	2014	1	1	English	STATA JOURNAL	103-118	16	1	WOS:000334150900006	Stata users often need to combine text, tables, and figures. The author's command, lpdf, generates reports and other PDF documents. lpdf compiles text stored in global macros, tables stored as dataset tables or LATEX table input files, and figures stored as Stata graphs or PDF figure files. LATEX must be installed, but familiarity with LATEX is not necessary. lpdf performs every step through Stata and with Stata syntax. It generates documents in report or article style and portrait or landscape orientation. The default author name, document title, and date can be modified. Further format options include the font and margin sizes. For each table and figure, the width and layout can be adapted. Stata users with LATEX skills may benefit from additional possibilities. The lpdf ado-file includes two other useful commands called latexize and latext. latexize processes the content of string variables to properly type special characters and symbols in LATEX input files. latext modifies text stored in global macros in the same way.
5062364	WOS:000349842900001	CB7XU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	TSclust: An R Package for Time series clustering	Journal	Article	2014	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-43	43	1	WOS:000349842900001	Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity measure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to implement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.
5076540	WOS:000347512800005	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	dhreg, xtdhreg, and bootdhreg: Commands to implement double-hurdle regression	Journal	Article	2014	1	1	English	STATA JOURNAL	778-797	20	1	WOS:000347512800005	The dhreg command implements maximum likelihood estimation of the double-hurdle model for continuously distributed outcomes. The command includes the option to fit a p-tobit model, that is, a model that estimates only an intercept for the hurdle equation. The bootcrhreg command (the bootstrap version of ahreg) may be convenient if the data-generating process is more complicated or if heteroskedasticity is suspected. The xtdhreg command is a random-effects version of dhreg applicable to panel data. However, this estimator differs from standard random-effects estimators in the sense that the outcome of the first hurdle applies to the complete set of observations for a given subject instead of applying at the level of individual observations. Command options include estimation of a correlation parameter capturing dependence between the two hurdles.
5103037	WOS:000348651700014	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	qmethod: A Package to Explore Human Perspectives Using Q Methodology	Journal	Article	2014	12	1	English	R JOURNAL	163-173	11	1	WOS:000348651700014	Q is a methodology to explore the distinct subjective perspectives that exist within a group. It is used increasingly across disciplines. The methodology is semi-qualitative and the data are analysed using data reduction methods to discern the existing patterns of thought. This package is the first to perform Q analysis in R, and it provides many advantages to the existing software: namely, it is fully cross-platform, the algorithms can be transparently examined, it provides results in a clearly structured and tabulated form ready for further exploration and modelling, it produces a graphical summary of the results, and it generates a more concise report of the distinguishing and consensus statements. This paper introduces the methodology and explains how to use the package, its advantages as well as its limitations. I illustrate the main functions with a dataset on value patterns about democracy.
5117349	WOS:000332113200001	AB9KW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Momocs: Outline Analysis Using R	Journal	Article	2014	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000332113200001	We introduce here Momocs, a package intended to ease and popularize modern morphometrics with R, and particularly outline analysis, which aims to extract quantitative variables from shapes. It mostly hinges on the functions published in the book entitled Modern Morphometrics Using R by Claude (2008). From outline extraction from raw data to multivariate analysis, Momocs provides an integrated and convenient toolkit to students and researchers who are, or may become, interested in describing the shape and its variation. The methods implemented so far in Momocs are introduced through a simplistic case study that aims to test if two sets of bottles have different shapes.
5118536	WOS:000349845100001	CB7YP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	RgoogleMaps and loa: Unleashing R Graphics Power on Map Tiles	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000349845100001	The RgoogleMaps package provides (1) an R interface to query the Google and the OpenStreetMap servers for Static maps in the form of PNGs, and (2) enables the user to overlay plots on those maps within R. The loa package provides dedicated panel functions to integrate RgoogleMaps within the lattice plotting environment. In addition to solving the generic task of plotting on a map background in R, we introduce several specific algorithms to detect and visualize spatio-temporal cluster. This task can often be-reduced to detecting over-densities in space relative to a background density. The relative density estimation is framed as a binary classification problem. An integrated hotspot visualizer is presented which allows the efficient identification and visualization of cluster in one environment. Competing clustering methods such as the scan statistic and the density scan offer higher detection power at a much larger computational cost. Such clustering method can then be extended using the lattice trellis framework to provide further insight into the relationship between clusters and potentially influential parameters. While there are other options for such map 'mashups' we believe that the integration of RgoogleMaps and lattice using loa can in certain circumstances be advantageous, e.g., by providing a highly intuitive working environment for multivariate analysis and flexible testbed for the rapid development of novel data visualization .
5132965	WOS:000340586400001	AN4VJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Three-Way Component Analysis Using the R Package ThreeWay	Journal	Article	2014	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	23	1	WOS:000340586400001	The R package Three Way is presented and its main features are illustrated. The aim of Three Way is too ff er a suit of functions for handling three- way arrays. In particular, the most relevant available functions are T3 and CP, which implement, respectively, the Tucker3 and Candecomp/ Parafac methods. They are the two most popular tools for summarizing three-way arrays in terms of components. After briefly recalling both techniques from a theoretical point of view, the functions T3 and CP are described by considering three real life examples.
5143441	WOS:000349840300001	CB7WW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bergm: Bayesian Exponential Random Graphs in R	Journal	Article	2014	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000349840300001	In this paper we describe the main features of the Bergm package for the open-source R software which provides a comprehensive framework for Bayesian analysis of exponential random graph models: tools for parameter estimation, model selection and goodness-of fit diagnostics. We illustrate the capabilities of this package describing the algorithms through a tutorial analysis of three network datasets.
5143538	WOS:000347512800015	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Collecting and organizing Stata graphs	Journal	Article	2014	1	1	English	STATA JOURNAL	965-974	10	1	WOS:000347512800015	Stata includes a powerful set of tools for constructing a wide array of graphs. Stata's graphing capabilities are well suited for describing exploratory or preliminary data analyses as well as producing publication-quality graphics. Currently, however, Stata does not have a built-in suite of commands for constructing various types of files (for example, HTML,TEX, or RTF files) to display multiple graphs. Such files can be invaluable for organizing and facilitating the interpretation of the numerous graphs needed throughout an analysis or in the final stage of a project. In this article, we provide an overview of two commands, graphsto and graphout, designed to organize and process multiple graphs across various file types.
5161554	WOS:000349843700001	CB7YC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	envlp: A MATLAB Toolbox for Computing Envelope Estimators in Multivariate Analysis	Journal	Article	2014	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000349843700001	Envelope models and methods represent new constructions that can lead to substantial increases in estimation efficiency in multivariate analyses. The envlp toolbox implements a variety of envelope estimators under the framework of multivariate linear regression, including the envelope model, partial envelope model, heteroscedastic envelope model, inner envelope model, scaled envelope model, and envelope model in the predictor space. The toolbox also implements the envelope model for estimating a multivariate mean. The capabilities of this toolbox include estimation of the model parameters, as well as performing standard multivariate inference in the context of envelope models; for example, prediction and prediction errors, F test for two nested models, the standard errors for contrasts or linear combinations of coefficients, and more. Examples and datasets are contained in the toolbox to illustrate the use of each model. All functions and datasets are documented.
5193349	WOS:000343788100006	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The RWiener Package: an R Package Providing Distribution Functions for the Wiener Diffusion Model	Journal	Article	2014	6	1	English	R JOURNAL	49-56	8	1	WOS:000343788100006	We present the RWiener package that provides R functions for the Wiener diffusion model. The core of the package are the four distribution functions dwiener, pwiener, qwiener and rwiener, which use up-to-date methods, implemented in C, and provide fast and accurate computation of the density, distribution, and quantile function, as well as a random number generator for the Wiener diffusion model. We used the typical Wiener diffusion model with four parameters: boundary separation, non-decision time, initial bias and drift rate parameter. Beyond the distribution functions, we provide extended likelihood-based functions that can be used for parameter estimation and model selection. The package can be obtained via CRAN.
5197755	WOS:000334150900001	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	sreweight: A Stata command to reweight survey data to external totals	Journal	Article	2014	1	1	English	STATA JOURNAL	4-21	18	1	WOS:000334150900001	This article describes sreweight, a Stata command to reweight survey data to external aggregate totals.
5200427	WOS:000347512800016	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Design plots for graphical summary of a response given factors	Journal	Article	2014	1	1	English	STATA JOURNAL	975-990	16	1	WOS:000347512800016	Design plots, as defined in this article, show summaries of a response variable given the classes or distinct levels of numeric or string variables presented as influencing factors. Any summarize results can be plotted using statsby as an engine to produce summaries for groups of observations defined by classes and their cross-combinations, graph dot is used by default, but graphs may readily be recast using graph hbar or graph bar. Such plots offer scope for detailed yet concise data exploration and reporting.
5208911	WOS:000341642600001	AO8YT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Optimal Asset Pricing	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000341642600001	We describe an R package for determining the optimal price of an asset which is "perishable" in a certain sense, given the intensity of customer arrivals and a time-varying price sensitivity function which specifies the probability that a customer will purchase an asset offered at a given price at a given time. The package deals with the case of customers arriving in groups, with a probability distribution for the group size being specified. The methodology and software allow for both discrete and continuous pricing. The class of possible models for price sensitivity functions is very wide, and includes piecewise linear models. A mechanism for constructing piecewise linear price sensitivity functions is provided.
5215993	WOS:000349842500001	CB7XR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	mdscore : An R Package to Compute Improved Score Tests in Generalized Linear Models	Journal	Article	2014	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000349842500001	Improved score tests are modifications of the score test such that the null distribution of the modified test statistic is better approximated by the chi-squared distribution. The literature includes theoretical and empirical evidence favoring the improved test over its unmodified version. However, the developed methodology seems to have been overlooked by data analysts in practice, possibly because of the difficulties associated with the computation of the modified test. In this article, we describe the mdscore package to compute improved score tests in generalized linear models, given a fitted model by the glm () function in R. The package is suitable for applied statistics and simulation experiments. Examples based on real and simulated data are discussed.
5239667	WOS:000347512800009	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	femlogit-Implementation of the multinomial logit model with fixed effects	Journal	Article	2014	1	1	English	STATA JOURNAL	847-862	16	1	WOS:000347512800009	Fixed-effects models have become increasingly popular in social-science research. The possibility to control for unobserved heterogeneity makes these models a prime tool for causal analysis. Fixed-effects models have been derived and implemented for many statistical software packages for continuous, dichotomous, and count-data dependent variables. Chamberlain (1980, Review of Economic Studies 47: 225-238) derived the multinomial logistic regression with fixed effects. However, this model has not yet been implemented in any statistical software package. Possible applications would be analyses of effects on employment status, with special consideration of part-time or irregular employment, and analyses of effects on voting behavior that implicitly control for long-time party identification rather than measuring it directly. This article introduces an implementation of this model with the new command femlogit. I show its application with British election panel data.
5241783	WOS:000345289600001	AU0CB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Convex Optimization in R	Journal	Article	2014	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	23	1	WOS:000345289600001	Convex optimization now plays an essential role in many facets of statistics. We briefly survey some recent developments and describe some implementations of these methods in R. Applications of linear and quadratic programming are introduced including quantile regression, the Huber M-estimator and various penalized regression methods. Applications to additively separable convex problems subject to linear equality and inequality constraints such as nonparametric density estimation and maximum likelihood estimation of general nonparametric mixture models are described, as are several cone programming problems. We focus throughout primarily on implementations in the R environment that rely on solution methods linked to R, like MOSEK by the package Rmosek. Code is provided in R to illustrate several of these problems. Other applications are available in the R package REBayes, dealing with empirical Bayes estimation of nonparametric mixture models.
5296586	WOS:000341584400001	AO8DU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Smoothing Spline ANOVA Models: R Package gss	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000341584400001	This document provides a brief introduction to the R package gss for nonparametric statistical modeling in a variety of problem settings including regression, density estimation, and hazard estimation. Functional ANOVA (analysis of variance) decompositions are built into models on product domains, and modeling and inferential tools are provided for tasks such as interval estimates, the "testing" of negligible model terms, the handling of correlated data, etc. The methodological background is outlined, and data analysis is illustrated using real-data examples.
5306764	WOS:000342606300009	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Adaptive Markov chain Monte Carlo sampling and estimation in Mata	Journal	Article	2014	1	1	English	STATA JOURNAL	623-661	39	1	WOS:000342606300009	I describe algorithms for drawing from distributions using adaptive Markov chain Monte Carlo (MCMC) methods; I introduce a Mata function for performing adaptive MCMC, amcmc (); and I present a suite of functions, amcmc_*(), that allows an alternative implementation of adaptive MCMC. amcmc () and amcmc_*() can be used with models set up to work with Mata's moptimize () (see [M-5] moptimize()) or optimize () (see [M-5] optimize()) or with standalone functions. To show how the routines can be used in estimation problems, I give two examples of what Chernozhukov and Hong (2003, Journal of Econometrics 115: 293-346) refer to as quasi-Bayesian or Laplace-type estimators simulation-based estimators using MCMC sampling. In the first example, I illustrate basic ideas and show how a simple linear model can be fit by simulation. In the next example, I describe simulation-based estimation of a censored quantile regression model following Powell (1986, Journal of Econometrics 32: 143-155); the discussion describes the workings of the command mcmccqreg. I also present an example of how the routines can be used to draw from distributions without a normalizing constant and used in Bayesian estimation of a mixed logit model. This discussion introduces the command bayesmixedlogit.
5309915	WOS:000343788100008	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	rotations: An R Package for SO(3) Data	Journal	Article	2014	6	1	English	R JOURNAL	68-78	11	1	WOS:000343788100008	In this article we introduce the rotations package which provides users with the ability to simulate, analyze and visualize three-dimensional rotation data. More specifically it includes four commonly used distributions from which to simulate data, four estimators of the central orientation, six confidence region estimation procedures and two approaches to visualizing rotation data. All of these features are available for two different parameterizations of rotations: three-by-three matrices and quaternions. In addition, two datasets are included that illustrate the use of rotation data in practice.
5311690	WOS:000342606300005	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	treatrew: A user-written command for estimating average treatment effects by reweighting on the propensity score	Journal	Article	2014	1	1	English	STATA JOURNAL	541-561	21	1	WOS:000342606300005	Reweighting is a popular statistical technique to deal with inference in the presence of a nonrandom sample, and various reweighting estimators have been proposed in the literature. This article presents the user-written command treatrew, which implements reweighting on the propensity-score estimator as proposed by Rosenbaum and Rubin (1983, Biometrika 70: 41-55) in their seminal article. The main contribution of this command lies in providing analytical standard errors for the average treatment effects in the whole population, in the subpopulation of the treated, and in that of the untreated. Standard errors are calculated using the approximation suggested by Wooldridge (2010, 920-930, Econometric Analysis of Cross Section and Panel Data [MIT Press]), but bootstrapped standard errors can also be easily computed. Because an implementation of this estimator with analytic standard errors and nonnormalized weights is missing in Stata, this article and the accompanying ado-file aim to provide the community with an easy-to-use method for reweighting on the propensity-score. The estimator proves to be a valuable tool for estimating average treatment effects under selection on observables.
5321949	WOS:000341792800001	AP0YO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ART: A Data Aggregation Program for the Behavioral Sciences	Journal	Article	2014	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000341792800001	Today, many experiments in the field of behavioral sciences are conducted using a computer. While there is a broad choice of computer programs facilitating the process of conducting experiments as well as programs for statistical analysis there are relatively few programs facilitating the intermediate step of data aggregation. ART has been developed in order to fill this gap and to provide a computer program for data aggregation that has a graphical user interface such that aggregation can be done more easily and without any programming. All "rules" that are necessary to extract variables can be seen "at a glance" which helps the user to conduct even complex aggregations with several hundreds of variables and makes aggregation more resistant against errors. ART runs with Windows XP, Vista, 7, and 8 and it is free.
5332487	WOS:000332110300001	AB9JY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	POBE : A Computer Program for Optimal Design of Multi-Subject Blocked fMRI Experiments	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000332110300001	For functional magnetic resonance imaging (fMRI) studies, researchers can use multi-subject blocked designs to identify active brain regions for a certain stimulus type of interest. Before performing such an experiment, careful planning is necessary to obtain efficient stimulus effect estimators within the available financial resources. The optimal number of subjects and the optimal scanning time for a multi-subject blocked design with fixed experimental costs can be determined using optimal design methods. In this paper, the user-friendly computer program POBE 1.2 (program for optimal design of blocked experiments, version 1.2) is presented. POBE provides a graphical user interface for fMRI researchers to easily and efficiently design their experiments. The computer program POBE calculates the optimal number of subjects and the optimal scanning time for user specified experimental factors and model parameters so that the statistical efficiency is maximised for a given study budget. POBE can also be used to determine the minimum budget for a given power. Furthermore, a maximin design can be determined as efficient design for a possible range of values for the unknown model parameters. In this paper, the computer program is described and illustrated with typical experimental factors for a blocked fMRI experiment.
5353432	WOS:000343788100005	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	brainR: Interactive 3 and 4D Images of High Resolution Neuroimage Data	Journal	Article	2014	6	1	English	R JOURNAL	41-48	8	1	WOS:000343788100005	We provide software tools for displaying and publishing interactive 3-dimensional (3D) and 4-dimensional (4D) figures to html webpages, with examples of high-resolution brain imaging. Our framework is based in the R statistical software using the rgl package, a 3D graphics library. We build on this package to allow manipulation of figures including rotation and translation, zooming, coloring of brain substructures, adjusting transparency levels, and addition/or removal of brain structures. The need for better visualization tools of ultra high dimensional data is ever present; we are providing a clean, simple, web-based option. We also provide a package (brainR) for users to readily implement these tools.
5364479	WOS:000342606300014	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of An Introduction to Stata for Health Researchers, Fourth Edition, by Juul and Frydenberg	Journal	Review	2014	1	1	English	STATA JOURNAL	697-700	4	1	WOS:000342606300014	In this article, I review An Introduction to Stata for Health Researchers, Fourth Edition; by Svend Juul and Morten Frydenberg (2014 [Stata Press]).
5378023	WOS:000334020100001	AE5ID	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Dixon Test. CriticalValues: A Computer Code to Calculate Critical Values for the Dixon Statistical Data Treatment Approach	Journal	Article	2014	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-12	12	1	WOS:000334020100001	The computer program DixonText. Critical Values is written in VB.NET to extend the quadrature approach to calculate the critical values with accuracy upto 6 significant digits for Dixon's ratios. Its use in creating the critical values tables in Excel is illustrated.
5390752	WOS:000348651700008	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	ngspatial: A Package for Fitting the Centered Autologistic and Sparse Spatial Generalized Linear Mixed Models for Areal Data	Journal	Article	2014	12	1	English	R JOURNAL	81-95	15	1	WOS:000348651700008	Two important recent advances in areal modeling are the centered autologistic model and the sparse spatial generalized linear mixed model (SGLMM), both of which are reparameterizations of traditional models. The reparameterizations improve regression inference by alleviating spatial confounding, and the sparse SGLMM also greatly speeds computing by reducing the dimension of the spatial random effects. Package ngspatial ('ng' = non-Gaussian) provides routines for fitting these new models. The package supports composite likelihood and Bayesian inference for the centered autologistic model, and Bayesian inference for the sparse SGLMM.
5404380	WOS:000341792500001	AP0YL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The R Package survsim for the Simulation of Simple and Complex Survival Data	Journal	Article	2014	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000341792500001	We present an R package for the simulation of simple and complex survival data. It covers different situations, including recurrent events and multiple events. The main simulation routine allows the user to introduce an arbitrary number of distributions, each corresponding to a new event or episode, with its parameters, choosing between the Weibull (and exponential as a particular case), log-logistic and log-normal distributions.
5417005	WOS:000341583500001	AO8DM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Flexible Generation of E-learning Exams in R: Moodle Quizzes, OLAT Assessments, and Beyond	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-36	36	1	WOS:000341583500001	The capabilities of the packages exams for automatic generation (statistical) exams in R are extended adding support foe learing mansgemnt systems: As in earlier versions - of the package exam generation is still based on separate sweave files for each exercise - but rather than just producing different types of pdf output files, the package can now render the same exercises into a wide variety of output formats. these include IITML (with exams in learning management systems such as moodle or OLAT. The flexibility is accomplished by a new modular and extensible desgn of the package that alloes for reading all weaved exercises into R and manging associated supplementary files (such as graphics or data files the manuscript discuss , and how new functioality can be built on top of the existing tools.
5417801	WOS:000332107900001	AB9JD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	STARS: An ArcGIS Toolset Used to Calculate the Spatial Information Needed to Fit Spatial Statistical Models to Stream Network Data	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000332107900001	This paper describes the STARS ArcGIS geoprocessing toolset,which is used to calcul ate the spatial information needed to fit spatial statistical models to stream network data using the SSN package. The STARS toolset is designed for use with a landscape network (LSN),which is at opological data model produced by the FLoWS ArcGIS geoprocessing toolset. An overview of the FLoWS LSN structure and a few particularly useful tools is also provided so that users will have a clear understanding of the underlying data structure that the STARS toolset depends on. This document may be used as an introduction to new users. The methods used to calculate the spatial information and format the final.ssn object are also explicitly described so that users may create their own.ssn object using other data models and software.
5418667	WOS:000334150900010	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	strel2: A command for estimating excess hazard and relative survival in large population-based studies	Journal	Article	2014	1	1	English	STATA JOURNAL	176-190	15	1	WOS:000334150900010	In this article, we describe strel2, a Stata command for the estimation of excess hazard and relative survival in large population-based datasets. strel2 implements the maximum-likelihood estimation approach developed by Esteve et al. (1990, Statistics in Medicine 9: 529-538) and assumes that the excess hazard is a piecewise constant function. Categorical covariates can be incorporated into the model, allowing the user to obtain interval- and covariate-specific estimates of the quantities of interest. Although alternative and more powerful commands for relative survival exist, strel2 is a simple tool that is particularly convenient for users who may not have strong statistical skills and want to analyze very large datasets.
5448681	WOS:000341806800001	AP1DO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Tidy Data	Journal	Article	2014	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000341806800001	A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.
5464269	WOS:000343788100019	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Web Technologies Task View	Journal	Editorial Material	2014	6	1	English	R JOURNAL	178-181	4	1	WOS:000343788100019	This article presents the CRAN Task View on Web Technologies. We describe the most important aspects of Web Technologies and Web Scraping and list some of the packages that are currently available on CRAN. Finally, we plot the network of Web Technology related package dependencies.
5467915	WOS:000343788100014	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The gridSVG Package	Journal	Article	2014	6	1	English	R JOURNAL	133-143	11	1	WOS:000343788100014	The gridSVG package can be used to generate a grid-based R plot in an SVG format, with the ability to add special effects to the plot. The special effects include animation, interactivity, and advanced graphical features, such as masks and filters. This article provides a basic introduction to important functions in the gridSVG package and discusses the advantages and disadvantages of gridSVG compared to similar R packages.
5484710	WOS:000349842000001	CB7XM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	coneproj: An R Package for the Primal or Dual Cone Projections with Routines for Constrained Regression	Journal	Article	2014	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000349842000001	The coneproj package contains routines for cone projection and quadratic programming, plus applications in estimation and inference for constrined parametric regression and shape-restricted regression problems. A short routine check_irred is included to chek the irreducibility of a matrix, whose rows are supposed to be a set of cone edges used by coneA OR coneB. For the coneA and coneB functions, the vector to project is provided by the user, along with the cone specifications, the vector. For coneB, the cone edges are provided. The coneA and coneB algorithms have been coded and compiled in C++, and are called by R. The qprog function transforms a quadratic programming problems into a cone projection problem and calls coneA. The constreg function does estimation and inference for parametric least-squares regression with constrainst on the parameters (using coneA). A p value for the "one-sided" test is provided. The shapereg function uses coneB to provide a least a least-squares estimator for a regression function with several choices of constraints including isotonic and convex regression function, as well as significance of the effects are also provided. This package is now available from the Comprehensive R Archive Network at http://CRAN.R-project.org/package=coneproj.
5516651	WOS:000341793300001	AP0YS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	mediation: R Package for Causal Mediation Analysis	Journal	Article	2014	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	38	1	WOS:000341793300001	In this paper, we describe the R package mediation for conducting causal mediation analysis in applied empirical research. In many scientific disciplines, the goal of researchers is not only estimating causal effects of a treatment but also understanding the process in which the treatment causally affects the outcome. Causal mediation analysis is frequently used to assess potential causal mechanisms. The mediation package implements a comprehensive suite of statistical tools for conducting such an analysis. The package is organized into two distinct approaches. Using the model-based approach, researchers can estimate causal mediation effects and conduct sensitivity analysis under the standard research design. Furthermore, the design-based approach provides several analysis tools that are applicable under different experimental designs. This approach requires weaker assumptions than the model-based approach. We also implement a statistical method for dealing with multiple (causally dependent) mediators, which are often encountered in practice. Finally, the package also offers a methodology for assessing causal mediation in the presence of treatment noncompliance, a common problem in randomized trials.
5529352	WOS:000343788100018	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	sgr: A Package for Simulating Conditional Fake Ordinal Data	Journal	Article	2014	6	1	English	R JOURNAL	164-177	14	1	WOS:000343788100018	Many self-report measures of attitudes, beliefs, personality, and pathology include items that can be easily manipulated by respondents. For example, an individual may deliberately attempt to manipulate or distort responses to simulate grossly exaggerated physical or psychological symptoms in order to reach specific goals such as, for example, obtaining financial compensation, avoiding being charged with a crime, avoiding military duty, or obtaining drugs. This article introduces the package sgr that can be used to perform fake data analysis according to the sample generation by replacement approach. The package includes functions for making simple inferences about discrete/ordinal fake data. The package allows to quantify uncertainty in inferences based on possible fake data as well as to study the implications of fake data for empirical results.
5554996	WOS:000348651700007	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Applying spartan to Understand Parameter Uncertainty in Simulations	Journal	Article	2014	12	1	English	R JOURNAL	63-80	18	1	WOS:000348651700007	In attempts to further understand the dynamics of complex systems, the application of computer simulation is becoming increasingly prevalent. Whereas a great deal of focus has been placed in the development of software tools that aid researchers develop simulations, similar focus has not been applied in the creation of tools that perform a rigorous statistical analysis of results generated through simulation: vital in understanding how these results offer an insight into the captured system. This encouraged us to develop spartan, a package of statistical techniques designed to assist researchers in understanding the relationship between their simulation and the real system. Previously we have described each technique within spartan in detail, with an accompanying immunology case study examining the development of lymphoid tissue. Here we provide a practical introduction to the package, demonstrating how each technique is run in R, to assist researchers in integrating this package alongside their chosen simulation platform.
5607694	WOS:000338091300012	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Stata command for calculating adverse event and efficacy stopping boundaries for phase II single-arm trials	Journal	Article	2014	1	1	English	STATA JOURNAL	407-417	11	1	WOS:000338091300012	Many programs and functions in statistical packages focus on the final stage of clinical trials, that is, the data analysis. In this article, I aim to assist in the early stages of clinical trials, specifically, the design of phase II single-arm trials. I present the new command stopbound, which calculates stopping boundaries and operating characteristics based on monitoring an adverse event, efficacy, or an adverse event and efficacy.
5641626	WOS:000349841900001	CB7XL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fitting Accelerated Failure Time Models in Routine Survival Analysis with R Package aftgee	Journal	Article	2014	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000349841900001	Accelerated failure time (AFT) models are alternatives to relative risk models which are used extensively to examine the covariate effects on event times in censored data regression. Nevertheless, AFT models have been much less utilized in practice due to lack of reliable computing methods and software. This paper describes an R package aft g gee that implements recently developed inference procedures for AFT models with both the rank-based approach and the least squares approach. For the rank-based approach, the package allows various weight choices and uses an induced smoothing procedure that leads to much more efficient computation than the linear programming method. With the rank-based estimator as an initial value, the generalized estimating equation approach is used as an extension of the least squares approach to the multivariate case. Additional sampling weights are incorporated to handle missing data needed as in case-cohort studies or general sampling schemes. A simulated dataset and two real life examples from biomedical research are employed to illustrate the usage of the package.
5651088	WOS:000341806900001	AP1DP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	hmmm: An R Package for Hierarchical Multinomial Marginal Models	Journal	Article	2014	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	25	1	WOS:000341806900001	In this paper we show how complete hierarchical multinomial marginal (HMM) models for categorical variables can be defined, estimated and tested using the R package hmmm. Models involving equality and inequality constraints on marginal parameters are needed to define hypotheses of conditional independence, stochastic dominance or notions of positive dependence, or when the parameters are allowed to depend on covariates. The hmmm package also serves the need of estimating and testing HMM models under equality and inequality constraints on marginal interactions.
5682068	WOS:000341807200001	AP1DS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Capabilities of R Package mixAK for Clustering Based on Multivariate Continuous and Discrete Longitudinal Data	Journal	Article	2014	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-38	38	1	WOS:000341807200001	R package mixAK originally implemented routines primarily for Bayesian estimation of finite normal mixture models for possibly interval-censored data. The functionality of the package was considerably enhanced by implementing methods for Bayesian estimation of mixtures of multivariate generalized linear mixed models proposed in Komarek and Komarkova (2013). Among other things, this allows for a cluster analysis (classification) based on multivariate continuous and discrete longitudinal data that arise whenever multiple outcomes of a different nature are recorded in a longitudinal study. This package also allows for a data-driven selection of a number of clusters as methods for selecting a number of mixture components were implemented. A model and clustering methodology for multivariate continuous and discrete longitudinal data is overviewed. Further, a step-by-step cluster analysis based jointly on three longitudinal variables of different types (continuous, count, dichotomous) is given, which provides a user manual for using the package for similar problems.
5690229	WOS:000349841700001	CB7XJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	rFerns: An Implementation of the Random Ferns Method for General-Purpose Machine Learning	Journal	Article	2014	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000349841700001	Random ferns is a very simple yet powerful classification method originally introduced for specific computer vision tasks. In this paper, I show that this algorithm may be considered as a constrained decision tree ensemble and use this interpretation to introduce a series of modifications which enable the use of random ferns in general machine learning problems. Moreover, I extend the method with an internal error approximation and an attribute importance measure based on corresponding features of the random forest algorithm. I also present the R package rFerns containing an efficient implementation of this modified version of random ferns.
5691164	WOS:000349840900001	CB7XC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Nbclust: An R Package for Determining the Relevant Number of Clusters in a Data Set	Journal	Article	2014	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-36	36	1	WOS:000349840900001	Clustering is the partitioning of a set of objects into groups (clusters) so that objects within a group are more similar to each others than objects in different groups. Most of the clustering algorithms depend on some assumptions in order to define the subgroups present in a data set. As a consequence, the resulting clustering scheme requires some sort of evaluation as regards its validity. The evaluation procedure has to tackle difficult problems such as the quality of clusters, the degree with which a clustering scheme fits a specific data set and the optimal number of clusters in a partitioning. In the literature, a wide variety of indices have been proposed to find the optimal number of clusters in a partitioning of a data set during the clustering process. However, for most of indices proposed in the literature, programs are unavailable to test these indices and compare them. The R package NbClust has been developed for that purpose. It provides 30 indices which determine the number of clusters in a data set and it offers also the best clustering scheme from different results to the user. In addition, it provides a function to perform k-means and hierarchical clustering with different distance measures and aggregation methods. Any combination of validation indices and clustering methods can be requested in a single function call. This enables the user to simultaneously evaluate several clustering schemes while varying the number of clusters, to help determining the most appropriate number of clusters for the data set of interest.
5696109	WOS:000338091300013	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Application of multiple imputation using the two-fold fully conditional specification algorithm in longitudinal clinical data	Journal	Article	2014	1	1	English	STATA JOURNAL	418-431	14	1	WOS:000338091300013	Electronic health records of longitudinal clinical data are a valuable resource for health care research. One obstacle of using databases of health records in epidemiological analyses is that general practitioners mainly record data if they are clinically relevant. We can use existing methods to handle missing data, such as multiple imputation (MI), if we treat the unavailability of measurements as a missing-data problem. Most software implementations of MI do not take account of the longitudinal and dynamic structure of the data and are difficult to implement in large databases with millions of individuals and long follow-up. Nevalainen, Kenward, and Virtanen (2009, Statistics in Medicine 28: 3657-3669) proposed the two-fold fully conditional specification algorithm to impute missing data in longitudinal data. It imputes missing values at a given time point, conditional on information at the same time point and immediately adjacent time points. In this article, we describe a new command, twofold, that implements the two-fold fully conditional specification algorithm. It is extended to accommodate MI of longitudinal clinical records in large databases.
5710049	WOS:000334150900003	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	miivfind: A command for identifying model-implied instrumental variables for structural equation models in Stata	Journal	Article	2014	1	1	English	STATA JOURNAL	60-75	16	1	WOS:000334150900003	This article presents a new Stata command, miivfind, that implements an algorithm developed by Bollen and Bauer (2004, Sociological Methods and Research 32: 425-452) to find the model-implied instrumental variables (MIIVs) from an identified structural equation model. MIIVs allow researchers to draw on instrumental-variable estimators, such as two-stage least-squares estimators, to obtain estimates for the parameters of a hypothesized structural equation model. It can be difficult to identify MIIVs by inspection of either a diagram of the model or the model equations. Two examples are provided that illustrate the use of miivfind to identify MIIVs and some of the advantages of a MIIV estimator as compared with a maximum likelihood estimator. By assisting in the process of finding MIIVs, miivfind facilitates the use of an alternative class of estimators, instrumental-variable estimators, to the standard maximum-likelihood and asymptotic-distribution free estimators available for structural equation models.
5714210	WOS:000348651700009	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	sgof: An R Package for Multiple Testing Problems	Journal	Article	2014	12	1	English	R JOURNAL	96-113	18	1	WOS:000348651700009	In this paper we present a new R package called sgof for multiple hypothesis testing. The principal aim of this package is to implement SGoF-type multiple testing methods, known to be more powerful than the classical false discovery rate (FDR) and family-wise error rate (FWER) based methods in certain situations, particularly when the number of tests is large. This package includes Binomial and Conservative SGoF and the Bayesian and Beta-Binomial SGoF multiple testing procedures, which are adaptations of the original SGoF method to the Bayesian setting and to possibly correlated tests, respectively. The sgof package also implements the Benjamini-Hochberg and Benjamini-Yekutieli FDR controlling procedures. For each method the package provides (among other things) the number of rejected null hypotheses, estimation of the corresponding FDR, and the set of adjusted p values. Some automatic plots of interest are implemented too. Two real data examples are used to illustrate how sgof works.
5730387	WOS:000341584800001	AO8DY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	%HPGLIMMIX: A High-Performance SAS Macro for GLMM Estimation	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000341584800001	Generalized linear mixed models (GLMMs) comprise a class of widely used statistical tools for data analysis with fixed and random effects when the response variable has a conditional distribution in the exponential family. GLMM analysis also has a close relationship with actuarial credibility theory. While readily available programs such as the GLIMMIX procedure in SAS and the Ime4 package in R are powerful tools for using this class of models, these programs are not able to handle models with thousands of levels of fixed and random effects. By using sparse-matrix and other high performance techniques, procedures such as HYMIXed in SAS can easily fit models with thousands of factor levels, but only for normally distributed response variables. In this paper, we present the %HPGLIMMIX SAS macro that fits GLMMS with large number of sparsely populated design matrices using the doubly-iterative linearization (pseudo-likelihood) method, in which the sparse-matrix-based HPMIXED is used for the linear iterations with the pesudo-variable constructed for the inverse-link function and the chosen model. Although the macro does not have the full functionality of the GLIMMIX procedure, time and memory savings can be large with the new macro. In applications in which design matrices contain many zeros and there are hundreds or thousands of factors levels, models cab be fitted without exhausting computer memory, and 90% or better reduction in running time can be observed. Examples with a Poisson, binomial, and gamma conditional distribution are presented to demonstrate the usage and efficiency of this macro.
5735927	WOS:000334020300001	AE5IF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The YUIMA Project: A Computational Framework for Simulation and Inference of Stochastic Differential Equations	Journal	Article	2014	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-51	51	1	WOS:000334020300001	The YUIMA Project is an open source and collaborative effort aimed at developing the R package yuima for simulation and inference of stochastic differential equations. In the yuima package stochastic differential equations can be of very abstract type, multidimensional, driven by Wiener process or fractional Brownian motion with general Hurst parameter, with or without jumps specified as Levy noise. The yuima package is intended to offer the basic infrastructure on which complex models and inference procedures can be built on. This paper explains the design of the yuima package and provides some examples of applications.
5760192	WOS:000349841100001	CB7XD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	WebBUGS : Conducting Bayesian Statistical Analysis Online	Journal	Article	2014	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000349841100001	A web interface, named WebBUGS, is developed to conduct Bayesian analysis online over the Internet through Open BUGS and R. WebBUGS can be used with the minimum requirement of a web browser both remotely and locally. WebBUGS has many collaborative features such as email notification and sharing. WebBUGS also eases the use of Open BUGS by providing built-in model templates, data management module, and other useful modules. In this paper, the use of WebBUGS is illustrated and discussed.
5782624	WOS:000338091300011	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Note on Lilien and modified Lilien index	Journal	Article	2014	1	1	English	STATA JOURNAL	398-406	9	1	WOS:000338091300011	This article is a companion to the Lilien (lilien) and modified Lilien commands for computing relative indices in Stata. In this article, we illustrate the main features of the commands with an application to the structural determinants of regional unemployment.
5783944	WOS:000334150900007	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating adjusted associations between random effects from multilevel models: The reffadjust package	Journal	Article	2014	1	1	English	STATA JOURNAL	119-140	22	1	WOS:000334150900007	We describe a method to estimate associations between random effects from multilevel models. We provide two new postestimation commands, reffadjustsim and reffadjust4nlcom, which are distributed as the reffadjust package. These commands produce the estimates and their associated confidence intervals. The commands are used after official Stata multilevel model estimation commands mixed, meqrlogit, and meqrpoisson (formerly named xtmixed, xtmelogit, and xtmepoisson, respectively, before Stata 13) and with models fit in the MLwiN statistical software package via the runmlwin command. We demonstrate our commands with several simulated datasets and for a bivariate outcome model investigating the relationship between weight and mean arterial pressure in pregnant women using data from the Avon Longitudinal Study of Parents and Children. Our method and commands help to improve the interpretability of estimated random-effects variance components from multilevel models.
5788027	WOS:000343788100009	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	ROSE: A Package for Binary Imbalanced Learning	Journal	Article	2014	6	1	English	R JOURNAL	79-89	11	1	WOS:000343788100009	The ROSE package provides functions to deal with binary classification problems in the presence of imbalanced classes. Artificial balanced samples are generated according to a smoothed bootstrap approach and allow for aiding both the phases of estimation and accuracy evaluation of a binary classifier in the presence of a rare class. Functions that implement more traditional remedies for the class imbalance and different metrics to evaluate accuracy are also provided. These are estimated by holdout, bootstrap, or cross-validation methods.
5792544	WOS:000334150900011	AE7BE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating marginal treatment effects using parametric and semiparametric methods	Journal	Article	2014	1	1	English	STATA JOURNAL	191-217	27	1	WOS:000334150900011	We describe the new command margte, which computes marginal and average treatment effects for a model with a binary treatment and a continuous outcome given selection on unobservables and returns. Marginal treatment effects differ from average treatment effects in instances where the impact of treatment varies within a population in correlation with unobserved characteristics. Both parametric and semiparametric estimation methods can be used with margte, and we provide evidence from a Monte Carlo simulation for when each is preferable.
5842117	WOS:000347512800014	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	adjcatlogit, ccrlogit, and ucrlogit: Fitting ordinal logistic regression models	Journal	Article	2014	1	1	English	STATA JOURNAL	947-964	18	1	WOS:000347512800014	In this article, I present three commands that perform adjacent-category logistic regression (adjcatlogit), constrained continuation-ratio logistic regression (ccrlogit), and unconstrained continuation-ratio logistic regression (ucrlogit) for ordered response data.
5864480	WOS:000345289200001	AU0BX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	trust Optim : An R Package for Trust Region Optimization with Sparse Hessians	Journal	Article	2014	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000345289200001	Trust region algorithms are nonlinear optimization tools that tend to be stable and reliable when the objective function is non-concave, ill-conditioned, or exhibits regions that are nearly at. Additionally, most freely-available optimization routines do not exploit the sparsity of the Hessian when such sparsity exists, as in log posterior densities of Bayesian hierarchical models. The trustOptim package for the R programming language addresses both of these issues. It is intended to be robust, scalable and e_cient for a large class of nonlinear optimization problems that are often encountered in statistics, such as _nding posterior modes. The user must supply the objective function, gradient and Hessian. However, when used in conjunction with the sparseHessianFD package, the user does not need to supply the exact sparse Hessian, as long as the sparsity structure is known in advance. For models with a large number of parameters, but for which most of the cross-partial derivatives are zero (i.e., the Hessian is sparse), trustOptim o_ers dramatic performance improvements over existing options, in terms of computational time and memory footprint.
5866586	WOS:000332110800001	AB9KC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Classification Accuracy and Consistency under Item Response Theory Models Using the Package classify	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000332110800001	The R package classify presents a number of useful functions which can be used to estimate the classification accuracy and consistency of assessments. Classification accuracy refers to the probability that an examinee's achieved grade classification on an assessment refects their true grade. Classification consistency refers to the probability that an examinee will be classified in to the same grade classification under repeated administrations of an assessment. Understanding the classification accuracy and consistency of assessments is important where key decisions are being taken on the basis of grades or classifications. The study of classification accuracy can help to improve the design of assessments and aid public understanding and confidence in those assessments.
5871449	WOS:000343788100011	AR7WQ	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Rankcluster: An R Package for Clustering Multivariate Partial Rankings	Journal	Article	2014	6	1	English	R JOURNAL	101-110	10	1	WOS:000343788100011	The Rankcluster package is the first R package proposing both modeling and clustering tools for ranking data, potentially multivariate and partial. Ranking data are modeled by the Insertion Sorting Rank (ISR) model, which is a meaningful model parametrized by a central ranking and a dispersion parameter. A conditional independence assumption allows multivariate rankings to be taken into account, and clustering is performed by means of mixtures of multivariate ISR models. The parameters of the cluster (central rankings and dispersion parameters) help the practitioners to interpret the clustering. Moreover, the Rankcluster package provides an estimate of the missing ranking positions when rankings are partial. After an overview of the mixture of multivariate ISR models, the Rankcluster package is described and its use is illustrated through the analysis of two real datasets.
5883615	WOS:000342606300006	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Modeling count data with generalized distributions	Journal	Article	2014	1	1	English	STATA JOURNAL	562-579	18	1	WOS:000342606300006	We present motivation and new commands for modeling count data. While our focus is to present new commands for estimating count data, we also discuss generalized binomial regression and present the zero-inflated versions of each model.
5887603	WOS:000332110000001	AB9JV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Estimating Extensive Form Games in R	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000332110000001	This article introduces new software, the games package, for estimating strategic statistical models in R. In these models, the probability distribution over outcomes corresponds to the equilibrium of an underlying game form. We review such models and provide derivations for one example, including discussion of alternative motivations for the stochastic component of the models. We introduce the basic functionality of the games package, such as how to estimate players' utilities for outcomes as a function of covariates. The package implements maximum likelihood estimation for the most commonly used models of strategic choice, including three extensive form games and an ultimatum bargaining model. The software also includes functions for bootstrapping, plotting fitted values with their confidence intervals, performing non-nested model comparisons, and checking global convergence failures. We use the new software to replicate Leblang's (2003) analysis of speculative currency attacks.
5888680	WOS:000338091300009	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Importing Chinese historical stock market quotations from NetEase	Journal	Article	2014	1	1	English	STATA JOURNAL	381-388	8	1	WOS:000338091300009	In this article, we describe a command, cntrade, that imports historical stock quotations from Net Ease, a key Chinese website for stocks listed on China's two exchanges.
5899683	WOS:000338091300005	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Density-based empirical likelihood procedures for testing symmetry of data distributions and K-sample comparisons	Journal	Article	2014	1	1	English	STATA JOURNAL	304-328	25	1	WOS:000338091300005	In practice, parametric likelihood-ratio techniques are powerful statistical tools. In this article, we propose and examine novel and simple distribution-free test statistics that efficiently approximate parametric likelihood ratios to analyze and compare distributions of K groups of observations. Using the density-based empirical likelihood methodology, we develop a Stata package that applies to a test for symmetry of data distributions and compares K-sample distributions. Recognizing that recent statistical software packages do not sufficiently address K-sample nonparametric comparisons of data distributions, we propose a new Stata command, vxdbel, to execute exact density-based empirical likelihood-ratio tests using K samples. To calculate p-values of the proposed tests, we use the following methods: 1) a classical technique based on Monte Carlo p-value evaluations; 2) an interpolation technique based on tabulated critical values; and 3) a new hybrid technique that combines methods 1 and 2. The third, cutting-edge method is shown to be very efficient in the context of exact-test p-value computations. This Bayesian-type method considers tabulated critical values as prior information and Monte Carlo generations of test statistic values as data used to depict the likelihood function. In this case, a nonparametric Bayesian method is proposed to compute critical values of exact tests.
5903282	WOS:000341584600001	AO8DW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	conting: An R Package for Bayesian Analysis of Complete and Incomplete Contingency Tables	Journal	Article	2014	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	27	1	WOS:000341584600001	The aim of this paper is to demonstrate the R package conting for the Bayesian analysis of complete and incomplete contingency tables using hierarchical log-linear models. This package allows a user to identify interactions between categorical factors (via complete contingency tables) and to estimate closed population sizes using capture-recapture studies (via incomplete contingency tables). The models are fitted using Markov chain Monte Carlo methods. In particular, implementations of the Metropolis-Hastings and reversible jump algorithms appropriate for log-linear models are employed. The conting package is demonstrated on four real examples.
5926736	WOS:000348651700006	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Flexible R Functions for Processing Accelerometer Data, with Emphasis on NHANES 2003-2006	Journal	Article	2014	12	1	English	R JOURNAL	52-62	11	1	WOS:000348651700006	Accelerometers are a valuable tool for measuring physical activity (PA) in epidemiological studies. However, considerable processing is needed to convert time-series accelerometer data into meaningful variables for statistical analysis. This article describes two recently developed R packages for processing accelerometer data. The package accelerometry contains functions for performing various data processing procedures, such as identifying periods of non-wear time and bouts of activity. The functions are flexible, computationally efficient, and compatible with uniaxial or triaxial data. The package nhanesaccel is specifically for processing data from the National Health and Nutrition Examination Survey (NHANES), years 2003-2006. Its primary function generates measures of PA volume, intensity, frequency, and patterns according to user-specified data processing methods. This function can process the NHANES 2003-2006 dataset in under one minute, which is a drastic improvement over existing software. This article highlights important features of packages accelerometry and nhanesaccel and demonstrates typical usage for PA researchers.
5934735	WOS:000345288200001	AU0BM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Numerical Optimization in R: Beyond optim	Journal	Article	2014	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	3	1	WOS:000345288200001	Numerical optimization is often an essential aspect of mathematical analysis in science, technology and other areas. The function optim() provides basic optimization capabilities and is among the most widely used functions in R. Additionally, there are various packages and functions for solving various types of optimization problem (the optimization task view on Comprehensive R Archive Network provides a comprehensive list of available options for solving optimization problems in R). In this special volume, four papers are presented which discuss some of the areas in numerical optimization where significant developments have been made recently to enhance the capabilities in R. This introduction provides a brief overview of the volume.
5947441	WOS:000349842200001	CB7XO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	BayesLCA: An R Package for Bayesian Latent Class Analysis	Journal	Article	2014	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000349842200001	The BayesLCA package for R provides tools for performing latent class analysis within a Bayesian setting. Three methods for fitting the model are provided, incorporating an expectation-maximization algorithm, Gibbs sampling and a variational Bayes approximation. The article briefly outlines the methodology behind each of these techniques and discusses some of the technical difficulties associated with them. Methods to remedy these problems are also described. Visualization methods for each of these techniques are included, as well as criteria to aid model selection.
5962875	WOS:000334020600001	AE5II	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	PySSM : APython Module for Bayesian Inference of Linear Gaussian State Space Models	Journal	Article	2014	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-37	37	1	WOS:000334020600001	PySSM is a Python package that has been developed for the analysis of time series using linear Gaussian state space models. PySSM is easy to use; models can be set up quickly and efficiently and a variety of different settings are available to the user. It also takes advantage of scientific libraries NumPy and SciPy and other high level features of the Python language. PySSM is also used as a platform for interfacing between optimized and parallelized Fortran routines. These Fortran routines heavily utilize basic linear algebra and linear algebra Package functions for maximum performance. PySSM contains classes for filtering, classical smoothing as well as simulation smoothing.
5967847	WOS:000338091300014	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Self and others	Journal	Article	2014	1	1	English	STATA JOURNAL	432-444	13	1	WOS:000338091300014	Data for people in a family, firms in a sector, or members of any kind of group are often analyzed using data for the other members in each group. Examples are the number of older children for each child in a family or the mean investment of other firms in the same year. Many such problems yield to simple logic, spelled out here in terms of useful Stata functions and commands. Operations under by:, or an equivalent by() option, are frequently called for, while the egen command offers a key tool for many calculations. Even when looping is needed, some helpful tricks can be identified.
5970418	WOS:000341806600001	AP1DM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Kenward-Roger Approximation and Parametric Bootstrap Methods for Tests in Linear Mixed Models - The R Package pbkrtest	Journal	Article	2014	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000341806600001	When testing for reduction of the mean value structure in linear mixed models, it is common to use an asymptotic chi(2) test. Such tests can, however, be very poor for small and moderate sample sizes. The pbkrtest package implements two alternatives to such approximate chi(2) tests: The package implements (1) a Kenward-Roger approximation for performing F tests for reduction of the mean structure and (2) parametric bootstrap methods for achieving the same goal. The implementation is focused on linear mixed models with independent residual errors. In addition to describing the methods and aspects of their implementation, the paper also contains several examples and a comparison of the various methods.
5972114	WOS:000348651700015	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	gset: An R Package for Exact Sequential Test of Equivalence Hypothesis Based on Bivariate Non-Central t-Statistics	Journal	Article	2014	12	1	English	R JOURNAL	174-184	11	1	WOS:000348651700015	The R package gset calculates equivalence and futility boundaries based on the exact bivariate non-central t test statistics. It is the first R package that targets specifically at the group sequential test of equivalence hypotheses. The exact test approach adopted by gset neither assumes the large-sample normality of the test statistics nor ignores the contribution to the overall Type I error rate from rejecting one out of the two one-sided hypotheses under a null value. The features of gset include: error spending functions, computation of equivalence boundaries and futility boundaries, either binding or nonbinding, depiction of stagewise boundary plots, and operating characteristics of a given group sequential design including empirical Type I error rate, empirical power, expected sample size, and probability of stopping at an interim look due to equivalence or futility.
5976057	WOS:000349840600001	CB7WZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ldr: An R Software Package for Likelihood-Based Sufficient Dimension Reduction	Journal	Article	2014	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000349840600001	In regression settings, a sufficient dimension reduction (SDR) method seeks the core information in a p-vector predictor that completely captures its relationship with a response. The reduced predictor may reside in a lower dimension d < p, improving ability to visualize data and predict future observations, and mitigating dimensionality issues when carrying out further analysis. We introduce ldr, a new R software package that implements three recently proposed likelihood-based methods for SDR: covariance reduction, likelihood acquired directions, and principal fitted components. All three methods reduce the dimensionality of the data by projection into lower dimensional subspaces. The package also implements a variable screening method built upon principal fitted components which makes use of flexible basis functions to capture the dependencies between the predictors and the response. Examples are given to demonstrate likelihood-based SDR analyses using ldr, including estimation of the dimension of reduction subspaces and selection of basis functions. The ldr package provides a framework that we hope to grow into a comprehensive library of likelihood-based SDR methodologies.
5981340	WOS:000332108200001	AB9JG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SSN: An R Package for Spatial Statistical Modeling on Stream Networks	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-45	45	1	WOS:000332108200001	The SSN package for R provides a set of functions for modeling stream network data. The package can import geographic information systems data or simulate new data as a 'SpatialStreamNetwork', a new object class that builds on the spatial sp classes. Functions are provided that fit spatial linear models (SLMs) for the 'SpatialStreamNetwork' object. The covariance matrix of the SLMs use distance metrics and geostatistical models that are unique to stream networks; these models account for the distances and topological configuration of stream networks, including the volume and direction of flowing water. In addition, traditional models that use Euclidean distance and simple random effects are included, along with Poisson and binomial families, for a generalized linear mixed model framework. Plotting and diagnostic functions are provided. Prediction (kriging) can be performed for missing data or for a separate set of unobserved locations, or block prediction (block kriging) can be used over sets of stream segments. This article summarizes the SSN package for importing, simulating, and modeling of stream network data, including diagnostics and prediction.
6003493	WOS:000341020600001	AO0RX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Clustering via Nonparametric Density Estimation: The R Package pdfCluster	Journal	Article	2014	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	26	1	WOS:000341020600001	The R package pdfCluster performs cluster analysis based on a nonparametric estimate of the density of the observed variables. Functions are provided to encompass the whole process of clustering, from kernel density estimation, to clustering itself and subsequent graphical diagnostics. After summarizing the main aspects of the methodology, we describe the features and the usage of the package, and finally illustrate its application with the aid of two data sets.
6022118	WOS:000341792300001	AP0YJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	cancerclass: An R Package for Development and Validation of Diagnostic Tests from High-Dimensional Molecular Data	Journal	Article	2014	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000341792300001	Progress in molecular high-throughput techniques has led to the opportunity of a comprehensive monitoring of biomolecules in medical samples. In the era of personalized medicine, these data form the basis for the development of diagnostic, prognostic and predictive tests for cancer. Because of the high number of features that are measured simultaneously in a relatively low number of samples, supervised learning approaches are sensitive to overfitting and performance overestimation. Bioinformatic methods were developed to cope with these problems including control of accuaracy and precision. However, there is demand for easy-to-use software that integrates methods for classifier construction, performance assessment and development of diagnostic tests. To contribute to filling of this gap, we developed a comprehensive R package for the development and validation of diagnostic tests from high-dimensional molecular data. An important focus of the package is a careful validation of the classification results. To this end, we implemented an extended version of the multiple random validation protocol, a validation method that was introduced before. The package includes methods for continuous prediction scores. This is important in a clinical setting, because scores can be converted to probabilities and help to distinguish between clear-cut and borderline classification results. The functionality of the package is illustrated by the analysis of two cancer microarray data sets.
6033112	WOS:000341021200001	AO0SD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	truncSP: An R Package for Estimation of Semi-Parametric Truncated Linear Regression Models	Journal	Article	2014	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000341021200001	Problems with truncated data occur in many areas, complicating estimation and inference. Regarding linear regression models, the ordinary least squares estimator is inconsistent and biased for these types of data and is therefore unsuitable for use. Alternative estimators, designed for the estimation of truncated regression models, have been developed. This paper presents the R package truncSP. The package contains functions for the estimation of semi-parametric truncated linear regression models using three different estimators: the symmetrically trimmed least squares, quadratic mode, and left truncated estimators, all of which have been shown to have good asymptotic and finite sample properties. The package also provides functions for the analysis of the estimated models. Data from the environmental sciences are used to illustrate the functions in the package.
6035060	WOS:000341806500001	AP1DL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	dglars: An R Package to Estimate Sparse Generalized Linear Models	Journal	Article	2014	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-40	40	1	WOS:000341806500001	dglars is a publicly available R package that implements the method proposed in Augugliaro, Mineo, and Wit (2013), developed to study the sparse structure of a generalized linear model. This method, called dgLARS, is based on a differential geometrical extension of the least angle regression method proposed in Efron, Hastie, Johnstone, and Tibshirani (2004). The core of the dglars package consists of two algorithms implemented in Fortran 90 to efficiently compute the solution curve: a predictor-corrector algorithm, proposed in Augugliaro et al. (2013), and a cyclic coordinate descent algorithm, proposed in Augugliaro, Mineo, and Wit (2012). The latter algorithm, as shown here, is significantly faster than the predictor-corrector algorithm. For comparison purposes, we have implemented both algorithms.
6050049	WOS:000338091300015	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Introduction to Time Series Using Stata by Sean Becketti	Journal	Review	2014	1	1	English	STATA JOURNAL	445-448	4	1	WOS:000338091300015	In this article, I review Introduction to Time Series Using Stata by Sean Becketti (2013 [Stata Press]).
6104213	WOS:000347512800011	AY3XM	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Lee (2009) treatment-effect bounds for nonrandom sample selection	Journal	Article	2014	1	1	English	STATA JOURNAL	884-894	11	1	WOS:000347512800011	Nonrandom sample selection may render estimated treatment effects biased even if assignment of treatment is purely random. Lee (2009, Review of Economic Studies, 76: 1071-1102) proposes an estimator for treatment-effect bounds that limit the possible range of the treatment effect. In this approach, the lower and upper bound correspond to extreme assumptions about the missing information that are consistent with the observed data. In contrast to conventional parametric approaches to correcting for sample-selection bias, Lee's bounds estimator rests on very few assumptions. I introduce the new command leebounds, which implements the estimator in Stata. The command allows for several options, such as tightening bounds by using covariates.
6108010	WOS:000332108600001	AB9JJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	R Implementation of a Polyhedral Approximation to a 3D Set of Points Using the alpha-Shape	Journal	Article	2014	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	19	1	WOS:000332108600001	This work presents the implementation in R of the alpha-shape of a finite set of points in the three-dimensional space R-3. This geometric structure generalizes the convex hull and allows to recover the shape of non-convex and even non-connected sets in 3D, given a random sample of points taken into it. Besides the computation of the alpha-shape, the R package alphashape3d provides users with tools to facilitate the three-dimensional graphical visualization of the estimated set as well as the computation of important characteristics such as the connected components or the volume, among others.
6115586	WOS:000338091300007	AK0GQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	From Stata to aML	Journal	Article	2014	1	1	English	STATA JOURNAL	342-362	21	1	WOS:000338091300007	In this article, I explain how to exploit Stata to run multilevel multiprocess regressions with applied maximum likelihood (aML). I show how a single do-file can prepare the dataset, write the control files, input the starting values, and run the regressions without the need to manually open the aML's Command Prompt window. If desired, results can be brought back to Stata for postestimation. I also provide an example that illustrates how well Stata and aML work together.
6117337	WOS:000342606300008	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Space-filling location selection	Journal	Article	2014	1	1	English	STATA JOURNAL	605-622	18	1	WOS:000342606300008	In this article, we describe an implementation of a space-filling location-selection algorithm. The objective is to select a subset from a list of locations so that the spatial coverage of the locations by the selected subset is optimized according to a geometric criterion. Such an algorithm designed for geographical site selection is useful for determining a grid of points that "covers" a data matrix as needed in various nonparametric estimation procedures.
6119020	WOS:000348651700012	CA1CX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Farewell's Linear Increments Model for Missing Data: The FLIM Package	Journal	Article	2014	12	1	English	R JOURNAL	137-150	14	1	WOS:000348651700012	Missing data is common in longitudinal studies. We present a package for Farewell's Linear Increments Model for Missing Data (the FLIM package), which can be used to fit linear models for observed increments of longitudinal processes and impute missing data. The method is valid for data with regular observation patterns. The end result is a list of fitted models and a hypothetical complete dataset corresponding to the data we might have observed had individuals not been missing. The FLIM package may also be applied to longitudinal studies for causal analysis, by considering counterfactual data as missing data - for instance to compare the effect of different treatments when only data from observational studies are available. The aim of this article is to give an introduction to the FLIM package and to demonstrate how the package can be applied.
6125677	WOS:000342606300010	AQ2HL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	csvconvert: A simple command to gather comma-separated value files into Stata	Journal	Article	2014	1	1	English	STATA JOURNAL	662-669	8	1	WOS:000342606300010	This command meets the need of a researcher who holds multiple data files in comma-separated value format differing by a period variable (for example, year or quarter) or by a cross-sectional variable (for example, country or firm) and must combine them into one Stata-format file.
6163923	WOS:000328129600001	267WS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	texreg: Conversion of Statistical Model Output in R to LATEX and HTML Tables	Journal	Article	2013	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000328129600001	A recurrent task in applied statistics is the (mostly manual) preparation of model output for inclusion in LATEX, Microsoft Word, or HTML documents - usually with more than one model presented in a single table along with several goodness-of-fit statistics. However, statistical models in R have diverse object structures and summary methods, which makes this process cumbersome. This article first develops a set of guidelines for converting statistical model output to LATEX and HTML tables, then assesses to what extent existing packages meet these requirements, and finally presents the texreg package as a solution that meets all of the criteria set out in the beginning. After providing various usage examples, a blueprint for writing custom model extensions is proposed.
6168170	WOS:000317249700003	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Sar: Automatic generation of statistical reports using Stata and Microsoft Word for Windows	Journal	Article	2013	1	1	English	STATA JOURNAL	39-64	26	1	WOS:000317249700003	The output provided by most Stata commands is plain text not suitable to be presented or published. After the numerical and graphical outputs are obtained, the user has to copy them into a word processor to complete the editing process. Some Stata commands help you to obtain well-formatted output, especially tabulated results in LATEX or other formats, but they are not a complete solution nor are they friendly tools. Stata automatic report (Sar) is an easy-to-use macro for Microsoft Word for Windows that allows a powerful integration between Stata and Word. With Sar, the user can retrieve numerical results and graphs from Stata and automatically insert them into a well-formatted Word document, exploiting all the functions of Word. This process is managed by Word while Stata is executed in the background. Sar requires Stata commands and some specific Sar commands to be written in ordinary Word comments. Thus the report is well documented, and this can encourage the sharing of the workflow of data analysis and the reproducibility of the research. With Sar, the user can create an automatic report, that is, a Word document that can be automatically updated if data have changed. Sar works only on Windows systems.
6190835	WOS:000326873100001	250SA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Greedy Algorithm for Representative Sampling: repsample in Stata	Journal	Article	2013	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000326873100001	Quantitative empirical analyses of a population of interest usually aim to estimate the causal effect of one or more independent variables on a dependent variable. However, only in rare instances is the whole population available for analysis. Researchers tend to estimate causal effects on a selected sample and generalize their conclusions to the whole population. The validity of this approach rests on the assumption that the sample is representative of the population on certain key characteristics. A study using a non-representative sample is lacking in external validity by failing to minimize population choice bias. When the sample is large and non-response bias is not an issue, a random selection process is adequate to ensure external validity. If that is not the case, however, researchers could follow a more deterministic approach to ensure representativeness on the selected characteristics, provided these are known, or can be estimated, in the parent population. Although such approaches exist for matched sampling designs, research on representative sampling and the similarity between the sample and the parent population seems to be lacking. In this article we propose a greedy algorithm for obtaining a representative sample and quantifying representativeness in Stata.
6205353	WOS:000315019200001	090ZT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fast and Elegant Numerical Linear Algebra Using the RcppEigen Package	Journal	Article	2013	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000315019200001	The RcppEigen package provides access from R (R Core Team 2012a) to the Eigen (Guennebaud, Jacob, and others 2012) C++ template library for numerical linear algebra. Rcpp (Eddelbuettel and Francois 2011, 2012) classes and specializations of the C++ templated functions as and wrap from Rcpp provide the "glue" for passing objects from R to C++ and back. Several introductory examples are presented. This is followed by an in-depth discussion of various available approaches for solving least-squares problems, including rank-revealing methods, concluding with an empirical run-time comparison. Last but not least, sparse matrix methods are discussed.
6209993	WOS:000325947500001	238LH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Warping Functional Datain Rand C via a Bayesian Multi resolution Approach	Journal	Article	2013	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	22	1	WOS:000325947500001	Phase variation in functional data obscures the true amplitude variation when a typical cross-sectional analysis of these responses would be performed. Time warping or curve registration aims at eliminating the phase variation, typically by applying transformations, the warping functions tau(n), to the function arguments. We propose a warping method that jointly estimates a decomposition of the warping function in warping components, and amplitude components. For the estimation routine, adaptive MCMC calculations are performed and implemented in C rather than R to increase computational speed. The R-C interface makes the program user-friendly, in that no knowledge of C is required and all input and output will be handled through R. The R package MRwarping contains all needed files.
6246306	WOS:000321944400002	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	RTextTools: A Supervised Learning Package for Text Classification	Journal	Article	2013	6	1	English	R JOURNAL	6-12	7	1	WOS:000321944400002	Social scientists have long hand-labeled texts to create datasets useful for studying topics from congressional policymaking to media reporting. Many social scientists have begun to incorporate machine learning into their toolkits. RTextTools was designed to make machine learning accessible by providing a start-to-finish product in less than 10 steps. After installing RTextTools, the initial step is to generate a document term matrix. Second, a container object is created, which holds all the objects needed for further analysis. Third, users can use up to nine algorithms to train their data. Fourth, the data are classified. Fifth, the classification is summarized. Sixth, functions are available for performance evaluation. Seventh, ensemble agreement is conducted. Eighth, users can cross-validate their data. Finally, users write their data to a spreadsheet, allowing for further manual coding if required.
6287238	WOS:000324372200001	217PR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	survPresmooth: An R Package for Presmoothed Estimation in Survival Analysis	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000324372200001	The survPresmooth package for R implements nonparametric presmoothed estimators of the main functions studied in survival analysis (survival, density, hazard and cumulative hazard functions). Presmoothed versions of the classical nonparametric estimators have been shown to increase efficiency if the presmoothing bandwidth is suitably chosen. The survPresmooth package provides plug-in and bootstrap bandwidth selectors, also allowing the possibility of using fixed bandwidths.
6302062	WOS:000330193300010	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Dynamic Parallelization of R Functions	Journal	Article	2013	12	1	English	R JOURNAL	89-97	9	1	WOS:000330193300010	R offers several extension packages that allow it to perform parallel computations. These operate on fixed points in the program flow and make it difficult to deal with nested parallelism and to organize parallelism in complex computations in general. In this article we discuss, first, of how to detect parallelism in functions, and second, how to minimize user intervention in that process. We present a solution that requires minimal code changes and enables to flexibly and dynamically choose the degree of parallelization in the resulting computation. An implementation is provided by the R package parallelize. dynamic and practical issues are discussed with the help of examples. The R language (Ihaka and Gentleman, 1996) can be used to program in the functional paradigm, i.e. return values of functions only depend on their arguments and values of variables bound at the moment of function definition. Assuming a functional R program, it follows that calls to a given set of functions are independent as long as their arguments do not involve return values of each other. This property of function calls can be exploited and several R packages allow to compute function calls in parallel, e. g. packages parallel, Rsge (Bode, 2012) or for each (Michael et al., 2013; Revolution Analytics and Weston, 2013). A natural point in the program flow where to employ parallelization is where use of the apply-family of functions is made. These functions take a single function (here called the compute-function) as their first argument together with a set of values as their second argument (here called the compute-arguments) each member of which is passed to the compute-function. The calling mechanism guarantees that function calls cannot see each others return values and are thereby independent. This family includes the apply, sapply, lapply, and tapply functions called generically Apply in the following. Examples of packages helping to parallelize Apply functions include parallel and Rsge among others and we will focus on these functions in this article as well. In these packages, a given Apply function is replaced by a similar function from the package that performs the same computation in a parallel way. Fixing a point of parallelism introduces some potential problems. For example, the bootstrap package boot (Davison and Hinkley, 1997; Canty and Ripley, 2013) allows implicit use of the parallel package. If bootstrap computations become nested within larger computations the parallelization option of the boot function potentially has to be changed to allow parallelization at a higher level once the computation scenario changes. In principle, the degree of parallelism could depend on parameter values changing between computations thereby making it difficult to choose an optimal code point at which to parallelize. Another shortcoming of existing solutions is that only a single Apply function gets parallelized thereby ignoring parallelism that spans different Apply calls
6304488	WOS:000316316900001	108SR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MIXREGLS: A Program for Mixed-Effects Location Scale Analysis	Journal	Article	2013	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-38	38	1	WOS:000316316900001	MIXREGLS is a program which provides estimates for a mixed-effects location scale model assuming a (conditionally) normally-distributed dependent variable. This model can be used for analysis of data in which subjects may be measured at many observations and interest is in modeling the mean and variance structure. In terms of the variance structure, covariates can by specified to have effects on both the between-subject and within-subject variances. Another use is for clustered data in which subjects are nested within clusters (e.g., clinics, hospitals, schools, etc.) and interest is in modeling the between-cluster and within-cluster variances in terms of covariates. MIXREGLS was written in Fortran and uses maximum likelihood estimation, utilizing both the EM algorithm and a Newton-Raphson solution. Estimation of the random effects is accomplished using empirical Bayes methods. Examples illustrating stand-alone usage and features of MIXREGLS are provided, as well as use via the SAS and R software packages.
6312847	WOS:000325442200010	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Financial portfolio selection using the multifactor capital asset pricing model and imported options data	Journal	Article	2013	1	1	English	STATA JOURNAL	603-617	15	1	WOS:000325442200010	Diversification and portfolio selection are integral parts of a finance curriculum. In this article, a multifactor capital asset pricing model is fit for components of the Dow Jones Composite Index using data from Yahoo! Finance. Along with the capital asset pricing model's Beta, other statistics that are common criteria for portfolio selection are calculated: historic standard deviation (total risk), total return, average daily return, and Sharpe and Treynor measures. Two new commands are introduced, fetchcomponents and fetchportfolio, that automate the entire process. A third new command, fetchyahoooptions, is provided to download and parse equity options data from Yahoo! Finance webpages and, optionally, to calculate the implied volatilities for the downloaded options.
6319083	WOS:000322051800001	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Maximum likelihood and generalized spatial two-stage least-squares estimators for a spatial-autoregressive model with spatial-autoregressive disturbances	Journal	Article	2013	1	1	English	STATA JOURNAL	221-241	21	1	WOS:000322051800001	We describe the spreg command, which implements a maximum likelihood estimator and a generalized spatial two-stage least-squares estimator for the parameters of a linear cross-sectional spatial-autoregressive model with spatial-autoregressive disturbances.
6329412	WOS:000329680600007	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Testing for zero inflation in count models: Bias correction for the Vuong test	Journal	Article	2013	1	1	English	STATA JOURNAL	810-835	26	1	WOS:000329680600007	The proportion of zeros in event-count processes may be inflated by an additional mechanism by which zeros are created. This has given rise to statistical models that accommodate zero inflation; these are available in Stata through the zip and zinb commands. The Vuong (1989, Econometrica 57: 307-333) test is regularly used to determine whether estimating a zero-inflation component is appropriate or whether a single-equation count model should be used. The use of the Vuong test in this case is complicated by the fact that zero-inflated models involve the estimation of several more parameters than the single-equation models. Although Vuong (1989, Econometrica 57: 307-333) suggested corrections to the test statistic to address the comparison of models with different numbers of parameters, Stata does not implement any such correction. The result is that the Vuong test used by Stata is biased toward supporting the model with a zero-inflation component, even when no zero inflation exists in the generative process. We provide new Stata commands for computing the Vuong statistic with corrections based on the Akaike and Bayesian (Schwarz) information criteria. In an extensive Monte Carlo study, we illustrate the bias inherent in using the uncorrected Vuong test, and we examine the relative merits of the Akaike and Schwarz corrections. Then, in an empirical example from international relations research, we show that errors in selecting an event-count model can have clear implications for substantive conclusions.
6341584	WOS:000323909600001	211LR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	dbEmpLikeGOF: An R Package for Nonparametric Likelihood Ratio Tests for Goodness-of-Fit and Two-Sample Comparisons Based on Sample Entropy	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000323909600001	We introduce and examine dbEmpLikeGOF, an R package for performing goodness-of-fit tests based on sample entropy. This package also performs the two sample distribution comparison test. For a given vector of data observations, the provided function dbEmpLikeGOF tests the data for the proposed null distributions, or tests for distribution equality between two vectors of observations. The proposed methods represent a distribution-free density-based empirical likelihood technique applied to nonparametric testing. The proposed procedure performs exact and very efficient p values for each test statistic obtained from a Monte Carlo (MC) resampling scheme. Note by using an MC scheme, we are assured exact level alpha tests that approximate nonparametrically most powerful Neyman-Pearson decision rules. Although these entropy based tests are known in the theoretical literature to be very efficient, they have not been well addressed in statistical software. This article briefly presents the proposed tests and introduces the package, with applications to real data. We apply the methods to produce a novel analysis of a recently published dataset related to coronary heart disease.
6350378	WOS:000318236800001	134TF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fast and Robust Bootstrap for Multivariate Inference: The R Package FRB	Journal	Article	2013	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000318236800001	We present the FRB package for R, which implements the fast and robust bootstrap. This method constitutes an alternative to ordinary bootstrap or asymptotic inference procedures when using robust estimators such as S-, MM- or GS-estimators. The package considers three multivariate settings: principal components analysis, Hotelling tests and multivariate regression. It provides both the robust point estimates and uncertainty measures based on the fast and robust bootstrap. In this paper we give some background on the method, discuss the implementation and provide various examples.
6365591	WOS:000317249700002	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Versatile sample-size calculation using simulation	Journal	Article	2013	1	1	English	STATA JOURNAL	21-38	18	1	WOS:000317249700002	I present a new Stata command, simsam, that uses simulation to determine the sample size required to achieve a given statistical power for any hypothesis test under any probability model that can be programmed in Stata. simsam returns the smallest sample size (or smallest multiple of 5, 10, or some other user-specified increment) so that the estimated power exceeds the target. The user controls the precision of the power estimate, and power is reported with a confidence interval. The sample size returned is reliable to the extent that if simsam is repeated, it will, nearly every time, give a sample size no more than one increment away.
6378300	WOS:000325442200004	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Computing adjusted risk ratios and risk differences in Stata	Journal	Article	2013	1	1	English	STATA JOURNAL	492-509	18	1	WOS:000325442200004	In this article, we explain how to calculate adjusted risk ratios and risk differences when reporting results from logit, probit, and related nonlinear models. Building on Stata's margins command, we create a new postestimation command, adjrr, that calculates adjusted risk ratios and adjusted risk differences after running a logit or probit model with a binary, a multinomial, or an ordered outcome. adjrr reports the point estimates, delta-method standard errors, and 95% confidence intervals and can compute these for specific values of the variable of interest. It automatically adjusts for complex survey design as in the fit model. Data from the Medical Expenditure Panel Survey and the National Health and Nutrition Examination Survey are used to illustrate multiple applications of the command.
6383294	WOS:000330193300007	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	ExactCIdiff: An R Package for Computing Exact Confidence Intervals for the Difference of Two Proportions	Journal	Article	2013	12	1	English	R JOURNAL	63-71	9	1	WOS:000330193300007	Comparing two proportions through the difference is a basic problem in statistics and has applications in many fields. More than twenty confidence intervals (Newcombe, 1998a,b) have been proposed. Most of them are approximate intervals with an asymptotic infimum coverage probability much less than the nominal level. In addition, large sample may be costly in practice. So exact optimal confidence intervals become critical for drawing valid statistical inference with accuracy and precision. Recently, Wang (2010, 2012) derived the exact smallest (optimal) one-sided 1 - alpha confidence intervals for the difference of two paired or independent proportions. His intervals, however, are computer-intensive by nature. In this article, we provide an R package ExactCIdiff to implement the intervals when the sample size is not large. This would be the first available package in R to calculate the exact confidence intervals for the difference of proportions. Exact two-sided 1 - alpha interval can be easily obtained by taking the intersection of the lower and upper one-sided 1 - alpha/2 intervals. Readers may jump to Examples 1 and 2 to obtain these intervals.
6399954	WOS:000328131500001	267XK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	CARBayes: An R Package for Bayesian Spatial Modeling with Conditional Autoregressive Priors	Journal	Article	2013	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000328131500001	Conditional autoregressive models are commonly used to represent spatial autocorrelation in data relating to a set of non-overlapping areal units, which arise in a wide variety of applications including agriculture, education, epidemiology and image analysis. Such models are typically specified in a hierarchical Bayesian framework, with inference based on Markov chain Monte Carlo (MCMC) simulation. The most widely used software to fit such models is WinBUGS or OpenBUGS, but in this paper we introduce the R package CARBayes. The main advantage of CARBayes compared with the BUGS software is its ease of use, because: (1) the spatial adjacency information is easy to specify as a binary neighbourhood matrix; and (2) given the neighbourhood matrix the models can be implemented by a single function call in R. This paper outlines the general class of Bayesian hierarchical models that can be implemented in the CARBayes software, describes their implementation via MCMC simulation techniques, and illustrates their use with two worked examples in the fields of house price analysis and disease mapping.
6401742	WOS:000324372900001	217PX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Estimation of Social Exclusion Indicators from Complex Surveys: The R Package laeken	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000324372900001	Units sampled from finite populations typically come with different inclusion probabilities. Together with additional preprocessing steps of the raw data, this yields unequal sampling weights of the observations. Whenever indicators are estimated from such complex samples, the corresponding sampling weights have to be taken into account. In addition, many indicators suffer from a strong influence of outliers, which are a common problem in real-world data. The R package laeken is an object-oriented toolkit for the estimation of indicators from complex survey samples via standard or robust methods. In particular the most widely used social exclusion and poverty indicators are implemented in the package. A general calibrated bootstrap method to estimate the variance of indicators for common survey designs is included as well. Furthermore, the package contains synthetically generated close-to-reality data for the European Union Statistics on Income and Living Conditions and the Structure of Earnings Survey, which are used in the code examples throughout the paper. Even though the paper is focused on showing the functionality of package laeken, it also provides a brief mathematical description of the implemented indicator methodology.
6423441	WOS:000326872500001	250RV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	BiDimRegression: Bidimensional Regression Modeling Using R	Journal	Article	2013	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-11	11	1	WOS:000326872500001	Tobler (1965) introduced bidimensional regression to the research field of geography in 1965 to provide a method for estimating mapping relations between two planes on the basis of regression modeling. The bidimensional regression method has been widely used within geographical research. However, the applicability in assessing the degree of similarity of two-dimensional patterns has not much explored in the area of psychological research, particularly in the domains of cognitive maps, face research and comparison of 2D-data patterns. Describing Tobler's method in detail, Friedman and Kohler (2003) made an attempt to bridge the gulf between geographical methodological knowledge and psychological research practice. Still, the method has not been incorporated into psychologists' standard methodical repertoire to date. The present paper aims to make bidimensional regression applicable also for researchers and users unfamiliar with its theoretical basis. The BiDimRegression function provides a manageable computing option for bidimensional regression models with affine and Euclidean transformation, which makes it easy to assess the similarity of any planar configuration of points. Typical applications are, for instance, assessments of the similarity of facial images defined by discrete features or of (cognitive) maps characterized by landmarks. BiDimRegression can be a valuable tool since it provides estimation, statistical inference, and goodness-of-fit measures for bidimensional regression.
6437069	WOS:000316316400001	108SM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Recursive Numerical Evaluation of the Cumulative Bivariate Normal Distribution	Journal	Article	2013	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000316316400001	We propose an algorithm for evaluation of the cumulative bivariate normal distribution, building upon Marsaglia's ideas for evaluation of the cumulative univariate normal distribution. The algorithm delivers competitive performance and can easily be extended to arbitrary precision.
6445853	WOS:000317249700014	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Flexible Parametric Survival Analysis Using Stata: Beyond the Cox Model by Patrick Royston and Paul C. Lambert	Journal	Review	2013	1	1	English	STATA JOURNAL	212-216	5	1	WOS:000317249700014	In this article, I review Flexible Parametric Survival Analysis Using Stata: Beyond the Cox Model, by Patrick Royston and Paul C. Lambert (2011 [Stata Press]).
6446300	WOS:000329680600011	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A score test for group comparisons in single-index models	Journal	Article	2013	1	1	English	STATA JOURNAL	876-883	8	1	WOS:000329680600011	In this article, I derive a score test for the equality of one or more parameters across groups of observations following estimation of a single-index model. The test has a wide array of applications and nests Pearson's chi-squared test as a particular case. The postestimation command scoregrp implements the test and works with logit, logistic, probit, poisson, or regress (see [R] logit, [R] logistic, [R] probit, [R] poisson, and [R] regress). Finally, I show some applications of the test.
6447409	WOS:000315019100001	090ZS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	lgcp: Inference with Spatial and Spatio-Temporal Log-Gaussian Cox Processes in R	Journal	Article	2013	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	40	1	WOS:000315019100001	This paper introduces an R package for spatial and spatio-temporal prediction and forecasting for log-Gaussian Cox processes. The main computational tool for these models is Markov chain Monte Carlo (MCMC) and the new package, lgcp, therefore also provides an extensible suite of functions for implementing MCMC algorithms for processes of this type. The modelling framework and details of inferential procedures are first presented before a tour of lgcp functionality is given via a walk-through data-analysis. Topics covered include reading in and converting data, estimation of the key components and parameters of the model, specifying output and simulation quantities, computation of Monte Carlo expectations, post-processing and simulation of data sets.
6448888	WOS:000320040300001	159IY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	New Approaches in Visualization of Categorical Data: R Package extracat	Journal	Article	2013	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000320040300001	The R package extracat provides two new graphical methods for displaying categorical data extending the concepts of multiple barcharts and parallel coordinates plots. The first method called rmb plot uses a crossover of mosaicplots and multiple barcharts to display the frequencies of a data table split up into conditional relative frequencies of one target variable and the absolute frequencies of the corresponding combinations of the remaining explanatory variables. It provides a well-structured representation of the data which is easy to interpret and allows precise comparisons. The graphic can additionally be used as a generalization of spineplots or with barcharts for the conditional relative frequencies. Several options, including ceiling censored zooming, residual shadings and a choice of color palettes, are provided. An interactive version based on the R package iWidgets is also presented. The second graphic cpcp uses the interactive parallel coordinates plots in the iplots package to visualize categorical data. Sequences of points are used to represent each of the variable categories, while ordering algorithms are applied to represent a hierarchical structure in the data and keep the arrangement clear. This interactive graphic is well-suited for exploratory analysis and allows a visual interpretation even for a higher number of variables and a mixture of categorical and numeric scales.
6457546	WOS:000321944400020	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Possible Directions for Improving Dependency Versioning in R	Journal	Article	2013	6	1	English	R JOURNAL	197-206	10	1	WOS:000321944400020	One of the most powerful features of R is its infrastructure for contributed code. The built-in package manager and complementary repositories provide a great system for development and exchange of code, and have played an important role in the growth of the platform towards the de-facto standard in statistical computing that it is today. However, the number of packages on CRAN and other repositories has increased beyond what might have been foreseen, and is revealing some limitations of the current design. One such problem is the general lack of dependency versioning in the infrastructure. This paper explores this problem in greater detail, and suggests approaches taken by other open source communities that might work for R as well. Three use cases are defined that exemplify the issue, and illustrate how improving this aspect of package management could increase reliability while supporting further growth of the R community.
6467725	WOS:000320040800001	159JD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for R and Python	Journal	Article	2013	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000320040800001	The fastcluster package is a C++ library for hierarchical, agglomerative clustering. It provides a fast implementation of the most efficient, current algorithms when the input is a dissimilarity index. Moreover, it features memory-saving routines for hierarchical clustering of vector data. It improves both asymptotic time complexity (in most cases) and practical performance (in all cases) compared to the existing implementations in standard software: several R packages, MATLAB, Mathematica, Python with SciPy. The fastcluster package presently has interfaces to R and Python. Part of the functionality is designed as a drop-in replacement for the methods hclust and flashClust in R and scipy. cluster. hierarchy. linkage in Python, so that existing programs can be effortlessly adapted for improved performance.
6478596	WOS:000325442200007	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Copula-based maximum-likelihood estimation of sample-selection models	Journal	Article	2013	1	1	English	STATA JOURNAL	547-573	27	1	WOS:000325442200007	I describe the commands heckmancopula and switchcopula, which implement copula-based maximum-likelihood estimations of sample-selection models.
6489908	WOS:000322051800006	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Generating Manhattan plots in Stata	Journal	Article	2013	1	1	English	STATA JOURNAL	323-328	6	1	WOS:000322051800006	Genome-wide association studies hold the potential for discovering genetic causes for a wide range of diseases, traits, and behaviors. However, the incredible amount of data handling, advanced statistics, and visualization have made conducting these studies difficult for researchers. Here we provide a tool, manhattan, for helping investigators easily visualize genome-wide association studies data in Stata.
6554243	WOS:000321944400017	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	beadarrayFilter: An R Package to Filter Beads	Journal	Article	2013	6	1	English	R JOURNAL	171-180	10	1	WOS:000321944400017	Microarrays enable the expression levels of thousands of genes to be measured simultaneously. However, only a small fraction of these genes are expected to be expressed under different experimental conditions. Nowadays, filtering has been introduced as a step in the microarray preprocessing pipeline. Gene filtering aims at reducing the dimensionality of data by filtering redundant features prior to the actual statistical analysis. Previous filtering methods focus on the Affymetrix platform and can not be easily ported to the Illumina platform. As such, we developed a filtering method for Illumina bead arrays. We developed an R package, beadarrayFilter, to implement the latter method. In this paper, the main functions in the package are highlighted and using many examples, we illustrate how beadarrayFilter can be used to filter bead arrays.
6612412	WOS:000330193300014	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	On Sampling from the Multivariate t Distribution	Journal	Article	2013	12	1	English	R JOURNAL	130-137	8	1	WOS:000330193300014	The multivariate normal and the multivariate t distributions belong to the most widely used multivariate distributions in statistics, quantitative risk management, and insurance. In contrast to the multivariate normal distribution, the parameterization of the multivariate t distribution does not correspond to its moments. This, paired with a non-standard implementation in the R package mvtnorm, provides traps for working with the multivariate t distribution. In this paper, common traps are clarified and corresponding recent changes to mvtnorm are presented.
6620883	WOS:000321944400015	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	ggmap: Spatial Visualization with ggplot2	Journal	Article	2013	6	1	English	R JOURNAL	144-161	18	1	WOS:000321944400015	In spatial statistics the ability to visualize data and models superimposed with their basic social landmarks and geographic context is invaluable. ggmap is a new tool which enables such visualization by combining the spatial information of static maps from Google Maps, OpenStreetMap, Stamen Maps or CloudMade Maps with the layered grammar of graphics implementation of ggplot2. In addition, several new utility functions are introduced which allow the user to access the Google Geocoding, Distance Matrix, and Directions APIs. The result is an easy, consistent and modular framework for spatial graphics with several convenient tools for spatial data analysis.
6621928	WOS:000329680600004	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Flexible parametric illness-death models	Journal	Article	2013	1	1	English	STATA JOURNAL	759-775	17	1	WOS:000329680600004	It is usual in time-to-event data to have more than one event of interest, for example, time to death from different causes. Competing risks models can be applied in these situations where events are considered mutually exclusive absorbing states. That is, we have some initial state for example, alive with a diagnosis of cancer and we are interested in several different endpoints, all of which are final. However, the progression of disease will usually consist of one or more intermediary events that may alter the progression to an endpoint. These events are neither initial states nor absorbing states. Here we consider one of the simplest multistate models, the illness-death model. stpm2illd is a postestimation command used after fitting a flexible parametric survival model with stpm2 to estimate the probability of being in each of four states as a function of time. There is also the option to generate confidence intervals and transition hazard functions. The new command is illustrated through a simple example.
6642326	WOS:000323909400001	211LP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	adabag: An R Package for Classification with Boosting and Bagging	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-35	35	1	WOS:000323909400001	Boosting and bagging are two widely used ensemble methods for classification. Their common goal is to improve the accuracy of a classifier combining single classifiers which are slightly better than random guessing. Among the family of boosting algorithms, AdaBoost (adaptive boosting) is the best known, although it is suitable only for dichotomous tasks. AdaBoost.M1 and SAMME (stagewise additive modeling using a multi-class exponential loss function) are two easy and natural extensions to the general case of two or more classes. In this paper, the adabag R package is introduced. This version implements AdaBoost.M1, SAMME and bagging algorithms with classification trees as base classifiers. Once the ensembles have been trained, they can be used to predict the class of new samples. The accuracy of these classifiers can be estimated in a separated data set or through cross validation. Moreover, the evolution of the error as the ensemble grows can be analysed and the ensemble can be pruned. In addition, the margin in the class prediction and the probability of each class for the observations can be calculated. Finally, several classic examples in classification literature are shown to illustrate the use of this package.
6689914	WOS:000325442200009	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Distribution-free estimation of heteroskedastic binary response models in Stata	Journal	Article	2013	1	1	English	STATA JOURNAL	588-602	15	1	WOS:000325442200009	In this article, we consider two recently proposed semiparametric estimators for distribution-free binary response models under a conditional median restriction. We show that these estimators can be implemented in Stata by using the n1 command through simple modifications to the nonlinear least-squares probit criterion function. We then introduce dfbr, a new Stata command that implements these estimators, and provide several examples of its usage. Although it is straightforward to carry out the estimation with n1, the dfbr implementation uses Mata for improved performance and robustness.
6701056	WOS:000325948000001	238LM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Performing the Kernel Method of Test Equating with the Package kequate	Journal	Article	2013	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	25	1	WOS:000325948000001	In standardized testing it is important to equate tests in order to ensure that the test takers, regardless of the test version given, obtain a fair test. Recently, the kernel method of test equating, which is a conjoint framework of test equating, has gained popularity. The kernel method of test equating includes five steps: (1) pre-smoothing, (2) estimation of the score probabilities, (3) continuization, (4) equating, and (5) computing the standard error of equating and the standard error of equating difference. Here, an implementation has been made for six different equating designs: equivalent groups, single group, counter balanced, non-equivalent groups with anchor test using either chain equating or post-stratification equating, and non-equivalent groups using covariates. An R package for the kernel method of test equating called kequate is presented. Included in the package are also diagnostic tools aiding in the search for a proper log-linear model in the pre-smoothing step for use in conjunction with the R function glm
6704325	WOS:000315019500001	090ZW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	moult: An R Package to Analyze Moult in Birds	Journal	Article	2013	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000315019500001	Moult is the process by which birds replace their feathers. It is a costly process in terms of energy and reduced flight ability but necessary for the maintenance of the plumage and its functions. Because birds generally avoid to moult while engaged with other energy demanding activities such as breeding and migration, the analysis of moult data gives insight into how birds fit this life stage into the annual cycle, on time constraints in the annual cycle, and on the effects of environmental variables on the timing of moult. The analysis of moult data requires non-standard statistical techniques. More than 20 years ago Underhill and Zucchini developed a likelihood approach for estimating duration, mean start date and variation in start date of a population of moulting birds. However, use of these models has been limited, mainly due to the lack of user-friendly software. The moult package for R implements the Underhill-Zucchini models, allowing the user to specify moult models in a regression type formula. In addition the functions allow the moult parameters (duration, and mean and variation in start date) to depend on explanatory variables. We here describe the package, give a brief summary of the theory and illustrate the models on two data sets included in the package.
6707967	WOS:000329680600005	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Implementation of a double-hurdle model	Journal	Article	2013	1	1	English	STATA JOURNAL	776-794	19	1	WOS:000329680600005	Corner solution responses are frequently observed in the social sciences. One common approach to model phenomena that give rise to corner solution responses is to use the tobit model. If the decision to participate in the market is decoupled from the consumption amount decision, then the tobit model is inappropriate. In these cases, the double-hurdle model presented in Cragg (1971, Econometrica 39: 829-844) is an appropriate alternative to the tobit model. In this article; I introduce a command, dblhurdle, that fits the double-hurdle model. The implementation allows the errors of the participation decision and the amount decision to be correlated. The capabilities of predict after dblhurdle are also discussed.
6709138	WOS:000322051800007	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Semiparametric fixed-effects estimator	Journal	Article	2013	1	1	English	STATA JOURNAL	329-336	8	1	WOS:000322051800007	In this article, we describe the Stata implementation of Baltagi and Li's (2002, Annals of Economics and Finance 3: 103-116) series estimator of partially linear panel-data models with fixed effects. After a brief description of the estimator itself, we describe the new command xtsemipar. We then simulate data to show that this estimator performs better than a fixed-effects estimator if the relationship between two variables is unknown or quite complex.
6723000	WOS:000325442200006	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Valid tests when instrumental variables do not perfectly satisfy the exclusion restriction	Journal	Article	2013	1	1	English	STATA JOURNAL	528-546	19	1	WOS:000325442200006	There is a growing consensus that it is difficult to pick instruments that perfectly satisfy the exclusion restriction. Drawing on results from Berkowitz, Caner, and Fang (2012, Journal of Econometrics 166: 255-266), we provide in this article a nontechnical summary of how valid inferences can be made when instrumental variables come close to satisfying the exclusion restriction. Although the Anderson-Rubin (1949, Annals of Mathematical Statistics 20: 46-63) test statistic is robust to weak identification, it assumes that the instruments are perfectly orthogonal to the structural error term and is therefore oversized under mild violations of the orthogonality condition. The fractionally resampled Anderson-Rubin (FAR) test is a modification of the Anderson-Rubin test that accounts for violations of the orthogonality condition. We show that in small samples, the size of the resampling block of the FAR test can be modified to obtain valid critical values and analyze its size and power properties. We focus on power and not on size-adjusted power because the FAR test uses only one critical value in its application. We also describe user-written commands to implement the Anderson-Rubin and FAR tests in Stata.
6734295	WOS:000322051800003	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A command for estimating spatial-autoregressive models with spatial-autoregressive disturbances and additional endogenous variables	Journal	Article	2013	1	1	English	STATA JOURNAL	287-301	15	1	WOS:000322051800003	We describe the spivreg command, which estimates the parameters of linear cross-sectional spatial-autoregressive models with spatial-autoregressive disturbances, where the model may also contain additional endogenous variables as well as exogenous variables. spivreg uses results and the literature cited in Kelejian and Prucha (1998, Journal of Real Estate Finance and Economics 17: 99-121; 1999, International Economic Review 40: 509-533; 2004, Journal of Econometrics 118: 27-50; 2010, Journal of Econometrics 157: 53-67); Arraiz et al. (2010, Journal of Regional Science 50: 592-614); and Drukker, Egger, and Prucha (2013, Econometric Reviews 32: 686-733).
6741071	WOS:000325947700001	238LJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Logarithmic Transformation-Based Gamma Random Number Generators	Journal	Article	2013	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000325947700001	Developing efficient gamma variate generators is important for Monte Carlo methods. With a brief review of existing methods for generating gamma random numbers, this article proposes two simple gamma variate generators that are obtained from the ratio-of-uniforms method and based on two logarithmic transformations of the gamma random variable. One transformation allows for the generators to work for all shape parameter values. The other is introduced to have improved efficiency for shape parameters smaller than or equal to one.
6756100	WOS:000317249700012	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Doubly robust estimation in generalized linear models	Journal	Article	2013	1	1	English	STATA JOURNAL	185-205	21	1	WOS:000317249700012	A common aim of epidemiological research is to assess the association between a particular exposure and a particular outcome, controlling for a set of additional covariates. This is often done by using a regression model for the outcome, conditional on exposure and covariates. A commonly used class of models is the generalized linear models. The model parameters are typically estimated through maximum likelihood. If the model is correct, then the maximum likelihood estimator is consistent but may otherwise be inconsistent. Recently, a new class of estimators known as doubly robust estimators has been proposed. These estimators use two regression models, one for the outcome and one for the exposure, and are consistent if either model is correct, not necessarily both. Thus doubly robust estimators give the analyst two chances instead of only one to make valid inference. In this article, we describe a new Stata command, drglm, that implements the most common doubly robust estimators for generalized linear models.
6757948	WOS:000328131100001	267XG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	EMMIXuskew: An R Package for Fitting Mixtures of Multivariate Skew t Distributions via the EM Algorithm	Journal	Article	2013	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000328131100001	This paper describes an algorithm for fitting finite mixtures of unrestricted Multivariate Skew t (FM-uMST) distributions. The package EMMIXuskew implements a closed-form expectation-maximization (EM) algorithm for computing the maximum likelihood (ML) estimates of the parameters for the (unrestricted) FM-MST model in R. EMMIXuskew also supports visualization of fitted contours in two and three dimensions, and random sample generation from a specified FM-uMST distribution. Finite mixtures of skew t distributions have proven to be useful in modelling heterogeneous data with asymmetric and heavy tail behaviour, for example, datasets from flow cytometry. In recent years, various versions of mixtures with multivariate skew t (MST) distributions have been proposed. However, these models adopted some restricted characterizations of the component MST distributions so that the E-step of the EM algorithm can be evaluated in closed form. This paper focuses on mixtures with unrestricted MST components, and describes an iterative algorithm for the computation of the ML estimates of its model parameters. Its implementation in R is presented with the package EMMIXuskew. The usefulness of the proposed algorithm is demonstrated in three applications to real datasets. The first example illustrates the use of the main function fmmst in the package by fitting a MST distribution to a bivariate unimodal flow cytometric sample. The second example fits a mixture of MST distributions to the Australian Institute of Sport (AIS) data, and demonstrates that EMMIXuskew can provide better clustering results than mixtures with restricted MST components. In the third example, EMMIXuskew is applied to classify cells in a trivariate flow cytometric dataset. Comparisons with some other available methods suggest that EMMIXuskew achieves a lower misclassification rate with respect to the labels given by benchmark gating analysis.
6861746	WOS:000321944400014	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Estimating Spatial Probit Models in R	Journal	Article	2013	6	1	English	R JOURNAL	130-143	14	1	WOS:000321944400014	In this article we present the Bayesian estimation of spatial probit models in R and provide an implementation in the package spatialprobit. We show that large probit models can be estimated with sparse matrix representations and Gibbs sampling of a truncated multivariate normal distribution with the precision matrix. We present three examples and point to ways to achieve further performance gains through parallelization of the Markov Chain Monte Carlo approach.
6872084	WOS:000322051800008	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Exact Wilcoxon signed-rank and Wilcoxon Mann-Whitney ranksum tests	Journal	Article	2013	1	1	English	STATA JOURNAL	337-343	7	1	WOS:000322051800008	We present new Stata commands for carrying out exact Wilcoxon one-sample and two-sample comparisons of the median. Nonparametric tests are often used in clinical trials, in which it is not uncommon to have small samples. In such situations, researchers are accustomed to making inferences by using exact statistics. The ranksum and signrank commands in Stata provide only asymptotic results, which assume normality. Because large-sample results are unacceptable in many clinical trials studies, these researchers must use other software packages. To address this, we have developed new commands for Stata that provide exact statistics in small samples. Additionally, when samples are large, we provide results based on the Student's t distribution that outperform those based on the normal distribution.
6893428	WOS:000325442200012	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	lclogit: A Stata command for fitting latent-class conditional logit models via the expectation-maximization algorithm	Journal	Article	2013	1	1	English	STATA JOURNAL	625-639	15	1	WOS:000325442200012	In this article, we describe lclogit, a Stata command for fitting a discrete-mixture or latent-class logit model via the expectation-maximization algorithm.
6907275	WOS:000325442200005	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	marginscontplot: Plotting the marginal effects of continuous predictors	Journal	Article	2013	1	1	English	STATA JOURNAL	510-527	18	1	WOS:000325442200005	I provide a new tool (marginscontplot) for plotting the marginal effect of continuous covariates in regression models. The plots may be univariate or according to levels or user-selected values of a second covariate. Nonlinear relationships involving transformed covariates may be plotted on the original scale.
6911532	WOS:000325947800001	238LK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Converting Odds Ratio to Relative Risk in Cohort Studies with Partial Data Information	Journal	Article	2013	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	11	1	WOS:000325947800001	In medical and epidemiological studies, the odds ratio is a commonly applied measure to approximate the relative risk or risk ratio in cohort studies. It is well known such an approximation is poor and can generate misleading conclusions, if the incidence rate of a study outcome is not rare. However, there are times when the incidence rate is not directly available in the published work. Motivated by real applications, this paper presents methods to convert the odds ratio to the relative risk when published data offers limited information. Specifically, the proposed new methods can convert the odds ratio to the relative risk, if an odds ratio and/or a confidence interval as well as the sample sizes for the treatment and control group are available. In addition, the developed methods can be utilized to approximate the relative risk based on the adjusted odds ratio from logistic regression or other multiple regression models. In this regard, this paper extends a popular method by Zhang and Yu (1998) for converting odds ratios to risk ratios. The objective is novelly mapped into a constrained nonlinear optimization problem, which is solved with both a grid search and nonlinear optimization algorithm. The methods are implemented in R package orsk (Wang 2013) which contains R functions and a Fortran subroutine for efficiency. The proposed methods and software are illustrated with real data applications.
6928937	WOS:000317249700013	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Data Analysis Using Stata, Third Edition, by Kohler and Kreuter	Journal	Review	2013	1	1	English	STATA JOURNAL	206-211	6	1	WOS:000317249700013	This article reviews Data Analysis Using Stata, Third Edition, by Ulrich Kohler and Frauke Kreuter (2012 [Stata Press]).
6931927	WOS:000321944400013	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Let Graphics Tell the Story - Datasets in R	Journal	Article	2013	6	1	English	R JOURNAL	117-129	13	1	WOS:000321944400013	Graphics are good for showing the information in datasets and for complementing modelling. Sometimes graphics show information models miss, sometimes graphics help to make model results more understandable, and sometimes models show whether information from graphics has statistical support or not. It is the interplay of the two approaches that is valuable. Graphics could be used a lot more in R examples and we explore this idea with some datasets available in R packages.
6944756	WOS:000324371700001	217PN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Simultaneous Optimization of Multiple Responses with the R Package JOP	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000324371700001	A joint optimization plot, shortly JOP, graphically displays the result of a loss function based robust parameter design for multiple responses. Different importance of reaching a target value can be assigned to the individual responses by weights. The JOP method simultaneously runs through a whole range of possible weights. For each weight matrix a parameter setting is derived which minimizes the estimated expected loss. The joint optimization plot displays these settings together with corresponding expected values and standard deviations of the response variable. The R package JOP provides all tools necessary to apply the JOP approach to a given data set. It also returns parameter settings for a desirable compromise of achieved expected responses chosen from the plot.
6949428	WOS:000317249700010	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Two-stage nonparametric bootstrap sampling with shrinkage correction for clustered data	Journal	Article	2013	1	1	English	STATA JOURNAL	141-164	24	1	WOS:000317249700010	This article describes a new Stata command, tsb, for performing a stratified two-stage nonparametric bootstrap resampling procedure for clustered data. Estimates for uncertainty around the point estimate, such as standard error and confidence intervals, are derived from the resultant bootstrap samples. A shrinkage estimator proposed for correcting possible overestimation due to second-stage sampling is implemented as default. Although this command is written with cost effectiveness analyses alongside cluster trials in mind, it is applicable to the analysis of continuous endpoints in cluster trials more generally. The use of this command is exemplified with a case study of a cost effectiveness analysis undertaken alongside a cluster randomized trial. We also report bootstrap confidence interval coverage by using data from a published simulation study.
6956972	WOS:000315019600001	090ZX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	edcc: An R Package for the Economic Design of the Control Chart	Journal	Article	2013	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000315019600001	The basic purpose of the economic design of the control charts is to find the optimum control charts parameters to minimize the process cost. In this paper, an R package, edcc (economic design of control charts), which provides a numerical method to find the optimum chart parameters is presented using the unified approach of the economic design. Also, some examples are given to illustrate how to use this package. The types of the control chart available in the edcc package are (X) over bar, CUSUM (cumulative sum), and EWMA (exponentially-weighted moving average) control charts.
6959029	WOS:000324372000001	217PP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	GLIMMPSE: Online Power Computation for Linear Models with and without a Baseline Covariate	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000324372000001	GLIMMPSE is a free, web-based software tool that calculates power and sample size for the general linear multivariate model with Gaussian errors (http://glimmpse.SampleSizeShop.org/). GLIMMPSE provides a user-friendly interface for the computation of power and sample size. We consider models with fixed predictors, and models with fixed predictors and a single Gaussian covariate. Validation experiments demonstrate that GLIMMPSE matches the accuracy of previously published results, and performs well against simulations. We provide several online tutorials based on research in head and neck cancer. The tutorials demonstrate the use of GLIMMPSE to calculate power and sample size.
6973727	WOS:000328129900001	267WV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	On Circulant Embedding for Gaussian Random Fields in R	Journal	Article	2013	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000328129900001	The high-dimensionality typically associated with discretized approximations to Gaussian random fields is a considerable hinderance to computationally efficient methods for their simulation. Many direct approaches require spectral decompositions of the associated covariance matrix and so are unable to complete the solving process in a timely fashion, if at all. However under certain conditions, we may construct block-circulant versions of the covariance matrix at hand thereby allowing access to fast-Fourier methods to perform the required operations with impressive speed. We demonstrate how circulant embedding and subsequent simulation can be performed directly in the R language. The approach is currently implemented in C for the R package Random Fields, and used in the recently released package lgcp. Motivated by applications dealing with spatial point processes we restrict attention to stationary Gaussian fields on R-2, where sparsity of the covariance matrix cannot necessarily be assumed.
7021966	WOS:000318237300001	134TK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	cts: An R Package for Continuous Time Autoregressive Models via Kalman Filter	Journal	Article	2013	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000318237300001	We describe an R package cts for fitting a modified form of continuous time autoregressive model, which can be particularly useful with unequally sampled time series. The estimation is based on the application of the Kalman filter. The paper provides the methods and algorithms implemented in the package, including parameter estimation, spectral analysis, forecasting, model checking and Kalman smoothing. The package contains R functions which interface underlying Fortran routines. The package is applied to geophysical and medical data for illustration.
7043999	WOS:000317249700005	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Trial sequential boundaries for cumulative meta-analyses	Journal	Article	2013	1	1	English	STATA JOURNAL	77-91	15	1	WOS:000317249700005	We present a new command, met acumbounds, for the estimation of trial sequential monitoring boundaries in cumulative meta-analyses. The approach is based on the Lan-DeMets method for estimating group sequential boundaries in individual randomized controlled trials by using the package ldbounds in R statistical software. Through Stata's metan command, metacumbounds plots the Lan-DeMets bounds, z-values, and p-values obtained from both fixed and random-effects cumulative meta-analyses. The analysis can be performed with count data or on the hazard scale for time-to-event data.
7067666	WOS:000321944400006	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	osmar: OpenStreetMap and R	Journal	Article	2013	6	1	English	R JOURNAL	53-63	11	1	WOS:000321944400006	OpenStreetMap provides freely accessible and editable geographic data. The osmar package smoothly integrates the OpenStreetMap project into the R ecosystem. The osmar package provides infrastructure to access OpenStreetMap data from different sources, to enable working with the OSM data in the familiar R idiom, and to convert the data into objects based on classes provided by existing R packages. This paper explains the package's concept and shows how to use it. As an application we present a simple navigation device.
7100902	WOS:000324372600001	217PU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	bcrm: Bayesian Continual Reassessment Method Designs for Phase I Dose-Finding Trials	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	26	1	WOS:000324372600001	This paper presents the R package bcrm for conducting and assessing Bayesian continual reassessment method (CRM) designs in Phase I dose-escalation trials. CRM designs are a class of adaptive design that select the dose to be given to the next recruited patient based on accumulating toxicity data from patients already recruited into the trial, often using Bayesian methodology. Despite the original CRM design being proposed in 1990, the methodology is still not widely implemented within oncology Phase I trials. The aim of this paper is to demonstrate, through example of the bcrm package, how a variety of possible designs can be easily implemented within the R statistical software, and how properties of the designs can be communicated to trial investigators using simple textual and graphical output obtained from the package. This in turn should facilitate an iterative process to allow a design to be chosen that is suitable to the needs of the investigator. Our bcrm package is the first to offer a large comprehensive choice of CRM designs, priors and escalation procedures, which can be easily compared and contrasted within the package through the assessment of operating characteristics
7106298	WOS:000328130700001	267XC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Hybrids of Gibbs Point Process Models and Their Implementation	Journal	Article	2013	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-43	43	1	WOS:000328130700001	We describe a simple way to construct new statistical models for spatial point pattern data. Taking two or more existing models (finite Gibbs spatial point processes) we multiply the probability densities together and renormalise to obtain a new probability density. We call the resulting model a hybrid. We discuss stochastic properties of hybrids, their statistical implications, statistical inference, computational strategies and software implementation in the R package spatstat. Hybrids are particularly useful for constructing models which exhibit interaction at different spatial scales. The methods are demonstrated on a real data set on human social interaction. Software and data are provided.
7110899	WOS:000329680600001	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Attributable and unattributable risks and fractions and other scenario comparisons	Journal	Article	2013	1	1	English	STATA JOURNAL	672-698	27	1	WOS:000329680600001	Scenarios are alternative versions of the same dataset with the same variables but different observations or values. Applied scientists frequently want to predict how much good an intervention will do by comparing outcomes from the same model between different scenarios. Alternatively, they may want to compare outcomes between different models applied to the same scenario, for instance, when standardizing statistics from different subpopulations to a common gender and age distribution. Standard Stata tools for scenario means and comparisons are margins and pwcompare. A suite of packages is presented for estimating scenario means and comparisons by using margins, together with normalizing and variance-stabilizing transformations implemented by using n1com. margprev estimates marginal prevalences; marglmean estimates marginal arithmetic means; regpar estimates the difference between two marginal prevalences (the population attributable risk); punaf estimates the ratio between two marginal arithmetic means (the population unattributable fraction); and punaf cc estimates a marginal mean between-scenario risk or hazard ratio for case control or survival data (also known as a population unattributable fraction). The population unattributable fraction and its confidence limits are subtracted from 1 to estimate the population attributable fraction. Formulas and examples are presented, including an example from the Global Allergy and Asthma European Network.
7123988	WOS:000330193300015	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	betategarch: Simulation, estimation and forecasting of Beta-Skew-t-EGARCH Models	Journal	Article	2013	12	1	English	R JOURNAL	138-148	11	1	WOS:000330193300015	This paper illustrates the usage of the betategarch package, a package for the simulation, estimation and forecasting of Beta-Skew-t-EGARCH models. The Beta-Skew-t-EGARCH model is a dynamic model of the scale or volatility of financial returns. The model is characterised by its robustness to jumps or outliers, and by its exponential specification of volatility. The latter enables richer dynamics, since parameters need not be restricted to be positive to ensure positivity of volatility. In addition, the model also allows for heavy tails and skewness in the conditional return (i.e. scaled return), and for leverage and a time-varying long-term component in the volatility specification. More generally, the model can be viewed as a model of the scale of the error in a dynamic regression.
7135814	WOS:000329680600008	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Parametric inference using structural break tests	Journal	Article	2013	1	1	English	STATA JOURNAL	836-861	26	1	WOS:000329680600008	We present methods for testing hypotheses and estimating confidence sets for structural parameters of economic models in the presence of instabilities and breaks of unknown form. These methods constructively explore information generated by changes in the data-generating process to improve the inference of parameters that remain stable over time. The proposed methods are suitable for models cast in the generalized method of moments framework, which makes their application wide. Moreover, they are robust to the presence of weak instruments. The genstest command in Stata implements these methods to conduct hypothesis tests and to estimate confidence sets.
7162505	WOS:000321944400011	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	An Introduction to the EcoTroph R Package: Analyzing Aquatic Ecosystem Trophic Networks	Journal	Article	2013	6	1	English	R JOURNAL	98-107	10	1	WOS:000321944400011	Recent advances in aquatic ecosystem modelling have particularly focused on trophic network analysis through trophodynamic models. We present here a R package devoted to a recently developed model, EcoTroph. This model enables the analysis of aquatic ecological networks and the related impacts of fisheries. It was available through a plug-in in the well-known Ecopath with Ecosim software or through implementations in Excel sheets. The R package we developed simplifies the access to the EcoTroph model and offers a new interfacing between two widely used software, Ecopath and R.
7178831	WOS:000322051800002	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Creating and managing spatial-weighting matrices with the spmat command	Journal	Article	2013	1	1	English	STATA JOURNAL	242-286	45	1	WOS:000322051800002	We present the spmat command for creating, managing, and storing spatial-weighting matrices, which are used to model interactions between spatial or more generally cross-sectional units. spmat can store spatial-weighting matrices in a general and banded form. We illustrate the use of the spmat command and discuss some of the underlying issues by using United States county and postal-code-level data.
7191001	WOS:000324371300001	217PJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Sustainable, Extensible Documentation Generation Using inlinedocs	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	20	1	WOS:000324371300001	This article presents inlinedocs, an R package for generating documentation from comments. The concept of structured, interwoven code and documentation has existed for many years, but existing systems that implement this for the R programming language do not tightly integrate with R code, leading to several drawbacks. This article attempts to address these issues and presents 2 contributions for documentation generation for the R community. First, we propose a new syntax for inline documentation of R code within comments adjacent to the relevant code, which allows for highly readable and maintainable code and documentation. Second, we propose an extensible system for parsing these comments, which allows the syntax to be easily augmented.
7206752	WOS:000322051800012	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Bonferroni and Holm approximations for Sidak and Holland-Copenhaver q-values	Journal	Article	2013	1	1	English	STATA JOURNAL	379-381	3	1	WOS:000322051800012	I describe the use of the Bonferroni and Holm formulas as approximations for Sidak and Holland-Copenhaver formulas when issues of precision are encountered, especially with q-values corresponding to very small p-values.
7221547	WOS:000321944400008	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Statistical Software from a Blind Person's Perspective	Journal	Article	2013	6	1	English	R JOURNAL	73-79	7	1	WOS:000321944400008	Blind people have experienced access issues to many software applications since the advent of the Windows operating system; statistical software has proven to follow the rule and not be an exception. The ability to use R within minutes of download with next to no adaptation has opened doors for accessible production of statistical analyses for this author (himself blind) and blind students around the world. This article shows how little is required to make R the most accessible statistical software available today. There is any number of ramifications that this opportunity creates for blind students, especially in terms of their future research and employment prospects. There is potential for making R even better for blind users. The extensibility of R makes this possible through added functionality being made available in an add-on package called BrailleR. Functions in this package are intended to make graphical information available in text form.
7225388	WOS:000324372700001	217PV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An R Package for Probabilistic Latent Feature Analysis of Two-Way Two-Mode Frequencies	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	29	1	WOS:000324372700001	A common strategy for the analysis of object-attribute associations is to derive a low-dimensional spatial representation of objects and attributes which involves a compensatory model (e.g., principal components analysis) to explain the strength of object-attribute associations. As an alternative, probabilistic latent feature models assume that objects and attributes can be represented as a set of binary latent features and that the strength of object-attribute associations can be explained as a non-compensatory (e.g., disjunctive or conjunctive) mapping of latent features. In this paper, we describe the R package plfm which comprises functions for conducting both classical and Bayesian probabilistic latent feature analysis with disjunctive or a conjunctive mapping rules. Print and summary functions are included to summarize results on parameter estimation, model selection and the goodness of fit of the models. As an example the functions of plfm are used to analyze product-attribute data on the perception of car models, and situation-behavior associations on the situational determinants of anger-related behavior.
7227051	WOS:000321944400007	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	ftsa: An R Package for Analyzing Functional Time Series	Journal	Article	2013	6	1	English	R JOURNAL	64-72	9	1	WOS:000321944400007	Recent advances in computer recording and storing technology have tremendously increased the presence of functional data, whose graphical representation can be infinite-dimensional curve, image, or shape. When the same functional object is observed over a period of time, such data are known as functional time series. This article makes first attempt to describe several techniques (centered around functional principal component analysis) for modeling and forecasting functional time series from a computational aspect, using a readily-available R addon package. These methods are demonstrated using age-specific Australian fertility rate data from 1921 to 2006, and monthly sea surface temperature data from January 1950 to December 2011.
7243905	WOS:000322051800010	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Goodness-of-fit tests for categorical data	Journal	Article	2013	1	1	English	STATA JOURNAL	356-365	10	1	WOS:000322051800010	A significant aspect of data modeling with categorical predictors is the definition of a saturated model. In fact, there are different ways of specifying it the casewise, the contingency table, and the collapsing approaches and they strictly depend on the unit of analysis considered. The analytical units of reference could be the subjects or, alternatively, groups of subjects that have the same covariate pattern. In the first case, the goal is to predict the probability of success (failure) for each individual; in the second case, the goal is to predict the proportion of successes (failures) in each group. The analytical unit adopted does not affect the estimation process; however, it does affect the definition of a saturated model. Consequently, measures and tests of goodness of fit can lead to different results and interpretations. Thus one must carefully consider which approach to choose. In this article, we focus on the deviance test for logistic regression models. However, the results and the conclusions are easily applicable to other linear models involving categorical regressors. We show how Stata 12.1 performs when implementing goodness of fit. In this situation, it is important to clarify which one of the three approaches is implemented as default. Furthermore, a prominent role is played by the shape of the dataset considered (individual format or events trials format) in accordance with the analytical unit choice. In fact, the same procedure applied to different data structures leads to different approaches to a saturated model. Thus one must attend to practical and theoretical statistical issues to avoid inappropriate analyses.
7272662	WOS:000321944400005	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Hypothesis Tests for Multivariate Linear Models Using the car Package	Journal	Article	2013	6	1	English	R JOURNAL	39-52	14	1	WOS:000321944400005	The multivariate linear model is [GRAPHICS] = [GRAPHICS] [GRAPHICS] + [GRAPHICS] The multivariate linear model can be fit with the lm function in R, where the left-hand side of the model comprises a matrix of response variables, and the right-hand side is specified exactly as for a univariate linear model (i.e., with a single response variable). This paper explains how to use the Anova and linear Hypothesis functions in the car package to perform convenient hypothesis tests for parameters in multivariate linear models, including models for repeated-measures data.
7285061	WOS:000330193300009	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Temporal Disaggregation of Time Series	Journal	Article	2013	12	1	English	R JOURNAL	81-88	8	1	WOS:000330193300009	Temporal disaggregation methods are used to disaggregate low frequency time series to higher frequency series, where either the sum, the average, the first or the last value of the resulting high frequency series is consistent with the low frequency series. Temporal disaggregation can be performed with or without one or more high frequency indicator series. The package tempdisagg is a collection of several methods for temporal disaggregation.
7294180	WOS:000320041200001	159JH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Distributed Procedure for Computing Stochastic Expansions with Mathematica	Journal	Article	2013	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000320041200001	The solution of a (stochastic) differential equation can be locally approximated by a (stochastic) expansion. If the vector field of the differential equation is a polynomial, the corresponding expansion is a linear combination of iterated integrals of the drivers and can be calculated using Picard Iterations. However, such expansions grow exponentially fast in their number of terms, due to their specific algebra, rendering their practical use limited. We present a Mathematica procedure that addresses this issue by reparametrizing the polynomials and distributing the load in as small as possible parts that can be processed and manipulated independently, thus alleviating large memory requirements and being perfectly suited for parallelized computation. We also present an iterative implementation of the shuffle product (as opposed to a recursive one, more usually implemented) as well as a fast way for calculating the expectation of iterated Stratonovich integrals for Brownian motion.
7344808	WOS:000322051800005	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Importing U.S. exchange rate data from the Federal Reserve and standardizing country names across datasets	Journal	Article	2013	1	1	English	STATA JOURNAL	315-322	8	1	WOS:000322051800005	fxrates is a command to import historical U.S. exchange rate data from the Federal Reserve and to calculate the daily change of the exchange rates. Because many cross-country datasets use different spellings and conventions for country names, we also introduce a second command, countrynames, to convert country names to a common naming standard.
7359141	WOS:000330193300006	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Performance Attribution for Equity Portfolios	Journal	Article	2013	12	1	English	R JOURNAL	53-62	10	1	WOS:000330193300006	The pa package provides tools for conducting performance attribution for long-only, single currency equity portfolios. The package uses two methods: the Brinson-Hood-Beebower model (hereafter referred to as the Brinson model) and a regression-based analysis. The Brinson model takes an ANOVA-type approach and decomposes the active return of any portfolio into asset allocation, stock selection, and interaction effect. The regression-based analysis utilizes estimated coefficients, based on a regression model, to attribute active return to different factors.
7360996	WOS:000325442200011	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Allocation of ordered exclusive choices	Journal	Article	2013	1	1	English	STATA JOURNAL	618-624	7	1	WOS:000325442200011	In this article, I describe the alloch command, which helps to allocate exclusive choices among individuals who have ordered preferences over available alternatives.
7373307	WOS:000325442200013	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Trimming to taste	Journal	Article	2013	1	1	English	STATA JOURNAL	640-666	27	1	WOS:000325442200013	Trimmed means are means calculated after setting aside zero or more values in each tail of a sample distribution. Here we focus on trimming equal numbers in each tail. Such trimmed means define a family or function with mean and median as extreme members and are attractive as simple and easily understood summaries of the general level (location, central tendency) of a variable. This article provides a tutorial review of trimmed means, emphasizing the scope for trimming to varying degrees in describing and exploring data. Detailed remarks are included on the idea's history, plotting of results, and confidence interval procedures. Examples are given using astronomical and medical data. The new Stata commands trimmean and trimplot are also included.
7375865	WOS:000330193300004	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	RNetCDF - A Package for Reading and Writing NetCDF Datasets	Journal	Article	2013	12	1	English	R JOURNAL	29-36	8	1	WOS:000330193300004	This paper describes the RNetCDF package (version 1.6), an interface for reading and writing files in Unidata NetCDF format, and gives an introduction to the NetCDF file format. NetCDF is a machine independent binary file format which allows storage of different types of array based data, along with short metadata descriptions. The package presented here allows access to the most important functions of the NetCDF C-interface for reading, writing, and modifying NetCDF datasets. In this paper, we present a short overview on the NetCDF file format and show usage examples of the package.
7400718	WOS:000321944400016	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	mpoly: Multivariate Polynomials in R	Journal	Article	2013	6	1	English	R JOURNAL	162-170	9	1	WOS:000321944400016	The mpoly package is a general purpose collection of tools for symbolic computing with multivariate polynomials in R. In addition to basic arithmetic, mpoly can take derivatives of polynomials, compute Grobner bases of collections of polynomials, and convert polynomials into a functional form to be evaluated. Among other things, it is hoped that mpoly will provide an R-based foundation for the computational needs of algebraic statisticians.
7404699	WOS:000330193300005	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Surface Melting Curve Analysis with R	Journal	Article	2013	12	1	English	R JOURNAL	37-52	16	1	WOS:000330193300005	Nucleic acid Melting Curve Analysis is a powerful method to investigate the interaction of double stranded nucleic acids. Many researchers rely on closed source software which is not ubiquitously available, and gives only little control over the computation and data presentation. R in contrast, is open source, highly adaptable and provides numerous utilities for data import, sophisticated statistical analysis and presentation in publication quality. This article covers methods, implemented in the MBmca package, for DNA Melting Curve Analysis on microbead surfaces. Particularly, the use of the second derivative melting peaks is suggested as an additional parameter to characterize the melting behavior of DNA duplexes. Examples of microbead surface Melting Curve Analysis on fragments of human genes are presented.
7442651	WOS:000320041000001	159JF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Minimum-Size Mixed-Level Orthogonal Fractional Factorial Designs Generation: A SAS-Based Algorithm	Journal	Article	2013	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000320041000001	Orthogonal fractional factorial designs (OFFDs) are frequently used in many fields of application, including medicine, engineering and agriculture. In this paper we present a software tool for the generation of minimum size OFFDs. The software, that has been implemented in SAS/IML, puts neither a restriction on the number of levels of each factor nor on the orthogonality constraints. The algorithm is based on the joint use of polynomial counting function and complex coding of levels and follows the approach presented in Fontana (2013).
7448355	WOS:000330193300012	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	lfe: Linear Group Fixed Effects	Journal	Article	2013	12	1	English	R JOURNAL	105-117	13	1	WOS:000330193300012	Linear models with fixed effects and many dummy variables are common in some fields. Such models are straightforward to estimate unless the factors have too many levels. The R package lfe solves this problem by implementing a generalization of the within transformation to multiple factors, tailored for large problems.
7499942	WOS:000330193300003	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	spMC: Modelling Spatial Random Fields with Continuous Lag Markov Chains	Journal	Article	2013	12	1	English	R JOURNAL	16-28	13	1	WOS:000330193300003	Currently, a part of the R statistical software is developed in order to deal with spatial models. More specifically, some available packages allow the user to analyse categorical spatial random patterns. However, only the spMC package considers a viewpoint based on transition probabilities between locations. Through the use of this package it is possible to analyse the spatial variability of data, make inference, predict and simulate the categorical classes in unobserved sites. An example is presented by analysing the well-known Swiss Jura data set.
7520445	WOS:000324371600001	217PM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Fortran 90 Program for the Generalized Order-Restricted Information Criterion	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	19	1	WOS:000324371600001	The generalized order-restricted information criterion (GORIC) is a generalization of the Akaike information criterion such that it can evaluate hypotheses that take on specific, but widely applicable, forms (namely, closed convex cones) for multivariate normal linear models. It can examine the traditional hypotheses H-0: beta(1,1) = ... = beta(t,k) and H-u: beta(1,1),..., beta(t,k) and hypotheses containing simple order restrictions H-m: beta(1,1) >= ... >= beta(t,k), where any ">=" may be replaced by "=" and m is the model/hypothesis index; with beta(h,j) the parameter for the h-th dependent variable and the j-th predictor in a t-variate regression model with k predictors (which might include the intercept). But, the GORIC can also be applied to restrictions of the form H-m: R-1 beta = r(1); R-2 beta >= r(2), with beta a vector of length tk, R-1 a c(m1) x tk matrix, r(1) a vector of length c(m1), R-2 a c(m2) x t(k) matrix, and r(2) a vector of length c(m2). It should be noted that [R-1(inverted perpendicular), R-2(inverted perpendicular)](inverted perpendicular) should be of full rank when [r(1)(inverted perpendicular), r(2)(inverted perpendicular)](inverted perpendicular) not equal 0. In practice, this implies that one cannot examine range restrictions (e. g., 0 < beta(1,1) < 2 or beta(1,2) < beta(1,1) < 2 beta(1,2)) with the GORIC. A Fortran 90 program is presented, which enables researchers to compute the GORIC for hypotheses in the context of multivariate regression models. Additionally, an R package called goric is made by Daniel Gerhard and the first author.
7539192	WOS:000317249700004	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Within and between estimates in random-effects models: Advantages and drawbacks of correlated random effects and hybrid models	Journal	Article	2013	1	1	English	STATA JOURNAL	65-76	12	1	WOS:000317249700004	Correlated random-effects (Mundlak, 1978, Econometrica 46: 69-85; Wooldridge, 2010, Econometric Analysis of Cross Section and Panel Data [MIT Press]) and hybrid models (Allison, 2009, Fixed Effects Regression Models [Sage]) are attractive alternatives to standard random-effects and fixed-effects models because they provide within estimates of level 1 variables and allow for the inclusion of level 2 variables. I discuss these models, give estimation examples, and address some complications that arise when interaction effects are included.
7541838	WOS:000329680600002	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Dealing with identifier variables in data management and analysis	Journal	Article	2013	1	1	English	STATA JOURNAL	699-718	20	1	WOS:000329680600002	Identifier variables are prominent in most data files and, more often than not, are essential to fully use the information in a Stata dataset. However, rendering them in the proper format and relevant number of digits appropriate for data management and statistical analysis might pose unnerving challenges to inexperienced or even veteran Stata users. To lessen these challenges, I provide some useful tips and guard against some pitfalls by featuring two official Stata routines: the string() function and its elaborated wrapper, the tostring command. I illustrate how to use these two routines to address the difficulties caused by identifier variables in managing and analyzing data from private institutions and U.S. government agencies.
7550875	WOS:000316316800001	108SQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	runmlwin: A Program to Run the MLwiN Multilevel Modeling Software from within Stata	Journal	Article	2013	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-40	40	1	WOS:000316316800001	We illustrate how to fit multilevel models in the MLwiN package seamlessly from within Stata using the Stata program runmlwin. We argue that using MLwiN and Stata in combination allows researchers to capitalize on the best features of both packages. We provide examples of how to use runmlwin to fit continuous, binary, ordinal, nominal and mixed response multilevel models by both maximum likelihood and Markov chain Monte Carlo estimation.
7553651	WOS:000323909900001	211LT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	RMatlab-app2web: Web Deployment of R/MATLAB Applications	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-11	11	1	WOS:000323909900001	This paper presents the RMatlab-app2web tool which enables the use of R or MATLAB scripts as CGI programs for generating dynamic web content. RMatlab-app2web is highly adjustable. It can be run on both, Windows and Unix-like systems. CGI scripts written in PHP take information entered on web-based forms on the client browser, pass it to R or MATLAB on the server and display the output on the client browser. Adjustable to the servers requirements, the data transfer procedure can use either the GET or the POST routine. The application allows to call R or MATLAB to run previously written scripts. It does not allow to run completely flexible user code. We run a multivariate OLS regression to demonstrate the use of the RMatlab-app2web tool.
7560342	WOS:000324372300001	217PS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	mixsmsn: Fitting Finite Mixture of Scale Mixture of Skew-Normal Distributions	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000324372300001	We present the R package mixsmsn, which implements routines for maximum likelihood estimation (via an expectation maximization EM-type algorithm) in finite mixture models with components belonging to the class of scale mixtures of the skew-normal distribution, which we call the FMSMSN models. Both univariate and multivariate responses are considered. It is possible to fix the number of components of the mixture to be fitted, but there exists an option that transfers this responsibility to an automated procedure, through the analysis of several models choice criteria. Plotting routines to generate histograms, plug-in densities and contour plots using the fitted models output are also available. The precision of the EM estimates can be evaluated through their estimated standard deviations, which can be obtained by the provision of an approximation of the associated information matrix for each particular model in the FMSMSN family. A function to generate artificial samples from several elements of the family is also supplied. Finally, two real data sets are analyzed in order to show the usefulness of the package.
7586939	WOS:000318236500001	134TC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	animation: An R Package for Creating Animations and Demonstrating Statistical Methods	Journal	Article	2013	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000318236500001	Animated graphs that demonstrate statistical ideas and methods can both attract interest and assist understanding. In this paper we first discuss how animations can be related to some statistical topics such as iterative algorithms, random simulations, (re)sampling methods and dynamic trends, then we describe the approaches that may be used to create animations, and give an overview to the R package animation, including its design, usage and the statistical topics in the package. With the animation package, we can export the animations produced by R into a variety of formats, such as a web page, a GIF animation, a Flash movie, a PDF document, or an MP4/AVI video, so that users can publish the animations fairly easily. The design of this package is flexible enough to be readily incorporated into web applications, e.g., we can generate animations online with Rweb, which means we do not even need R to be installed locally to create animations. We will show examples of the use of animations in teaching statistics and in the presentation of statistical reports using Sweave or knitr. In fact, this paper itself was written with the knitr and animation package, and the animations are embedded in the PDF document, so that readers can watch the animations in real time when they read the paper (the Adobe Reader is required). Animations can add insight and interest to traditional static approaches to teaching statistics and reporting, making statistics a more interesting and appealing subject.
7619870	WOS:000321944400010	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	QCA: A Package for Qualitative Comparative Analysis	Journal	Article	2013	6	1	English	R JOURNAL	87-97	11	1	WOS:000321944400010	We present QCA, a package for performing Qualitative Comparative Analysis (QCA). QCA is becoming increasingly popular with social scientists, but none of the existing software alternatives covers the full range of core procedures. This gap is now filled by QCA. After a mapping of the method's diffusion, we introduce some of the package's main capabilities, including the calibration of crisp and fuzzy sets, the analysis of necessity relations, the construction of truth tables and the derivation of complex, parsimonious and intermediate solutions.
7654416	WOS:000328129200001	267WO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The RAppArmor Package: Enforcing Security Policies in R Using Dynamic Sandboxing on Linux	Journal	Article	2013	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000328129200001	The increasing availability of cloud computing and scientific super computers brings great potential for making R accessible through public or shared resources. This allows us to efficiently run code requiring lots of cycles and memory, or embed R functionality into, e.g., systems and web services. However some important security concerns need to be addressed before this can be put in production. The prime use case in the design of R has always been a single statistician running R on the local machine through the interactive console. Therefore the execution environment of R is entirely unrestricted, which could result in malicious behavior or excessive use of hardware resources in a shared environment. Properly securing an R process turns out to be a complex problem. We describe various approaches and illustrate potential issues using some of our personal experiences in hosting public web services. Finally we introduce the RAppArmor package: a Linux based reference implementation for dynamic sandboxing in R on the level of the operating system.
7668854	WOS:000318236600001	134TD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	stpp: An R Package for Plotting, Simulating and Analyzing Spatio-Temporal Point Patterns	Journal	Article	2013	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000318236600001	stpp is an R package for analyzing, simulating and displaying space-time point patterns. It covers many of the models encountered in applications of point process methods to the study of spatio-temporal phenomena. The package also includes estimators of the space-time inhomogeneous K-function and pair correlation function. stpp is the first dedicated unified computational environment in the area of spatio-temporal point processes. In this paper we describe space-time point processes and introduce the package stpp to new users.
7719152	WOS:000321944400004	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Multiple Factor Analysis for Contingency Tables in the FactoMineR Package	Journal	Article	2013	6	1	English	R JOURNAL	29-38	10	1	WOS:000321944400004	We present multiple factor analysis for contingency tables (MFACT) and its implementation in the FactoMineR package. This method, through an option of the MFA function, allows us to deal with multiple contingency or frequency tables, in addition to the categorical and quantitative multiple tables already considered in previous versions of the package. Thanks to this revised function, either a multiple contingency table or a mixed multiple table integrating quantitative, categorical and frequency data can be tackled. The FactoMineR package (Le et al., 2008; Husson et al., 2011) offers the most commonly used principal component methods: principal component analysis (PCA), correspondence analysis (CA; Benzecri, 1973), multiple correspondence analysis (MCA; Lebart et al., 2006) and multiple factor analysis (MFA; Escofier and Pages, 2008). Detailed presentations of these methods enriched by numerous examples can be consulted at the website http://factominer.free.fr/. An extension of the MFA function that considers contingency or frequency tables as proposed by Becue-Bertaut and Pages (2004, 2008) is detailed in this article. First, an example is presented in order to motivate the approach. Next, the mortality data used to illustrate the method are introduced. Then we briefly describe multiple factor analysis (MFA) and present the principles of its extension to contingency tables. A real example on mortality data illustrates the handling of the MFA function to analyse these multiple tables and, finally, conclusions are presented.
7719221	WOS:000317249700011	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Joint modeling of longitudinal and survival data	Journal	Article	2013	1	1	English	STATA JOURNAL	165-184	20	1	WOS:000317249700011	The joint modeling of longitudinal and survival data has received remarkable attention in the methodological literature over the past decade; however, the availability of software to implement the methods lags behind. The most common form of joint model assumes that the association between the survival and the longitudinal processes is underlined by shared random effects. As a result, computationally intensive numerical integration techniques such as adaptive Gauss-Hermite quadrature are required to evaluate the likelihood. We describe a new user-written command, stjm, that allows the user to jointly model a continuous longitudinal response and the time to an event of interest. We assume a linear mixed-effects model for the longitudinal submodel, allowing flexibility through the use of fixed or random fractional polynomials of time. Four choices are available for the survival submodel: the exponential, Weibull or Gompertz proportional hazard models, and the flexible parametric model (stpm2). Flexible parametric models are fit on the log cumulative-hazard scale, which has direct computational benefits because it avoids the use of numerical integration to evaluate the cumulative hazard. We describe the features of stjm through application to a dataset investigating the effect of serum bilirubin level on time to death from any cause in 312 patients with primary biliary cirrhosis.
7738068	WOS:000321944400021	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Translating Probability Density Functions: From R to BUGS and Back Again	Journal	Article	2013	6	1	English	R JOURNAL	207-209	3	1	WOS:000321944400021	The ability to implement statistical models in the BUGS language facilitates Bayesian inference by automating MCMC algorithms. Software packages that interpret the BUGS language include OpenBUGS, WinBUGS, and JAGS. R packages that link BUGS software to the R environment, including rjags and R2WinBUGS, are widely used in Bayesian analysis. Indeed, many packages in the Bayesian task view on CRAN (http://cran.rproject.org/web/views/Bayesian.html) depend on this integration. However, the R and BUGS languages use different representations of common probability density functions, creating a potential for errors to occur in the implementation or interpretation of analyses that use both languages. Here we review different parameterizations used by the R and BUGS languages, describe how to translate between the languages, and provide an R function, r2bugs.distributions, that transforms parameterizations from R to BUGS and back again.
7790412	WOS:000317249700008	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A menu-driven facility for sample-size calculations in cluster randomized controlled trials	Journal	Article	2013	1	1	English	STATA JOURNAL	114-135	22	1	WOS:000317249700008	We introduce the Stata menu-driven command clustersampsi, which calculates sample sizes, detectable differences, and power for cluster randomized controlled trials. The command permits continuous, binary, and rate outcomes (with normal approximations) for comparisons of two-sided tests in two equal-sized arms. The command allows for specification of the number of clusters available, or the cluster size, or the average cluster size along with an estimate of the variation of cluster sizes. When the number of clusters available is insufficient to detect the required difference at the prespecified power, clustersampsi will return the minimum number of clusters required under the prespecified design along with the minimum detectable difference and maximum achievable power (both for the prespecified number of clusters). Cluster heterogeneity can be parameterized by using either the intracluster correlation or the coefficient of variation. The command is illustrated via examples.
7817781	WOS:000321944400019	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	RcmdrPlugin.temis, a Graphical Integrated Text Mining Solution in R	Journal	Article	2013	6	1	English	R JOURNAL	188-196	9	1	WOS:000321944400019	We present the package RcmdrPlugin.temis, a graphical user interface for user-friendly text mining in R. Built as a plug-in to the R Commander provided by the Rcmdr package, it brings together several existing packages and provides new features streamlining the process of importing, managing and analyzing a corpus, in addition to saving results and plots to a report file. Beyond common file formats, automated import of corpora from the Dow Jones Factiva content provider and Twitter is supported. Featured analyses include vocabulary and dissimilarity tables, terms frequencies, terms specific of levels of a variable, term co-occurrences, time series, correspondence analysis and hierarchical clustering.
7835145	WOS:000322051800004	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A command for Laplace regression	Journal	Article	2013	1	1	English	STATA JOURNAL	302-314	13	1	WOS:000322051800004	We present the new laplace command for estimating Laplace regression, which models quantiles of a possibly censored outcome variable given covariates. We illustrate laplace with an example from a clinical trial on survival in patients with metastatic renal carcinoma. We also report the results of a small simulation study.
7854589	WOS:000325947100001	238LD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Costationarity of Locally Stationary Time Series Using costat	Journal	Article	2013	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000325947100001	This article describes the R package costat. This package enables a user to (i) perform a test for time series stationarity; (ii) compute and plot time-localized autocovariances, and (iii) to determine and explore any costationary relationship between two locally stationary time series. Two locally stationary time series are said to be costationary if there exists two time-varying combination functions such that the linear combination of the two series with the functions produces another time series which is stationary. Costationarity existing between two time series indicates a relationship between the series that might be usefully exploited in a number of ways. Sometimes the relationship itself is of interest, sometimes the derived stationary series is of interest and useful as a substitute for either of the original stationary series in some applications.
7869528	WOS:000322051800013	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Fitting the generalized multinomial logit model in Stata	Journal	Article	2013	1	1	English	STATA JOURNAL	382-397	16	1	WOS:000322051800013	In this article, we describe the gmnl Stata command, which can be used to fit the generalized multinomial logit model and its special cases.
7907236	WOS:000318237000001	134TH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	GA: A Package for Genetic Algorithms in R	Journal	Article	2013	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-37	37	1	WOS:000318237000001	Genetic algorithms (GAs) are stochastic search algorithms inspired by the basic principles of biological evolution and natural selection. GAs simulate the evolution of living organisms, where the fittest individuals dominate over the weaker ones, by mimicking the biological mechanisms of evolution, such as selection, crossover and mutation. GAs have been successfully applied to solve optimization problems, both for continuous (whether differentiable or not) and discrete functions. This paper describes the R package G A, a collection of general purpose functions that provide a flexible set of tools for applying a wide range of genetic algorithm methods. Several examples are discussed, ranging from mathematical functions in one and two dimensions known to be hard to optimize with standard derivative-based methods, to some selected statistical problems which require the optimization of user de fined objective functions. (This paper contains animations that can be viewed using the Adobe Acrobat PDF viewer.)
7925658	WOS:000329680600003	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Stochastic frontier analysis using Stata	Journal	Article	2013	1	1	English	STATA JOURNAL	719-758	40	1	WOS:000329680600003	This article describes sfcross and sfpanel, two new Stata commands for the estimation of cross-sectional and panel-data stochastic frontier models. sfcross extends the capabilities of the frontier command by including additional models (Greene, 2003, Journal of Productivity Analysis 19: 179-190; Wang, 2002, Journal of Productivity Analysis 18: 241-253) and command functionalities, such as the possibility of managing complex survey data characteristics. Similarly, sfpanel allows one to fit a much wider range of time-varying inefficiency models compared with the xtfrontier command, including the model of Cornwell, Schmidt, and Sickles (1990, Journal of Econometrics 46: 185-200); the model of Lee and Schmidt (1993, in The Measurement of Productive Efficiency: Techniques and Applications), a production frontier model with flexible temporal variation in technical efficiency; the flexible model of Kumbhakar (1990, Journal of Econometrics 46: 201-211); the inefficiency effects model of Battese and Coelli (1995 Empirical Economics 20: 325-332); and the "true" fixed- and random-effects models of Greene (2005a, Journal of Econometrics 126: 269-303). A brief overview of the stochastic frontier literature, a description of the two commands and their options, and examples using simulated and real data are provided.
7936974	WOS:000321944400009	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	PIN: Measuring Asymmetric Information in Financial Markets with R	Journal	Article	2013	6	1	English	R JOURNAL	80-86	7	1	WOS:000321944400009	The package PIN computes a measure of asymmetric information in financial markets, the so-called probability of informed trading. This is obtained from a sequential trade model and is used to study the determinants of an asset price. Since the probability of informed trading depends on the number of buy- and sell-initiated trades during a trading day, this paper discusses the entire modelling cycle, from data handling to the computation of the probability of informed trading and the estimation of parameters for the underlying theoretical model.
7940871	WOS:000322051800009	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Extending the flexible parametric survival model for competing risks	Journal	Article	2013	1	1	English	STATA JOURNAL	344-355	12	1	WOS:000322051800009	Competing risks are present when the patients within a dataset could experience one or more of several exclusive events and the occurrence of any one of these could impede the event of interest. One of the measures of interest for analyses of this type is the cumulative incidence function. stpm2cif is a postestimation command used to generate predictions of the cumulative incidence function after fitting a flexible parametric survival model using stpm2. There is also the option to generate confidence intervals, cause-specific hazards, and two other measures that will be discussed in further detail. The new command is illustrated through a simple example.
7948148	WOS:000321944400003	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Generalized Simulated Annealing for Global Optimization: The GenSA Package	Journal	Article	2013	6	1	English	R JOURNAL	13-28	16	1	WOS:000321944400003	Many problems in statistics, finance, biology, pharmacology, physics, mathematics, economics, and chemistry involve determination of the global minimum of multidimensional functions. R packages for different stochastic methods such as genetic algorithms and differential evolution have been developed and successfully used in the R community. Based on Tsallis statistics, the R package GenSA was developed for generalized simulated annealing to process complicated non-linear objective functions with a large number of local minima. In this paper we provide a brief introduction to the R package and demonstrate its utility by solving a non-convex portfolio optimization problem in finance and the Thomson problem in physics. GenSA is useful and can serve as a complementary tool to, rather than a replacement for, other widely used R packages for optimization.
7950665	WOS:000315018900001	090ZQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Modeling Dependence with C- and D-Vine Copulas: The R Package CDVine	Journal	Article	2013	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	27	1	WOS:000315018900001	Flexible multivariate distributions are needed in many areas. The popular multivariate Gaussian distribution is however very restrictive and cannot account for features like asymmetry and heavy tails. Therefore dependence modeling using copulas is nowadays very common to account for such patterns. The use of copulas is however challenging in higher dimensions, where standard multivariate copulas suffer from rather inflexible structures. Vine copulas overcome such limitations and are able to model complex dependency patterns by benefiting from the rich variety of bivariate copulas as building blocks. This article presents the R package CDVine which provides functions and tools for statistical inference of canonical vine (C-vine) and D-vine copulas. It contains tools for bivariate exploratory data analysis and for bivariate copula selection as well as for selection of pair-copula families in a vine. Models can be estimated either sequentially or by joint maximum likelihood estimation. Sampling algorithms and graphical methods are also included.
7963670	WOS:000315018800001	090ZP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ergm.userterms: A Template Package for Extending statnet	Journal	Article	2013	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	25	1	WOS:000315018800001	Exponential-family random graph models (ERGMs) represent a powerful and flexible class of models for the statistical analysis of networks. statnet is a suite of software packages that implement these models. This paper details how the capabilities for ERGM modeling can be expanded and customized by programming additional network statistics that may be included in ERGMs. We describe a template R package called ergm.userterms that can be modified for this purpose. It is designed to make this process as straight forward as possible. We also explain some of the internal workings of statnet that will help users develop their own network analysis capabilities.
7993730	WOS:000330193300013	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The R in Robotics rosR: A New Language Extension for the Robot Operating System	Journal	Article	2013	12	1	English	R JOURNAL	118-129	12	1	WOS:000330193300013	The aim of this contribution is to connect two previously separated worlds: robotic application development with the Robot Operating System (ROS) and statistical programming with R. This fruitful combination becomes apparent especially in the analysis and visualization of sensory data. We therefore introduce a new language extension for ROS that allows to implement nodes in pure R. All relevant aspects are described in a step-by-step development of a common sensor data transformation node. This includes the reception of raw sensory data via the ROS network, message interpretation, bag-file analysis, transformation and visualization, as well as the transmission of newly generated messages back into the ROS network.
8006286	WOS:000330193300008	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	rlme: An R Package for Rank-Based Estimation and Prediction in Random Effects Nested Models	Journal	Article	2013	12	1	English	R JOURNAL	72-80	9	1	WOS:000330193300008	There is a lack of robust statistical analyses for random effects linear models. In practice, statistical analyses, including estimation, prediction and inference, are not reliable when data are unbalanced, of small size, contain outliers, or not normally distributed. It is fortunate that rank-based regression analysis is a robust nonparametric alternative to likelihood and least squares analysis. We propose an R package that calculates rank-based statistical analyses for two-and three-level random effects nested designs. In this package, a new algorithm which recursively obtains robust predictions for both scale and random effects is used, along with three rank-based fitting methods.
8018478	WOS:000320041300001	159JI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	stgenreg: A Stata Package for General Parametric Survival Analysis	Journal	Article	2013	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000320041300001	In this paper we present the Stata package stgenreg for the parametric analysis of survival data. Any user-defined hazard function can be specified, with the model estimated using maximum likelihood utilising numerical quadrature. Models that can be fitted range from the Weibull proportional hazards model to the generalized gamma model, mixture models, cure rate models, accelerated failure time models and relative survival models. We illustrate the features of stgenreg through application to a cohort of women diagnosed with breast cancer with outcome all-cause death.
8025960	WOS:000325947200001	238LE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Nonparametric Regression via StatLSSVM	Journal	Article	2013	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000325947200001	We present a new MATLAB toolbox under Windows and Linux for nonparametric regression estimation based on the statistical library for least squares support vector machines (StatLSSVM). The StatLSSVM toolbox is written so that only a few lines of code are necessary in order to perform standard nonparametric regression, regression with correlated errors and robust regression. In addition, construction of additive models and pointwise or uniform confidence intervals are also supported. A number of tuning criteria such as classical cross-validation, robust cross-validation and cross-validation for correlated errors are available. Also, minimization of the previous criteria is available without any user interaction.
8035886	WOS:000322051800011	186NR	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Standardizing anthropometric measures in children and adolescents with functions for egen: Update	Journal	Article	2013	1	1	English	STATA JOURNAL	366-378	13	1	WOS:000322051800011	In this article, we describe an extension to the egen functions zanthro () and zbmicat () (Vidmar et al., 2004, Stata Journal 4: 50-55). All functionality of the original version remains unchanged. In the 2004 version of zanthro (), z scores could be generated using the 2000 U.S. Centers for Disease Control and Prevention Growth Reference and the British 1990 Growth Reference. More recent growth references are now available For measurement-for-age charts, age can now be adjusted for gestational age. The zbmicat () function previously categorized children according to body mass index (weight/height(2)) as normal weight, overweight, or obese. "Normal weight" is now split into normal weight and three grades of thinness. Finally, this updated version uses cubic rather than linear interpolation to calculate the values of L, M, and S for the child's decimal age between successive ages (or length/height for weight-for-length/height charts).
8036629	WOS:000330193300016	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Changes to grid for R 3.0.0	Journal	Article	2013	12	1	English	R JOURNAL	149-161	13	1	WOS:000330193300016	From R 3.0.0, there is a new recommended way to develop new grob classes in grid. In a nutshell, two new "hook" functions, makeContext() and makeContent() have been added to grid to provide an alternative to the existing hook functions preDrawDetails(), drawDetails(), and postDrawDetails(). There is also a new function called grid. force(). This article discusses why these changes have been made, provides a simple demonstration of the use of the new functions, and discusses some of the implications for packages that build on grid.
8048261	WOS:000329680600009	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	cmpute: A tool to generate or replace a variable	Journal	Article	2013	1	1	English	STATA JOURNAL	862-866	5	1	WOS:000329680600009	I provide a new programming tool, cmpute, to manage conveniently the creation of a new variable or the replacement of an existing variable interactively or within a Stata program.
8080120	WOS:000315019400001	090ZV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Spreadsheets in the Cloud - Not Ready Yet	Journal	Article	2013	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	14	1	WOS:000315019400001	Cloud computing is a relatively new technology that facilitates collaborative creation and modification of documents over the internet in real time. Here we provide an introductory assessment of the available statistical functions in three leading cloud spreadsheets namely Google Spreadsheet, Microsoft Excel Web App, and Zoho Sheet. Our results show that the developers of cloud-based spreadsheets are not performing basic quality control, resulting in statistical computations that are misleading and erroneous. Moreover, the developers do not provide sufficient information regarding the software and the hardware, which can change at any time without notice. Indeed, rerunning the tests after several months we obtained different and sometimes worsened results.
8083319	WOS:000329680600006	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Little's test of missing completely at random	Journal	Article	2013	1	1	English	STATA JOURNAL	795-809	15	1	WOS:000329680600006	In missing-data analysis, Little's test (1988, Journal of the American Statistical Association 83: 1198-1202) is useful for testing the assumption of missing completely at random for multivariate, partially observed quantitative data. I introduce the mcartest command, which implements Little's missing completely at random test and its extension for testing the covariate-dependent missingness. The command also includes an option to perform the likelihood-ratio test with adjustment for unequal variances. I illustrate the use of mcartest through an example and evaluate the finite-sample performance of these tests in simulation studies.
8085920	WOS:000325442200008	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A short guide and a forest plot command (ipdforest) for one-stage meta-analysis	Journal	Article	2013	1	1	English	STATA JOURNAL	574-587	14	1	WOS:000325442200008	In this article, we describe a new individual patient data meta-analysis postestimation command, ipdforest. The command produces a forest plot following a one-stage meta-analysis with xtmixed or xtmelogit. (These commands have been renamed in Stata 13 to mixed and meqrlogit, respectively; ipdforest is currently not compatible with the new names.) The overall effect is obtained from the preceding mixed-effects regression and the study effects from linear or logistic regressions on each study, which are executed within ipdforest. Individual patient data meta-analysis models with Stata are discussed.
8116372	WOS:000323909300001	211LO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fitting Additive Binomial Regression Models with the R Package blm	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000323909300001	The R package blm provides functions for fitting a family of additive regression models to binary data. The included models are the binomial linear model, in which all covariates have additive effects, and the linear-expit (lexpit) model, which allows some covariates to have additive effects and other covariates to have logisitc effects. Additive binomial regression is a model of event probability, and the coefficients of linear terms estimate covariate-adjusted risk differences. Thus, in contrast to logistic regression, additive binomial regression puts focus on absolute risk and risk differences. In this paper, we give an overview of the methodology we have developed to fit the binomial linear and lexpit models to binary outcomes from cohort and population-based case-control studies. We illustrate the blm package's methods for additive model estimation, diagnostics, and inference with risk association analyses of a bladder cancer nested case-control study in the NIH-AARP Diet and Health Study.
8147695	WOS:000325442200002	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Simulation-based sample-size calculation for designing new clinical trials and diagnostic test accuracy studies to update an existing meta-analysis	Journal	Article	2013	1	1	English	STATA JOURNAL	451-473	23	1	WOS:000325442200002	In this article, we describe a suite of commands that enable the user to estimate the probability that the conclusions of a meta-analysis will change with the inclusion of a new study, as described previously by Sutton et al. (2007, Statistics in Medicine 26: 2479-2500). Using the metasim command, we take a simulation approach to estimating the effects in future studies. The method assumes that the effect sizes of future studies are consistent with those observed previously, as represented by the current meta-analysis. Two-arm randomized controlled trials and studies of diagnostic test accuracy are considered for a variety of outcome measures. Calculations are possible under both fixed- and random-effects assumptions, and several approaches to inference, including statistical significance and limits of clinical significance, are possible. Calculations for specific sample sizes can be conducted (by using metapow). Plots, akin to traditional power curves, can be produced (by using metapowplot) to indicate the probability that a new study will change inferences for a range of sample sizes. Finally, plots of the simulation results are overlaid on extended funnel plots by using extfunnel, described in Crowther, Langan, and Sutton (2012, Stata Journal 12: 605-622), which can help to intuitively explain the results of such calculations of sample size. We hope the command will be useful to trialists who want to assess the potential impact new trials will have on the overall evidence base and to meta-analysts who want to assess the robustness of the current meta-analysis to the inclusion of future data.
8163511	WOS:000325442200003	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Response mapping to translate health outcomes into the generic health-related quality-of-life instrument EQ-5D: Introducing the mrs2eq and oks2eq commands	Journal	Article	2013	1	1	English	STATA JOURNAL	474-491	18	1	WOS:000325442200003	Reliable and accurate mapping techniques that translate health-related quality-of-life data into EQ-5D index values are now in demand by researchers conducting economic evaluation of health care technologies. In this article, we present two commands (mrs2eq and oks2eq) that translate data from two widely used disease-specific instruments into EQ-5D index values and predicted probabilities of being at a particular level on each EQ-5D domain. mrs2eq conducts a response mapping approach to transform data from the stroke-specific modified Rankin scale into index values from the generic quality-of-life EQ-5D instrument. oks2eq uses a response mapping model to estimate EQ-5D index values based on patients' responses to the Oxford Knee Score.
8186962	WOS:000315019300001	090ZU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The textcat Package for n-Gram Based Text Categorization in R	Journal	Article	2013	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	17	1	WOS:000315019300001	Identifying the language used will typically be the first step in most natural language processing tasks. Among the wide variety of language identification methods discussed in the literature, the ones employing the Cavnar and Trenkle (1994) approach to text categorization based on character n-gram frequencies have been particularly successful. This paper presents the R extension package textcat for n-gram based text categorization which implements both the Cavnar and Trenkle approach as well as a reduced n-gram approach designed to remove redundancies of the original approach. A multi-lingual corpus obtained from the Wikipedia pages available on a selection of topics is used to illustrate the functionality of the package and the performance of the provided language identification methods.
8207452	WOS:000325442200001	231TV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation of multivalued treatment effects under conditional independence	Journal	Article	2013	1	1	English	STATA JOURNAL	407-450	44	1	WOS:000325442200001	This article discusses the poparms command, which implements two semiparametric estimators for multivalued treatment effects discussed in Cattaneo (2010, Journal of Econometrics 155: 138-154). The first is a properly reweighted inverse-probability weighted estimator, and the second is an efficient-influence-function estimator, which can be interpreted as having the double-robust property. Our implementation jointly estimates means and quantiles of the potential-outcome distributions, allowing for multiple, discrete treatment levels. These estimators are then used to estimate a variety of multivalued treatment effects. We discuss pre- and postestimation approaches that can be used in conjunction with our main implementation. We illustrate the program and provide a simulation study assessing the finite-sample performance of the inference procedures.
8219673	WOS:000320040600001	159JB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	profdpm: An R Package for MAP Estimation in a Class of Conjugate Product Partition Models	Journal	Article	2013	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000320040600001	The profdpm package facilitates inference at the posterior mode for a class of product partition models. Dirichlet process mixtures are currently the only available class members. Several methods are implemented to search for the maximum posterior estimate of the data partition. This article discusses the relevant theory, the R and underlying C implementation, and examples of high level functionality.
8245028	WOS:000317249700009	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating Geweke's (1982) measure of instantaneous feedback	Journal	Article	2013	1	1	English	STATA JOURNAL	136-140	5	1	WOS:000317249700009	In this article, we describe the gwke82 command, which implements a measure of instantaneous feedback for two time series following Geweke (1982, Journal of the American Statistical Association 77: 304-313).
8245090	WOS:000317249700007	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	kmlmap: A Stata command for producing Google's Keyhole Markup Language	Journal	Article	2013	1	1	English	STATA JOURNAL	107-113	7	1	WOS:000317249700007	kmlmap produces a Keyhole Markup Language file by using Stata data and geographical coordinates. This file produces a map when uploaded to Google Maps. The resulting map is a so-called choropleth or thematic map, where units of analysis are colored according to values of a variable. You can click on units of analysis on the map to display more information, to zoom, and to do all other things that Google Maps can do. The units of analysis can be points or polygons.
8255551	WOS:000317249700006	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Regression anatomy, revealed	Journal	Article	2013	1	1	English	STATA JOURNAL	92-106	15	1	WOS:000317249700006	The regression anatomy theorem (Angrist and Pischke, 2009, Mostly Harmless Econometrics: An Empiricist's Companion [Princeton University Press]) is an alternative formulation of the Frisch-Waugh-Lovell theorem (Frisch and Waugh, 1933, Econometrica 1: 387-401; Lovell, 1963, Journal of the American Statistical Association 58: 993-1010), a key finding in the algebra of ordinary least-squares multiple regression models. In this article, I present a command, reganat, to implement graphically the method of regression anatomy. This addition complements the built-in Stata command avplot in the validation of linear models, producing bidimensional scatterplots and regression lines obtained by controlling for the other covariates, along with several fine-tuning options. Moreover, I provide 1) a fully worked-out proof of the regression anatomy theorem and 2) an explanation of how the regression anatomy and the Frisch-Waugh-Lovell theorems relate to partial and semipartial correlations, whose coefficients are informative when evaluating relevant variables in a linear regression model.
8272956	WOS:000318237500001	134TM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Interactive Network Exploration with Orange	Journal	Article	2013	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000318237500001	Network analysis is one of the most widely used techniques in many areas of modern science. Most existing tools for that purpose are limited to drawing networks and computing their basic general characteristics. The user is not able to interactively and graphically manipulate the networks, select and explore subgraphs using other statistical and data mining techniques, add and plot various other data within the graph, and so on. In this paper we present a tool that addresses these challenges, an add-on for exploration of networks within the general component-based environment Orange.
8275764	WOS:000328130300001	267WZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The lifecontingencies Package: Performing Financial and Actuarial Mathematics Calculations in R	Journal	Article	2013	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	36	1	WOS:000328130300001	It is possible to model life contingency insurances with the life contingencies R package, which is capable of performing financial and actuarial mathematics calculations. Its functions permit one to determine both the expected value and the stochastic distribution of insured bene fits. Therefore, life insurance coverage can be priced and portfolios risk-based capital requirements can be assessed. This paper briefly summarizes the theory regarding life contingencies that is based on financial mathematics and demographic concepts. Then, with the aid of applied examples, it shows how the lifecontingencies package can be a useful tool for executing routine, deterministic, or stochastic calculations for life-contingencies actuarial mathematics.
8290744	WOS:000329680600010	289KE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	group2: Generating the finest partition that is coarser than two given partitions	Journal	Article	2013	1	1	English	STATA JOURNAL	867-875	9	1	WOS:000329680600010	In this article, I develop a useful interpretation of the function group() based on partitions belonging to mathematical set theory, an interpretation that in turn engenders a related command here called group2. In the context of the partitioning of sets, while the function group() creates a variable that generates the coarsest partition that is finer than the finest partition generated by the variables used as arguments, the group2 command will create a variable that generates the finest partition that is coarser than the coarsest partition generated by the variables used as arguments. This latter operation has proven very useful in several problems of database management. An introduction of this new command in the context of mathematical partitions is provided, and two examples of its application are presented.
8304733	WOS:000330193300011	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	CompLognormal: An R Package for Composite Lognormal Distributions	Journal	Article	2013	12	1	English	R JOURNAL	98-104	7	1	WOS:000330193300011	In recent years, composite models based on the lognormal distribution have become popular in actuarial sciences and related areas. In this short note, we present a new R package for computing the probability density function, cumulative density function, and quantile function, and for generating random numbers of any composite model based on the lognormal distribution. The use of the package is illustrated using a real data set.
8321474	WOS:000323909800001	211LS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	isocir: An R Package for Constrained Inference Using Isotonic Regression for Circular Data, with an Application to Cell Biology	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000323909800001	In many applications one may be interested in drawing inferences regarding the order of a collection of points on a unit circle. Due to the underlying geometry of the circle standard constrained inference procedures developed for Euclidean space data are not applicable. Recently,statistical inference for parameters under such order constraints on a unit circle was discussed in Rueda ,Fernandez, and Peddada (2009) and Fernandez, Rueda,and Peddada(2012).In this paper we introduce the R package isocir which provides as et of functions that can be used for analyzing angular data subject to order constraints on a unit circle. Since this work is motivated by applications in cellbiology, we illustrate the proposed package using a relevant cell cycle data.
8331603	WOS:000328131700001	267XM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Scalable Strategies for Computing with Massive Data	Journal	Article	2013	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000328131700001	This paper presents two complementary statistical computing frameworks that address challenges in parallel processing and the analysis of massive data. First, the for e a c h package allows users of the R programming environment to de fine parallel loops that may be run sequentially on a single machine, in parallel on a symmetric multiprocessing (SMP) machine, or in cluster environments without platform-specific code. Second, the bigmemory package implements memory-and file-mapped data structures that provide (a) access to arbitrarily large data while retaining a look and feel that is familiar to R users and (b) data structures that are shared across processor cores in order to support efficient parallel computing techniques. Although these packages may be used independently, this paper shows how they can be used in combination to address challenges that have effectively been beyond the reach of researchers who lack specialized software development skills or expensive hardware.
8343679	WOS:000317249700001	121MU	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Stata as a numerical tool for scientific thought experiments: A tutorial with worked examples	Journal	Article	2013	1	1	English	STATA JOURNAL	3-20	18	1	WOS:000317249700001	Thought experiments based on simulation can be used to explain the impact of the chosen study design, statistical analysis strategy, or the sensitivity of results to fellow researchers. In this article, we demonstrate with two examples how to implement quantitative thought experiments in Stata. The first example uses a large-sample approach to study the impact on the estimated effect size of dichotomizing an exposure variable at different values. The second example uses simulations of datasets of realistic size to illustrate the necessity of using sampling fractions as inverse probability weights in statistical analysis for protection against bias in a complex sampling design. We also give a brief outline of the general steps needed for implementing quantitative thought experiments in Stata. We demonstrate how Stata provides programming facilities for conveniently implementing such thought experiments, with the advantage of saving researchers time, speculation, and debate as well as improving communication in interdisciplinary research groups.
8356852	WOS:000330193300002	296OA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	factorplot: Improving Presentation of Simple Contrasts in Generalized Linear Models	Journal	Article	2013	12	1	English	R JOURNAL	4-15	12	1	WOS:000330193300002	Recent statistical literature has paid attention to the presentation of pairwise comparisons either from the point of view of the reference category problem in generalized linear models (GLMs) or in terms of multiple comparisons. Both schools of thought are interested in the parsimonious presentation of sufficient information to enable readers to evaluate the significance of contrasts resulting from the inclusion of qualitative variables in GLMs. These comparisons also arise when trying to interpret multinomial models where one category of the dependent variable is omitted as a reference. While considerable advances have been made, opportunities remain to improve the presentation of this information, especially in graphical form. The factorplot package provides new functions for graphically and numerically presenting results of hypothesis tests related to pairwise comparisons resulting from qualitative covariates in GLMs or coefficients in multinomial logistic regression models.
8367481	WOS:000324371500001	217PL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ebalance: A Stata Package for Entropy Balancing	Journal	Article	2013	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	18	1	WOS:000324371500001	The Stata package ebalance implements entropy balancing, a multivariate reweighting method described in Hainmueller (2012) that allows users to reweight a dataset such that the covariate distributions in the reweighted data satisfy a set of specified moment conditions. This can be useful to create balanced samples in observational studies with a binary treatment where the control group data can be reweighted to match the covariate moments in the treatment group. Entropy balancing can also be used to reweight a survey sample to known characteristics from a target population.
8394456	WOS:000321944400012	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	stellaR: A Package to Manage Stellar Evolution Tracks and Isochrones	Journal	Article	2013	6	1	English	R JOURNAL	108-116	9	1	WOS:000321944400012	We present the R package stellaR, which is designed to access and manipulate publicly available stellar evolutionary tracks and isochrones from the Pisa low-mass database. The procedures for extracting important stages in the evolution of a star from the database, for constructing isochrones from stellar tracks and for interpolating among tracks are discussed and demonstrated. Due to the advance in the instrumentation, nowadays astronomers can deal with a huge amount of high-quality observational data. In the last decade impressive improvements of spectroscopic and photometric observational capabilities made available data which stimulated the research in the globular clusters field. The theoretical effort of recovering the evolutionary history of the clusters benefits from the computation of extensive databases of stellar tracks and isochrones, such as Pietrinferni et al. (2006); Dotter et al. (2008); Bertelli et al. (2008). We recently computed a large data set of stellar tracks and isochrones, "The Pisa low-mass database" (Dell'Omodarme et al., 2012), with up to date physical and chemical inputs, and made available all the calculations to the astrophysical community at the Centre de Donnes astronomiques de Strasbourg (CDS)(1), a data center dedicated to the collection and worldwide distribution of astronomical data. In most databases, the management of the information and the extraction of the relevant evolutionary properties from libraries of tracks and/ or isochrones is the responsibility of the end users. Due to its extensive capabilities of data manipulation and analysis, however, R is an ideal choice for these tasks. Nevertheless R is not yet well known in astrophysics; up to December 2012 only seven astronomical or astrophysical-oriented packages have been published on CRAN (see the CRAN Task View Chemometrics and Computational Physics). The package stellaR (Dell'Omodarme and Valle, 2012) is an effort to make available to the astrophysical community a basic tool set with the following capabilities: retrieve the required calculations from CDS; plot the information in a suitable form; construct by interpolation tracks or isochrones of compositions different to the ones available in the database; construct isochrones for age not included in the database; extract relevant evolutionary points from tracks or isochrones.
8403027	WOS:000314392600001	082LF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The cg Package for Comparison of Groups	Journal	Article	2013	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000314392600001	In research of medicines, the comparison of treatments, test articles, conditions, administrations, etc., is very common. Studies are completed, and the data are then most often analyzed with a default mixture of equal variance t tests, analysis of variance,and multiple comparison procedures. But even for an implicit, presumed one-factor linear model to compare groups,more often than not there is the added need to accommodate data which is better suited for expression of multiplicative effects, potential outliers, and limits of detection. Base R and contributed packages provide all the pieces to develop a comprehensive strategy to account for these needs. Such an approach includes exploration of the data, fitting models, formal analysis to gauge the magnitude of effects, and checking of assumptions. The cg package is developed with those goals in mind, using a flow of wrapper functions to guide the full analysis and interpretation of the data. Examples from our non-clinical world of research will be used to illustrate the package and strategy.
8414639	WOS:000321944400018	185DC	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Fast Pure R Implementation of GEE: Application of the Matrix Package	Journal	Article	2013	6	1	English	R JOURNAL	181-187	7	1	WOS:000321944400018	Generalized estimating equation solvers in R only allow for a few pre-determined options for the link and variance functions. We provide a package, geeM, which is implemented entirely in R and allows for user specified link and variance functions. The sparse matrix representations provided in the Matrix package enable a fast implementation. To gain speed, we make use of analytic inverses of the working correlation when possible and a trick to find quick numeric inverses when an analytic inverse is not available. Through three examples, we demonstrate the speed of geeM, which is not much worse than C implementations like geepack and gee on small data sets and faster on large data sets.
8434432	WOS:000312290100001	053RG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	tmle: An R Package for Targeted Maximum Likelihood Estimation	Journal	Article	2012	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-35	35	1	WOS:000312290100001	Targeted maximum likelihood estimation (TMLE) is a general approach for constructing an efficient double-robust semi-parametric substitution estimator of a causal effect parameter or statistical association measure. tmle is a recently developed R package that implements TMLE of the effect of a binary treatment at a single point in time on an outcome of interest, controlling for user supplied covariates, including an additive treatment effect, relative risk, odds ratio, and the controlled direct effect of a binary treatment controlling for a binary intermediate variable on the pathway from treatment to the outcome. Estimation of the parameters of a marginal structural model is also available. The package allows outcome data with missingness, and experimental units that contribute repeated records of the point-treatment data structure, thereby allowing the analysis of longitudinal data structures. Relevant factors of the likelihood may be modeled or fit data-adaptively according to user specifications, or passed in from an external estimation procedure. Effect estimates, variances, p values, and 95% confidence intervals are provided by the software.
8442738	WOS:000313139600002	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A generalized missing-indicator approach to regression with imputed covariates	Journal	Article	2012	1	1	English	STATA JOURNAL	575-604	30	1	WOS:000313139600002	We consider estimation of a linear regression model using data where some covariate values are missing but imputations are available to fill in the missing values. This situation generates a tradeoff between bias and precision when estimating the regression parameters of interest. Using only the subsample of complete observations does not cause bias but may imply a substantial loss of precision because the complete cases may be too few. On the other hand, filling in the missing values with imputations may cause bias. We provide the new Stata command gmi, which handles such tradeoff by using either model reduction or Bayesian model averaging techniques in the context of the generalized missing-indicator approach recently proposed by Dardanoni, Modica, and Peracchi (2011, Journal of Econometrics 162: 362-368). If multiple imputations are available, gmi can also be combined with the built-in Stata prefix mi estimate to account for extra variability due to imputation. We illustrate the use of gmi with an empirical application in the health domain, where item nonresponse is substantial.
8446644	WOS:000301073100001	902WD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Graphical Independence Networks with the gRain Package for R	Journal	Article	2012	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000301073100001	In this paper we present the R package gRain for propagation in graphical independence networks (for which Bayesian networks is a special instance). The paper includes a description of the theory behind the computations. The main part of the paper is an illustration of how to use the package. The paper also illustrates how to turn a graphical model and data into an independence network.
8448956	WOS:000326872100001	250RR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Basic Functions for Supporting an Implementation of Choice Experiments in R	Journal	Article	2012	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000326872100001	The package support.CEs provides seven basic functions that support the implementation of choice experiments (CEs) in R : two functions for creating a CE design, which is based on orthogonal main-effect arrays; a function for converting a CE design into questionnaire format; a function for converting a CE design into a design matrix; a function for making the data set suitable for the implementation of a conditional logit model; a function for calculating the goodness-of-fit measures of an estimated model; and a function for calculating the marginal willingness to pay for the attributes and/or levels of the estimated model.
8460784	WOS:000326871100001	250RJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Separation-Resistant and Bias-Reduced Logistic Regression: STATISTICA Macro	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-12	12	1	WOS:000326871100001	Logistic regression is one of the most popular techniques used to describe the relationship between a binary dependent variable and a set of independent variables. However, the application of logistic regression to small data sets is often hindered by the complete or quasicomplete separation. Under the separation scenario, results obtained via maximum likelihood should not be trusted, since at least one parameter estimate diverges to innity. Firth's approach to logistic regression is a theoretically sound procedure, which is guaranteed to arrive at finite estimates even in a separation case. Firth's procedure was also proved to significantly reduce the small sample bias of maximum likelihood estimates. The main goal of the paper is to introduce the STATISTICA macro, which performs Firth-type logistic regression.
8478378	WOS:000312289800001	053RD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MixSim: An R Package for Simulating Data to Study Performance of Clustering Algorithms	Journal	Article	2012	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000312289800001	The R package MixSim is a new tool that allows simulating mixtures of Gaussian distributions with different levels of overlap between mixture components. Pairwise overlap, defined as a sum of two misclassification probabilities, measures the degree of interaction between components and can be readily employed to control the clustering complexity of datasets simulated from mixtures. These datasets can then be used for systematic performance investigation of clustering and finite mixture modeling algorithms. Among other capabilities of MixSim, there are computing the exact overlap for Gaussian mixtures, simulating Gaussian and non-Gaussian data, simulating outliers and noise variables, calculating various measures of agreement between two partitionings, and constructing parallel distribution plots for the graphical display of finite mixture models. All features of the package are illustrated in great detail. The utility of the package is highlighted through a small comparison study of several popular clustering algorithms.
8500221	WOS:000326872300001	250RT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	%GI: A SAS Macro for Measuring and Testing Global Imbalance of Covariates within Subgroups	Journal	Article	2012	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000326872300001	The global imbalance (GI) measure is a way for checking balance of baseline covariates that confound efforts to draw valid conclusions about treatment effects on outcomes of interest. In addition, GI is tested by means of a multivariate test. The GI measure and its test overcome some limitations of the common way for assessing the presence of imbalance in observed covariates that were discussed in D'Attoma and Camillo (2011). A user written SAS macro called %GI, to simultaneously measure and test global imbalance of baseline covariates is described. Furthermore, %GI also assesses global imbalance by subgroups obtained through several matching or classification methods (e. g., cluster analysis, propensity score subclassification, Rosenbaum and Rubin 1984), no matter how many groups are examined. %GI works with mixed categorical, ordinal and continuous covariates. Continuous baseline covariates need to be split into categories. It also works in the multi-treatment case. The use of the %GI macro will be illustrated using two artificial examples.
8529806	WOS:000313139600010	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Incorporating complex sample design effects when only final survey weights are available	Journal	Article	2012	1	1	English	STATA JOURNAL	718-725	8	1	WOS:000313139600010	In this article, we consider the situation that arises when a survey data producer has collected data from a sample with a complex design (possibly featuring stratification of the population, cluster sampling, and unequal probabilities of selection) and for various reasons only provides secondary analysts of those survey data with a final survey weight for each respondent and "average" design effects for survey estimates computed from the data. In general, these "average" design effects, presumably computed by the data producer in a way that fully accounts for all the complex sampling features, already incorporate possible increases in sampling variance due to the use of the survey weights in estimation. The secondary analyst of the survey data-who uses the provided information to compute weighted estimates; computes design-based standard errors reflecting variance in the weights (by using Taylor series linearization, for example); and inflates the estimated variances using the "average" design effects provided-is applying a "double" adjustment to the standard errors for the effect of weighting on the variance estimates, leading to overly conservative inferences. We propose a simple method to prevent this problem and provide a Stata program for applying appropriate adjustments to variance estimates in this situation. We illustrate two applications of the method with survey data from the Monitoring the Future study and conclude with suggested directions for future research in this area.
8530362	WOS:000305065000001	956BO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Classification Trees for Ordinal Responses in R: The rpartScore Package	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000305065000001	This paper introduces rpartScore (Galimberti, Soffritti, and Di Maso 2012), a new R package for building classification trees for ordinal responses, that can be employed whenever a set of scores is assigned to the ordered categories of the response. This package has been created to overcome some problems that produced unexpected results from the package rpartOrdinal (Archer 2010). Explanations for the causes of these unexpected results are provided. The main functionalities of rpartScore are described, and its use is illustrated through some examples.
8539678	WOS:000305065100001	956BP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Causal Inference Using Graphical Models with the R Package pcalg	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000305065100001	The pcalg package for R can be used for the following two purposes: Causal structure learning and estimation of causal effects from observational data. In this document, we give a brief overview of the methodology, and demonstrate the package's functionality in both toy examples and applications.
8557244	WOS:000326870600001	250RG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	%PROC_R: ASAS Macro that Enables Native R Programming in the Base SAS Environment	Journal	Article	2012	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000326870600001	In this paper, we describe %PROC_R, a SAS macro that enables native R language to be embedded in and executed along with a SAS program in the base SAS environment under Windows OS. This macro executes a user-defined R code in batch mode by calling the unnamed pipe method within base SAS. The R textual and graphical output can be routed to the SAS output window and result viewer, respectively. Also, this macro automatically converts data between SAS datasets and R data frames such that the data and results from each statistical environment can be utilized by the other environment. The objective of this work is to leverage the strength of the R programming language within the SAS environment in a systematic manner. Moreover, this macro helps statistical programmers to learn a new statistical language while staying in a familiar environment.
8567232	WOS:000308909900001	007TA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Spherical k-Means Clustering	Journal	Article	2012	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000308909900001	Clustering text documents is a fundamental task in modern data analysis, requiring approaches which perform well both in terms of solution quality and computational efficiency. Spherical k-means clustering is one approach to address both issues, employing cosine dissimilarities to perform prototype-based partitioning of term weight representations of the documents. This paper presents the theory underlying the standard spherical k-means problem and suitable extensions, and introduces the R extension package skmeans which provides a computational environment for spherical k-means clustering featuring several solvers: a fixed-point and genetic algorithm, and interfaces to two external solvers (CLUTO and Gmeans). Performance of these solvers is investigated by means of a large scale benchmark experiment.
8568097	WOS:000305117800001	956VN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Flexible Rasch Mixture Models with Package psychomix	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000305117800001	Measurement invariance is an important assumption in the Rasch model and mixture models constitute a flexible way of checking for a violation of this assumption by detecting unobserved heterogeneity in item response data. Here, a general class of Rasch mixture models is established and implemented in R, using conditional maximum likelihood estimation of the item parameters (given the raw scores) along with flexible specification of two model building blocks: (1) Mixture weights for the unobserved classes can be treated as model parameters or based on covariates in a concomitant variable model. (2) The distribution of raw score probabilities can be parametrized in two possible ways, either using a saturated model or a specification through mean and variance. The function raschmix () in the R package psychomix provides these models, leveraging the general infrastructure for fitting mixture models in the flexmix package. Usage of the function and its associated methods is illustrated on artificial data as well as empirical data from a study of verbally aggressive behavior.
8573819	WOS:000306347300001	973GF	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A robust instrumental-variables estimator	Journal	Article	2012	1	1	English	STATA JOURNAL	169-181	13	1	WOS:000306347300001	The classical instrumental-variables estimator is extremely sensitive to the presence of outliers in the sample. This is a concern because outliers can strongly distort the estimated effect of a given regressor on the dependent variable. Although outlier diagnostics exist, they frequently fail to detect atypical observations because they are themselves based on nonrobust (to outliers) estimators. Furthermore, they do not take into account the combined influence of outliers in the first and second stages of the instrumental-variables estimator. In this article, we present a robust instrumental-variables estimator, initially proposed by Cohen Freue, Ortiz-Molina, and Zamar (2011, Working paper: http://www.stat.ubc.ca/similar to ruben/website/cv/cohen-zamar.pdf), that we have programmed in Stata and made available via the robivreg command. We have improved on their estimator in two different ways. First, we use a weighting scheme that makes our estimator more efficient and allows the computations of the usual identification and overidentifying restrictions tests. Second, we implement a generalized Hausman test for the presence of outliers.
8585977	WOS:000303616500008	936UW	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Output to order	Journal	Article	2012	1	1	English	STATA JOURNAL	147-158	12	1	WOS:000303616500008	Wanting to present Stata output in a different way is a very common desire that lies behind many substantial user-written programs. Here I start at the beginning with some basic tips and tricks. I discuss using display to replay the results you want; putting results into new variables so that they can be listed, tabulated, or plotted; and using existing reduction commands to produce new datasets containing results.
8612543	WOS:000303616500003	936UW	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Age-period-cohort models in Stata	Journal	Article	2012	1	1	English	STATA JOURNAL	45-60	16	1	WOS:000303616500003	In this article, I describe and illustrate Stata programs that facilitate i) the fitting of smooth age-period-cohort models to event data and ii) the plotting of observed and fitted rates. The programs include postestimation functionality and flexibility to fit models not possible using Stata's glm command. What distinguishes this article from a recent Stata Journal article on age-period-cohort models by Rutherford, Lambert, and Thompson (2010, Stata Journal 10: 606-627) is that the emphasis here is on extrapolating the model fit to make projections into the future.
8634358	WOS:000305990100001	968NS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Graphical User Interface for R in a Rich Client Platform for Ecological Modeling	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000305990100001	For many ecological analyses powerful statistical tools are required for a profound analysis of spatial and time based data sets. In order to avoid many common errors of analysis and data acquisition a graphical user interface can help to focus on the task of the analysis and minimize the time to fulfill certain tasks in a programming language like R. In this paper we present a graphical user interface for R embedded in the ecological modeling software Bio7 which is based on an Eclipse rich client platform. We demonstrate that within the Bio7 platform R can not only be effectively combined with Java but also with the powerful components of Eclipse. Additionally we present some custom Bio7 components which interact with R and make use of some useful advanced concepts and libraries of this connection. Our overview on the Bio7 R interface also emphasizes a broad applicability for disciplines beyond ecological modelling.
8642679	WOS:000313198000002	066CR	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	What's in a Name? The Importance of Naming grid Grobs When Drawing Plots in R	Journal	Article	2012	12	1	English	R JOURNAL	5-12	8	1	WOS:000313198000002	Any shape that is drawn using the grid graphics package can have a name associated with it. If a name is provided, it is possible to access, query, and modify the shape after it has been drawn. These facilities allow for very detailed customisations of plots and also for very general transformations of plots that are drawn by packages based on grid.
8664598	WOS:000305991700001	968OG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	RKWard: A Comprehensive Graphical User Interface and Integrated Development Environment for Statistical Analysis with R	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000305991700001	R is a free open-source implementation of the S statistical computing language and programming environment. The current status of R is a command line driven interface with no advanced cross-platform graphical user interface (GUI), but it includes tools for building such. Over the past years, proprietary and non-proprietary GUI solutions have emerged, based on internal or external tool kits, with different scopes and technological concepts. For example, Rgui.exe and Rgui.app have become the de facto GUI on the Microsoft Windows and Mac OS X platforms, respectively, for most users. In this paper we discuss RKWard which aims to be both a comprehensive GUI and an integrated development environment for R. RKWard is based on the KDE software libraries. Statistical procedures and plots are implemented using an extendable plugin architecture based on ECMAScript (JavaScript), R, and XML. RKWard provides an excellent tool to manage different types of data objects; even allowing for seamless editing of certain types. The objective of RKWard is to provide a portable and extensible R interface for both basic and advanced statistical and graphical analysis, while not compromising on flexibility and modularity of the R programming environment itself.
8677409	WOS:000326870900001	250RH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Intake_epis_food (): An R Function for Fitting a Bivariate Nonlinear Measurement Error Model to Estimate Usual and Energy Intake for Episodically Consumed Foods	Journal	Article	2012	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000326870900001	We consider a Bayesian analysis using WinBUGS to estimate the distribution of usual intake for episodically consumed foods and energy (calories). The model uses measures of nutrition and energy intakes via a food frequency questionnaire (FFQ) along with repeated 24 hour recalls and adjusting covariates. In order to estimate the usual intake of the food, we phrase usual intake in terms of person-specific random effects, along with day-to-day variability in food and energy consumption. Three levels are incorporated in the model. The first level incorporates information about whether an individual in fact reported consumption of a particular food item. The second level incorporates the amount of intake from those individuals who reported consumption of the food, and the third level incorporates the energy intake. Estimates of posterior means of parameters and distributions of usual intakes are obtained by using Markov chain Monte Carlo calculations. This R function reports to users point estimates and credible intervals for parameters in the model, samples from their posterior distribution, samples from the distribution of usual intake and usual energy intake, trace plots of parameters and summary statistics of usual intake, usual energy intake and energy adjusted usual intake.
8692814	WOS:000309735300009	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Partial frontier efficiency analysis	Journal	Article	2012	1	1	English	STATA JOURNAL	461-478	18	1	WOS:000309735300009	Despite their frequent use in applied work, nonparametric approaches to efficiency analysis-namely, data envelopment analysis and free disposal hull-have had reputations among econometricians. This is mainly because data envelopment analysis and free disposal hull represent deterministic approaches that are highly sensitive to outliers and measurement errors. However, so-called partial frontier approaches have recently been developed, namely, order-m and order-alpha. These approaches generalize free disposal hull by allowing for superefficient observations to be located beyond the estimated production-possibility frontier. Although these methods are also purely nonparametric, the sensitivity to outliers is substantially reduced by partial frontier approaches enveloping just a subsample of observations. In this article, I introduce the new Stata commands orderm and orderalpha, which implement order-m, order-alpha, and free disposal hull efficiency analysis in Stata. The commands allow for several options, such as statistical inference based on subsampling bootstrapping.
8703267	WOS:000313197700008	066CO	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	maxent: An R Package for Low-memory Multinomial Logistic Regression with Support for Semi-automated Text Classification	Journal	Article	2012	6	1	English	R JOURNAL	56-59	4	1	WOS:000313197700008	maxent is a package with tools for data classification using multinomial logistic regression, also known as maximum entropy. The focus of this maximum entropy classifier is to minimize memory consumption on very large datasets, particularly sparse document-term matrices represented by the tm text mining package.
8710288	WOS:000312289200001	053QX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	spacetime: Spatio-Temporal Data in R	Journal	Article	2012	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000312289200001	This document describes classes and methods designed to deal with different types of spatio-temporal data in R implemented in the R package spa c e time, and provides examples for analyzing them. It builds upon the classes and methods for spatial data from package sp, and for time series data from package xts. The goal is to cover a number of useful representations for spatio-temporal sensor data, and results from predicting (spatial and/or temporal interpolation or smoothing), aggregating, or subsetting them, and to represent trajectories. The goals of this paper is to explore how spatio-temporal data can be sensibly represented in classes, and to find out which analysis and visualisation methods are useful and feasible. We discuss the time series convention of representing time intervals by their starting time only. This document is the main reference for the R package spacetime, and is available (in updated form) as a vignette in this package.
8763701	WOS:000305991300001	968OC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The RcmdrPlugin.survival Package: Extending the R Commander Interface to Survival Analysis	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000305991300001	The R Commander graphical user interface to R is extensible via plug-in packages, which integrate seamlessly with the R Commander's menu structure, data, and model handling. The paper describes the RcmdrPlugin.survival package, which makes many of the facilities of the survival package for R available through the R Commander, including Cox and parametric survival models. We explain the structure, capabilities, and limitations of this plug-in package and illustrate its use.
8786143	WOS:000305991000001	968OA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	tourrGui: A gWidgets GUI for the Tour to Explore High-Dimensional Data Using Low-Dimensional Projections	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-12	12	1	WOS:000305991000001	This paper describes a graphical user interface (GUI) for the tourr package in R. The tour is a dynamic graphical method for viewing multivariate data. The GUI allows users to interact with a tour in order to explore the data for structures like clustering, outliers, nonlinear dependence. Users can pause the tour, choose a subset of variables, color points by other variables, and switch between several different types of tours.
8796340	WOS:000301231700001	904XH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multivariate Generalizations of the Multiplicative Binomial Distribution: Introducing the MM Package	Journal	Article	2012	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000301231700001	We present two natural generalizations of the multinomial and multivariate binomial distributions, which arise from the multiplicative binomial distribution of Altham (1978). The resulting two distributions are discussed and we introduce an R package, MM, which includes associated functionality.
8842999	WOS:000303803600001	939IS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	GeoXp: An R Package for Exploratory Spatial Data Analysis	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000303803600001	We present GeoXp, an R package implementing interactive graphics for exploratory spatial data analysis. We use a data set concerning public schools of the French Midi-Pyrenees region to illustrate the use of these exploratory techniques based on the coupling between a statistical graph and a map. Besides elementary plots like boxplots, histograms or simple scatterplots, GeoXp also couples maps with Moran scatterplots, variogram clouds, Lorenz curves and other graphical tools. In order to make the most of the multidimensionality of the data, GeoXp includes dimension reduction techniques such as principal components analysis and cluster analysis whose results are also linked to the map.
8867046	WOS:000313139600007	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Simulating complex survival data	Journal	Article	2012	1	1	English	STATA JOURNAL	674-687	14	1	WOS:000313139600007	Simulation studies are essential for understanding and evaluating both current and new statistical models. When simulating survival times, one often assumes an exponential or Weibull distribution for the baseline hazard function, with survival times generated using the method of Bender, Augustin, and Blettner (2005, Statistics in Medicine 24: 1713-1723). Assuming a constant or monotonic hazard can be considered too simplistic and can lack biological plausibility in many situations. We describe a new user-written command, survsim, which allows the user to simulate survival times from two-component parametric mixture models, providing much more flexibility in the underlying hazard. Standard parametric distributions can also be used, including the exponential, Weibull, and Gompertz. Furthermore, survival times can be simulated from the all-cause distribution of cause-specific hazards for competing risks by using the method of Beyersmann et al. (2009, Statistics in Medicine 24: 956-971). A multinomial distribution is used to create the event indicator, whereby the probability of experiencing each event at a simulated time t is the cause-specific hazard divided by the all-cause hazard evaluated at time t. Baseline covariates can be included in all scenarios. We also describe the extension to incorporate nonproportional hazards in standard parametric and competing-risks scenarios.
8869800	WOS:000305065300001	956BR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	tclust: An R Package for a Trimming Approach to Cluster Analysis	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000305065300001	Outlying data can heavily influence standard clustering methods. At the same time, clustering principles can be useful when robustifying statistical procedures. These two reasons motivate the development of feasible robust model-based clustering approaches. With this in mind, an R package for performing non-hierarchical robust clustering, called tclust, is presented here. Instead of trying to "fit" noisy data, a proportion alpha of the most outlying observations is trimmed. The tclust package efficiently handles different cluster scatter constraints. Graphical exploratory tools are also provided to help the user make sensible choices for the trimming proportion as well as the number of clusters to search for.
8903477	WOS:000306914000001	980SS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MortalitySmooth: An R Package for Smoothing Poisson Counts with P-Splines	Journal	Article	2012	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000306914000001	The MortalitySmooth package provides a framework for smoothing count data in both one- and two-dimensional settings. Although general in its purposes, the package is specifically tailored to demographers, actuaries, epidemiologists, and geneticists who may be interested in using a practical tool for smoothing mortality data over ages and/or years. The total number of deaths over a specified age- and year-interval is assumed to be Poisson-distributed, and P-splines and generalized linear array models are employed as a suitable regression methodology. Extra-Poisson variation can also be accommodated. Structured in an S 3 object orientation system, MortalitySmooth has two main functions which fit the data and define two classes of objects: Mort1Dsmooth and Mort2Dsmooth. The methods for these classes (print, summary, plot, predict, and residuals) are also included. These features make it easy for users to extract and manipulate the outputs. In addition, a collection of mortality data is provided. This paper provides an overview of the design, aims, and principles of MortalitySmooth, as well as strategies for applying it and extending its use.
8952793	WOS:000312289700001	053RC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	parfm: Parametric Frailty Models in R	Journal	Article	2012	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000312289700001	Frailty models are getting more and more popular to account for overdispersion and/or clustering in survival data. When the form of the baseline hazard is somehow known in advance, the parametric estimation approach can be used advantageously. Nonetheless, there is no unified widely available software that deals with the parametric frailty model. The new parfm package remedies that lack by providing a wide range of parametric frailty models in R. The gamma, inverse Gaussian, and positive stable frailty distributions can be specified, together with five different baseline hazards. Parameter estimation is done by maximising the marginal log-likelihood, with right-censored and possibly left-truncated data. In the multivariate setting, the inverse Gaussian may encounter numerical difficulties with a huge number of events in at least one cluster. The positive stable model shows analogous difficulties but an ad-hoc solution is implemented, whereas the gamma model is very resistant due to the simplicity of its Laplace transform.
8955509	WOS:000306347300009	973GF	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Using the margins command to estimate and interpret adjusted predictions and marginal effects	Journal	Article	2012	1	1	English	STATA JOURNAL	308-331	24	1	WOS:000306347300009	Many researchers and journals place a strong emphasis on the sign and statistical significance of effects-but often there is very little emphasis on the substantive and practical significance of the findings. As Long and Freese (2006, Regression Models for Categorical Dependent Variables Using Stata [Stata Press]) show, results can often be made more tangible by computing predicted or expected values for hypothetical or prototypical cases. Stata 11 introduced new tools for making such calculations-factor variables and the margins command. These can do most of the things that were previously done by Stata's own adjust and mfx commands, and much more. Unfortunately, the complexity of the margins syntax, the daunting 50-page reference manual entry that describes it, and a lack of understanding about what margins offers over older commands that have been widely used for years may have dissuaded some researchers from examining how the margins command could benefit them. In this article, therefore, I explain what adjusted predictions and marginal effects are, and how they can contribute to the interpretation of results. I further explain why older commands, like adjust and mfx, can often produce incorrect results, and how factor variables and the margins command can avoid these errors. The relative merits of different methods for setting representative values for variables in the model (marginal effects at the means, average marginal effects, and marginal effects at representative values) are considered. I shows how the marginsplot command (introduced in Stata 12) provides a graphical and often much easier means for presenting and understanding the results from margins, and explain why margins does not present marginal effects for interaction terms.
8956736	WOS:000310774000001	033DM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	speedR: An R Package for Interactive Data Import, Filtering and Ready-to-Use Code Generation	Journal	Article	2012	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-12	12	1	WOS:000310774000001	Emerging technologies in the experimental sciences have opened the way for large-scale experiments. Such experiments generate ever growing amounts of data from which researchers need to extract relevant pieces for subsequent analysis. R offers a great environment for statistical analysis. However, due to the diversity of possible data sources and formats, data preprocessing and import can be time consuming especially with data that require user interaction such as editing, filtering or formatting. Writing a code for these tasks can be time-consuming, error prone and rather complex. We present speedR, an R- package for interactive data import, filtering and code generation in order to address these needs. Using speedR, researchers can import new data, make basic corrections, examine current R session objects, open them in the speedR environment for filtering (subsetting), put the filtered data back into R, and even create new R functions with applied import and filtering constraints to speed up their productivity.
8957987	WOS:000303804800001	939JD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MPCI: An R Package for Computing Multivariate Process Capability Indices	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000303804800001	Manufacturing processes are often based on more than one quality characteristic. When these variables are correlated the process capability analysis should be performed using multivariate statistical methodologies. Although there is a growing interest in methods for evaluating the capability of multivariate processes, little attention has been given to developing user friendly software for supporting multivariate capability analysis. In this work we introduce the package MPCI for R, which allows to compute multivariate process capability indices. MPCI aims to provide a useful tool for dealing with multivariate capability assessment problems. We illustrate the use of MPCI package through both simulated and real examples.
9010493	WOS:000313197700010	066CO	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Who Did What? The Roles of R Package Authors and How to Refer to Them	Journal	Article	2012	6	1	English	R JOURNAL	64-69	6	1	WOS:000313197700010	Computational infrastructure for representing persons and citations has been available in R for several years, but has been restructured through enhanced classes "person" and "bibentry" in recent versions of R. The new features include support for the specification of the roles of package authors (e. g. maintainer, author, contributor, translator, etc.) and more flexible formatting/printing tools among various other improvements. Here, we introduce the new classes and their methods and indicate how this functionality is employed in the management of R packages. Specifically, we show how the authors of R packages can be specified along with their roles in package 'DESCRIPTION' and/or 'CITATION' files and the citations produced from it.
9028791	WOS:000305990500001	968NW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Closing the Gap between Methodologists and End-Users: R as a Computational Back-End	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000305990500001	The R environment provides a natural platform for developing new statistical methods due to the mathematical expressiveness of the language, the large number of existing libraries, and the active developer community. One drawback to R, however, is the learning curve; programming is a deterrent to non-technical users, who typically prefer graphical user interfaces (GUIs) to command line environments. Thus, while statisticians develop new methods in R, practitioners are often behind in terms of the statistical techniques they use as they rely on GUI applications. Meta-analysis is an instructive example; cutting-edge meta-analysis methods are often ignored by the overwhelming majority of practitioners, in part because they have no easy way of applying them. This paper proposes a strategy to close the gap between the statistical state-of-the-science and what is applied in practice. We present open-source meta-analys is software that uses R as the underlying statistical engine, and Python for the GUI. We present a frame work that allows methodologists to implement new methods in R that are then automatically integrated into the GUI for use by end-users, so long as the programmer conforms to our interface. Such an approach allows an intuitive interface for non-technical users while leveraging the latest advanced statistical methods implemented by methodologists
9062474	WOS:000309735300011	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Exact and mid-p confidence intervals for the odds ratio	Journal	Article	2012	1	1	English	STATA JOURNAL	505-514	10	1	WOS:000309735300011	The odds ratio is a frequently used effect measure for two independent binomial proportions. Unfortunately, the confidence intervals that are available for it in Stata and other standard software packages are generally wider than necessary, particularly for small-sample and exact estimation. The performance of the Cornfield exact interval-the only widely available exact interval for the odds ratio-may be improved by incorporating a small modification attributed to Baptista and Pike (1977, Journal of the Royal Statistical Society, Series C 26: 214-220). A further improvement is achieved when the Baptista-Pike method is combined with the mid-p approach. In this article, I present; the command merci (mid-p and exact odds-ratio confidence intervals) and its immediate version mercii, which calculate the Cornfield exact, Cornfield mid-p, Baptista-Pike exact, and Baptista-Pike mid-p confidence intervals for the odds ratio. I compare these intervals with three well-known logit intervals. I strongly recommend the Baptista-Pike mid-p interval.
9089446	WOS:000309735300010	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Sensible parameters for univariate and multivariate splines	Journal	Article	2012	1	1	English	STATA JOURNAL	479-504	26	1	WOS:000309735300010	The package bspline, downloadable from Statistical Software Components, now has three commands. The first, bspline, generates a basis of Schoenberg B-splines. The second, frencurv, generates a basis of reference splines whose parameters in the regression model are simply values of the spline at reference points on the X axis. The recent addition, flexcurv, is an easy-to-use version of frencurv that generates reference splines with automatically generated, sensibly spaced knots. frencurv and flexcurv now have the additional option of generating an incomplete basis of reference splines, with the reference spline for a baseline reference point omitted or set to 0. This incomplete basis can be completed by adding the standard unit vector to the design matrix and can then be used to estimate differences between values of the spline at the remaining reference points and the value of the spline at the baseline reference point. Reference splines therefore model continuous factor variables as indicator variables (or "dummies") model discrete factor variables. The method can be extended in a similar way to define factor-product bases, allowing the user to estimate factor-combination means, subset-specific effects, or even factor interactions involving multiple continuous or discrete factors.
9113406	WOS:000312289400001	053QZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multiple-Table Data in R with the multitable Package	Journal	Article	2012	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-38	38	1	WOS:000312289400001	Data frames are integral to R. They provide a standard format for passing data to model-fitting and plotting functions, and this standard makes it easier for experienced users to learn new functions that accept data as a single data frame. Still, many data sets do not easily fit into a single data frame; data sets in ecology with a so-called fourth-corner problem provide important examples. Manipulating such inherently multiple-table data using several data frames can result in long and difficult-to-read workflows. We introduce the R multitable package to provide new data storage objects called data.list objects,which extend the data.frame concept to explicitly multiple-table settings. Like data frames, data lists are lists of variables stored as vectors; what is new is that these vectors have dimension attributes that make accessing and manipulating them easier. As data.list objects can be coerced to data.frame objects, they can be used with all R functions that accept an object that is coercible to a data.frame.
9114850	WOS:000305992000001	968OI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	gWidgetsWWW: Creating Interactive Web Pages within R	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-12	12	1	WOS:000305992000001	The gWidgetsWWW package provides a framework for easily developing interactive web pages from within R. It uses the API developed in the gWidgets programming interface to specify the layout of the controls and the relationships between them. The web pages may be served locally under R's built-in web server for help pages or from an rApache-enabled web server.
9144952	WOS:000313197700005	066CO	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Foreign Library Interface	Journal	Article	2012	6	1	English	R JOURNAL	30-40	11	1	WOS:000313197700005	We present an improved Foreign Function Interface (FFI) for R to call arbitary native functions without the need for C wrapper code. Further we discuss a dynamic linkage framework for binding standard C libraries to R across platforms using a universal type information format. The package rdyncall comprises the framework and an initial repository of cross-platform bindings for standard libraries such as (legacy and modern) OpenGL, the family of SDL libraries and Expat. The package enables system-level programming using the R language; sample applications are given in the article. We outline the underlying automation tool-chain that extracts cross-platform bindings from C headers, making the repository extendable and open for library developers.
9155599	WOS:000309735300006	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Easy demand-system estimation with quaids	Journal	Article	2012	1	1	English	STATA JOURNAL	433-446	14	1	WOS:000309735300006	Previously, to fit an almost-ideal demand system in Stata, one would have to use the nlsur command and write a function evaluator program as described in [R] nlsur and Poi (2008, Stata Journal 8: 554-556). hi this article, I introduce the command quaids, which obviates the need for any programming by the user. The command fits Deaton and Muellbauer's (1980b, American Economic Review 70: 312-326) original almost-ideal demand-system model as well as Banks, Blundell, and Lewbel's (1997, Review of Economics and Statistics 79: 527-539) quadratic variant. Demographic variables can also be included in the model. Postestimation tools calculate expenditure and price elasticities.
9159854	WOS:000308910400001	007TF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	nparLD: An R Software Package for the Nonparametric Analysis of Longitudinal Data in Factorial Experiments	Journal	Article	2012	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000308910400001	Longitudinal data from factorial experiments frequently arise in various fields of study, ranging from medicine and biology to public policy and sociology. In most practical situations, the distribution of observed data is unknown and there may exist a number of atypical measurements and outliers. Hence, use of parametric and semiparametric procedures that impose restrictive distributional assumptions on observed longitudinal samples becomes questionable. This, in turn, has led to a substantial demand for statistical procedures that enable us to accurately and reliably analyze longitudinal measurements in factorial experiments with minimal conditions on available data, and robust nonparametric methodology offering such a possibility becomes of particular practical importance. In this article, we introduce a new R package nparLD which provides statisticians and researchers from other disciplines an easy and user-friendly access to the most up-to-date robust rank-based methods for the analysis of longitudinal data in factorial settings. We illustrate the implemented procedures by case studies from dentistry, biology, and medicine.
9163989	WOS:000305991500001	968OE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Deducer: A Data Analysis GUI for R	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000305991500001	While R has proven itself to be a powerful and flexible tool for data exploration and analysis, it lacks the ease of use present in other software such as SPSS and Minitab. An easy to use graphical user interface (GUI) can help new users accomplish tasks that would otherwise be out of their reach, and improves the efficiency of expert users by replacing fifty key strokes with five mouse clicks. With this in mind, Deducer presents dialogs that are understandable for the beginner, and yet contain all (or most) of the options that an experienced statistician, performing the same task, would want. An Excel-like spreadsheet is included for easy data viewing and editing. Deducer is based on Java's Swing GUI library and can be used on any common operating system. The GUI is independent of the specific R console and can easily be used by calling a text-based menu system. Graphical menus are provided for the JGR console and the Windows R GUI.
9164439	WOS:000306347300003	973GF	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	From resultssets to resultstables in Stata	Journal	Article	2012	1	1	English	STATA JOURNAL	191-213	23	1	WOS:000306347300003	The listtab package supersedes the listtex package. It inputs a list of variables and outputs them as a table in one of several formats, including TEX, LATEX, HTML, Microsoft Rich Text Format, or possibly future XML-based formats. It works with a team of four other packages: sdecode, chardef, xrewide, and ingap, which are downloaded from the Statistical Software Components archive. The sdecode package is an extension of decode; it allows the user to add prefixes or suffixes and to output exponents as TEX, HTML, or Rich Text Format superscripts. It is called by another package, bmjcip, to convert estimates, confidence limits, and p-values from numeric to string. The chardef package is an extension of char define; it allows the user to define a characteristic for a list of variables, adding prefixes or suffixes. The xrewide package is an extension of reshape wide; it allows the user to store additional results in variable characteristics or local macros. The ingap package inserts gap observations into a dataset to form gap rows when the dataset is converted to a table. Together, these packages form a general toolkit to convert Stata datasets (or resultssets) to tables, complete with row labels and column labels. Examples are demonstrated using LATEX and also using the rtfutil package to create Rich Text Format documents. This article uses data from the Avon Longitudinal Study of Parents and Children, based at Bristol University, UK.
9169346	WOS:000305117100001	956VH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	lavaan: An R Package for Structural Equation Modeling	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-36	36	1	WOS:000305117100001	Structural equation modeling (SEM) is a vast field and widely used by many applied researchers in the social and behavioral sciences. Over the years, many software packages for structural equation modeling have been developed, both free and commercial. However, perhaps the best state-of-the-art software packages in this field are still closed-source and/or commercial. The R package lavaan has been developed to provide applied researchers, teachers, and statisticians, a free, fully open-source, but commercial-quality package for latent variable modeling. This paper explains the aims behind the development of the package, gives an overview of its most important features, and provides some examples to illustrate how lavaan works in practice.
9215694	WOS:000308910200001	007TD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Evaluating Random Forests for Survival Analysis Using Prediction Error Curves	Journal	Article	2012	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000308910200001	Prediction error curves are increasingly used to assess and compare predictions in survival analysis. This article surveys the R package pec which provides a set of functions for efficient computation of prediction error curves. The software implements inverse probability of censoring weights to deal with right censored data and several variants of cross-validation to deal with the apparent error problem. In principle, all kinds of prediction models can be assessed, and the package readily supports most traditional regression modeling strategies, like Cox regression or additive hazard regression, as well as state of the art machine learning methods such as random forests, a nonparametric method which provides promising alternatives to traditional strategies in low and high-dimensional settings. We show how the functionality of pec can be extended to yet unsupported prediction models. As an example, we implement support for random forest prediction models based on the R packages randomSurvivalForest and party. Using data of the Copenhagen Stroke Study we use pec to compare random forests to a Cox regression model derived from stepwise variable selection.
9223287	WOS:000306347300010	973GF	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Transforming the time axis	Journal	Article	2012	1	1	English	STATA JOURNAL	332-341	10	1	WOS:000306347300010	The time variable is most commonly plotted precisely as recorded in graphs showing change over time. However, if the most interesting part of the graph is very crowded, then transforming the time axis to give that part more space is worth consideration. In this column, I discuss logarithmic scales, square root scales, and scale breaks as possible solutions.
9234830	WOS:000309735300005	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A review of Stata commands for fixed-effects estimation in normal linear models	Journal	Review	2012	1	1	English	STATA JOURNAL	406-432	27	1	WOS:000309735300005	Availability of large multilevel longitudinal databases in various fields of research, including labor economics (with workers and firms observed over time) and education (with students, teachers, and schools observed over time), has increased the application of models with one level or multiple levels of fixed effects (for example, teacher and student effects). There has been a corresponding rapid development of Stata commands designed for fitting these types of models. The commands parameterize the fixed-effects portions of models differently. In cases where estimates of the fixed-effects parameters are of interest, it is critical to understand precisely what parameters are being estimated by different commands. In this article, we catalog the estimates of reported fixed effects provided by different commands for several canonical cases of both one-level and two-level fixed-effects models. We also discuss issues regarding computational efficiency and standard-error estimation.
9238709	WOS:000312289500001	053RA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Estimating Population Abundance Using Sightability Models: R Sightability Model Package	Journal	Article	2012	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000312289500001	Sightability models are binary logistic-regression models used to estimate and adjust for visibility bias in wildlife-population surveys (Steinhorst and Samuel 1989). Estimation proceeds in 2 stages: (1) Sightability trials are conducted with marked individuals, and logistic regression is used to estimate the probability of detection as a function of available covariates (e.g., visual obstruction, group size). (2) The fitted model is used to adjust counts (from future surveys) for animals that were not observed. A modified Horvitz-Thompson estimator is used to estimate abundance: counts of observed animal groups are divided by their inclusion probabilites (determined by plot-level sampling probabilities and the detection probabilities estimated from stage 1). We provide a brief historical account of the approach, clarifying and documenting suggested modifications to the variance estimators originally proposed by Steinhorst and Samuel (1989). We then introduce a new R package, SightabilityModel, for estimating abundance using this technique. Lastly, we illustrate the software with a series of examples using data collected from moose (Alcesalces) in northeastern Minnesota and mountain goats (Oreamnos americanus) in Washington State.
9259799	WOS:000309735300007	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A generalized Hosmer-Lemeshow goodness-of-fit test for multinomial logistic regression models	Journal	Article	2012	1	1	English	STATA JOURNAL	447-453	7	1	WOS:000309735300007	Testing goodness of fit is an important step in evaluating a statistical model. For binary logistic regression models, the Hosmer-Lemeshow goodness-of-fit test is often used. For rnultinomial logistic regression models, however, few tests are available. We present the mlogitgof command, which implements a goodness-of-fit test for multinomial logistic regression models. This test can also be used for binary logistic regression models, where it gives results identical to the Hosmer-Lemeshow test.
9328183	WOS:000313139600005	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Tools to simulate realistic censored survival-time distributions	Journal	Article	2012	1	1	English	STATA JOURNAL	639-654	16	1	WOS:000313139600005	Simulation of realistic censored survival times is challenging. Most research studies use highly simplified models, such as the exponential, that do not adequately reflect the patterns of time to event and censoring seen in real datasets. In this article, I present a general method of simulating such data based on flexible parametric survival models (Royston and Parmar, 2002, Statistics in Medicine 21: 2175-2197). A key component of the approach is modeling not only the time to event but also the time to censoring. I illustrate the methods in data from clinical trials and from a prognostic study. I also describe a new Stata program, stsurvsim, that does the necessary calculations.
9331323	WOS:000305118500001	956VU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000305118500001	Beta regression -an increasingly popular approach for modeling rates and proportions - is extended in various directions: (a) bias correction/reduction of the maximum likelihood estimator, (b) beta regression tree models by means of recursive partitioning, (c) latent class beta regression by means of finite mixture models. All three extensions may be of importance for enhancing the beta regression toolbox in practice to provide more reliable inference and capture both observed and unobserved/latent heterogeneity in the data. Using the analogy of Smithson and Verkuilen (2006), these extensions make beta regression not only "a better lemon squeezer" (compared to classical least squares regression) but a full-fledged modern juicer offering lemon-based drinks: shaken and stirred (bias correction and reduction), mixed (finite mixture model), or partitioned (tree model). All three extensions are provided in the R package betareg (at least 2.4-0), building on generic algorithms and implementations for bias correction/reduction, model-based recursive partioning, and finite mixture models, respectively. Specifically, the new functions betatree () and betamix () reuse the object-oriented flexible implementation from the R packages party and flexmix, respectively.
9340269	WOS:000313198000010	066CR	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The State of Naming Conventions in R	Journal	Editorial Material	2012	12	1	English	R JOURNAL	74-75	2	1	WOS:000313198000010	Most programming language communities have naming conventions that are generally agreed upon, that is, a set of rules that governs how functions and variables are named. This is not the case with R, and a review of unofficial style guides and naming convention usage on CRAN shows that a number of different naming conventions are currently in use. Some naming conventions are, however, more popular than others and as a newcomer to the R community or as a developer of a new package this could be useful to consider when choosing what naming convention to adopt.
9347723	WOS:000313197700002	066CO	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Analysing Seasonal Data	Journal	Article	2012	6	1	English	R JOURNAL	5-10	6	1	WOS:000313197700002	Many common diseases, such as the flu and cardiovascular disease, increase markedly in winter and dip in summer. These seasonal patterns have been part of life for millennia and were first noted in ancient Greece by both Hippocrates and Herodotus. Recent interest has focused on climate change, and the concern that seasons will become more extreme with harsher winter and summer weather. We describe a set of R functions designed to model seasonal patterns in disease. We illustrate some simple descriptive and graphical methods, a more complex method that is able to model non-stationary patterns, and the case-crossover to control for seasonal confounding.
9352371	WOS:000313198000009	066CR	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Graphical Markov Models with Mixed Graphs in R	Journal	Article	2012	12	1	English	R JOURNAL	65-73	9	1	WOS:000313198000009	In this paper we provide a short tutorial illustrating the new functions in the package ggm that deal with ancestral, summary and ribbonless graphs. These are mixed graphs (containing three types of edges) that are important because they capture the modified independence structure after marginalisation over, and conditioning on, nodes of directed acyclic graphs. We provide functions to verify whether a mixed graph implies that A is independent of B given C for any disjoint sets of nodes and to generate maximal graphs inducing the same independence structure of non-maximal graphs. Finally, we provide functions to decide on the Markov equivalence of two graphs with the same node set but different types of edges.
9365976	WOS:000303804000001	939IW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	frailtypack: An R Package for the Analysis of Correlated Survival Data with Frailty Models Using Penalized Likelihood Estimation or Parametrical Estimation	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000303804000001	Frailty models are very useful for analysing correlated survival data, when observations are clustered into groups or for recurrent events. The aim of this article is to present the new version of an R package called frailtypack. This package allows to fit Cox models and four types of frailty models (shared, nested, joint, additive) that could be useful for several issues within biomedical research. It is well adapted to the analysis of recurrent events such as cancer relapses and/or terminal events (death or lost to follow-up). The approach uses maximum penalized likelihood estimation. Right-censored or left-truncated data are considered. It also allows stratification and time-dependent covariates during analysis
9388543	WOS:000305065500001	956BT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An Exact Algorithm for Weighted-Mean Trimmed Regions in Any Dimension	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000305065500001	Trimmed regions are a powerful tool of multivariate data analysis. They describe a probability distribution in Euclidean d-space regarding location, dispersion, and shape, and they order multivariate data with respect to their centrality. Dyckerhoff and Mosler (2011) have introduced the class of weighted-mean trimmed regions, which possess attractive properties regarding continuity, subadditivity, and monotonicity. We present an exact algorithm to compute the weighted-mean trimmed regions of a given data cloud in arbitrary dimension d. These trimmed regions are convex polytopes in R-d. To calculate them, the algorithm builds on methods from computational geometry. A characterization of a region's facets is used, and information about the adjacency of the facets is extracted from the data. A key problem consists in ordering the facets. It is solved by the introduction of a tree-based order, by which the whole surface can be traversed efficiently with the minimal number of computations. The algorithm has been programmed in C++ and is available as the R package WMTregions.
9402553	WOS:000306347300004	973GF	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Menu-driven X-12-ARIMA seasonal adjustment in Stata	Journal	Article	2012	1	1	English	STATA JOURNAL	214-241	28	1	WOS:000306347300004	The X-12-ARIMA software of the U.S. Census Bureau is one of the most popular methods for seasonal adjustment; the program x12a.exe is widely used around the world. Some software also provides X-12-ARIMA seasonal adjustments by using x12a.exe as a plug-in or externally. In this article, we illustrate a menu-driven X-12-ARIMA seasonal-adjustment method in Stata. Specifically, the main utilities include how to specify the input file and run the program, how to make a diagnostics table, how to import data, and how to make graphs.
9430730	WOS:000301231300001	904XD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fast R Functions for Robust Correlations and Hierarchical Clustering	Journal	Article	2012	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000301231300001	Many high-throughput biological data analyses require the calculation of large correlation matrices and/or clustering of a large number of objects. The standard R function for calculating Pearson correlation can handle calculations without missing values efficiently, but is inefficient when applied to data sets with a relatively small number of missing data. We present an implementation of Pearson correlation calculation that can lead to substantial speedup on data with relatively small number of missing entries. Further, we parallelize all calculations and thus achieve further speedup on systems where parallel processing is available. A robust correlation measure, the biweight midcorrelation, is implemented in a similar manner and provides comparable speed. The functions c or and bicor for fast Pearson and biweight midcorrelation, respectively, are part of the updated, freely available R package WGCNA. The hierarchical clustering algorithm implemented in R function hclust is an order n(3) (n is the number of clustered objects) version of a publicly available clustering algorithm (Murtagh 2012). We present the package flashClust that implements the original algorithm which in practice achieves order approximately n(2), leading to substantial time savings when clustering large data sets.
9437946	WOS:000313139600003	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Graphical augmentations to the funnel plot to assess the impact of a new study on an existing meta-analysis	Journal	Article	2012	1	1	English	STATA JOURNAL	605-622	18	1	WOS:000313139600003	Funnel plots are currently advocated to investigate the presence of publication bias (and other possible sources of bias) in meta-analysis. A previously described augmentation to the funnel plot to aid its interpretation in assessing publication biases is the addition of statistical contours indicating regions where studies would have to be for a given level of significance, as implemented in the Stata package confunnel by Palmer et al. (2008, Stata Journal 8: 242-254). In this article, we describe the implementation of a new range of overlay augmentations to the funnel plot, many described in detail recently by Langan et al. (2012, Journal of Clinical Epidemiology 65: 511-519). The purpose of these overlays is to display the potential impact a new study would have on an existing meta-analysis, providing an indication of the robustness of the meta-analysis to the addition of new evidence. Thus these overlays extend the use of the funnel plot beyond assessments of publication biases. Two main graphical displays are described: 1) statistical significance contours, which define regions of the funnel plot where a new study would have to be located to change the statistical significance of the meta-analysis; and 2) heterogeneity contours, which show how a new study would affect the extent of heterogeneity in a given meta-analysis. We present the extfunnel command, which implements the methods of Langan et al. (2012, Journal of Clinical Epidemiology 65: 511-519), and, furthermore, we extend the graphical displays to illustrate the impact a new study has on lower and upper confidence interval values and the confidence interval width of the pooled meta-analytic result. We also describe overlays for the impact of a future study on user-defined limits of clinical equivalence. We implement inverse-variance weighted methods by using both explicit formulas for contour lines and a simulation approach optimized in Mata.
9462920	WOS:000313139600004	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Fitting and modeling cure in population-based cancer studies within the framework of flexible parametric survival models	Journal	Article	2012	1	1	English	STATA JOURNAL	623-638	16	1	WOS:000313139600004	When the mortality among a cancer patient group returns to the same level as in the general population, that is, when the patients no longer experience excess mortality, the patients still alive are considered "statistically cured". Cure models can be used to estimate the cure proportion as well as the survival function of the "uncured". One limitation of parametric cure models is that the functional form of the survival of the uncured has to be specified. It can sometimes be hard to find a survival function flexible enough to fit the observed data, for example, when there is high excess hazard within a few months from diagnosis, which is common among older age groups. This has led to the exclusion of older age groups in population-based cancer studies using cure models. Here we use flexible parametric survival models that incorporate cure as a special case to estimate the cure proportion and the survival of the uncured. Flexible parametric survival models use splines to model the underlying hazard function; therefore, no parametric distribution has to be specified. We have updated the stpm2 command for flexible parametric models to enable cure modeling.
9463220	WOS:000313198000008	066CR	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Rfit: Rank-based Estimation for Linear Models	Journal	Article	2012	12	1	English	R JOURNAL	57-64	8	1	WOS:000313198000008	In the nineteen seventies, Jureckova and Jaeckel proposed rank estimation for linear models. Since that time, several authors have developed inference and diagnostic methods for these estimators. These rank-based estimators and their associated inference are highly efficient and are robust to outliers in response space. The methods include estimation of standard errors, tests of general linear hypotheses, confidence intervals, diagnostic procedures including studentized residuals, and measures of influential cases. We have developed an R package, Rfit, for computing of these robust procedures. In this paper we highlight the main features of the package. The package uses standard linear model syntax and includes many of the main inference and diagnostic functions.
9478890	WOS:000301068800001	902UV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	BIEMS: A Fortran 90 Program for Calculating Bayes Factors for Inequality and Equality Constrained Models	Journal	Article	2012	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-39	39	1	WOS:000301068800001	This paper discusses a Fortran 90 program referred to as BIEMS (Bayesian inequality and equality constrained model selection) that can be used for calculating Bayes factors of multivariate normal linear models with equality and/or inequality constraints between the model parameters versus a model containing no constraints, which is referred to as the unconstrained model. The prior that is used under the unconstrained model is the conjugate expected-constrained posterior prior and the prior under the constrained model is proportional to the unconstrained prior truncated in the constrained space. This results in Bayes factors that appropriately balance between model fit and complexity for a broad class of constrained models. When the set of equality and/or inequality constraints in the model represents a hypothesis that applied researchers have in, for instance, (M)AN(C)OVA, (multivariate) regression, or repeated measurements, the obtained Bayes factor can be used to determine how much evidence is provided by the data in favor of the hypothesis in comparison to the unconstrained model. If several hypotheses are under investigation, the Bayes factors between the constrained models can be calculated using the obtained Bayes factors from BIEMS. Furthermore, posterior model probabilities of constrained models are provided which allows the user to compare the models directly with each other.
9501547	WOS:000309735300002	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	The Chen-Shapiro test for normality	Journal	Article	2012	1	1	English	STATA JOURNAL	368-374	7	1	WOS:000309735300002	The Chen-Shapiro test for normality (Chen and Shapiro, 1995, Journal of Statistical Computation and Simulation 53: 269-288) has been shown in simulation studies to be generally slightly more powerful than the commonly used Shapiro-Wilk and Shapiro-Francia tests, implemented in Stata official commands swilk and sfrancia. I present the chens command, which performs the Chen-Shapiro test in Stata.
9506282	WOS:000306347300002	973GF	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	What hypotheses do "nonparametric" two-group tests actually test?	Journal	Article	2012	1	1	English	STATA JOURNAL	182-190	9	1	WOS:000306347300002	In this article, I discuss measures of effect size for two-group comparisons where data are not appropriately analyzed by least-squares methods. The Mann-Whitney test calculates a statistic that is a very useful measure of effect size, particularly suited to situations in which differences are measured on scales that either are ordinal or use arbitrary scale units. Both the difference in medians and the median difference between groups are also useful measures of effect size.
9542258	WOS:000313197700006	066CO	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Vdgraph: A Package for Creating Variance Dispersion Graphs	Journal	Article	2012	6	1	English	R JOURNAL	41-44	4	1	WOS:000313197700006	This article introduces the package Vdgraph that is used for making variance dispersion graphs of response surface designs. The package includes functions that make the variance dispersion graph of one design or compare variance dispersion graphs of two designs, which are stored in data frames or matrices. The package also contains several minimum run response surface designs (stored as matrices) that are not available in other R packages.
9582581	WOS:000299641400001	883NR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Interactive and Animated Scalable Vector Graphics and R Data Displays	Journal	Article	2012	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-88	88	1	WOS:000299641400001	We describe an approach to creating interactive and animated graphical displays using R's graphics engine and Scalable Vector Graphics, an XML vocabulary for describing two-dimensional graphical displays. We use the svg() graphics device inR and then post-process the resulting XML documents. The post-processing identifies the elements in the SVG that correspond to the different components of the graphical display, e.g., points, axes, labels, lines. One can then annotate these elements to add interactivity and animation effects. One can also use JavaScript to provide dynamic interactive effects to the plot, enabling rich user interactions and compelling visualizations. The resulting SVG documents can be embedded within HTML documents and can involve JavaScript code that integrates the SVG and HTML objects. The functionality is provided via the SVG Annotation package and makes static plots generated via R graphics functions available as stand-alone, interactive and animated plots for the Web and other venues.
9586531	WOS:000313197700004	066CO	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	openair - Data Analysis Tools for the Air Quality Community	Journal	Article	2012	6	1	English	R JOURNAL	20-29	10	1	WOS:000313197700004	The openair package contains data analysis tools for the air quality community. This paper provides an overview of data importers, main functions, and selected utilities and workhorse functions within the package and the function output class, as of package version 0.4-14. It is intended as an explanation of the rationale for the package and a technical description for those wishing to work more interactively with the main functions or develop additional functions to support 'higher level' use of openair and R.
9594813	WOS:000306914200001	980SU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	NParCov3: A SAS/IML Macro for Nonparametric Randomization-Based Analysis of Covariance	Journal	Article	2012	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000306914200001	Analysis of covariance serves two important purposes in a randomized clinical trial. First, there is a reduction of variance for the treatment effect which provides more powerful statistical tests and more precise confidence intervals. Second, it provides estimates of the treatment effect which are adjusted for random imbalances of covariates between the treatment groups. The nonparametric analysis of covariance method of Koch, Tangen, Jung, and Amara (1998) defines a very general methodology using weighted least-squares to generate covariate-adjusted treatment effects with minimal assumptions. This methodology is general in its applicability to a variety of outcomes, whether continuous, binary, ordinal, incidence density or time-to-event. Further, its use has been illustrated in many clinical trial setting, such as multi-center, dose-response and non-inferiority trials. NParCov3 is a SAS/IML macro written to conduct the nonparametric randomization-based covariance analyses of Koch et al. (1998). The software can analyze a variety of outcomes and can account for stratification. Data from multiple clinical trials will be used for illustration.
9600936	WOS:000303803800001	939IU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The R Package bgmm: Mixture Modeling with Uncertain Knowledge	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000303803800001	Classical supervised learning enjoys the luxury of accessing the true known labels for the observations in a modeled dataset. Real life, however, poses an abundance of problems, where the labels are only partially defined, i.e., are uncertain and given only for a subset of observations. Such partial labels can occur regardless of the knowledge source. For example, an experimental assessment of labels may have limited capacity and is prone to measurement errors. Also expert knowledge is often restricted to a specialized area and is thus unlikely to provide trustworthy labels for all observations in the dataset. Partially supervised mixture modeling is able to process such sparse and imprecise input. Here, we present an R package called bgmm, which implements two partially supervised mixture modeling methods: soft-label and belief-based modeling. For completeness, we equipped the package also with the functionality of unsupervised, semi-and fully supervised mixture modeling. On real data we present the usage of bgmm for basic model-fitting in all modeling variants. The package can be applied also to selection of the best-fitting from a set of models with different component numbers or constraints on their structures. This functionality is presented on an artificial dataset, which can be simulated in bgmm from a distribution defined by a given model.
9613606	WOS:000313139600012	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Modeling underdispersed count data with generalized Poisson regression	Journal	Article	2012	1	1	English	STATA JOURNAL	736-747	12	1	WOS:000313139600012	We present motivation and new Stata commands for modeling count data. While the focus of this article is on modeling data with underdispersion, the new command for fitting generalized Poisson regression models is also suitable as an alternative to negative binomial regression for overdispersed data.
9623131	WOS:000306914400001	980SW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	GrassmannOptim: An R Package for Grassmann Manifold Optimization	Journal	Article	2012	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	18	1	WOS:000306914400001	The optimization of a real-valued objective function f(U), where U is a p x d, p > d, semi-orthogonal matrix such that (UU)-U-inverted perpendicular = I-d, and f is invariant under right orthogonal transformation of U, is often referred to as a Grassmann manifold optimization. Manifold optimization appears in a wide variety of computational problems in the applied sciences. In this article, we present GrassmannOptim, an R package for Grassmann manifold optimization. The implementation uses gradient-based algorithms and embeds a stochastic gradient method for global search. We describe the algorithms, provide some illustrative examples on the relevance of manifold optimization and finally, show some practical usages of the package.
9640666	WOS:000301070300001	902VF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An SPSS R-Menu for Ordinal Factor Analysis	Journal	Article	2012	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000301070300001	Exploratory factor analysis is a widely used statistical technique in the social sciences. It attempts to identify underlying factors that explain the pattern of correlations with in a set of observed variables. A statistical software package is needed to perform the calculations. However, there are some limitations with popular statistical software packages,like SPSS. The R programming language is a free software package for statistical and graphical computing. It offers many packages written by contributors from all over the world and programming resources that allow it to overcome the dialog limitations of SPSS. This paper offers an SPSS dialog written in the R programming language with the help of some packages,so that researchers with little or no knowledge in programming, or those who are accustomed to making their calculations based on statistical dialogs, have more options when applying factor analysis to their data and hence can adopt a better approach when dealing with ordinal, Likert-type data.
9645867	WOS:000312289600001	053RB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Meta-Statistics for Variable Selection: The R Package BioMark	Journal	Article	2012	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000312289600001	Biomarker identification is an ever more important topic in the life sciences. With the advent of measurement methodologies based on microarrays and mass spectrometry, thousands of variables are routinely being measured on complex biological samples. Often, the question is what makes two groups of samples different. Classical hypothesis testing suffers from the multiple testing problem; however, correcting for this often leads to a lack of power. In addition, choosing alpha cutoff levels remains somewhat arbitrary. Also in a regression context, a model depending on few but relevant variables will be more accurate and precise, and easier to interpret biologically. We propose an R package, BioMark, implementing two meta-statistics for variable selection. The first, higher criticism, presents a data-dependent selection threshold for significance, instead of a cookbook value of alpha = 0.05. It is applicable in all cases where two groups are compared. The second, stability selection, is more general, and can also be applied in a regression context. This approach uses repeated subsampling of the data in order to assess the variability of the model coefficients and selects those that remain consistently important. It is shown using experimental spike-in data from the field of metabolomics that both approaches work well with real data. BioMark also contains functionality for simulating data with specific characteristics for algorithm development and testing.
9651627	WOS:000313139600008	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A command to calculate age-standardized rates with efficient interval estimation	Journal	Article	2012	1	1	English	STATA JOURNAL	688-701	14	1	WOS:000313139600008	In this article, we illustrate the command distrate, which calculates age-standardized rates with efficient interval estimation by using formulas developed by Tiwari, Clegg, and Zou (2006, Statistical Methods in Medical Research 15: 547-569) as a modification of the method proposed by Fay and Feuer (1997, Statistics in Medicine 16: 791-801). This method is currently used in the Surveillance, Epidemiology, and End Results Program of the National Cancer Institute in Bethesda, Maryland; the Italian Association of Cancer Registries (Associazione Italiana Registro Tumori, AIRTUM); and the Lombardy Mesothelioma and Sinonasal Cancer Registry in Northern Italy. The command produces a compact output and allows for the possibility of specifying a rate multiplier, for instance, x100,000 or x1,000,000. Furthermore, rates and confidence limits can be easily exported to an external dataset for further processing (for example, for making graphs). The command distrate is a useful addition to the official Stata command dstdize.
9657332	WOS:000309735300012	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Long-run covariance and its applications in cointegration regression	Journal	Article	2012	1	1	English	STATA JOURNAL	515-542	28	1	WOS:000309735300012	Long-run covariance plays a major role in much of time-series inference, such as heteroskedasticity- and autocorrelation-consistent standard errors, generalized method of moments estimation, and cointegration regression. We propose a Stata command, lrcov, to compute long-run covariance with a prewhitening strategy and various kernel functions. We illustrate how long-run covariance matrix estimation can be used to obtain heteroskedasticity- and autocorrelation-consistent standard errors via the new hacreg command; we also illustrate cointegration regression with the new cointreg command. hacreg has several improvements compared with the official newey command, such as more kernel functions, automatic determination of the lag order, and prewhitening of the data. cointreg enables the estimation of cointegration regression using fully modified ordinary least squares, dynamic ordinary least squares, and canonical cointegration regression methods. We use several classical examples to demonstrate the use of these commands.
9661493	WOS:000305117900001	956VO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Random Generation of Response Patterns under Computerized Adaptive Testing with the R Package catR	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000305117900001	This paper outlines a computerized adaptive testing (CAT) framework and presents an R package for the simulation of response patterns under CAT procedures. This package, called catR, requires a bank of items, previously calibrated according to the four-parameter logistic (4PL) model or any simpler logistic model. The package proposes several methods to select the early test items, several methods for next item selection, different estimators of ability (maximum likelihood, Bayes modal, expected a posteriori, weighted likelihood), and three stopping rules (based on the test length, the precision of ability estimates or the classification of the examinee). After a short description of the different steps of a CAT process, the commands and options of the catR package are presented and practically illustrated.
9667568	WOS:000307534100001	989BB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The R Package metaLik for Likelihood Inference in Meta-Analysis	Journal	Article	2012	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000307534100001	Meta-analysis is a statistical method for combining information from different studies about the same issue of interest. Meta-analysis is widely diffuse in medical investigation and more recently it received a growing interest also in social disciplines. Typical applications involve a small number of studies, thus making ordinary inferential methods based on first-order asymptotics unreliable. More accurate results can be obtained by exploiting the theory of higher-order asymptotics. This paper describes the metaLik package which provides an R implementation of higher-order likelihood methods in meta-analysis. The extension to meta-regression is included. Two real data examples are used to illustrate the capabilities of the package.
9670962	WOS:000303616500004	936UW	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating panel time-series models with heterogeneous slopes	Journal	Article	2012	1	1	English	STATA JOURNAL	61-71	11	1	WOS:000303616500004	This article introduces a new Stata command, xtmg, that implements three panel time-series estimators; allowing for heterogeneous slope coefficients across group members: the Pesaran and Smith (1995, Journal of Econometrics 68: 79-113) mean group estimator, the Pesaran (2006, Econometrica 74: 967-1012) common correlated effects mean group estimator, and the augmented mean group estimator introduced by Eberhardt and Teal (2010, Discussion Paper 515, Department of Economics, University of Oxford). The latter two estimators further allow for unobserved correlation across panel members (cross-section dependence).
9676433	WOS:000305989900001	968NQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Glotaran: A Java-Based Graphical User Interface for the R Package TIMP	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000305989900001	In this work the software application called Glotaran is introduced as a Java-based graphical user interface to the R package TIMP, a problem solving environment for fitting superposition models to multi-dimensional data. TIMP uses a command-line user interface for the interaction with data, the specification of models and viewing of analysis results. Instead, Glotaran provides a graphical user interface which features interactive and dynamic data inspection, easier assisted by the user interface model specification and interactive viewing of results. The interactivity component is especially helpful when working with large, multi-dimensional datasets as often result from time-resolved spectroscopy measurements, allowing the user to easily pre-select and manipulate data before analysis and to quickly zoom in to regions of interest in the analysis results. Glotaran has been developed on top of the NetBeans rich client platform and communicates with R through the Java-to-R interface R serve. The background and the functionality of the application are described here. In addition, the design, development and implementation process of Glotaran is documented in a generic way.
9681574	WOS:000309735300015	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Interpreting and Visualizing Regression Models Using Stata by Michael N. Mitchell	Journal	Review	2012	1	1	English	STATA JOURNAL	562-564	3	1	WOS:000309735300015	In this article, I review Interpreting and Visualizing Regression Models Using Stata, by Michael Mitchell (2012a [Stata Press]).
9695193	WOS:000305116900001	956VG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Psychoco: Psychometric Computing in R	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-5	5	1	WOS:000305116900001	This special volume features eleven contributions to psychometric computing, a research area that integrates psychometrics and computational methods in statistics. Topics covered include structural equation modeling, item response theory, probabilistic choice modeling, and other modeling approaches prevalent in or useful for psychometric research. Each contributed paper is accompanied by a software package published on the Comprehensive R Archive Network. This introduction gives a brief overview of the volume.
9714936	WOS:000305118200001	956VR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bradley-Terry Models in R: The BradleyTerry2 Package	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000305118200001	This is a short overview of the R add-on package BradleyTerry2, which facilitates the specification and fitting of Bradley-Terry logit, probit or cauchit models to pair-comparison data. Included are the standard 'unstructured' Bradley-Terry model, structured versions in which the parameters are related through a linear predictor to explanatory variables, and the possibility of an order or 'home advantage' effect or other 'contest-specific' effects. Model fitting is either by maximum likelihood, by penalized quasi-likelihood (for models which involve a random effect), or by bias-reduced maximum likelihood in which the first-order asymptotic bias of parameter estimates is eliminated. Also provided are a simple and efficient approach to handling missing covariate data, and suitably-defined residuals for diagnostic checking of the linear predictor.
9735131	WOS:000313139600013	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Matrices as look-up tables	Journal	Article	2012	1	1	English	STATA JOURNAL	748-758	11	1	WOS:000313139600013	Matrices in Stata can serve as look-up tables. Because Stata will accept references to matrix elements within many commands, most notably generate and replace, users can access and use values from a table in either vector or full matrix form. Examples are given for entry of small datasets, recoding of categorical variables, and quantile-based or similar binning of counted or measured variables. In the last case, the device grants easy exploration of the consequences of different binning conventions and the instability of bin allocation.
9744297	WOS:000309535500001	016QZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ClustOfVar: An R Package for the Clustering of Variables	Journal	Article	2012	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000309535500001	Clustering of variables is as a way to arrange variables into homogeneous clusters, i.e., groups of variables which are strongly related to each other and thus bring the same information. These approaches can then be useful for dimension reduction and variable selection. Several specific methods have been developed for the clustering of numerical variables. However concerning qualitative variables or mixtures of quantitative and qualitative variables, far fewer methods have been proposed. The R package ClustOfVar was specifically developed for this purpose. The homogeneity criterion of a cluster is defined as the sum of correlation ratios (for qualitative variables) and squared correlations (for quantitative variables) to a synthetic quantitative variable, summarizing "as good as possible" the variables in the cluster. This synthetic variable is the first principal component obtained with the PCAMIX method. Two clustering algorithms are proposed to optimize the homogeneity criterion: iterative relocation algorithm and ascendant hierarchical clustering. We also propose a bootstrap approach in order to determine suitable numbers of clusters. We illustrate the methodologies and the associated package on small datasets.
9800609	WOS:000301072600001	902VY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The R Package bild for the Analysis of Binary Longitudinal Data	Journal	Article	2012	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000301072600001	We present the R package bild for the parametric and graphical analysis of binary longitudinal data. The package performs logistic regression for binary longitudinal data, allowing for serial dependence among observations from a given individual and a random intercept term. Estimation is via maximization of the exact likelihood of a suitably defined model. Missing values and unbalanced data are allowed, with some restrictions. The code of bild is written partly in R language, partly in Fortran 77, interfaced through R. The package is built following the S4 formulation of R methods.
9802275	WOS:000309535800001	016RB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	msSurv: An R Package for Nonparametric Estimation of Multistate Models	Journal	Article	2012	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000309535800001	We present an R package, msSurv, to calculate the marginal (that is, not conditional on any covariates) state occupation probabilities, the state entry and exit time distributions, and the marginal integrated transition hazard for a general, possibly non-Markov, multistate system under left-truncation and right censoring. For a Markov model, msSurv also calculates and returns the transition probability matrix between any two states. Dependent censoring is handled via modeling the censoring hazard through observable covariates. Pointwise confidence intervals for the above mentioned quantities are obtained and returned for independent censoring from closed-form variance estimators and for dependent censoring using the bootstrap.
9814720	WOS:000305118400001	956VT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	prefmod: An R Package for Modeling Preferences Based on Paired Comparisons, Rankings, or Ratings	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000305118400001	The first part of this paper describes a series of loglinear preference models based on paired comparisons, a method of measurement whose aim is to order a set of objects according to an attribute of interest by asking subjects to compare pairs of objects. Based on the basic Bradley-Terry specification, two types of models, the loglinear Bradley-Terry model and a pattern approach are presented. Both methods are extended to include subject and object-specific covariates and some further structural effects. In addition, models for derived paired comparisons (based on rankings and ratings) are also included. Latent classes and missing values can be included. The second part of the paper describes the package prefmod that implements the above models in R. Illustrational applications are provided in the last part of the paper.
9817893	WOS:000313198000006	066CR	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	influence.ME: Tools for Detecting Influential Data in Mixed Effects Models	Journal	Article	2012	12	1	English	R JOURNAL	38-47	10	1	WOS:000313198000006	influence.ME provides tools for detecting influential data in mixed effects models. The application of these models has become common practice, but the development of diagnostic tools has lagged behind. influence. ME calculates standardized measures of influential data for the point estimates of generalized mixed effects models, such as DFBETAS, Cook's distance, as well as percentile change and a test for changing levels of significance. influence. ME calculates these measures of influence while accounting for the nesting structure of the data. The package and measures of influential data are introduced, a practical example is given, and strategies for dealing with influential data are suggested.
9823021	WOS:000305117600001	956VL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	New Developments in Mokken Scale Analysis in R	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000305117600001	Mokken (1971) developed a scaling procedure for both dichotomous and polytomous items that was later coined Mokken scale analysis (MSA). MSA has been developed ever since, and the developments until 2000 have been implemented in the software package MSP (Molenaar and Sijtsma 2000) and the R package mokken (Van der Ark 2007). This paper describes the new developments in MSA since 2000 that have been implemented in mokken since its first release in 2007. These new developments pertain to invariant item ordering, a new automated item selection procedure based on a genetic algorithm, inclusion of reliability coefficients, and the computation of standard errors for the scalability coefficients. We demonstrate all new applications using data obtained with a transitive reasoning test and a personality test.
9828280	WOS:000310774200001	033DO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	DiagTest3Grp: An R Package for Analyzing Diagnostic Tests with Three Ordinal Groups	Journal	Article	2012	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	24	1	WOS:000310774200001	Medical researchers endeavor to identify potentially useful biomarkers to develop marker based screening assays for disease diagnosis and prevention. Useful summary measures which properly evaluate the discriminative ability of diagnostic markers are critical for this purpose. Literature and existing software, for example, R packages nicely cover summary measures for diagnostic markers used for the binary case (e.g., healthy vs. diseased). An intermediate population at an early disease stage usually exists between the healthy and the fully diseased population in many disease processes. Supporting utilities for three-group diagnostic tests are highly desired and important for identifying patients at the early disease stage for timely treatments. However, application packages which provide summary measures for three ordinal groups are currently lacking. This paper focuses on two summary measures of diagnostic accuracy-volume under the receiver operating characteristic surface and the extended Youden index, with three diagnostic groups. We provide the R package DiagTest3Grp to estimate, under both parametric and nonparametric assumptions, the two summary measures and the associated variances, as well as the optimal cut-points for disease diagnosis. An omnibus test for multiple markers and a Wald test for two markers, on independent or paired samples, are incorporated to compare diagnostic accuracy across biomarkers. Sample size calculation under the normality assumption can be performed in the R package to design future diagnostic studies. A real world application evaluating the diagnostic accuracy of neuropsychological markers for Alzheimer's disease is used to guide readers through step-by-step implementation of DiagTest3Grp to demonstrate its utility
9832947	WOS:000326871200001	250RK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	IRTrees: Tree-Based Item Response Models of the GLMM Family	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000326871200001	A category of item response models is presented with two defining features: they all (i) have a tree representation, and (ii) are members of the family of generalized linear mixed models (GLMM). Because the models are based on trees, they are denoted as IRTree models. The GLMM nature of the models implies that they can all be estimated with the glmer function of the lme4 package in R. The aim of the article is to present four subcategories of models, the first two of which are based on a tree representation for response categories: 1. linear response tree models (e.g., missing response models), 2. nested response tree models (e.g., models for parallel observations regarding item responses such as agreement and certainty), while the last two are based on a tree representation for latent variables: 3. linear latent-variable tree models (e.g., models for change processes), and 4. nested latent-variable tree models (e.g., bi-factor models). The use of the g l m er function is illustrated for all four subcategories. Simulated example data sets and two service functions useful in preparing the data for IRTree modeling with glmer are provided in the form of an R package, irtrees. For all four subcategories also a real data application is discussed.
9833684	WOS:000301071900001	902VS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Introducing multivator: A Multivariate Emulator	Journal	Article	2012	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	20	1	WOS:000301071900001	A multivariate generalization of the emulator technique described by Hankin (2005) is presented in which random multivariate functions may be assessed. In the standard univariate case (Oakley 1999), a Gaussian process, a finite number of observations is made; here, observations of different types are considered. The technique has the property that marginal analysis (that is, considering only a single observation type) reduces exactly to the univariate theory. The associated software is used to analyze datasets from the field of climate change.
9835363	WOS:000326871800001	250RP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Random Number Generation in gretl	Journal	Article	2012	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000326871800001	The increasing popularity and complexity of random number intensive methods such as simulation and bootstrapping in econometrics requires researchers to have a good grasp of random number generation in general, and the specific generators that they employ in particular. Here, we discuss the random number generation options, their specifications, and their implementations in gretl. We also assess the performance and the reliability of gretl in this department by conducting extensive empirical testing using the TestU01 library. Our results show that the available alternatives are soundly implemented and should be sufficient for most econometric applications.
9897991	WOS:000301070700001	902VJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multi-Objective Parameter Selection for Classifiers	Journal	Article	2012	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000301070700001	Setting the free parameters of classifiers to different values can have a profound impact on their performance. For some methods, specialized tuning algorithms have been developed. These approaches mostly tune parameters according to a single criterion, such as the cross-validation error. However, it is sometimes desirable to obtain parameter values that optimize several concurrent - often conflicting - criteria. The TunePareto package provides a general and highly customizable framework to select optimal parameters for classifiers according to multiple objectives. Several strategies for sampling and optimizing parameters are supplied. The algorithm determines a set of Pareto-optimal parameter configurations and leaves the ultimate decision on the weighting of objectives to the researcher. Decision support is provided by novel visualization techniques.
9906651	WOS:000303805200001	939JH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Coordinate Descent Methods for the Penalized Semiparametric Additive Hazards Model	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000303805200001	For survival data with a large number of explanatory variables, lasso penalized Cox regression is a popular regularization strategy. However, a penalized Cox model may not always provide the best fit to data and can be difficult to estimate in high dimension because of its intrinsic nonlinearity. The semiparametric additive hazards model is a flexible alternative which is a natural survival analogue of the standard linear regression model. Building on this analogy, we develop a cyclic coordinate descent algorithm for fitting the lasso and elastic net penalized additive hazards model. The algorithm requires no nonlinear optimization steps and offers excellent performance and stability. An implementation is available in the R package ahaz. We demonstrate this implementation in a small timing study and in an application to real data.
9920390	WOS:000307534500001	989BF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Nonparametric Kernel Distribution Function Estimation with kerdiest: An R Package for Bandwidth Choice and Applications	Journal	Article	2012	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000307534500001	The R package kerdiest has been designed for computing kernel estimators of the distribution function and other related functions. Because of its usefulness in real applications, the bandwidth parameter selection problem has been considered, and a cross-validation method and two of plug-in type have been implemented. Moreover, three relevant functions in nature hazards have also been programmed. The package is completed with two interesting data sets, one of geological type (a complete catalogue of the earthquakes occurring in the northwest of the Iberian Peninsula) and another containing the maximum peak flow levels of a river in the United States of America.
9928192	WOS:000312288800001	053QT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A tm Plug-In for Distributed Text Mining in R	Journal	Article	2012	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000312288800001	R has gained explicit text mining support with the tm package enabling statisticians to answer many interesting research questions via statistical analysis or modeling of (text) corpora. However, we typically face two challenges when analyzing large corpora: (1) the amount of data to be processed in a single machine is usually limited by the available main memory (i.e., RAM), and (2) the more data to be analyzed the higher the need for efficient procedures for calculating valuable results. Fortunately, adequate programming models like MapReduce facilitate parallelization of text mining tasks and allow for processing data sets beyond what would fit into memory by using a distributed file system possibly spanning over several machines, e. g., in a cluster of workstations. In this paper we present a plug-in package to tm called tm.plugin.dc implementing a distributed corpus class which can take advantage of the Hadoop MapReduce library for large scale text mining tasks. We show on the basis of an application in culturomics that we can efficiently handle data sets of significant size.
9937769	WOS:000309735300014	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Axis practice, or what goes where on a graph	Journal	Article	2012	1	1	English	STATA JOURNAL	549-561	13	1	WOS:000309735300014	Conventions about what information goes on each axis of a two-way plot are precisely that, conventions. This column discusses-historically, syntactically, and by example-the idea that flouting convention in various ways can lead to small but useful improvements in graph display. Putting y-axis information on the right or on the top, or putting x-axis information on the top, often is useful. The most substantial examples are for multiple quantile plots, for which the new command multqplot is offered, and table-like graphs, which are made even more table-like by mimicking column headers.
9963800	WOS:000309735300004	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Adjusting for age effects in cross-sectional distributions	Journal	Article	2012	1	1	English	STATA JOURNAL	393-405	13	1	WOS:000309735300004	Income and wealth differ over the life cycle. In cross-sectional distributions of income or wealth, classical inequality measures such as the Gini could therefore find substantial inequality even if everyone has the same lifetime income or wealth. We describe the adjusted Gini index (Almas and Mogstad, 2012, Scandinavian Journal of Economics 114: 24-54), which is a generalization of the classical Gini index with attractive properties, and we describe the adgini command, which provides the adjusted Gini index and the classical Gini index. The adgini command also provides options to produce other well-known age-adjusted inequality measures, such as the Paglin-Gini (Paglin, 1975, American Economic Review 65: 598-609) and the Wertz-Gini (Wertz, 1979, American Economic Review 69: 670-672), and provides efficient estimation of the classical Gird coefficient.
9975024	WOS:000305117700001	956VM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	mirt: A Multidimensional Item Response Theory Package for the R Environment	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000305117700001	Item response theory (IRT) is widely used in assessment and evaluation research to explain how participants respond to item level stimuli. Several R packages can be used to estimate the parameters in various IRT models, the most flexible being the ltm (Rizopoulos 2006), eRm (Mair and Hatzinger 2007), and MCMCpack (Martin, Quinn, and Park 2011) packages. However these packages have limitations in that ltm and eRm can only analyze unidimensional IRT models effectively and the exploratory multidimensional extensions available in MCMCpack requires prior understanding of Bayesian estimation convergence diagnostics and are computationally intensive. Most importantly, multidimensional confirmatory item factor analysis methods have not been implemented in any R package. The mirt package was created for estimating multidimensional item response theory parameters for exploratory and confirmatory models by using maximum-likelihood methods. The Gauss-Hermite quadrature method used in traditional EM estimation (e.g., Bock and Aitkin 1981) is presented for exploratory item response models as well as for confirmatory bifactor models (Gibbons and Hedeker 1992). Exploratory and confirmatory models are estimated by a stochastic algorithm described by Cai (2010a,b). Various program comparisons are presented and future directions for the package are discussed.
9978666	WOS:000303616500001	936UW	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A new system for formatting estimation tables	Journal	Article	2012	1	1	English	STATA JOURNAL	3-28	26	1	WOS:000303616500001	I present an entirely rewritten version of the outreg command, which creates tables from the results of Stata estimation commands and generates formatted Microsoft Word or LATEX files. My objective is to provide as complete control as is practical over the layout and formatting of the estimation tables in both file formats. outreg provides a wide range of estimation statistics (including confidence intervals and marginal effects), can control the number and arrangement of the statistics displayed, and can merge subsequent estimation results into the same table. Users can specify numeric formats, font sizes, and font types at the table cell level, as well as lines in the table and row spacing. Multiple tables can be written to the same document, making it possible to create a fully formatted statistical appendix from a do-file. I demonstrate in examples the numerous formatting options for the outreg command.
9986301	WOS:000306347300005	973GF	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Faster estimation of a discrete-time proportional hazards model with gamma frailty	Journal	Article	2012	1	1	English	STATA JOURNAL	242-256	15	1	WOS:000306347300005	Fitting a complementary log-log model that accounts for gamma-distributed unobserved heterogeneity often takes a significant amount of time. This is in part because numerical derivatives are used to approximate the gradient vector and Hessian matrix. The main contribution of this article is the use of Mata and a gf2 evaluator to express the gradient vector and Hessian matrix. Gradient vector expression allows one to use a few different options and postestimation commands. Furthermore, expression of the gradient vector and Hessian matrix increases the speed at which a likelihood function is maximized. In this article, I present a complementary log-log model, show how the gamma distribution has been incorporated, and point out why the gradient vector and Hessian matrix can be expressed. I then discuss the speed at which a maximum is achieved, and I apply sampling weights that require an expression of the gradient vector. I introduce a new command for fitting this model. To demonstrate how this model can be applied, I will examine information on when young males first try marijuana.
10036946	WOS:000309735300013	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Kernel-smoothed cumulative distribution function estimation with akdensity	Journal	Article	2012	1	1	English	STATA JOURNAL	543-548	6	1	WOS:000309735300013	In this article, I describe estimation of the kernel-smoothed cumulative distribution function with the user-written package akdensity, with formulas and an example.
10058511	WOS:000306914300001	980SV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	bspmma: An R Package for Bayesian Semiparametric Models for Meta-Analysis	Journal	Article	2012	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000306914300001	We introduce an R package, bspmma, which implements a Dirichlet-based random effects model specific to meta-analysis. In meta-analysis, when combining effect estimates from several heterogeneous studies, it is common to use a random-effects model. The usual frequentist or Bayesian models specify a normal distribution for the true effects. However, in many situations, the effect distribution is not normal, e. g., it can have thick tails, be skewed, or be multi-modal. A Bayesian nonparametric model based on mixtures of Dirichlet process priors has been proposed in the literature, for the purpose of accommodating the non-normality. We review this model and then describe a competitor, a semiparametric version which has the feature that it allows for a well-defined centrality parameter convenient for determining whether the overall effect is significant. This second Bayesian model is based on a different version of the Dirichlet process prior, and we call it the "conditional Dirichlet model." The package contains functions to carry out analyses based on either the ordinary or the conditional Dirichlet model, functions for calculating certain Bayes factors that provide a check on the appropriateness of the conditional Dirichlet model, and functions that enable an empirical Bayes selection of the precision parameter of the Dirichlet process. We illustrate the use of the package on two examples, and give an interpretation of the results in these two different scenarios.
10071400	WOS:000303803400001	939IQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	splm: Spatial Panel Data Models in R	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-38	38	1	WOS:000303803400001	splm is an R package for the estimation and testing of various spatial panel data specifications. We consider the implementation of both maximum likelihood and generalized moments estimators in the context of fixed as well as random effects spatial panel data models. This paper is a general description of splm and all functionalities are illustrated using a well-known example taken from Munnell (1990) with productivity data on 48 US states observed over 17 years. We perform comparisons with other available software; and, when this is not possible, Monte Carlo results support our original implementation.
10074581	WOS:000313139600011	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Robinson's square root of N consistent semiparametric regression estimator in Stata	Journal	Article	2012	1	1	English	STATA JOURNAL	726-735	10	1	WOS:000313139600011	In this article, we describe Robinson's (1988, Econometrica 56: 931-954) double residual semiparametric regression estimator and Hardle and Mammen's (1993, Annals of Statistics 21: 1926-1947) specification test implementation in Stata. We use some simple simulations to illustrate how this newly coded estimator outperforms the already available semiparametric plreg command (Lokshin, 2006, Stata Journal 6: 377-383).
10093636	WOS:000305117200001	956VI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	semPLS: Structural Equation Modeling Using Partial Least Squares	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000305117200001	Structural equation models (SEM) are very popular in many disciplines. The partial least squares (PLS) approach to SEM offers an alternative to covariance-based SEM, which is especially suited for situations when data is not normally distributed. PLS path modelling is referred to as soft-modeling-technique with minimum demands regarding measurement scales, sample sizes and residual distributions. These m PLS package provides the capability to estimate PLS path models within the R programming environment. Different setups for the estimation of factor scores can be used. Furthermore it contains modular methods for computation of bootstrap confidence intervals, model parameters and several quality indices. Various plot functions help to evaluate the model. The well known mobile phone dataset from marketing research is used to demonstrate the features of the package.
10098086	WOS:000303804900001	939JE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Variable Penalty Dynamic Time Warping Code for Aligning Mass Spectrometry Chromatograms in R	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000303804900001	Aligment of mass spectrometry (MS) chromatograms is sometimes required prior to sample comparison and data analysis. Without alignment, direct comparison of chromatograms would lead to inaccurate results. We demonstrate a new method for computing a high quality alignment of full length MS chromatograms using variable penalty dynamic time warping. This method aligns signals using local linear shifts without excessive warping that can alter the shape (and area) of chromatogram peaks. The software is available as the R package VPdtw on the Comprehensive R Archive Network and we highlight how one can use this package here.
10109705	WOS:000305065700001	956BV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	bayesclust: An R Package for Testing and Searching for Significant Clusters	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000305065700001	The detection and determination of clusters has been of special interest among researchers from different fields for a long time. In particular,assessing whether the clusters are significant is a question that has been asked by a number of experimenters. In Fuentes and Casella (2009), the authors put forth a new methodology for analyzing clusters. It tests the hypothesis II0: kappa - 1 versus II1: kappa - k in a Bayesian setting,where kappa denotes the number of clusters in a population. The bayesclust package implements this approach in R. Here we give an overview of the algorithm and a detailed description of the functions available in the package. The routines in bayesclust allow the user to test for the existence of clusters, and then pick out optimal partitionings of the data. We demonstrate the testing procedure with simulated datasets.
10110985	WOS:000301071600001	902VQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Neural Networks in R Using the Stuttgart Neural Network Simulator: RSNNS	Journal	Article	2012	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000301071600001	Neural networks are important standard machine learning procedures for classifi cation and regression. We describe the R package RSNNS that provides a convenient interface to the popular Stuttgart Neural Network Simulator SNNS. The main features are (a) encapsulation of the relevant SNNS parts in a C++ class, for sequential and parallel usage of different networks, (b) accessibility of all of the SNNS algorithmic functionality from R using a low-level interface, and (c) a high-level interface for convenient, R-style usage of many standard neural network procedures. The package also includes functions for visualization and analysis of the models and the training procedures, as well as functions for data input/output from/to the original SNNS file formats.
10112290	WOS:000306347300008	973GF	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	The S-estimator of multivariate location and scatter in Stata	Journal	Article	2012	1	1	English	STATA JOURNAL	299-307	9	1	WOS:000306347300008	In this article, we introduce a new Stata command, smultiv, that implements the S-estimator of multivariate location and scatter. Using simulated data, we show that smultiv outperforms mcd, an alternative robust estimator. Finally, we use smultiv to perform robust principal component analysis and least-squares regression on a real dataset.
10118044	WOS:000303616500002	936UW	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Scrambled Halton sequences in Mata	Journal	Article	2012	1	1	English	STATA JOURNAL	29-44	16	1	WOS:000303616500002	In this article, I discuss the need for Halton sequences and discuss the Mata implementation of scrambling of Halton sequences, providing several examples of scrambling procedures.
10125873	WOS:000306347300007	973GF	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Fitting nonparametric mixed logit models via expectation-maximization algorithm	Journal	Article	2012	1	1	English	STATA JOURNAL	284-298	15	1	WOS:000306347300007	In this article, I provide an illustrative, step-by-step implementation of the expectation-maximization algorithm for the nonparametric estimation of mixed logit models. In particular, the proposed routine allows users to fit straight-forwardly latent-class logit models with an increasing number of mass points so as to approximate the unobserved structure of the mixing distribution.
10150737	WOS:000313197700003	066CO	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	MARSS: Multivariate Autoregressive State-space Models for Analyzing Time-series Data	Journal	Article	2012	6	1	English	R JOURNAL	11-19	9	1	WOS:000313197700003	MARSS is a package for fitting multivariate autoregressive state-space models to time-series data. The MARSS package implements state-space models in a maximum likelihood framework. The core functionality of MARSS is based on likelihood maximization using the Kalman filter/smoother, combined with an EM algorithm. To make comparisons with other packages available, parameter estimation is also permitted via direct search routines available in 'optim'. The MARSS package allows data to contain missing values and allows a wide variety of model structures and constraints to be specified (such as fixed or shared parameters). In addition to model-fitting, the package provides bootstrap routines for simulating data and generating confidence intervals, and multiple options for calculating model selection criteria (such as AIC).
10160749	WOS:000303616500005	936UW	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Respondent-driven sampling	Journal	Article	2012	1	1	English	STATA JOURNAL	72-93	22	1	WOS:000303616500005	Respondent-driven sampling is a network sampling technique typically employed for hard-to-reach populations (for example, drug users, men who have sex with men, people with HIV). Similarly to snowball sampling, initial seed respondents recruit additional respondents from their network of friends. The recruiting process repeats iteratively, thereby forming long referral chains. Unlike in snowball sampling, it is crucial to obtain estimates of respondents' personal network sizes (that is, number of acquaintances in the target population) and information about who recruited whom. Markov chain theory makes it possible to derive population estimates and sampling weights. We introduce a new Stata command for respondent-driven sampling and illustrate its use.
10163711	WOS:000313198000003	066CR	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	It's Not What You Draw, It's What You Don't Draw Drawing Paths with Holes in R Graphics	Journal	Article	2012	12	1	English	R JOURNAL	13-18	6	1	WOS:000313198000003	The R graphics engine has new support for drawing complex paths via the functions polypath() and grid.path(). This article explains what is meant by a complex path and demonstrates the usefulness of complex paths in drawing non-trivial shapes, logos, customised data symbols, and maps.
10204825	WOS:000308909400001	007SV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	solaR: Solar Radiation and Photovoltaic Systems with R	Journal	Article	2012	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000308909400001	The solaR package allows for reproducible research both for photovoltaics (PV) systems performance and solar radiation. It includes a set of classes, methods and functions to calculate the sun geometry and the solar radiation incident on a photovoltaic generator and to simulate the performance of several applications of the photovoltaic energy. This package performs the whole calculation procedure from both daily and intradaily global horizontal irradiation to the final productivity of grid-connected PV systems and water pumping PV systems. It is designed using a set of S4 classes whose core is a group of slots with multivariate time series. The classes share a variety of methods to access the information and several visualization methods. In addition, the package provides a tool for the visual statistical analysis of the performance of a large PV plant composed of several systems. Although solaR is primarily designed for time series associated to a location defined by its latitude/longitude values and the temperature and irradiation conditions, it can be easily combined with spatial packages for space-time analysis.
10211234	WOS:000309735300008	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Importing presidential approval poll results	Journal	Article	2012	1	1	English	STATA JOURNAL	454-460	7	1	WOS:000309735300008	The American Presidency Project (http://www.presidency.ucsb.edu) provides presidential job approval poll results. These data are available for each U.S. president since President Franklin D. Roosevelt and for all the job approval polls conducted since his presidency. In this article, we propose the Stata command approval, which downloads these approval poll results in their original format, an HTML table. approval then parses the HTML table and prepares the data as a usable Stata dataset.
10226417	WOS:000313197700007	066CO	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	xgrid and R: Parallel Distributed Processing Using Heterogeneous Groups of Apple Computers	Journal	Article	2012	6	1	English	R JOURNAL	45-55	11	1	WOS:000313197700007	The Apple Xgrid system provides access to groups (or grids) of computers that can be used to facilitate parallel processing. We describe the xgrid package which facilitates access to this system to undertake independent simulations or other long-running jobs that can be divided into replicate runs within R. Detailed examples are provided to demonstrate the interface, along with results from a simulation study of the performance gains using a variety of grids. Use of the grid for "embarassingly parallel" independent jobs has the potential for major speedups in time to completion. Appendices provide guidance on setting up the workflow, utilizing add-on packages, and constructing grids using existing machines.
10247855	WOS:000303804200001	939IY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	High-Dimensional Bayesian Clustering with Variable Selection: The R Package bclus	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000303804200001	The R package bclust is useful for clustering high-dimensional continuous data. The package uses a parametric spike-and-slab Bayesian model to downweight the effect of noise variables and to quantify the importance of each variable in agglomerative clustering. We take advantage of the existence of closed-form marginal distributions to estimate the model hyper-parameters using empirical Bayes, thereby yielding a fully automatic method. We discuss computational problems arising in implementation of the procedure and illustrate the usefulness of the package through examples.
10250053	WOS:000313197700009	066CO	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Sumo: An Authenticating Web Application with an Embedded R Session	Journal	Article	2012	6	1	English	R JOURNAL	60-63	4	1	WOS:000313197700009	Sumo is a web application intended as a template for developers. It is distributed as a Java 'war' file that deploys automatically when placed in a Servlet container's 'webapps' directory. If a user supplies proper credentials, Sumo creates a session-specific Secure Shell connection to the host and a user-specific R session over that connection. Developers may write dynamic server pages that make use of the persistent R session and user-specific file space. The supplied example plots a data set conditional on preferences indicated by the user; it also displays some static text. A companion server page allows the user to interact directly with the R session. Sumo's novel feature set complements previous efforts to supply R functionality over the internet.
10278958	WOS:000305989500001	968NO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Integrated Degradation Models in R Using iDEMO	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000305989500001	Degradation models are widely used to assess the lifetime information for highly reliable products with quality characteristics whose degradation over time can be related to reliability. The performance of a degradation model largely depends on an appropriate model description of the product's degradation path. The cross-platform package iDEMO (integrated degradation models) is developed in R and the interface is built using the Tcl/Tk bindings provided by the tcltk and tcltk2 packages included with R. It is a tool to build a linear degradation model which can simultaneously consider the unit-to-unit variation, time-dependent structure and measurement error in the degradation paths. The package iDEMO provides the maximum likelihood estimates of the unknown parameters, mean-time-to-failure and q-th quantile, and their corresponding confidence intervals based on the different information matrices. In addition, degradation model selection and goodness-of-fit tests are provided to determine and diagnose the degradation model for the user's current data by the commonly used criteria. By only enabling user interface elements when necessary, input errors are minimized.
10279742	WOS:000305992100001	968OJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Oscars and Interfaces	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000305992100001	Graphical user interfaces (GUIs) are gradually becoming more powerful and more accepted. They are the standard way of interacting with the web and play an increasing role in many software applications. Nevertheless, they have not been generally adopted, and critics point to particular weaknesses and disadvantages. Many of these are due more to flaws in design and implementation than to the basic concepts of GUIs. More attention could be paid to what users want to do and how a GUI might be developed to support these goals. Using a dataset about Oscar nominees and winners, this paper considers what analyses statisticians might carry out and what kind of GUI would be appropriate for these tasks. (It also offers some insights into the Oscars dataset.)
10301684	WOS:000303804400001	939JA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Greedy Algorithm for Unimodal Kernel Density Estimation by Data Sharpening	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000303804400001	We consider the problem of nonparametric density estimation where estimates are constrained to be unimodal. Though several methods have been proposed to achieve this end, each of them has its own drawbacks and none of them have readily-available computer codes. The approach of Braun and Hall (2001), where a kernel density estimator is modified by data sharpening, is one of the most promising options, but optimization difficulties make it hard to use in practice. This paper presents a new algorithm and MATLAB code for finding good unimodal density estimates under the Braun and Hall scheme. The algorithm uses a greedy, feasibility-preserving strategy to ensure that it always returns a unimodal solution. Compared to the incumbent method of optimization, the greedy method is easier to use, runs faster, and produces solutions of comparable quality. It can also be extended to the bivariate case.
10348971	WOS:000313139600006	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A programmer's command to build formatted statistical tables	Journal	Article	2012	1	1	English	STATA JOURNAL	655-673	19	1	WOS:000313139600006	The frmttable command is a tool for experienced users and programmers to create formatted tables from statistics and write them to Word or LAT(E)X files. My objective is to provide as much control over the layout and formatting of the statistical tables as possible in both file formats while keeping the syntax simple. Users can create rectangular tables with any configuration of data and text; specify numeric formats, font sizes, and font types at the table cell level; specify row spacing; and place lines in or around the table. A complex table can be built by merging or appending new statistics to an existing table, and multiple tables can be included in the same document, making it possible to create a fully formatted statistical appendix from a single do-file. In this article, I provide examples of the ways in which programmers call frmttable to create formatted tables of statistics.
10359192	WOS:000312288900001	053QU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Package to Study the Performance of Step-Up and Step-Down Test Procedures	Journal	Article	2012	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000312288900001	The package can be used to analyze the performance of step-up and step-down procedures. It can be used to compare powers, calculate the "false discovery rate", to study the effects of reduced step procedures, and to calculate P[U <= k], where U is the number of rejected true hypotheses. It can be used to determine the maximum number of steps that can be made and still guarantee (with a given probability) that the number of false rejections will not exceed some specified number. The test statistics are assumed to have a multivariate-t distribution. Examples are included.
10383297	WOS:000305117400001	956VK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	qgraph: Network Visualizations of Relationships in Psychometric Data	Journal	Article	2012	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000305117400001	We present the qgraph package for R, which provides an interface to visualize data through network modeling techniques. For instance, a correlation matrix can be represented as a network in which each variable is a node and each correlation an edge; by varying the width of the edges according to the magnitude of the correlation, the structure of the correlation matrix can be visualized. A wide variety of matrices that are used in statistics can be represented in this fashion, for example matrices that contain (implied) covariances, factor loadings, regression parameters and p values. qgraph can also be used as a psychometric tool, as it performs exploratory and confirmatory factor analysis, using sem and lavaan; the output of these packages is automatically visualized in qgraph, which may aid the interpretation of results. In this article, we introduce qgraph by applying the package functions to data from the NEO-PI-R, a widely used personality questionnaire.
10386140	WOS:000306347300006	973GF	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Threshold regression for time-to-event analysis: The stthreg package	Journal	Article	2012	1	1	English	STATA JOURNAL	257-283	27	1	WOS:000306347300006	In this article, we introduce the stthreg package of Stata commands to fit the threshold regression model, which is based on the first hitting time of a boundary by the sample path of a Wiener diffusion process and is well suited to applications involving time-to-event and survival data. The threshold regression model serves as an important alternative to the Cox proportional hazards model. The four commands that comprise this package for the threshold regression model are the model-fitting command stthreg, the postestimation command trhr for hazard-ratio calculation, the postestimation command trpredict for prediction, and the model diagnostics command sttrkm. These commands can also be used to implement an extended threshold regression model that accommodates applications where a cure rate exists.
10423967	WOS:000305987700001	968NC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Graphical User Interfaces for R	Journal	Article	2012	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-8	8	1	WOS:000305987700001	Since R was first launched, it has managed to gain the support of an ever-increasing percentage of academic and professional statisticians. However, the spread of its use among novice and occasional users of statistics have not progressed at the same pace, which can be atributed partially to the lack of a graphical user interface (GUI). Nevertheless, this situation has changed in the last years and there is currently several projects that have added GUIs to R. This article discusses briefly the history of GUIs for data analysis and then introduces the papers submitted to an special issue of the Journal of Statistical Software on GUIs for R
10426302	WOS:000313198000004	066CR	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Debugging grid Graphics	Journal	Article	2012	12	1	English	R JOURNAL	19-27	9	1	WOS:000313198000004	A graphical scene that has been produced using the grid graphics package consists of grobs (graphical objects) and viewports. This article describes functions that allow the exploration and inspection of the grobs and viewports in a grid scene, including several functions that are available in a new package called gridDebug. The ability to explore the grobs and viewports in a grid scene is useful for adding more drawing to a scene that was produced using grid and for understanding and debugging the grid code that produced a scene.
10433156	WOS:000313139600009	065HO	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	HTML output in Stata	Journal	Article	2012	1	1	English	STATA JOURNAL	702-717	16	1	WOS:000313139600009	In this article, we present a suite of basic commands that facilitate the production of HTML files in Stata. Creating HTML files in Stata allows for the programmatic production of formatted statistical reports that users can easily open without proprietary software, a feature long desired by many Stata statisticians.
10434478	WOS:000301071000001	902VL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	HDclassif: An R Package for Model-Based Clustering and Discriminant Analysis of High-Dimensional Data	Journal	Article	2012	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000301071000001	This paper presents the R package HDclassif which is devoted to the clustering and the discriminant analysis of high-dimensional data. The classification methods proposed in the package result from a new parametrization of the Gaussian mixture model which combines the idea of dimension reduction and model constraints on the covariance matrices. The supervised classification method using this parametrization is called high dimensional discriminant analysis (HDDA). In a similar manner, the associated clustering method is called high dimensional data clustering (HDDC) and uses the expectation-maximization algorithm for inference. In order to correctly fit the data, both methods estimate the specific subspace and the intrinsic dimension of the groups. Due to the constraints on the covariance matrices, the number of parameters to estimate is significantly lower than other model-based methods and this allows the methods to be stable and efficient in high dimensions. Two introductory examples illustrated with R codes allow the user to discover the hdda and hddc functions. Experiments on simulated and real datasets also compare HDDC and HDDA with existing classification methods on high-dimensional datasets. HDclassif is a free software and distributed under the general public license, as part of the R software project.
10437417	WOS:000303616500006	936UW	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Stata graph library for network analysis	Journal	Article	2012	1	1	English	STATA JOURNAL	94-129	36	1	WOS:000303616500006	Network analysis is a multidisciplinary research method that is quickly becoming a popular and exciting field. Though some statistical programs possess sophisticated packages for analyzing networks, similar capabilities have yet to be made available in Stata. In an effort to motivate the use of Stata for network analysis, I designed in Mata the Stata graph library (SGL), which consists of algorithms that construct matrix representations of networks, compute centrality measures, calculate clustering coefficients, and solve maximum-flow problems. The SGL is designed for both directed and undirected one-mode networks containing edges that are either unweighted or weighted with positive values. Performance tests conducted between C++ and Stata graph library implementations indicate gross inefficiencies in current SGL routines, making the SGL impractical for large networks. The obstacles are, however, welcome challenges in the effort to spread the use of Stata for analyzing networks. Future developments will focus toward addressing computational time complexities and integrating additional capabilities into the SGL.
10438910	WOS:000313198000005	066CR	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	frailtyHL: A Package for Fitting Frailty Models with H-likelihood	Journal	Article	2012	12	1	English	R JOURNAL	28-37	10	1	WOS:000313198000005	We present the frailtyHL package for fitting semi-parametric frailty models using h-likelihood. This package allows lognormal or gamma frailties for random-effect distribution, and it fits shared or multilevel frailty models for correlated survival data. Functions are provided to format and summarize the frailtyHL results. The estimates of fixed effects and frailty parameters and their standard errors are calculated. We illustrate the use of our package with three well-known data sets and compare our results with various alternative R-procedures.
10442500	WOS:000313198000007	066CR	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The crs Package: Nonparametric Regression Splines for Continuous and Categorical Predictors	Journal	Article	2012	12	1	English	R JOURNAL	48-56	9	1	WOS:000313198000007	A new package crs is introduced for computing nonparametric regression (and quantile) splines in the presence of both continuous and categorical predictors. B-splines are employed in the regression model for the continuous predictors and kernel weighting is employed for the categorical predictors. We also develop a simple R interface to NOMAD, which is a mixed integer optimization solver used to compute optimal regression spline solutions.
10471488	WOS:000309735300001	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Diagnostics for multiple imputation in Stata	Journal	Article	2012	1	1	English	STATA JOURNAL	353-367	15	1	WOS:000309735300001	Our new command midiagplots makes diagnostic plots for multiple imputations created by mi impute. The plots compare the distribution of the imputed values with that of the observed values so that problems with the imputation model can be corrected before the imputed data are analyzed. We include an example and suggest extensions to other diagnostics.
10474880	WOS:000301231800001	904XI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	survivalBIV: Estimation of the Bivariate Distribution Function for Sequentially Ordered Events Under Univariate Censoring	Journal	Article	2012	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000301231800001	In many medical studies, patients can experience several events. The times between consecutive events (gap times) are often of interest and lead to problems that have received much attention recently. In this work we consider the estimation of the bivariate distribution function for censored gap times, using survivalBIV a software application for R. Some related problems such as the estimation of the marginal distribution of the second gap time is also discussed. It describes the capabilities of the program for estimating these quantities using four different approaches, all using the Kaplan-Meier estimator of survival. One of these estimators is based on Bayes' theorem and Kaplan-Meier survival function. Two estimators were recently proposed using the Kaplan-Meier estimator pertaining to the distribution of the total time to weight the bivariate data (de Una-Alvarez and Meira-Machado 2008 and de Una-Alvarez and Amorim 2011). The software can also be used to implement the estimator proposed in Lin, Sun, and Ying (1999), which is based on inverse probability of censoring weighted. The software is illustrated using data from a bladder cancer study.
10493034	WOS:000310773900001	033DL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	DiceKriging, DiceOptim: Two R Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization	Journal	Article	2012	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-55	55	1	WOS:000310773900001	We present two recently released R packages, DiceKriging and DiceOptim, for the approximation and the optimization of expensive-to-evaluate deterministic functions. Following a self-contained mini tutorial on Kriging-based approximation and optimization, the functionalities of both packages are detailed and demonstrated in two distinct sections. In particular, the versatility of DiceKriging with respect to trend and noise specifications, covariance parameter estimation, as well as conditional and unconditional simulations are illustrated on the basis of several reproducible numerical experiments. We then put to the fore the implementation of sequential and parallel optimization strategies relying on the expected improvement criterion on the occasion of DiceOptim's presentation. An appendix is dedicated to complementary mathematical and computational details.
10493963	WOS:000306914100001	980ST	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Data Analysis with the Morse-Smale Complex: The msr Package for R	Journal	Article	2012	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000306914100001	In many areas, scientists deal with increasingly high-dimensional data sets. An important aspect for these scientists is to gain a qualitative understanding of the process or system from which the data is gathered. Often, both input variables and an outcome are observed and the data can be characterized as a sample from a high-dimensional scalar function. This work presents the R package msr for exploratory data analysis of multivariate scalar functions based on the Morse-Smale complex. The Morse-Smale complex provides a topologically meaningful decomposition of the domain. The msr package implements a discrete approximation of the Morse-Smale complex for data sets. In previous work this approximation has been exploited for visualization and partition-based regression, which are both supported in the msr package. The visualization combines the Morse-Smale complex with dimension-reduction techniques for a visual summary representation that serves as a guide for interactive exploration of the high-dimensional function. In a similar fashion, the regression employs a combination of linear models based on the Morse-Smale decomposition of the domain. This regression approach yields topologically accurate estimates and facilitates interpretation of general trends and statistical comparisons between partitions. In this manner, the msr package supports high-dimensional data understanding and exploration through the Morse-Smale complex.
10499506	WOS:000301232000001	904XK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The benchden Package: Benchmark Densities for Nonparametric Density Estimation	Journal	Article	2012	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000301232000001	This article describes the benchden package which implements a set of 28 example densities for nonparametric density estimation in R. In addition to the usual functions that evaluate the density, distribution and quantile functions or generate random variates, a function designed to be specifically useful for larger simulation studies has been added. After describing the set of densities and the usage of the package, a small toy example of a simulation study conducted using the benchden package is given.
10524957	WOS:000326871000001	250RI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	R Functions for Sample Size and Probability Calculations for Assessing Consistency of Treatment Effects in Multi-Regional Clinical Trials	Journal	Article	2012	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-10	10	1	WOS:000326871000001	Multi-regional clinical trials have been widely used for efficient global new drug developments. Due to potential heterogeneity of patient populations, it is critical to evaluate consistency of treatment effects across different regions in a multi-regional trial in order to determine the applicability of the overall treatment effect to the patients in individual regions. Quan et al. (2010) proposed definitions for the assessments of consistency of treatment effects in multi-regional trials. To facilitate the application of their ideas to design multi-regional trials, in this paper, we provide the corresponding R functions for calculating the unconditional and conditional probabilities for demonstrating consistency in relationship with the overall/regional sample sizes and the anticipated treatment effects. Detailed step by step instructions and trial examples are also provided to illustrate the applications of these R functions.
10535257	WOS:000309735300003	019JH	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Apportionment methods	Journal	Article	2012	1	1	English	STATA JOURNAL	375-392	18	1	WOS:000309735300003	Apportionment methods are used to translate a set of positive natural numbers into a set of smaller natural numbers while keeping the proportions between the numbers very similar. The methods are used to allocate seats in a chamber Proportionally to the number of votes for a party in an election or proportionally to regional populations. In this article, we describe six apportionment methods and the user-written egen function apport(), which implements these methods.
10563356	WOS:000307476000001	988GB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	dlmap: An R Package for Mixed Model QTL and Association Analysis	Journal	Article	2012	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000307476000001	dlmap is a software package capable of mapping quantitative trait loci (QTL) in a variety of genetic studies. Unlike most other QTL mapping packages, dlmap is built on a linear mixed model platform, and thus can simultaneously handle multiple sources of genetic and environmental variation. Furthermore, it can accommodate both experimental crosses and association mapping populations within a versatile modeling framework. The software implements a mapping algorithm with separate detection and localization stages in a user-friendly manner. It accepts data in various common formats, has a flexible modeling environment, and summarizes results both graphically and numerically.
10563985	WOS:000310774500001	033DR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Statistical Computing in Functional Data Analysis: The R Package fda.usc	Journal	Article	2012	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000310774500001	This paper is devoted to the R package fda.usc which includes some utilities for functional data analysis. This package carries out exploratory and descriptive analysis of functional data analyzing its most important features such as depth measurements or functional outliers detection, among others. The R package fda.usc also includes functions to compute functional regression models, with a scalar response and a functional explanatory data via non-parametric functional regression, basis representation or functional principal components analysis. There are natural extensions such as functional linear models and semi-functional partial linear models, which allow non-functional covariates and factors and make predictions. The functions of this package complement and incorporate the two main references of functional data analysis: The R package fda and the functions implemented by Ferraty and Vieu (2006).
10612188	WOS:000303616500007	936UW	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	tt: Treelet transform with Stata	Journal	Article	2012	1	1	English	STATA JOURNAL	130-146	17	1	WOS:000303616500007	The treelet transform is a recent data reduction technique from the field of machine learning. Sharing many similarities with principal component analysis, the treelet transform can reduce a multidimensional dataset to the projections on a small number of directions or components that account for much of the variation in the original data. However, in contrast to principal component analysis, the treelet transform produces sparse components. This can greatly simplify interpretation. I describe the tt Stata add-on for performing the treelet transform. The add-on includes a Mata implementation of the treelet transform algorithm alongside other functionality to aid in the practical application of the treelet transform. I demonstrate an example of a basic exploratory data analysis using the tt add-on.
10618301	WOS:000326870400001	250RE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SIMEX R Package for Accelerated Failure time Model with cobariate Measurement Error	Journal	Article	2012	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	14	1	WOS:000326870400001	It has been well documented that ignoring measurement error may result in substantially biased estimates in many contexts including linear and nonlinear regressions. For survival data with measurement error in covariates, there has been extensive discussion in the literature with the focus typically centered on proportional hazards models. The impact of measurement error on inference under accelerated failure time models has received relatively little attention, although these models are very useful in survival data analysis. He et al. (2007) discussed accelerated failure time models with error-prone covariates and studied the bias induced by the naive approach of ignoring measurement error in covariates. To adjust for the effects of covariate measurement error, they described a simulation and extrapolation method. This method has theoretical advantages such as robustness to distributional assumptions for error prone covariates. Moreover, this method enjoys simplicity and flexibility for practical use. It is quite appealing to analysts who would like to accommodate covariate measurement error in their analysis. To implement this method, in this paper, we develop an R package for general users. Two data sets arising from clinical trials are employed to illustrate the use of the package.
10621439	WOS:000301069900001	902VC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Multi-Language Computing Environment for Literate Programming and Reproducible Research	Journal	Article	2012	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000301069900001	We present a new computing environment for authoring mixed natural and computer language documents. In this environment a single hierarchically-organized plain text source file may contain a variety of elements such as code in arbitrary programming languages, raw data, links to external resources, project management data, working notes, and text for publication. Code fragments may be executed in situ with graphical, numerical and textual output captured or linked in the file. Export to LATEX, HTML, LATEX beamer, DocBook and other formats permits working reports, presentations and manuscripts for publication to be generated from the file. In addition, functioning pure code files can be automatically extracted from the file. This environment is implemented as an extension to the Emacs text editor and provides a rich set of features for authoring both prose and code, as well as sophisticated project management capabilities.
10632266	WOS:000299494600006	881NY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Managing the US Census 2000 and World Development Indicators databases for statistical analysis in Stata	Journal	Article	2011	1	1	English	STATA JOURNAL	589-604	16	1	WOS:000299494600006	This article introduces a new Stata command, labcenswdi,(1) to automatically manage databases that provide variable descriptions on the second row in a dataset. While renaming all variables and converting them from string to numeric, labcenswdi automatically manages the variable descriptions including removing them from the second row to place them into Stata variable labels and saving them to a text file. The process yields a dataset ready for statistical analysis. I illustrate how this command can be used to efficiently manage datmets obtained from the U.S. Census 2000 and the World Development Indicators databases.
10654205	WOS:000289457800001	749IF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Rcpp: Seamless R and C plus plus Integration	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000289457800001	The Rcpp package simplifies integrating C++ code with R. It provides a consistent C++ class hierarchy that maps various types of R objects (vectors, matrices, functions, environments, ...) to dedicated C++ classes. Object interchange between R and C++ is managed by simple, flexible and extensible concepts which include broad support for C++ Standard Template Library idioms. C++ code can both be compiled, linked and loaded on the fly, or added via packages. Flexible error and exception code handling is provided. Rcpp substantially lowers the barrier for programmers wanting to combine C++ code with R.
10661645	WOS:000296718900001	844AH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Temporal and Spatial Independent Component Analysis for fMRI Data Sets Embedded in the AnalyzeFMRI R Package	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000296718900001	For statistical analysis of functional magnetic resonance imaging (fMRI) data sets, we propose a data-driven approach based on independent component analysis (ICA) implemented in a new version of the AnalyzeFMRI R package. For fMRI data sets, spatial dimension being much greater than temporal dimension, spatial ICA is the computationally tractable approach generally proposed. However, for some neuroscientific applications, temporal independence of source signals can be assumed and temporal ICA becomes then an attractive exploratory technique. In this work, we use a classical linear algebra result ensuring the tractability of temporal ICA. We report several experiments on synthetic data and real MRI data sets that demonstrate the potential interest of our R package.
10692094	WOS:000285981500001	703MH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Empirical Transition Matrix of Multi-State Models: The etm Package	Journal	Article	2011	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000285981500001	Multi-State models provide a relevant framework for modelling complex event histories. Quantities of interest are the transition probabilities that can be estimated by the empirical transition matrix, that is also referred to as the Aalen-Johansen estimator. In this paper, we present the R package etm that computes and displays the transition proabilities. etm also features a Greenwood-type estimator of the covariance matrix. The use of the package is illustrated through a prominent example in bone marrow transplant for leukaemia patients.
10692826	WOS:000292850600009	793UY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: MMXI and all that: Handling Roman numerals within Stata	Journal	Article	2011	1	1	English	STATA JOURNAL	126-142	17	1	WOS:000292850600009	The problem of handling Roman numerals in Stata is used to illustrate issues arising in the handling of classification codes in character string form and their numeric equivalents. The solutions include Stata programs and Mata functions for conversion from numeric to string and from string to numeric. Defining acceptable input and trapping and flagging incorrect or unmanageable inputs are key concerns in good practice. Regular expressions are especially valuable for this problem.
10709893	WOS:000292850600003	793UY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Visualization of social networks in Stata using multidimensional scaling	Journal	Article	2011	1	1	English	STATA JOURNAL	52-63	12	1	WOS:000292850600003	I describe and illustrate the use of multidimensional scaling methods for visualizing social networks in Stata. The procedure is implemented in the netplot command. I discuss limitations of the approach and sketch possibilities for improvement.
10713481	WOS:000294275400007	812FB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	M statistic commands: Interpoint distance distribution analysis	Journal	Article	2011	1	1	English	STATA JOURNAL	271-289	19	1	WOS:000294275400007	We implement the commands mstat and mtest to perform inference based on the M statistic, a statistic that can be used to compare the interpoint distance distribution across groups of observations. The analyses are based on the study of the interpoint distances between n points in a k-dimensional setting to produce a one-dimensional real-valued test statistic. The locations are distributed in a region of the plane. When we consider all (72) interpoint distances, the dependencies among them are difficult to express analytically, but their distribution is informative, and the M statistic can be built to summarize one aspect of this information. The two commands can be used on a wide class of datasets to test the null hypothesis that two groups have the same (spatial) distribution. mstat and mtest return the exact M test statistic. Moreover, mtest executes a Monte Carlo type permutation test, which returns the empirical p-value together with its confidence interval. This is the command to use in most situations, because the convergence of M to its asymptotic chi-squared distribution is slow. Both commands can be used to obtain graphical output of the empirical density function of the interpoint distance distributions in the two groups and the two-dimensional map of the 71 observations in the plane. The descriptions of the commands are accompanied by examples of applications with real and simulated data. We run the test on the Alt and Vach grave site dataset (Manjourides and Pagano, forthcoming, Statistics in Medicine) and reject the null hypothesis, in contradiction to other published analyses. We also show how to adapt the techniques to discrete datasets with more than one unit in each location. Finally, we report an extensive application on breast cancer data in Massachusetts; in the application, we show the compatibility of the M commands with Pisati's spmap package.
10734201	WOS:000208590100009	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Raster Images in R Graphics	Journal	Article	2011	6	1	English	R JOURNAL	48-54	7	1	WOS:000208590100009	The R graphics engine has new support for rendering raster images via the functions rasterImage() and grid. raster(). This leads to better scaling of raster images, faster rendering to screen, and smaller graphics files. Several examples of possible applications of these new features are described.
10736040	WOS:000285981300001	703MF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	p3state.msm: Analyzing Survival Data from an Illness-Death Model	Journal	Article	2011	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000285981300001	In longitudinal studies of disease, patients can experience several events across a follow-up period. Analysis of such studies can be successfully performed by multi-state models. In the multi-state framework, issues of interest include the study of the relationship between covariates and disease evolution, estimation of transition probabilities, and survival rates. This paper introduces p3state.msm, a software application for R which performs inference in an illness-death model. It describes the capabilities of the program for estimating semi-parametric regression models and for implementing nonparametric estimators for several quantities. The main feature of the package is its ability for obtaining non-Markov estimates for the transition probabilities. Moreover, the methods can also be used in progressive three-state models. In such a model, estimators for other quantities, such as the bivariate distribution function (for sequentially ordered events), are also given. The software is illustrated using data from the Stanford Heart Transplant Study.
10775899	WOS:000287815300001	727QE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	LDR: A Package for Likelihood-Based Sufficient Dimension Reduction	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000287815300001	We introduce a new M A T L A B software package that implements several recently proposed likelihood-based methods for sufficient dimension reduction. Current capabilities include estimation of reduced subspaces with a fixed dimension d, as well as estimation of d by use of likelihood-ratio testing, permutation testing and information criteria. The methods are suitable for preprocessing data for both regression and classification. Implementations of related estimators are also available. Although the software is more oriented to command-line operation, a graphical user interface is also provided for prototype computations.
10800694	WOS:000288204200001	732QQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Logcondens: Computations Related to Univariate Log-Concave Density Estimation	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000288204200001	Maximum likelihood estimation of a log-concave density has attracted considerable attention over the last few years. Several algorithms have been proposed to estimate such a density. Two of those algorithms, an iterative convex minorant and an active set algorithm, are implemented in the R package logcondens. While these algorithms are discussed elsewhere, we describe in this paper the use of the logcondens package and discuss functions and datasets related to log-concave density estimation contained in the package. In particular, we provide functions to (1) compute the maximum likelihood estimate (MLE) as well as a smoothed log-concave density estimator derived from the MLE, (2) evaluate the estimated density, distribution and quantile functions at arbitrary points, (3) compute the characterizing functions of the MLE, (4) sample from the estimated distribution, and finally (5) perform a two-sample permutation test using a modi fied Kolmogorov-Smirnov test statistic. In addition, logcondens makes two datasets available that have been used to illustrate log-concave density estimation.
10803112	WOS:000290086900001	757LQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Using the R Package crlmm for Genotyping and Copy Number Estimation	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000290086900001	Genotyping platforms such as Affymetrix can be used to assess genotype-phenotype as well as copy number-phenotype associations at millions of markers. While genotyping algorithms are largely concordant when assessed on HapMap samples, tools to assess copy number changes are more variable and often discordant. One explanation for the discordance is that copy number estimates are susceptible to systematic differences between groups of samples that were processed at different times or by different labs. Analysis algorithms that do not adjust for batch effects are prone to spurious measures of association. The R package crlmm implements a multilevel model that adjusts for batch effects and provides allele-specific estimates of copy number. This paper illustrates a workfow for the estimation of allele-specific copy number and integration of the marker-level estimates with complimentary Bioconductor software for inferring regions of copy number gain or loss. All analyses are performed in the statistical environment R
10834975	WOS:000298032600001	861PV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multiple Imputation by Chained Equations (MICE): Implementation in Stata	Journal	Article	2011	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000298032600001	Missing data are a common occurrence in real datasets. For epidemiological and prognostic factors studies in medicine, multiple imputation is becoming the standard route to estimating models with missing covariate data under a missing-at-random assumption. We describe ice, an implementation in Stata of the MICE approach to multiple imputation. Real data from an observational study in ovarian cancer are used to illustrate the most important of the many options available with ice. We remark briefly on the new database architecture and procedures for multiple imputation introduced in releases 11 and 12 of Stata.
10836201	WOS:000290527200001	763BE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	REGCMPNT - A Fortran Program for Regression Models with ARIMA Component Errors	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000290527200001	RegComponent models are time series models with linear regression mean functions and error terms that follow ARIMA (autoregressive-integrated-moving average) component time series models. Bell (2004) discusses these models and gives some underlying theoretical and computational results. The REGCMPNT program is a Fortran program for performing Gaussian maximum likelihood estimation, signal extraction, and forecasting with RegComponent models. In this paper we briefly examine the nature of RegComponent models, provide an overview of the REGCMPNT program, and then use three examples to show some important features of the program and to illustrate its application to various different RegComponent models.
10848145	WOS:000290527600001	763BI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	State Space Methods in RATS	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000290527600001	This paper uses several examples to show how the econometrics program RATS can be used to analyze state space models. It demonstrates Kalman filtering and smoothing, estimation of hyperparameters, unconditional and conditional simulation. It also provides a more complicated example where a dynamic simultaneous equations model is transformed into a proper state space representation and its unknown parameters are estimated.
10849559	WOS:000285981200001	703ME	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Analyzing Competing Risk Data Using the R timereg Pac	Journal	Article	2011	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000285981200001	In this paper we describe flexible competing risks regression models using the comp.risk () function available in the time re g package for R based on Scheike et al. (2008). Regression models are specified for the transition probabilities, that is the cumulative incidence in the competing risks setting. The model contains the Fine and Gray (1999) model as a special case. This can be used to do goodness-of-fit test for the subdistribution hazards' proportionality assumption (Scheike and Zhang 2008). The program can also construct confidence bands for predicted cumulative incidence curves. We apply the methods to data on follicular cell lymphoma from Pintilie (2007), where the competing risks are disease relapse and death without relapse. There is important non-proportionality present in the data, and it is demonstrated how one can analyze these data using the flexible regression models.
10863160	WOS:000208590200006	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Ckmeans.1d.dp: Optimal k-means Clustering in One Dimension by Dynamic Programming	Journal	Article	2011	12	1	English	R JOURNAL	29-33	5	1	WOS:000208590200006	The heuristic k-means algorithm, widely used for cluster analysis, does not guarantee optimality. We developed a dynamic programming algorithm for optimal one-dimensional clustering. The algorithm is implemented as an R package called Ckmeans.1d.dp. We demonstrate its advantage in optimality and runtime over the standard iterative k-means algorithm.
10868511	WOS:000298032200001	861PR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	State of the Multiple Imputation Software	Journal	Article	2011	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-7	7	1	WOS:000298032200001	Owing to its practicality as well as strong inferential properties, multiple imputation has been increasingly popular in the analysis of incomplete data. Methods that are not only computationally elegant but also applicable in wide spectrum of statistical incomplete data problems have also been increasingly implemented in a numerous computing environments. Unfortunately, however, the speed of this development has not been replicated in reaching to "sophisticated" users. While the researchers have been quite successful in developing the underlying software, documentation in a style that would be most reachable to the greater scientific society has been lacking. The main goal of this special volume is to close this gap by articles that illustrate these software developments. Here I provide a brief history of multiple imputation and relevant software and highlight the contents of the contributions. Potential directions for the future of the software development is also provided.
10905167	WOS:000288205000001	732QY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	lordif: An R Package fo rDetecting Differential Item Functioning Using Iterative Hybrid Ordinal Logistic Regression/Item Response Theory and Monte Carlo Simulations	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000288205000001	Logistic regression provides a flexible framework for detecting various types of differential item functioning (DIF). Previous efforts extended the framework by using item response theory (IRT) based trait scores, and by employing an iterative process using group-specific item parameters to account for DIF in the trait scores, analogous to purification approaches used in other DIF detection frameworks. The current investigation advances the technique by developing a computational platform integrating both statistical and IRT procedures into a single program. Furthermore, a Monte Carlo simulation approach was incorporated to derive empirical criteria for various DIF statistics and effect size measures. For purposes of illustration, the procedure was applied to data from a questionnaire of anxiety symptoms for detecting DIF associated with age from the Patient-Reported Outcomes Measurement Information System.
10913063	WOS:000290526200001	763AV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Statistical Software for State Space Methods	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000290526200001	In this paper we review the state space approach to time series analysis and establish the notation that is adopted in this special volume of the Journal of Statistical Software. We first provide some background on the history of state space methods for the analysis of time series. This is followed by a concise overview of linear Gaussian state space analysis including the modelling framework and appropriate estimation methods. We discuss the important class of unobserved component models which incorporate a trend, a seasonal, a cycle, and fixed explanatory and intervention variables for the univariate and multivariate analysis of time series. We continue the discussion by presenting methods for the computation of different estimates for the unobserved state vector: filtering, prediction, and smoothing. Estimation approaches for the other parameters in the model are also considered. Next, we discuss how the estimation procedures can be used for constructing confidence intervals, detecting outlier observations and structural breaks, and testing model assumptions of residual independence, homoscedasticity, and normality. We then show how ARIMA and ARIMA components models fit in the state space framework to time series analysis. We also provide a basic introduction for non-Gaussian state space models. Finally, we present an overview of the software tools currently available for the analysis of time series with state space methods as they are discussed in the other contributions to this special volume.
10922012	WOS:000288203800001	732QN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Hidden Semi Markov Models for Multiple Observation Sequences: The mhsmm Package for R	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000288203800001	This paper describes the R package mhsmm which implements estimation and prediction methods for hidden Markov and semi-Markov models for multiple observation sequences. Such techniques are of interest when observed data is thought to be dependent on some unobserved (or hidden) state. Hidden Markov models only allow a geometrically distributed sojourn time in a given state, while hidden semi-Markov models extend this by allowing an arbitrary sojourn distribution. We demonstrate the software with simulation examples and an application involving the modelling of the ovarian cycle of dairy cows.
10941130	WOS:000292098100001	783QD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The Scythe Statistical Library: An Open Source C plus plus Library for Statistical Computation	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000292098100001	The Scythe Statistical Library is an open source C++ library for statistical computation. It includes a suite of matrix manipulation functions, a suite of pseudo-random number generators, and a suite of numerical optimization routines. Programs written using Scythe are generally much faster than those written in commonly used interpreted languages, such as R and MATLAB; and can be compiled on any system with the GNU GCC compiler (and perhaps with other C++ compilers). One of the primary design goals of the Scythe developers has been ease of use for non-expert C++ programmers. Ease of use is provided through three primary mechanisms: (1) operator and function over loading, (2) numerous pre-fabricated utility functions, and (3) clear documentation and example programs. Additionally, Scythe is quite flexible and entirely extensible because the source code is available to all users under the GNU General Public License.
10957357	WOS:000208590100013	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Tips for Presenting Your Work	Journal	Editorial Material	2011	6	1	English	R JOURNAL	72-74	3	1	WOS:000208590100013	With the international R user conference, useR! 2011, approaching, many participants may be contemplating how to put their thoughts together for presentation. This paper provides some suggestions for giving presentations and making posters.
10993384	WOS:000208590100004	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Rmetrics - timeDate Package	Journal	Article	2011	6	1	English	R JOURNAL	19-24	6	1	WOS:000208590100004	The management of time and holidays can prove crucial in applications that rely on historical data. Atypical example is the aggregation of a data set recorded in different time zones and under different daylight saving time rules. Besides the time zone conversion function, which is well supported by default classes in R, one might need functions to handle special days or holidays. In this respect, the package timeDate enhances default date-time classes in R and brings new functionalities to time zone management and the creation of holiday calendars.
11000424	WOS:000293390400001	800TP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Analyzing Remote Sensing Data in R: The landsat Package	Journal	Article	2011	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	25	1	WOS:000293390400001	Research and development on atmospheric and topographic correction methods for multispectral satellite data such as Landsat images has far outpaced the availability of those methods in geographic information systems software. As Landsat and other data become more widely available, demand for these improved correction methods will increase. Open source R statistical software can help bridge the gap between research and implementation. Sophisticated spatial data routines are already available, and the ease of program development in R makes it straightforward to implement new correction algorithms and to assess the results. Collecting radiometric, atmospheric, and topographic correction routines into the landsat package will make them readily available for evaluation for particular applications
11027279	WOS:000296298000001	838NA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Logistic quantile regression in Stata	Journal	Article	2011	1	1	English	STATA JOURNAL	327-344	18	1	WOS:000296298000001	We present a set of Stata commands for the estimation, prediction, and graphical representation of logistic quantile regression described by Bottai, Cai, and McKeown (2010, Statistics in Medicine 29: 309-317). Logistic quantile regression models the quantiles of outcome variables that take on values within a bounded, known interval, such as proportions (or percentages) within 0 and 1, school grades between 0 and 100 points, and visual analog scales between 0 and 10 cm. We describe the syntax of the new commands and illustrate their use with data from a large cohort of Swedish men on lower urinary tract symptoms measured on the international prostate symptom score, a widely accepted score bounded between 0 and 35.
11044556	WOS:000296719700001	844AO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	FIAR: An R Package for Analyzing Functional Integration in the Brain	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000296719700001	Functional integration in the brain refers to distributed interactions among functionally segregated regions. Investigation of effective connectivity in brain networks, i.e, the directed causal influence that one brain region exerts over another region, is being increasingly recognized as an important tool for understanding brain function in neuroimaging studies. Methods for identifying intrinsic relationships among elements in a network are increasingly in demand. Over the last few decades several techniques such as Bayesian networks, Granger causality, and dynamic causal models have been developed to identify causal relations in dynamic systems. At the same time, established techniques such as structural equation modeling (SEM) are being modified and extended in order to reveal underlying interactions in imaging data. In the R package FIAR, which stands for Functional Integration Analysis in R, we have implemented many of the latest techniques for analyzing brain networks based on functional magnetic resonance imaging (fMRI) data. The package can be used to analyze experimental data, but also to simulate data under certain models.
11059922	WOS:000287815100001	727QC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Kalman Filtering in R	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000287815100001	Support in R for state space estimation via Kalman filtering was limited to one package, until fairly recently. In the last five years, the situation has changed with no less than four additional packages offering general implementations of the Kalman filter, including in some cases smoothing, simulation smoothing and other functionality. This paper reviews some of the offerings in R to help the prospective user to make an informed choice.
11060508	WOS:000292850600007	793UY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Stata utilities for geocoding and generating travel time and travel distance information	Journal	Article	2011	1	1	English	STATA JOURNAL	106-119	14	1	WOS:000292850600007	This article describes geocode and travel:time, two commands that use Google Maps to provide spatial information for data. The geocode command allows users to generate latitude and longitude for various types of locations, including addresses. The traveltime command takes latitude and longitude information and finds travel distances between points, as well as the time it would take to travel that distance by either driving, walking, or using public transportation.
11062401	WOS:000296228800001	837UG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Quantitative Analysis of Dynamic Contrast-Enhanced and Diffusion-Weighted Magnetic Resonance Imaging for Oncology in R	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000296228800001	The package dcemriS4 provides a complete set of data analysis tools for quantitative assessment of dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Image processing is provided for the ANALYZE and NIfTI data formats as input with all parameter estimates being output in NIfTI format. Estimation of T1 relaxation from multiple flip-angle acquisitions, using either constant or spatially-varying flip angles, is performed via nonlinear regression. Both literature-based and data-driven arterial input functions are available and may be combined with a variety of compartmental models. Kinetic parameters are obtained from nonlinear regression, Bayesian estimation via Markov chain Monte Carlo or Bayesian maximum a posteriori estimation. A non-parametric model, using penalized splines, is also available to characterize the contrast agent concentration time curves. Estimation of the apparent diffusion coefficient (ADC) is provided for diffusion-weighted imaging. Given the size of multi-dimensional data sets commonly acquired in imaging studies, care has been taken to maximize computational efficiency and minimize memory usage. All methods are illustrated using both simulated and real-world medical imaging data available in the public domain.
11070481	WOS:000293391100001	800TV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SAS Macros for Calculation of Population Attributable Fraction in a Cohort Study Design	Journal	Article	2011	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000293391100001	The population attributable fraction (PAF) is a useful measure for quantifying the impact of exposure to certain risk factors on a particular outcome at the population level. Recently, new model-based methods for the estimation of PAF and its confidence interval for different types of outcomes in a cohort study design have been proposed. In this paper, we introduce SAS macros implementing these methods and illustrate their application with a data example on the impact of different risk factors on type 2 diabetes incidence.
11071491	WOS:000293389700001	800TI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	bayesTFR: An R Package for Probabilistic Projections of the Total Fertility Rate	Journal	Article	2011	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000293389700001	The bayesTFR package for R provides a set of functions to produce probabilistic projections of the total fertility rate (TFR) for all countries. In the model, a random walk with drift is used to project the TFR during the fertility transition, using a Bayesian hierarchical model to estimate the parameters of the drift term. The TFR is modeled with a first order autoregressive process during the post-transition phase. The computationally intensive part of the projection model is a Markov chain Monte Carlo algorithm for estimating the parameters of the drift term. This article summarizes the projection model and describes the basic steps to generate probabilistic projections, as well as other functionalities such as projecting aggregate outcomes and dealing with missing values.
11081476	WOS:000296298000008	838NA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Fun and fluency with functions	Journal	Article	2011	1	1	English	STATA JOURNAL	460-471	12	1	WOS:000296298000008	Functions are the unsung heroes of Stata. This column is a tour of functions that might easily be missed or underestimated, with a potpourri of tips, tricks, and examples for a wide range of basic problems.
11099799	WOS:000296298000006	838NA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Comparing coefficients of nested nonlinear probability models	Journal	Article	2011	1	1	English	STATA JOURNAL	420-438	19	1	WOS:000296298000006	In a series of recent articles, Karlson, Holm, and Breen (Breen, Karlson, and Holm, 2011, http://papers.ssrn.com/sol3/papers.cfm?abstractid=1730065; Karlson and Holm, 2011, Research in Stratification and Social Mobility 29: 221 237;.Karlson, Holm, and Breen, 2010, http://www.yale.edu/ciqle/Breen_Scaling %20effects.pdf) have developed a method for comparing the estimated coefficients of two nested nonlinear probability models. In this article, we describe this method and the user-written program khb, which implements the method. The KHB method is a general decomposition method that is unaffected by the resealing or attenuation bias that arises in cross-model comparisons in nonlinear models. It recovers the degree to which a control variable, Z, mediates or explains the relationship between X and a latent outcome variable, Y*, underlying the nonlinear probability model. It also decomposes effects of both discrete and continuous variables, applies to average partial effects, and provides analytically derived statistical tests. The method can be extended to other models in the generalized linear model family.
11113977	WOS:000296228400001	837UD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	DATforDCEMRI: An R Package for Deconvolution Analysis and Visualization of DCE-MRI Data	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000296228400001	Numerical deconvolution is a powerful mathematical operation that can be used to extract the impulse response function of a linear, time-invariant system. We have found this method to be useful for preliminary analysis of dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) data, capable of quickly producing voxel-wise parametric maps describing the heterogeneity of contrast agent kinetics over the entire field of view, typically comprising tens of thousands of voxels. The statistical programming language R is well suited for this type of analysis and when combined with LATEX, via Sweave, allows one to perform all calculations and generate a report with a single script. The purpose of this manuscript is to describe the R package DATforDCEMRI, a Deconvolution Analysis Tool for DCE-MRI contrast agent concentration vs. time data, which allows the user to perform kinetic deconvolution analysis and visualize/explore the resulting voxel-wise parametric maps and associated data.
11116421	WOS:000292850600008	793UY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	eq5d: A command to calculate index values for the EQ-5D quality-of-life instrument	Journal	Article	2011	1	1	English	STATA JOURNAL	120-125	6	1	WOS:000292850600008	The eq5d command computes an index value using the individual mobility, self care, usual activities, pain or discomfort, and anxiety or depression responses from the EuroQol EQ-5D quality-of-life instrument. The command calculates index values using value sets from eight countries: the United Kingdom, the United States, Spain, Germany, the Netherlands, Denmark, japan, and Zimbabwe.
11116641	WOS:000299494600001	881NY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	gformula: Estimating causal effects in the presence of time-varying confounding or mediation using the g-computation formula	Journal	Article	2011	1	1	English	STATA JOURNAL	479-517	39	1	WOS:000299494600001	Tins article describes a new command, gformula, that is an implementation of the g-computation procedure. It is used to estimate the causal effect of time-varying exposures on an outcome in the presence of time-varying confounders that are themselves also affected by the exposures. The procedure also addresses the related problem of estimating direct and indirect effects when the causal effect of the exposures on an outcome is mediated by intermediate variables, and in particular when confounders of the mediator-outcome relationships are themselves affected by the exposures. A brief overview of the theory and a description of the command and its options are given, and illustrations using two simulated examples are provided.
11158332	WOS:000208590200012	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	R's Participation in the Google Summer of Code 2011	Journal	Article	2011	12	1	English	R JOURNAL	64-67	4	1	WOS:000208590200012	This article describes the activity of R in the 2011 Google Summer of Code (GSoC), which Wikipedia describes as follows. The Google Summer of Code (GSoC) is an annual program, first held from May to August 2005, in which Google awards stipends (of 5000 USD, as of 2010) to hundreds of students who successfully complete a requested free or open source software coding project during the summer.
11173752	WOS:000296228600001	837UF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	cudaBayesreg: Parallel Implementation of a Bayesian Multilevel Model for fMRI Data Analysis	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000296228600001	Graphic processing units (GPUs) are rapidly gaining maturity as powerful general parallel computing devices. A key feature in the development of modern GPUs has been the advancement of the programming model and programming tools. Compute Unified Device Architecture (CUDA) is a software platform for massively parallel high-performance computing on Nvidia many-core GPUs. In functional magnetic resonance imaging (fMRI), the volume of the data to be processed, and the type of statistical analysis to perform call for high-performance computing strategies. In this work, we present the main features of the R-CUDA package cudaBayesreg which implements in CUDA the core of a Bayesian multilevel model for the analysis of brain fMRI data. The statistical model implements a Gibbs sampler for multilevel/hierarchical linear models with a normal prior. The main contribution for the increased performance comes from the use of separate threads for fitting the linear regression model at each voxel in parallel. The R-CUDA implementation of the Bayesian model proposed here has been able to reduce significantly the run-time processing of Markov chain Monte Carlo (MCMC) simulations used in Bayesian fMRI data analyses. Presently, cudaBayesreg is only configured for Linux systems with Nvidia CUDA support.
11208330	WOS:000289228500001	746FM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Analyzing and Visualizing State Sequences in R with TraMineR	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-37	37	1	WOS:000289228500001	This article describes the many capabilities offered by the TraMineR toolbox for categorical sequence data. It focuses more specifically on the analysis and rendering of state sequences. Addressed features include the description of sets of sequences by means of transversal aggregated views, the computation of longitudinal characteristics of individual sequences and the measure of pairwise dissimilarities. Special emphasis is put on the multiple ways of visualizing sequences. The core element of the package is the state sequence object in which we store the set of sequences together with attributes such as the alphabet, state labels and the color palette. The functions can then easily retrieve this information to ensure presentation homogeneity across all printed and graphical displays. The article also demonstrates how TraMineR's outcomes give access to advanced analyses such as clustering and statistical modeling of sequence data.
11209830	WOS:000296719200001	844AK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Statistical Parametric Maps for Functional MRI Experiments in R: The Package fmri	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000296719200001	The purpose of the package fmri is the analysis of single subject functional magnetic resonance imaging (fMRI) data. It provides fMRI analysis from time series modeling by a linear model to signal detection and publication quality images. Specifically, it implements structural adaptive smoothing methods with signal detection for adaptive noise reduction which avoids blurring of activation areas. Within this paper we describe the complete pipeline for fMRI analysis using fmri. We describe data reading from various medical imaging formats and the linear modeling used to create the statistical parametric maps. We review the rationale behind the structural adaptive smoothing algorithms and explain their usage from the package fmri. We demonstrate the results of such analysis using two experimental datasets. Finally, we report on the usage of a graphical user interface for some of the package functions.
11213521	WOS:000294275400002	812FB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Fitting fully observed recursive mixed-process models with cmp	Journal	Article	2011	1	1	English	STATA JOURNAL	159-206	48	1	WOS:000294275400002	At the heart of many econometric models are a linear function and a normal error. Examples include the classical small-sample linear regression model and the probit, ordered probit, multinomial probit, tobit, interval regression, and truncated-distribution regression models. Because the normal distribution has a, natural multidimensional generalization, such models can be combined into multiequation systems in which the errors share a multivariate normal distribution. The literature has historically focused on multistage procedures for fitting mixed models, which are more efficient computationally, if less so statistically, than maximum likelihood. Direct maximum likelihood estimation has been made more practical by faster computers and simulated likelihood methods for estimating higher-dimensional cumulative normal distributions. Such simulated likelihood methods include the Geweke-Hajivassiliou-Keane algorithm (Geweke, 1989, Econometrics 57: :1317-1339; Hajivassiliou and McFadden, 1998, Econometrics 66: 863-896; Keane, 1994, Econometrics 62: 95-116). Maximum likelihood also facilitates a generalization to switching, selection, and other models in which the number and types of equations vary by observation. The Stata command cmp fits seemingly unrelated regressions models of this broad family. Its estimator is also consistent for recursive systems in which all endogenous variables appear on the right-hand sides as observed. If all the equations are structural, then estimation is full-inforrnation maximum likelihood. If only the final stage or stages are structural, then estimation is limited-information maximum likelihood. cmp can mimic a score of built-in and user-written Stata commands. It is also appropriate for a panoply of models that previously were hard to estimate. Heteroskedasticity, however, can render cmp inconsistent. This article explains the theory and implementation of cmp and of a related Mata function, ghk 2 (), that implements the Geweke-Hajivassiliou-Keane algorithm.
11229202	WOS:000292850600006	793UY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Right-censored Poisson regression model	Journal	Article	2011	1	1	English	STATA JOURNAL	95-105	11	1	WOS:000292850600006	I present the rcpoisson command for right-censored count-data models with a constant (Terza 1985, Economics Letters 18: 361-365) and variable censoring threshold (Caudill and Mixon 1995, Empirical Economics 20: 183-1.96). I show the effects of censoring on estimation results by comparing the censored Poisson model with the uncensored one.
11255418	WOS:000298032300001	861PS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multiple Imputation with Diagnostics (mi) in R: Opening Windows into the Black Box	Journal	Article	2011	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000298032300001	Our mi package in R has several features that allow the user to get inside the imputation process and evaluate the reasonableness of the resulting models and imputations. These features include: choice of predictors, models, and transformations for chained imputation models; standard and binned residual plots for checking the fit of the conditional distributions used for imputation; and plots for comparing the distributions of observed and imputed data. In addition, we use Bayesian models and weakly informative prior distributions to construct more stable estimates of imputation models. Our goal is to have a demonstration package that (a) avoids many of the practical problems that arise with existing multivariate imputation programs, and (b) demonstrates state-of-the-art diagnostics that can be applied more generally and can be in corporated in to the software of others.
11268134	WOS:000208590200002	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Creating and Deploying an Application with (R)Excel and R	Journal	Article	2011	12	1	English	R JOURNAL	5-11	7	1	WOS:000208590200002	We present some ways of using R in Excel and build an example application using the package rpart. Starting with simple interactive use of rpart in Excel, we eventually package the code into an Excel-based application, hiding all details (including R itself) from the end user. In the end, our application implements a service-oriented architecture (SOA) with a clean separation of presentation and computation layer.
11272050	WOS:000290527800001	763BK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	State Space Methods in Stata	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000290527800001	We illustrate how to estimate parameters of linear state-space models using the Stata program sspace. We provide examples of how to use sspace to estimate the parameters of unobserved-component models, vector autoregressive moving-average models, and dynamic-factor models. We also show how to compute one-step, filtered, and smoothed estimates of the series and the states; dynamic forecasts and their confidence intervals; and residuals.
11282629	WOS:000292096900001	783PR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	BARD: Better Automated Redistricting	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000292096900001	BARD is the first (and at time of writing, only) open source software package for general redistricting and redistricting analysis. BARD provides methods to create, display, compare, edit, automatically refine, evaluate, and profile political districting plans. BARD aims to provide a framework for scientific analysis of redistricting plans and to facilitate wider public participation in the creation of new plans. BARD facilitates map creation and refinement through command-line, graphical user interface, and automatic methods. Since redistricting is a computationally complex partitioning problem not amenable to an exact optimization solution, BARD implements a variety of selectable metaheuristics that can be used to refine existing or randomly-generated redistricting plans based on user-determined criteria. Furthermore, BARD supports automated generation of redistricting plans and profiling of plans by assigning different weights to various criteria, such as district compactness or equality of population. This functionality permits exploration of trade-offs among criteria. The intent of a redistricting authority may be explored by examining these trade-offs and inferring which reasonably observable plans were not adopted. Redistricting is a computationally-intensive problem for even modest-sized states. Performance is thus an important consideration in BARD's design and implementation. The program implements performance enhancements such as evaluation caching, explicit memory management, and distributed computing across snow clusters.
11295627	WOS:000208590200004	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Implementing the Compendium Concept with Sweave and DOCSTRIP	Journal	Article	2011	12	1	English	R JOURNAL	16-21	6	1	WOS:000208590200004	This article suggests an implementation of the compendium concept by combining Sweave and the LAT(E)X literate programming environment DOCSTRIP.
11306097	WOS:000296719000001	844AI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	neuRosim: An R Package for Generating fMRI Data	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000296719000001	Studies that validate statistical methods for functional magnetic resonance imaging (fMRI) data often use simulated data to ensure that the ground truth is known. However, simulated fMRI data are almost always generated using in-house procedures because a well-accepted simulation method is lacking. In this article we describe the R package neuRosim, which is a collection of data generation functions for neuroimaging data. We will demonstrate the possibilities to generate data from simple time series to complete 4D images and the possibilities for the user to create her own data generation method.
11314496	WOS:000289932800001	755LN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	spa: Semi-Supervised Semi-Parametric Graph-Based Estimation in R	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000289932800001	In this paper, we present an R package that combines feature-based (X) data and graph-based (G) data for prediction of the response Y. In this particular case, Y is observed for a subset of the observations (labeled) and missing for the remainder (unlabeled). We examine an approach for fitting (Y) over cap = X (beta) over cap + (f) over cap (G) where (beta) over cap is a coefficient vector and (f) over cap f is a function over the vertices of the graph. The procedure is semi-supervised in nature (trained on the labeled and unlabeled sets), requiring iterative algorithms for fitting this estimate. The package provides several key functions for fitting and evaluating an estimator of this type. The package is illustrated on a text analysis data set, where the observations are text documents (papers), the response is the category of paper (either applied or theoretical statistics), the X information is the name of the journal in which the paper resides, and the graph is a co-citation network, with each vertex an observation and each edge the number of times that the two papers cite a common paper. An application involving classification of protein location using a protein interaction graph and an application involving classification on a manifold with part of the feature data converted to a graph are also presented.
11315813	WOS:000292850600004	793UY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Pointwise confidence intervals for the covariate-adjusted survivor function in the Cox model	Journal	Article	2011	1	1	English	STATA JOURNAL	64-81	18	1	WOS:000292850600004	A graphical representation of the pointwise confidence intervals allows a researcher to easily assess the precision of estimators. In the absence of covariates, the official command sts graph can be used to plot these intervals for the survivor function or the cumulative hazard function; however, in the presence of covariates, sts graph is insufficient. The user-written command survci can be used to plot the pointwise intervals for the survivor function after the Cox model. In this article, I describe the current and new features of survci. The new features include pointwise confidence intervals for the cumulative hazard function and the support of stratified Cox models, as well as factor variables; available as of Stata 11. I describe the methods used in calculating pointwise confidence intervals in the Cox model for both the covariate-adjusted survivor function and the covariate-adjusted cumulative hazard function. I also demonstrate the syntax of survci using Stata's example cancer dataset, cancer dta.
11322837	WOS:000296298000007	838NA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	GMM estimation of the covariance structure of longitudinal data on earnings	Journal	Article	2011	1	1	English	STATA JOURNAL	439-459	21	1	WOS:000296298000007	In this article, we discuss generalized method of moments estimation of the covariance structure of longitudinal data on earnings, and we introduce and illustrate a Stata program that facilitates the implementation of the generalized method of moments approach in this context. The program, gmmcovearn, estimates a variety of models that encompass those most commonly used by labor economists. These include models where the permanent component of earnings follows a random growth or random walk process and where the transitory component can follow either an AR(1) or an ARMA(1,1) process. In addition, time-factor loadings and cohort-factor loadings may be incorporated in the transitory and permanent components.
11340476	WOS:000288205600001	732RC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Nested Archimedean Copulas Meet R: The nacopula Package	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000288205600001	The package nacopula provides procedures for constructing nested Archimedean copulas in any dimensions and with any kind of nesting structure, generating vectors of random variates from the constructed objects, computing function values and probabilities of falling into hypercubes, as well as evaluation of characteristics such as Kendall's tau and the tail-dependence coefficients. As by-products, algorithms for various distributions, including exponentially tilted stable and Sibuya distributions, are implemented. Detailed examples are given.
11368355	WOS:000292850600001	793UY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A procedure to tabulate and plot results after flexible modeling of a quantitative covariate	Journal	Article	2011	1	1	English	STATA JOURNAL	1-29	29	1	WOS:000292850600001	The use of flexible models for the relationship between a quantitative covariate and the response variable can be limited by the difficulty in interpreting the regression coefficients. in this article, we present a new postestimation command, xblc, that facilitates tabular and graphical presentation of these relationships. Cubic splines are given special emphasis. We illustrate the command through several worked examples using data from a large study of Swedish men on the relation between physical activity and the occurrence of lower urinary tract symptoms.
11405142	WOS:000208590100006	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Differential Evolution with DEoptim An Application to Non-Convex Portfolio Optimization	Journal	Article	2011	6	1	English	R JOURNAL	27-34	8	1	WOS:000208590100006	The R package DEoptim implements the Differential Evolution algorithm. This algorithm is an evolutionary technique similar to classic genetic algorithms that is useful for the solution of global optimization problems. In this note we provide an introduction to the package and demonstrate its utility for financial applications by solving a non-convex portfolio optimization problem.
11422019	WOS:000296228100001	837UA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Special Volume on Magnetic Resonance Imaging in R	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-6	6	1	WOS:000296228100001	The special volume on "Magnetic Resonance Imaging in R" features articles and packages related to a variety of imaging modalities: functional MRI, diffusion-weighted MRI, dynamic contrast-enhanced MRI, dynamic susceptibility-contrast MRI and structural MRI. The papers describe the methodology, software implementation and provide comprehensive examples and data.
11424393	WOS:000285981800001	703MK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	mstate: An R Package for the Analysis of Competing Risks and Multi-State Models	Journal	Article	2011	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000285981800001	Multi-state models are a very useful tool to answer a wide range of questions in survival analysis that cannot, or only in a more complicated way, be answered by classical models. They are suitable for both biomedical and other applications in which time-to-event variables are analyzed. However, they are still not frequently applied. So far, an important reason for this has been the lack of available software. To overcome this problem, we have developed the mstate package in R for the analysis of multi-state models. The package covers all steps of the analysis of multi-state models, from model building and data preparation to estimation and graphical representation of the results. It can be applied to non-and semi-parametric (Cox) models. The package is also suitable for competing risks models, as they are a special category of multi-state models. This article offers guidelines for the actual use of the software by means of an elaborate multi-state analysis of data describing post-transplant events of patients with blood cancer. The data have been provided by the EBMT (the European Group for Blood and Marrow Transplantation). Special attention will be paid to the modeling of different covariate effects (the same for all transitions or transition-specific) and different baseline hazard assumptions (different for all transitions or equal for some).
11449953	WOS:000296298000005	838NA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A closer examination of three small-sample approximations to the multiple-imputation degrees of freedom	Journal	Article	2011	1	1	English	STATA JOURNAL	403-419	17	1	WOS:000296298000005	Incomplete data is a common complication in applied research. In this study, we use simulation to compare two approaches to the multiple imputation of a continuous predictor: multiple imputation through chained equations and multivariate normal imputation. This study extends earlier work by being the first to 1) compare the small-sample approximations to the multiple-imputation degrees of freedom proposed by Barnard and Rubin (1999, Biometrika 86: 948-955); Lipsitz, Parzen, and Zhao (2002, Journal of Statistical Computation and Simulation 72: 309-318); and Reiter (2007,Biometrika 94: 502-508) and 2) ask if the sampling distribution of the t statistics is in fact a Student's t distribution with the specified degrees of freedom. In addition to varying the imputation method, we varied the number of imputations (m = 5,10,20, 100) that were averaged over 500,000 replications to obtain the combined estimates and standard errors for a linear model that regressed the log price of a home on its age (years) and size (square feet) in a sample of 25 observations. Six age values were randomly set equal to missing for each replication. As assessed by the absolute percentage and relative percentage bias, the two approaches performed similarly. The absolute bias of the regression coefficients for age and size was roughly -0.1% across the levels of m for both approaches; the absolute bias for the constant was 0.6% for the chained-equations approach and 1.0% for the multivariate normal model. The absolute biases of the standard errors for age, size, and the constant were 0.2%, 0.3%, and 1.2%, respectively. In general, the relative percentage bias was slightly smaller for the chained-equations approach. Graphical and numerical inspection of the empirical sampling distributions for the three t statistics suggested that the area from the shoulder to the tail was reasonably well approximated by a t distribution and that the small-sample approximations to the multiple-imputation degrees of freedom proposed by Barnard and Rubin and by Reiter performed satisfactorily.
11454873	WOS:000298032800001	861PX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multiple Imputation Using SAS Software	Journal	Article	2011	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000298032800001	Multiple imputation provides a useful strategy for dealing with data sets that have missing values. Instead of filling in a single value for each missing value, a multiple imputation procedure replaces each value to impute. These multiply imputed data sets are then analyzed by using standard procedures for complete data and combining the results from these analyses. No matter which complete-data analysis is used, the process of combining results of parameter estimates and their associated standard errors from different imputed data sets is essentially the same. This process results in valid statistical inferences that properly reflect the uncertainty due to missing values. This paper reviews methods for analyzing missing data and applications of multiple imputation techniques. This paper presents the SAS/STAT MI and MIANALYZE procedures, which perform inference by multiple imputation under numerous settings. PROC MI implements popular methods for creating imputations under monotone nonmonotone (arbitary) patterns of missing data, and PROC MIANALYZE analyzes results from multiply imputed data sets.
11470683	WOS:000292098300001	783QF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Synth: An R Package for Synthetic Control Methods in Comparative Case Studies	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000292098300001	The R package Synth implements synthetic control methods for comparative case studies designed to estimate the causal effects of policy interventions and other events of interest (Abadie and Gardeazabal 2003; Abadie, Diamond, and Hainmueller 2010). These techniques are particularly well-suited to investigate events occurring at an aggregate level (i.e., countries, cities, regions, etc.) and affecting a relatively small number of units. Benefits and features of the Synth package are illustrated using data from Abadie and Gardeazabal (2003), which examined the economic impact of the terrorist conflict in the Basque Country.
11474889	WOS:000293390000001	800TL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	LS2W: Implementing the Locally Stationary 2D Wavelet Process Approach in R	Journal	Article	2011	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000293390000001	Locally stationary process representations have recently been proposed and applied to both time series and image analysis applications. This article describes an implementation of the locally stationary two-dimensional wavelet process approach in R. This package permits construction of estimates of spatially localized spectra and localized autocovariance which can be used to characterize structure within images.
11522982	WOS:000296298000004	838NA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	The impact of different sources of body mass index assessment on smoking onset: An application of multiple-source information models	Journal	Article	2011	1	1	English	STATA JOURNAL	386-402	17	1	WOS:000296298000004	Multiple-source data are often collected to provide better information of some underlying construct that is difficult to measure or likely to be missing. In this article, we describe regression-based methods for analyzing multiple-source data in Stata. We use data from the BROMS Cohort Study, a cohort of Swedish adolescents who collected data on body mass index that was self-reported and that was measured by nurses. We draw together into a single frame of reference both source reports and relate these to smoking onset. This unified method has two advantages over traditional approaches: 1) the relative predictiveness of each source can be assessed and 2) all subjects contribute to the analysis. The methods are applicable to other areas of epidemiology where multiple-source reports are used.
11573631	WOS:000292097200001	783PU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multivariate and Propensity Score Matching Software with Automated Balance Optimization: The Matching Package for R	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-52	52	1	WOS:000292097200001	Matching is an R package which provides functions for multivariate and propensity score matching and for finding optimal covariate balance based on a genetic search algorithm. A variety of univariate and multivariate metrics to determine if balance actually has been obtained are provided. The underlying matching algorithm is written in C++, makes extensive use of system BLAS and scales efficiently with dataset size. The genetic algorithm which finds optimal balance is parallelized and can make use of multiple CPUs or a cluster of computers. A large number of options are provided which control exactly how the matching is conducted and how balance is evaluated
11585766	WOS:000208590200005	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Watch Your Spelling!	Journal	Article	2011	12	1	English	R JOURNAL	22-28	7	1	WOS:000208590200005	We discuss the facilities in base R for spell checking via Aspell, Hunspell or Ispell, which are useful in particular for conveniently checking the spelling of natural language texts in package Rd files and vignettes. Spell checking performance is illustrated using the Rd files in package stats. This example clearly indicates the need for a domain-specific statistical dictionary. We analyze the results of spell checking all Rd files in all CRAN packages and show how these can be employed for building such a dictionary.
11595450	WOS:000292850600002	793UY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Nonparametric item response theory using Stata	Journal	Article	2011	1	1	English	STATA JOURNAL	30-51	22	1	WOS:000292850600002	Item response theory is a set of models and methods allowing for the analysis of binary or ordinal variables (items) that are influenced by a latent variable or latent trait-that is, a variable that cannot be measured directly. The theory was originally developed in educational assessment but has many other applications in clinical research, ecology, psychiatry, and economics. The Mokken scales have been described by Mokken (1971, A Theory and Procedure of Scale Analysis [De Gruyter]). They are composed of items that satisfy the three fundamental assumptions of item response theory: unidimensionality, monotonicity, and local independence. They can be considered nonparametric models in item response theory. Traces of the items and Loevinger's H coefficients are particularly useful indexes for checking whether a set of items constitutes a Mokken scale. However, these indexes are not available in general statistical packages. We introduce Stata commands to compute them. We also describe the options available and provide examples of output.
11617781	WOS:000292096500001	783PN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An Introduction to the Special Volume on "Political Methodology"	Journal	Editorial Material	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-5	5	1	WOS:000292096500001	This special volume of the Journal of Statistical Software on political methodology includes 14 papers, with wide-ranging software contributions of political scientists to their own field, and more generally to statistical data analysis in the the social sciences and beyond. Special emphasis is given to software that is written in or can cooperate with the R system for statistical computing.
11633947	WOS:000299494600007	881NY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Causal mediation analysis	Journal	Article	2011	1	1	English	STATA JOURNAL	605-619	15	1	WOS:000299494600007	Estimating the mechanisms that connect explanatory variables with the explained variable, also known as "mediation analysis," is central to a variety of social-science fields, especially psychology, and increasingly to fields like epidemiology,. Recent work on the statistical methodology behind mediation analysis points to limitations in earlier methods. We implement in Stata computational approaches based on recent developments in the statistical methodology of mediation analysis. In particular, we provide functions for the correct calculation of causal mediation effects using several different types of parametric models, as well as the calculation of sensitivity analyses for violations to the key identifying assumption required for interpreting mediation results causally.
11687893	WOS:000288206100001	732RG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Computing the Two-Sided Kolmogorov-Smirov Distribution	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000288206100001	We propose an algorithm to compute the cumulative distribution function of the two-sided Kolmogorov-Smirnov test statistic D(n) and its complementary distribution in a fast and reliable way. Different approximations are used in different regions of (n,x). Java and C programs are available.
11698949	WOS:000208590100005	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The digitize Package: Extracting Numerical Data from Scatterplots	Journal	Article	2011	6	1	English	R JOURNAL	25-26	2	1	WOS:000208590100005	I present the small R package digitize, designed to extract data from scatterplots with a simple method and suited to small datasets. I present an application of this method to the extraction of data from a graph whose source is not available.
11706972	WOS:000296718600001	844AE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	mritc: A Package for MRI Tissue Classification	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000296718600001	This paper presents an R package for magnetic resonance imaging (MRI) tissue classification. The methods include using normal mixture models, hidden markov normal mixture models, and a higher resolution hidden Markov normal mixture model fitted by various optimization algorithms and by a Bayesian Markov chain Monte Carlo (MCMC) methods. Functions to obtain initial values of parameters of normal mixture models and spatial parameters are provided. Supported input formats are ANALYZE, NIfTI, and a raw byte format. the function slices3d in misc3d is used for visualizing data and results. Various performance evaluation indices are provided to evaluate classification results. To improve performance, table lookup methods are used in several places, and vectorized computation taking advantage of conditional independence properties are used. Some computations are performed by C code, and OpenMP is used to parallelize key loops in the C code.
11737113	WOS:000208590200009	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	GrapheR: a Multiplatform GUI for Drawing Customizable Graphs in R	Journal	Article	2011	12	1	English	R JOURNAL	45-53	9	1	WOS:000208590200009	This article presents GrapheR, a Graphical User Interface allowing the user to draw customizable and high-quality graphs without knowing any R commands. Six kinds of graph are available: histograms, box-and-whisker plots, bar plots, pie charts, curves and scatter plots. The complete process is described with the examples of a bar plot and a scatter plot illustrating the legendary puzzle of African and European swallows' migrations.
11782847	WOS:000294275400003	812FB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Poisson: Some convergence issues	Journal	Article	2011	1	1	English	STATA JOURNAL	207-212	6	1	WOS:000294275400003	In this article, we identify and illustrate some shortcomings of the poisson command in Stata. Specifically, we point out that the command fails to check for the existence of the estimates, and we show that it is very sensitive to numerical problems. While these are serious problems that may prevent users from obtaining estimates or may even produce spurious and misleading results, we show that the informed user often has simple workarounds available for addressing these problems.
11791384	WOS:000208590100008	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Cryptographic Boolean Functions with R	Journal	Article	2011	6	1	English	R JOURNAL	44-47	4	1	WOS:000208590100008	Anew package called boolfun is available for R users. The package provides tools to handle Boolean functions, in particular for cryptographic purposes. This document guides the user through some (code) examples and gives a feel of what can be done with the package.
11808356	WOS:000290528200001	763BO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A Bayesian Analysis of Unobserved Component Models Using Ox	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000290528200001	This article details a Bayesian analysis of the Nile river flow data, using a similar state space model as other articles in this volume. For this data set, Metropolis-Hastings and Gibbs sampling algorithms are implemented in the programming language Ox. These Markov chain Monte Carlo methods only provide output conditioned upon the full data set. For filtered output, conditioning only on past observations, the particle filter is introduced. The sampling methods are flexible, and this advantage is used to extend the model to incorporate a stochastic volatility process. The volatility changes both in the Nile data and also in daily S&P 500 return data are investigated. The posterior density of parameters and states is found to provide information on which elements of the model are easily identifiable, and which elements are estimated with less precision.
11824465	WOS:000288206200001	732RH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The Estimation of Item Response Models with the lmer Function from the lme4 Package in R	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000288206200001	In this paper we elaborate on the potential of the lmer function from the l m e 4 package in R for item response (IRT) modeling. In line with the package, an IRT framework is described based on generalized linear mixed modeling. The aspects of the framework refer to (a) the kind of covariates - their mode (person, item, person-by-item), and their being external vs. internal to responses, and (b) the kind of effects the covariates have - fixed vs. random, and if random, the mode across which the effects are random (persons, items). Based on this framework, three broad categories of models are described: Item covariate models, person covariate models, and person-by-item covariate models, and within each category three types of more specific models are discussed. The models in question are explained and the associated lmer code is given. Examples of models are the linear logistic test model with an error term, differential item functioning models, and local item dependency models. Because the l m e 4 package is for univariate generalized linear mixed models, neither the two-parameter, and three-parameter models, nor the item response models for polytomous response data, can be estimated with the lmer function.
11827684	WOS:000208590100003	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Content-Based Social Network Analysis of Mailing Lists	Journal	Article	2011	6	1	English	R JOURNAL	11-18	8	1	WOS:000208590100003	Social Network Analysis (SNA) provides tools to examine relationships between people. Text Mining (TM) allows capturing the text they produce in Web 2.0 applications, for example, however it neglects their social structure. This paper applies an approach to combine the two methods named "content-based SNA". Using the R mailing lists, R-help and R-devel, we show how this combination can be used to describe people's interests and to find out if authors who have similar interests actually communicate. We find that the expected positive relationship between sharing interests and communicating gets stronger as the centrality scores of authors in the communication networks increase.
11830399	WOS:000290526700001	763BA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	State Space Methods in Ox/SsfPack	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000290526700001	The use of state space models and their inference is illustrated using the package SsfPack for Ox. After a rather long introduction that explains the use of SsfPack and many of its functions, four case-studies illustrate the practical implementation of the software to real world problems through short sample programs. The first case consists in the analysis of the well-known (at least to time series analysis experts) Nile data with a local level model. The other case-studies deal with ARIMA and RegARIMA models applied to the (also well-known) Airline time series, structural time series models applied to the Italian industrial production index and stochastic volatility models applied to the FTSE100 index. In all applications inference on the model (hyper-) parameters is carried out by maximum likelihood, but in one case (stochastic volatility) also an MCMC-based approach is illustrated. Cubic splines are covered in a very short example as well.
11834351	WOS:000292097100001	783PT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	IndElec: A Software for Analyzing Party Systems and Electoral Systems	Journal	Software Review	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000292097100001	IndElec is a software addressed to compute a wide range of indices from electoral data, which are intended to analyze both party systems and electoral systems in political studies. Further, IndElec can calculate such indices from electoral data at several levels of aggregation, even when the acronyms of some political parties change across districts. As the amount of information provided by IndElec may be considerable, this software also aids the user in the analysis of electoral data through three capabilities. First, IndElec automatically elaborates preliminary descriptive statistical reports of computed indices. Second, IndElec saves the computed information into text files in data matrix format, which can be directly loaded by any statistical software to facilitate more sophisticated statistical studies. Third, IndElec provides results in several file formats (text, CSV, HTML, R) to facilitate their visualization and management by using a wide range of application softwares (word processors, spreadsheets, web browsers, etc.). Finally, a graphical user interface is provided for IndElec to manage calculation processes, but no visualization facility is available in this environment. In fact, both the inputs and outputs for IndElec are arranged in files with the aforementioned formats.
11851049	WOS:000208590100010	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Probabilistic Weather Forecasting in R	Journal	Article	2011	6	1	English	R JOURNAL	55-63	9	1	WOS:000208590100010	This article describes two R packages for probabilistic weather forecasting, ensembleBMA, which offers ensemble postprocessing via Bayesian model averaging (BMA), and Prob-ForecastGOP, which implements the geostatistical output perturbation (GOP) method. BMA forecasting models use mixture distributions, in which each component corresponds to an ensemble member, and the form of the component distribution depends on the weather parameter (temperature, quantitative precipitation or wind speed). The model parameters are estimated from training data. The GOP technique uses geostatistical methods to produce probabilistic forecasts of entire weather fields for temperature or pressure, based on a single numerical forecast on a spatial grid. Both packages include functions for evaluating predictive performance, in addition to model fitting and forecasting.
11852715	WOS:000289228100001	746FI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The Split-Apply-Combine Strategy for Data Analysis	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000289228100001	Many data analysis problems involve the application of a split-apply-combine strategy, where you break up a big problem into manageable pieces, operate on each piece independently and then put all the pieces back together. This insight gives rise to a new R package that allows you to smoothly apply this strategy, without having to worry about the type of structure in which your data is stored. The paper includes two case studies showing how these insights make it easier to work with batting records for veteran baseball players and a large 3d array of spatio-temporal ozone measurements.
11882114	WOS:000292098400001	783QG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Scaling Roll Call Votes with wnominate in R	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000292098400001	This paper presents a software package designed to estimate Poole and Rosenthal W-NOMINATE scores in R. The package uses a logistic regression model to analyze political choice data, usually (though not exclusively) from a legislative setting. In contrast to other scaling methods, W-NOMINATE explicitly assumes probabilistic voting based on a spatial utility function, where the parameters of the utility function and the spatial coordinates of the legislators and the votes can all be estimated on the basis of observed voting behavior. Building on software written by Poole in Fortran, the new wnominate package in R facilitates easier data input and manipulation, generates boots trapped standard errors, and includes a new suite of graphics functions to display the results. We demonstrate the functionality of this package by conducting a natural experiment using roll calls - an experiment which is greatly simplified by the data manipulation capabilities of the wnominate package in R.
11883823	WOS:000292097600001	783PY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MCMCpack: Markov Chain Monte Carlo in R	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000292097600001	We introduce MCMCpack, an R package that contains functions to perform Bayesian inference using posterior simulation for a number of statistical models. In addition to code that can be used to fit commonly used models, MCMCpack also contains some useful utility functions, including some additional density functions and pseudo-random number generators for statistical distributions, a general purpose Metropolis sampling algorithm, and tools for visualization.
11896112	WOS:000294231700001	811RV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Unifying Optimization Algorithms to Aid Software System Users: optimx for R	Journal	Article	2011	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000294231700001	R users can often solve optimization tasks easily using the tools in the optim function in the stats package provided by default on R installations. However, there are many other optimization and nonlinear modelling tools in R or in easily installed add-on packages. These present users with a bewildering array of choices. optimx is a wrapper to consolidate many of these choices for the optimization of functions that are mostly smooth with parameters at most bounds-constrained. We attempt to provide some diagnostic information about the function, its scaling and parameter bounds, and the solution characteristics. optimx runs a battery of methods on a given problem, thus facilitating comparative studies of optimization algorithms for the problem at hand. optimx can also be a useful pedagogical tool for demonstrating the strengths and pitfalls of different classes of optimization approaches including Newton, gradient, and derivative-free methods.
11898740	WOS:000289228400001	746FL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Dates and Times Made Easy with lubridate	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000289228400001	This paper presents the lubridate package for R, which facilitates working with dates and times. Date-times create various technical problems for the data analyst. The paper highlights these problems and offers practical advice on how to solve them using lubridate. The paper also introduces a conceptual framework for arithmetic with date-times in R.
11905013	WOS:000299494600008	881NY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Importing financial data	Journal	Article	2011	1	1	English	STATA JOURNAL	620-626	7	1	WOS:000299494600008	In this article, we describe two commands-fetchyahooquotes and fetchyahookeystats-that import historical financial data and key current financial statistics from Yahoo! Finance.
11948506	WOS:000299494600004	881NY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	mvdcmp: Multivariate decomposition for nonlinear response models	Journal	Article	2011	1	1	English	STATA JOURNAL	556-576	21	1	WOS:000299494600004	We developed a general-purpose multivariate decomposition command for nonlinear response models that incorporates several recent contributions to overcome various problems dealing with path dependence and identification. This work extends existing Stata packages in important ways by including additional models and allowing for weights and model offsets.
11953396	WOS:000289832600001	754DL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SPECIES: An R Package for Species Richness Estimation	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000289832600001	We introduce an R package SPECIES for species richness or diversity estimation. This package provides simple R functions to compute point and confidence interval estimates of species number from a few nonparametric and semi-parametric methods. For the methods based on nonparametric maximum likelihood estimation, the R functions are wrappers for Fortran codes for better efficiency. All functions in this package are illustrated using real data sets.
11987768	WOS:000289932900001	755LO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Simultaneous Analysis in S-PLUS: The Simult An Package	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000289932900001	In this paper we describe the SimultAn package dedicated to simultaneous analysis. Simultaneous analysis is a new factorial methodology developed for the joint treatment of a set of several data tables. Since the first stage of simultaneous analysis requires a correspondence analysis of each table the package comprises two parts, one for correspondence analysis and one for simultaneous analysis. The package can be used to perform classical correspondence analysis of frequency/contingency tables as well as to perform simultaneous analysis of a set of frequency/contingency tables. In this package, functions for computation, summaries and graphical visualization in two dimensions are provided, including options to display partial rows and supplementary points.
11993494	WOS:000287814900001	727QA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	sparr: Analyzing Spatial Relative Risk Using Fixed and Adaptive Kernel Density Estimation in R	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000287814900001	The estimation of kernel-smoothed relative risk functions is a useful approach to examining the spatial variation of disease risk. Though there exist several options for performing kernel density estimation in statistical software packages, there have been very few contributions to date that have focused on estimation of a relative risk function perse. Use of a variable or adaptive smoothing parameter for estimation of the individual densities has been shown to provide additional benefits in estimating relative risk and specific computational tools for this approach are essentially absent. Furthermore, little attention has been given to providing methods in available software for any kind of subsequent analysis with respect to an estimated risk function. To facilitate analyses in the field, the R package sparr is introduced, providing the ability to construct both fixed and adaptive kernel-smoothed densities and risk functions, identify statistically significant fluctuations in an estimated risk function through the use of asymptotic tolerance contours, and visualize these objects in flexible and attractive ways.
11995374	WOS:000296298000002	838NA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Nonparametric bounds for the causal effect in a binary instrumental-variable model	Journal	Article	2011	1	1	English	STATA JOURNAL	345-367	23	1	WOS:000296298000002	Instrumental variables can be used to make inferences about causal effects in the presence of unmeasured confounding. For a model in which the instrument, intermediate/treatment, and outcome variables are all binary, Balke and Pearl (1997, Journal of the American Statistical Association 92: 1172-1.176) derived nonparametric bounds for the intervention probabilities and the average causal effect. We have implemented these bounds in two commands: bpbounds and bpboundsi. We have also implemented several extensions to these bounds. One of these extensions applies when the instrument and outcome are measured in one sample and the instrument and intermediate are measured in another sample. We have also implemented the bounds for an instrument with three categories, as is common in Mendelian randomization analyses in epidemiology and for the case where a monotonic effect of the instrument on the intermediate can be assumed. In each case, we calculate the instrumental-variable inequality constraints as a check for gross violations of the instrumental-variable conditions. The use of the commands is illustrated with a re-creation of the original Balke and Pearl analysis and with a Mendelian randomization analysis. We also give a simulated example to demonstrate that the instrumental-variable inequality constraints can both detect and fail to detect violations of the instrumental-variable conditions.
12012942	WOS:000290391200001	761IK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Topicmodels: An R Package for Fitting Topic Models	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000290391200001	Topic models allow the probabilistic modeling of term frequency occurrences in documents. The fitted model can be used to estimate the similarity between documents as well as between a set of specified keywords using an additional layer of latent variables which are referred to as topics. The R package topicmodels provides basic infrastructure for fitting topic models based on data structures from the text mining package tm. The package includes interfaces to two algorithms for fitting topic models: the variational expectation-maximization algorithm provided by David M. Blei and co-authors and an algorithm using Gibbs sampling by Xuan-Hieu Phan and co-authors.
12017196	WOS:000289228600001	746FN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	DPpackage: Bayesian Semi- and Nonparametric Modeling in R	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000289228600001	Data analysis sometimes requires the relaxation of parametric assumptions in order to gain modeling flexibility and robustness against mis-specification of the probability model. In the Bayesian context, this is accomplished by placing a prior distribution on a function space, such as the space of all probability distributions or the space of all regression functions. Unfortunately, posterior distributions ranging over function spaces are highly complex and hence sampling methods play a key role. This paper provides an introduction to a simple, yet comprehensive, set of programs for the implementation of some Bayesian nonparametric and semiparametric models in R, DPpackage. Currently, DPpackage includes models for marginal and conditional density estimation, receiver operating characteristic curve analysis, interval-censored data, binary regression data, item response data, longitudinal and clustered data using generalized linear mixed models, and regression data using generalized additive models. The package also contains functions to compute pseudo-Bayes factors for model comparison and for eliciting the precision parameter of the Dirichlet process prior, and a general purpose Metropolis sampling algorithm. To maximize computational efficiency, the actual sampling for each model is carried out using compiled C, C++ or Fortran code.
12037160	WOS:000208590100011	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Analyzing an Electronic Limit Order Book	Journal	Article	2011	6	1	English	R JOURNAL	64-68	5	1	WOS:000208590100011	The orderbook package provides facilities for exploring and visualizing the data associated with an order book: the electronic collection of the outstanding limit orders for a financial instrument. This article provides an overview of the orderbook package and examples of its use.
12048020	WOS:000208805600001	V30GX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Passing in Command Line Arguments and Parallel Cluster/Multicore Batching in R with batch	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-11	11	1	WOS:000208805600001	It is often useful to rerun a command line R script with some slight change in the parameters used to run it - a new set of parameters for a simulation, a different dataset to process, etc. The R package batch provides a means to pass in multiple command line options, including vectors of values in the usual R format, easily into R. The same script can be setup to run things in parallel via different command line arguments. The R package batch also provides a means to simplify this parallel batching by allowing one to use R and an R-like syntax for arguments to spread a script across a cluster or local multicore/multiprocessor computer, with automated syntax for several popular cluster types. Finally it provides a means to aggregate the results together of multiple processes run on a cluster.
12050002	WOS:000208805800001	V30GZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Implementing Panel-Corrected Standard Errors in R: The pcse Package	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-11	11	1	WOS:000208805800001	Time-series-cross-section (TSCS) data are characterized by having repeated observations over time on some set of units, such as states or nations. TSCS data typically display both contemporaneous correlation across units and unit level heteroskedasity making inference from standard errors produced by ordinary least squares incorrect. Panel-corrected standard errors (PCSE) account for these these deviations from spherical errors and allow for better inference from linear models estimated from TSCS data. In this paper, we discuss an implementation of them in the R system for statistical computing. The key computational issue is how to handle unbalanced data.
12059131	WOS:000294837100001	819OV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	spikeSlabGAM: Bayesian Variable Selection, Model Choice and Regularization for Generalized Additive Mixed Models in R	Journal	Article	2011	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	24	1	WOS:000294837100001	The R package spikeSlabGAM implements Bayesian variable selection, model choice, and regularized estimation in (geo-) additive mixed models for Gaussian, binomial, and Poisson responses. Its purpose is to (1) choose an appropriate subset of potential covariates and their interactions, (2) to determine whether linear or more flexible functional forms are required to model the effects of the respective covariates, and (3) to estimate their shapes. Selection and regularization of the model terms is based on a novel spike-and-slab-type prior on coefficient groups associated with parametric and semi-parametric effects.
12077346	WOS:000292097900001	783QB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	poLCA: An R Package for Polytomous Variable Latent Class Analysis	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000292097900001	poLCA is a software package for the estimation of latent class and latent class regression models for polytomous outcome variables, implemented in the R statistical computing environment. Both models can be called using a single simple command line. The basic latent class model is a finite mixture model in which the component distributions are assumed to be multi-way cross-classification tables with all variables mutually independent. The latent class regression model further enables the researcher to estimate the effects of covariates on predicting latent class membership. poLCA uses expectation-maximization and Newton-Raphson algorithms to find maximum likelihood estimates of the model parameters.
12113062	WOS:000294275400009	812FB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Generating random samples from user-defined distributions	Journal	Article	2011	1	1	English	STATA JOURNAL	299-304	6	1	WOS:000294275400009	Generating random samples in Stata is very straightforward if the distribution drawn from is uniform or normal. With any other distribution, an inverse method can be used; but even in this case, the user is limited to the built-in functions. For any other distribution functions, their inverse must be derived analytically or numerical methods must be used if analytical derivation of the inverse function is tedious or impossible. In this article, I introduce a command that generates a random sample from any user-specified distribution function using numeric methods that make this command very generic.
12122311	WOS:000290528100001	763BN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	State Space Modeling Using SAS	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000290528100001	This article provides a brief introduction to the state space modeling capabilities in SAS, a well-known statistical software system. SAS provides state space modeling in a few different settings. SAS/ETS, the econometric and time series analysis module of the SAS system, contains many procedures that use state space models to analyze univariate and multivariate time series data. In addition, SAS/IML, an interactive matrix language in the SAS system, provides Kalman filtering and smoothing routines for stationary and nonstationary state space models. SAS/IML also provides support for linear algebra and nonlinear function optimization, which makes it a convenient environment for general-purpose state space modeling.
12124827	WOS:000285981900001	703ML	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multi-State Models for Panel Data: The msm Package for R	Journal	Article	2011	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000285981900001	Panel data are observations of a continuous-time process at arbitrary times, for example, visits to a hospital to diagnose disease status. Multi-state models for such data are generally based on the Markov assumption. This article reviews the range of Markov models and their extensions which can be fitted to panel-observed data, and their implementation in the msm package for R. Transition intensities may vary between individuals, or with piecewise-constant time-dependent covariates, giving an inhomogeneous Markov model. Hidden Markov models can be used for multi-state processes which are misclassified or observed only through a noisy marker. The package is intended to be straightforward to use, flexible and comprehensively documented. Worked examples are given of the use of msm to model chronic disease progression and screening. Assessment of model fit, and potential future developments of the software, are also discussed.
12134455	WOS:000296718800001	844AG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	TractoR: Magnetic Resonance Imaging and Tractography with R	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000296718800001	Statistical techniques play a major role in contemporary methods for analyzing magnetic resonance imaging (MRI) data. In addition to the central role that classical statistical methods play in research using MRI, statistical modeling and machine learning techniques are key to many modern data analysis pipelines. Applications for these techniques cover a broad spectrum of research, including many preclinical and clinical studies, and in some cases these methods are working their way into widespread routine use. In this manuscript we describe a software tool called TractoR (for "Tractography with R"), a collection of packages for the R language and environment, along with additional infrastructure for straightforwardly performing common image processing tasks. TractoR provides general purpose functions for reading, writing and manipulating MR images, as well as more specific code for fitting signal models to diffusion MRI data and performing tractography, a technique for visualizing neural connectivity.
12152349	WOS:000290526800001	763BB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	State Space Models in R	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000290526800001	We give an overview of some of the software tools available in R, either as built-in functions or contributed packages, for the analysis of state space models. Several illustrative examples are included, covering constant and time-varying models for both univariate and multivariate time series. Maximum likelihood and Bayesian methods to obtain parameter estimates are considered.
12155179	WOS:000294232100001	811RZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	osDesign: An R Package for the Analysis, Evaluation, and Design of Two-Phase and Case-Control Studies	Journal	Article	2011	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000294232100001	The two-phase design has recently received attention in the statistical literature as an extension to the traditional case-control study for settings where a predictor of interest is rare or subject to missclassification. Despite a thorough methodological treatment and the potential for substantial efficiency gains, the two-phase design has not been widely adopted. This may be due, in part, to a lack of general-purpose, readily-available soft-ware. The osDesign package for R provides a suite of functions for analyzing data from a two-phase and/or case-control design, as well as evaluating operating characteristics, including bias, efficiency and power. The evaluation is simulation-based, permitting flexible application of the package to a broad range of scientific settings. Using lung cancer mortality data from Ohio, the package is illustrated with a detailed case-study in which two statistical goals are considered: (i) the evaluation of small-sample operating characteristics for two-phase and case-control designs and (ii) the planning and design of a future two-phase study.
12177998	WOS:000292097500001	783PX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MatchIt: Nonparametric Preprocessing for Parametric Causal Inference	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	28	1	WOS:000292097500001	MatchIt implements the suggestions of Ho, Imai, King, and Stuart (2007) for improving parametric statistical models by preprocessing data with nonparametric matching methods. MatchIt implements a wide range of sophisticated matching methods, making it possible to greatly reduce the dependence of causal inferences on hard-to-justify, but commonly made, statistical modeling assumptions. The software also easily fits into existing research practices since, after preprocessing data with MatchIt, researchers can use whatever parametric model they would have used without MatchIt, but produce inferences with substantially more robustness and less sensitivity to modeling assumptions. MatchIt is an R program, and also works seamlessly with Zelig.
12202844	WOS:000208590200008	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Using the Google Visualisation API with R	Journal	Article	2011	12	1	English	R JOURNAL	40-44	5	1	WOS:000208590200008	The googleVis package provides an interface between R and the Google Visualisation API to create interactive charts which can be embedded into web pages. The best known of these charts is probably the Motion Chart, popularised by Hans Rosling in his TED talks. With the googleVis package users can easily create web pages with interactive charts based on R data frames and display them either via the local R HTTP help server or within their own sites.
12204263	WOS:000294275400010	812FB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Compared with ...	Journal	Article	2011	1	1	English	STATA JOURNAL	305-314	10	1	WOS:000294275400010	Many problems in data management center on relating values to values in other observations, either within a dataset as a. whole or within groups such as panels. This column reviews some basic Stata techniques helpful for such tasks, including the use of subscripts, summarize, by:, sum(), cond(), and egen. Several techniques exploit the fact that logical expressions yield 1 when true and 0 when false. Dividing by zero to yield missings is revealed as a surprisingly valuable device.
12211367	WOS:000294275400004	812FB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation of ordered response models with sample selection	Journal	Article	2011	1	1	English	STATA JOURNAL	213-239	27	1	WOS:000294275400004	We introduce two new Stata commands for the estimation of an ordered response model with sample selection. The opsel command uses a standard maximum-likelihood approach to lit a parametric specification of the model where errors are assumed to follow a bivariate Gaussian distribution. The snpopsel command uses the semi-nonparametric approach of Gallant and Nychka. (1987,Econornetrica. 55: 363-390) to fit a semiparametric specification of the model where the bivariate density function of the errors is approximated by a Hermite polynomial expansion. The snpopsel command extends the set of Stata routines for semi-nonparametric estimation of discrete response models. Compared to the other semi-nonparametric estimators, our routine is relatively faster because it is programmed in Mata. In addition, we provide new postestimation routines to compute linear predictions, predicted probabilities, and marginal effects. These improvements are also extended to the set of semi-nonparametric Stata commands originally written by Stewart (2004, Stata journal 4: 27-39) and De Luca (2008, Stata journal 8: 190-220). An illustration of the new opsel and snpopsel commands is provided through an empirical application on self-reported health with selectivity due to sample attrition.
12225269	WOS:000299494600005	881NY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Dynamic simulations of autoregressive relationships	Journal	Article	2011	1	1	English	STATA JOURNAL	577-588	12	1	WOS:000299494600005	This postestimation technique produces dynamic simulations of autoregressive ordinary least-squares models.
12226030	WOS:000296719400001	844AM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	dti: Beyond the Gaussian Model in Diffusion-Weighted Imaging	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000296719400001	Diffusion weighted imaging (DWI) is a magnetic resonance (MR) based method to investigate water diffusion in tissue like the human brain. Inference focuses on integral properties of the tissue microstructure. The acquired data are usually modeled using the diffusion tensor model, a three-dimensional Gaussian model for the diffusion process. Since the homogeneity assumption behind this model is not valid in large portion of the brain voxel more sophisticated approaches have been developed. This paper describes the R package dti. The package offers capabilities for the analysis of diffusion weighted MR experiments. Here, we focus on recent extensions of the package, for example models for high angular resolution diffusion weighted imaging (HARDI) data, including Q-ball imaging and tensor mixture models, and fiber tracking. We provide a detailed description of the package structure and functionality. Examples are used to guide the reader through a typical analysis using the package. Data sets and R scripts used are available as electronic supplements.
12237412	WOS:000298032500001	861PU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	mice: Multivariate Imputation by Chained Equations in R	Journal	Article	2011	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-67	67	1	WOS:000298032500001	The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice 2.9, which extends the functionality of mice 1.0 in several ways. In mice 2.9, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice 2.9 adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice 2.9 can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.
12244129	WOS:000285981100001	703MD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Special Issue about Competing Risks and Multi-State Models	Journal	Article	2011	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-4	4	1	WOS:000285981100001	There is a clear growing interest, at least in the statistical literature, in competing risks and multi-state models. With the rising interest in competing risks and multi-state models a number of software packages have been developed for the analysis of such models. The present special issue of the Journal of Statistical Software introduces a selection of R packages devoted to competing risks and multi-state models. This introduction to the special issue contains some background and highlights the contents of the contributions.
12249865	WOS:000294275400005	812FB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Simon's minimax and optimal and Jung's admissible two-stage designs with or without curtailment	Journal	Article	2011	1	1	English	STATA JOURNAL	240-254	15	1	WOS:000294275400005	This article describes a new Stata command called simontwostage, which calculates the critical values and sample sizes for two-stage designs for phase H oncology trials. Options are provided to determine the minimax and optimal designs proposed by Simon (1989, Controlled Clinical Trials 10: 1-10) and admissible designs described by Jung et al. (2004, Statistics in Medicine 23: 561-569). Furthermore, nonstochastic and stochastic curtailment rules can be implemented in both stages of the trial, and the properties of the curtailed designs can be examined.
12257952	WOS:000208590100002	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	testthat: Get Started with Testing	Journal	Article	2011	6	1	English	R JOURNAL	5-10	6	1	WOS:000208590100002	Software testing is important, but many of us don't do it because it is frustrating and boring. testthat is a new testing framework for R that is easy learn and use, and integrates with your existing workflow. This paper shows how, with illustrations from existing packages.
12259288	WOS:000208590200007	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Nonparametric Goodness-of-Fit Tests for Discrete Null Distributions	Journal	Article	2011	12	1	English	R JOURNAL	34-39	6	1	WOS:000208590200007	Methodology extending nonparametric goodness-of-fit tests to discrete null distributions has existed for several decades. However, modern statistical software has generally failed to provide this methodology to users. We offer a revision of R's ks. test () function and a new cvm. test () function that fill this need in the R language for two of the most popular nonparametric goodness-of-fit tests. This paper describes these contributions and provides examples of their usage. Particular attention is given to various numerical issues that arise in their implementation.
12262040	WOS:000294232000001	811RY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Unmarked: An R Package for Fitting Hierarchical Models of Wildlife Occurrence and Abundance	Journal	Article	2011	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000294232000001	Ecological research uses data collection techniques that are prone to substantial and unique types of measurement error to address scientific questions about species abundance and distribution. These data collection schemes include a number of survey methods in which unmarked individuals are counted, or determined to be present, at spatially-referenced sites. Examples include site occupancy sampling, repeated counts, distance sampling, removal sampling, and double observer sampling. To appropriately analyze these data, hierarchical models have been developed to separately model explanatory variables of both a latent abundance or occurrence process and a conditional detection process. Because these models have a straightforward interpretation paralleling mechanisms under which the data arose, they have recently gained immense popularity. The common hierarchical structure of these models is well-suited for a unified modeling interface. The R package unmarked provides such a unified modeling framework, including tools for data exploration, model fitting, model criticism, post-hoc analysis, and model comparison.
12279900	WOS:000288204000001	732QO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000288204000001	We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of l(1) and l(2) penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.
12299702	WOS:000285981700001	703MJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Using Lexis Objects for Multi-State Models in R	Journal	Article	2011	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000285981700001	The Lexis class in the R package Epi provides tools for creation, manipulation and display of data from multi-state models. Transitions between states are described by rates (intensities); Lexis objects represent this kind of data and provide tools to show states and transitions annotated by relevant summary numbers. Data can be transformed to a form that allows modelling of several transition rates with common parameters.
12323397	WOS:000208590200003	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	glm2: Fitting Generalized Linear Models with Convergence Problems	Journal	Article	2011	12	1	English	R JOURNAL	12-15	4	1	WOS:000208590200003	The R function glm uses step-halving to deal with certain types of convergence problems when using iteratively reweighted least squares to fit a generalized linear model. This works well in some circumstances but non-convergence remains a possibility, particularly with a nonstandard link function. In some cases this is because step-halving is never invoked, despite a lack of convergence. In other cases step-halving is invoked but is unable to induce convergence. One remedy is to impose a stricter form of step-halving than is currently available in glm, so that the deviance is forced to decrease in every iteration. This has been implemented in the glm2 function available in the glm2 package. Aside from a modified computational algorithm, glm2 operates in exactly the same way as glm and provides improved convergence properties. These improvements are illustrated here with an identity link Poisson model, but are also relevant in other contexts.
12332771	WOS:000289228900001	746FQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	R Package wgaim: QTL Analysis in Bi-Parental Populations Using Linear Mixed Models	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000289228900001	The wgaim (whole genome average interval mapping) package developed in the R system for statistical computing (R Development Core Team 2011) builds on linear mixed modelling techniques by incorporating a whole genome approach to detecting significant quantitative trait loci (QTL) in bi-parental populations. Much of the sophistication is inherited through the well established linear mixed modelling package ASReml-R (Butler et al. 2009). As wgaim uses an extension of interval mapping to incorporate the whole genome into the analysis, functions are provided which allow conversion of genetic data objects created with the qtl package of Broman and Wu (2010) available in R. Results of QTL analyses are available using summary and print methods as well as diagnostic summaries of the selection method. In addition, the package features a flexible linkage map plotting function that can be easily manipulated to provide an aesthetic viewable genetic map. As a visual summary, QTL obtained from one or more models can also be added to the linkage map.
12346076	WOS:000208806000001	V30HB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian Analysis for Food-Safety Risk Assessment: Evaluation of Dose-Response Functions within WinBUGS	Journal	Article	2011	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000208806000001	Bayesian methods are becoming increasingly popular in the field of food-safety risk assessment. Risk assessment models often require the integration of a dose-response function over the distribution of all possible doses of a pathogen ingested with a specific food. This requires the evaluation of an integral for every sample for a Markov chain Monte Carlo analysis of a model. While many statistical software packages have functions that allow for the evaluation of the integral, this functionality is lacking in WinBUGS. A probabilistic model, that incorporates a novel numerical integration technique, is presented to facilitate the use of WinBUGS for food-safety risk assessments. The numerical integration technique is described in the context of a typical food-saftey risk assesment, some theoretical results are given, and a snippet of WinBUGS code is provided.
12357366	WOS:000296298000003	838NA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Impact of interventions on discrete outcomes: Maximum likelihood estimation of the binary choice models with binary endogenous regressors	Journal	Article	2011	1	1	English	STATA JOURNAL	368-385	18	1	WOS:000296298000003	In this article, we describe the switch_probit command, which implements the maximum likelihood method to fit the model of the binary choice with binary endogenous regressors.
12365958	WOS:000288206300001	732RI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Hierarchical Generalized Linear Models: The R Package HGLMMM	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000288206300001	The R package HGLMMM has been developed to fit generalized linear models with random effects using the h-likelihood approach. The response variable is allowed to follow a binomial, Poisson, Gaussian or gamma distribution. The distribution of random effects can be specified as Gaussian, gamma, inverse-gamma or beta. Complex structures as multi-membership design or multilevel designs can be handled. Further, dispersion parameters of random components and the residual dispersion (overdispersion) can be modeled as a function of covariates. Overdispersion parameter can be fixed or estimated. Fixed effects in the mean structure can be estimated using extended likelihood or a first order Laplace approximation to the marginal likelihood. Dispersion parameters are estimated using first order adjusted profile likelihood.
12366861	WOS:000296228900001	837UH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Working with the DICOM and NIfTI Data Standards in R	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000296228900001	Two packages, oro.dicom and oro.nifti, are provided for the interaction with and manipulation of medical imaging data that conform to the DICOM standard or ANALYZE/NIfTI formats. DICOM data, from a single file or directory tree, may be uploaded into R using basic data structures: a data frame for the header information and a matrix for the image data. A list structure is used to organize multiple DICOM files. The S4 class framework is used to develop basic ANALYZE and NIfTI classes, where NIfTI extensions may be used to extend the fixed-byte NIfTI header. One example of this, that has been implemented, is an XML-based "audit trail" tracking the history of operations applied to a data set. The conversion from DICOM to ANALYZE/NIfTI is straightforward using the capabilities of both packages. The S4 classes have been developed to provide a user-friendly interface to the ANALYZE/NIfTI data formats; allowing easy data input, data output, image processing and visualization.
12404492	WOS:000298032700001	861PW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	REALCOM-IMPUTE Software for Multilevel Multiple Imputation with Mixed Response Types	Journal	Article	2011	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000298032700001	Multiple imputation is becoming increasingly established as the leading practical approach to modelling partially observed data, under the assumption that the data are missing at random. However, many medical and social datasets are multilevel, and this structures should be reflected not only in the model of interest, but also in the imputation model. In particular, the imputation model should reflect the differences between level 1 variables and level 2 variables (which are constant across level 1 units). This led us to develop the REALCOM-IMPUTE software, which we describe in this article. This software performs multilevel multiple imputation, and handles ordinal and unordered categorical data appropriately . Is is freely available on-line, and many be used either as a standalone package, or in conjuction with the multilevel software MLwiN or Stata.
12404649	WOS:000289228700001	746FO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	DEoptim: An R Package for Global Optimization by Differential Evolution	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000289228700001	This article describes the R package DEoptim, which implements the differential evolution algorithm for global optimization of a real-valued function of a real-valued parameter vector. The implementation of differential evolution in DEoptim interfaces with C code for efficiency. The utility of the package is illustrated by case studies in fitting a Parratt model for X-ray reflectometry data and a Markov-switching generalized autoregressive conditional heteroskedasticity model for the returns of the Swiss Market Index.
12407239	WOS:000296228300001	837UC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Markov Chain Monte Carlo Random Effects Modeling in Magnetic Resonance Image Processing Using the BRugs Interface to WinBUGS	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000296228300001	A common feature of many magnetic resonance image (MRI) data processing methods is the voxel-by-voxel (a voxel is a volume element) manner in which the processing is performed. In general, however, MRI data are expected to exhibit some level of spatial correlation, rendering an independent-voxels treatment inefficient in its use of the data. Bayesian random effect models are expected to be more efficient owing to their information-borrowing behaviour. To illustrate the Bayesian random effects approach, this paper outlines a Markov chain Monte Carlo (MCMC) analysis of a perfusion MRI dataset, implemented in R using the BRugs package. BRugs provides an interface to WinBUGS and its GeoBUGS add-on. WinBUGS is a widely used programme for performing MCMC analyses, with a focus on Bayesian random effect models. A simultaneous modeling of both voxels (restricted to a region of interest) and multiple subjects is demonstrated. Despite the low signal-to-noise ratio in the magnetic resonance signal intensity data, useful model signal intensity profiles are obtained. The merits of random effects modeling are discussed in comparison with the alternative approaches based on region-of-interest averaging and repeated independent voxels analysis. This paper focus on perfusion MRI for the purpose of illustration, the main proposition being that random effects modeling is expected to be beneficial in many other MRI applications in which the signal-to-noise ratio is a limiting factor.
12413893	WOS:000292850600005	793UY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation of hurdle models for overdispersed count data	Journal	Article	2011	1	1	English	STATA JOURNAL	82-94	13	1	WOS:000292850600005	Hurdle models based on the zero-truncated Poisson-lognormal distribution are rarely used in applied work, although they incorporate some advantages compared with their negative binomial alternatives. I present a command that enables Stata users to estimate Poisson-lognormal hurdle models. I use adaptive Gauss-Hermite quadrature to approximate the likelihood function, arid I evaluate the performance of the estimator in Monte Carlo experiments. The model is applied to the number of doctor visits in a sample of the U.S. Medical Expenditure Panel Survey.
12420081	WOS:000293389900001	800TK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	B2Z: An R Package for Bayesian Two-Zone Models	Journal	Article	2011	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000293389900001	A primary issue in industrial hygiene is the estimation of a worker's exposure to chemical, physical and biological agents. Mathematical modeling is increasingly being used as a method for assessing occupational exposures. However, predicting exposure in real settings is constrained by lack of quantitative knowledge of exposure determinants. Recently, Zhang, Banerjee, Yang, Lungu, and Ramachandran (2009) proposed Bayesian hierarchical models for estimating parameters and exposure concentrations for the two-zone differential equation models and for predicting concentrations in a zone near and far away from the source of contamination. Bayesian estimation, however, can often require substantial amounts of user-defined code and tuning. In this paper, we introduce a statistical software package, B2Z, built upon the R statistical computing platform that implements a Bayesian model for estimating model parameters and exposure concentrations in two-zone models. We discuss the algorithms behind our package and illustrate its use with simulated and real data examples.
12453678	WOS:000208590100007	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	rworldmap: A New R package for Mapping Global Data	Journal	Article	2011	6	1	English	R JOURNAL	35-43	9	1	WOS:000208590100007	rworldmap is a relatively new package available on CRAN for the mapping and visualisation of global data. The vision is to make the display of global data easier, to facilitate understanding and communication. The initial focus is on data referenced by country or grid due to the frequency of use of such data in global assessments. Tools to link data referenced by country (either name or code) to a map, and then to display the map are provided as are functions to map global gridded data. Country and gridded functions accept the same arguments to specify the nature of categories and colour and how legends are formatted. This package builds on the functionality of existing packages, particularly sp, maptools and fields. Example code is provided to produce maps, to link with the packages classInt, RColorBrewer and ncdf, and to plot examples of publicly available country and gridded data.
12455173	WOS:000294275400008	812FB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating adjusted risk ratios for matched and unmatched data: An update	Journal	Article	2011	1	1	English	STATA JOURNAL	290-298	9	1	WOS:000294275400008	The Stata 11 margins command makes it easier to estimate adjusted risk ratios, and the new robust variance option for xtpoisson, fe provides correct confidence intervals for adjusted risk ratios from matched-cohort data.
12456426	WOS:000208590200010	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	rainbow: An R Package for Visualizing Functional Time Series	Journal	Article	2011	12	1	English	R JOURNAL	54-59	6	1	WOS:000208590200010	Recent advances in computer technology have tremendously increased the use of functional data, whose graphical representation can be infinite-dimensional curves, images or shapes. This article describes four methods for visualizing functional time series using an R add-on package. These methods are demonstrated using age-specific Australian fertility data from 1921 to 2006 and monthly sea surface temperatures from January 1950 to December 2006.
12480033	WOS:000293390600001	800TQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multivariate L-1 Methods: The Package MNM	Journal	Article	2011	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000293390600001	In the paper we present an R package MNM dedicated to multivariate data analysis based on the L-1 norm. The analysis proceeds very much as does a traditional multivariate analysis. The regular L-2 norm is just replaced by different L-1 norms, observation vectors are replaced by their (standardized and centered) spatial signs, spatial ranks, and spatial signed-ranks, and so on. The procedures are fairly efficient and robust, and no moment assumptions are needed for asymptotic approximations. The background theory is briefly explained in the multivariate linear regression model case, and the use of the package is illustrated with several examples using the R package MNM.
12507711	WOS:000208805900001	V30HA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	fmixed: A SAS Macro for Smoothing-Spline-Based Functional Mixed Effects Models	Journal	Article	2011	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000208805900001	In this article we implement the smoothing-spline-based functional mixed effects models (Guo 2002) by a SAS macro by exploiting the connection between mixed effects models and smoothing splines. The macro can handle flexible design matrices and is easy to use. Input parameters and output results are described and explained. A numeric example and a real data example are used for illustration.
12519186	WOS:000299494600003	881NY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Treatment interactions with nonexperimental data in Stata	Journal	Article	2011	1	1	English	STATA JOURNAL	545-555	11	1	WOS:000299494600003	Treatment effects may vary with the observed. characteristics of the treated, often with important implications. In the context of experimental data, a growing literature deals with the problem of specifying treatment interaction terms that most effectively capture this variation. Sonic results of this literature are now implemented in Stata. With nonexperimental (observational) data., and in particular when selection into treatment depends on unmeasured factors, treatment effects can be estimated using Stata's treatreg command. Though not originally designed for this purpose, treatreg can be used to consistently estimate treatment interaction parameters. With interactions, however, adjustments are required to generate predicted values and estimate the average treatment effect. In this article, we introduce commands that perform this adjustment for multiplicative interactions, and we show the required adjustment for more complicated interactions.
12544157	WOS:000294232200001	811SA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Introduction to SamplerCompare	Journal	Article	2011	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-10	10	1	WOS:000294232200001	SamplerCompare is an R package for comparing the performance of Markov chain Monte Carlo (MCMC) samplers. It samples from a collection of distributions with a collection of MCMC methods over a range of tuning parameters. Then, using log density evaluations per uncorrelated observation as a figure of merit, it generates a grid of plots showing the results of the simulation. It comes with a collection of predefined distributions and samplers and provides R and C interfaces for defining additional ones. It also provides the means to import simulation data generated by external systems. This document provides background on the package and demonstrates the basics of running simulations, visualizing results, and defining distributions and samplers in R.
12561059	WOS:000294275400006	812FB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multivariate random-effects meta-regression: Updates to mvmeta	Journal	Article	2011	1	1	English	STATA JOURNAL	255-270	16	1	WOS:000294275400006	An extension of mvmeta, my program for multivariate random-effects meta-analysis, is described. The extension handles meta-regression. Estimation methods available are restricted maximum likelihood, maximum likelihood, method of moments, and fixed effects. The program also allows a wider range of models (Riley's overall correlation model and structured between-studies covariance); better estimation (using Mata for speed and correctly allowing for missing data); and new postestimation facilities (I-squared; standard errors and confidence intervals for between-studies standard deviations and correlations, and identification of the best intervention). The program is illustrated using a multipletreatments meta-analysis.
12561516	WOS:000294837000001	819OU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ipw: An R Package for Inverse Probability Weighting	Journal	Article	2011	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000294837000001	We describe the R package ipw for estimating inverse probability weights. We show how to use the package to fit marginal structural models through inverse probability weighting, to estimate causal effects. Our package can be used with data from a point treatment situation as well as with a time-varying exposure and time-varying confounders. It can be used with binomial, categorical, ordinal and continuous exposure variable.
12561803	WOS:000292098000001	783QC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Genetic Optimization Using Derivatives: The rgenoud Package for R	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000292098000001	genoud is an R function that combines evolutionary algorithm methods with a derivative-based (quasi-Newton) method to solve difficult optimization problems. genoud may also be used for optimization problems for which derivatives do not exist. genoud solves problems that are nonlinear or perhaps even discontinuous in the parameters of the function to be optimized. When the function to be optimized (for example, a log-likelihood) is nonlinear in the model's parameters, the function will generally not be globally concave and may have irregularities such as saddlepoints or discontinuities. Optimization methods that rely on derivatives of the objective function may be unable to find any optimum at all. Multiple local optima may exist, so that there is no guarantee that a derivative-based method will converge to the global optimum. On the other hand, algorithms that do not use derivative information (such as pure genetic algorithms) are for many problems needlessly poor at local hill climbing. Most statistical problems are regular in a neighborhood of the solution. Therefore, for some portion of the search space, derivative information is useful. The function supports parallel processing on multiple CPUs on a single machine or a cluster of computers.
12570912	WOS:000290527100001	763BD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The State Space Models Toolbox for MATLAB	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000290527100001	State Space Models (SSM) is a MATLAB toolbox for time series analysis by state space methods. The software features fully interactive construction and combination of models, with support for univariate and multivariate models, complex time-varying (dynamic) models, non-Gaussian models, and various standard models such as ARIMA and structural time-series models. The software includes standard functions for Kalman filtering and smoothing, simulation smoothing, likelihood evaluation, parameter estimation, signal extraction and forecasting, with incorporation of exact initialization for filters and smoothers, and support for missing observations and multiple time series input with common analysis structure. The software also includes implementations of TRAMO model selection and Hillmer-Tiao decomposition for ARIMA models. The software will provide a general toolbox for time series analysis on the MATLAB platform, allowing users to take advantage of its readily available graph plotting and general matrix computation capabilities.
12576068	WOS:000288204700001	732QV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An Open Source Approach for Modern Teaching Methods: The Interactive TGUI System	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000288204700001	In order to facilitate teaching complex topics in an interactive way, the authors developed a computer-assisted teaching system, a graphical user interface named TGUI (Teaching Graphical User Interface). TGUI was introduced at the beginning of 2009 in the Austrian Journal of Statistics (Dinges and Templ 2009) as being an effective instrument to train and teach staff on mathematical and statistical topics. While the fundamental principles were retained, the current TGUI system has been undergone a complete redesign. The ultimate goal behind the reimplementation was to share the advantages of TGUI and provide teachers and people who need to hold training courses with a strong tool that can enrich their lectures with interactive features. The idea was to go a step beyond the current modular blended-learning systems (see, e.g., Da Rin 2003) or the related teaching techniques of classroom-voting (see, e.g., Cline 2006). In this paper the authors have attempted to exemplify basic idea and concept of TGUI by means of statistics seminars held at Statistics Austria. The powerful open source software R (R Development Core Team 2010a) is the backend for TGUI which can therefore be used to process even complex statistical contents. However, with specifically created contents the interactive TGUI system can be used to support a wide range of courses and topics. The open source R packages TGUI Core and TGUI Teaching are freely available from the Comprehensive R Archive Network at http://CRAN.R-project.org/
12594186	WOS:000290528000001	763BM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	State Space Methods in gretl	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000290528000001	gretl is a general-purpose econometric package, whose most important characteristic is being free software. This ensures that its source code is freely available under the general public license (GPL) and, like most GPL software, that it can be used free of charge. As of version 1.8.1 (released in May 2009), it offers a mechanism for handling linear state space models in a reasonably general and efficient way. This article illustrates its main features with two examples.
12599034	WOS:000285981600001	703MI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Lexis: An R Class for Epidemiological Studies with Long-Term Follow-Up	Journal	Article	2011	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	12	1	WOS:000285981600001	The Lexis class in the R package Epi provides an object-based framework for managing follow-up time on multiple time scales, which is an important feature of prospective epidemiological studies with long duration. Follow-up time may be split either into fixed time bands, or on individual event times and the split data may be used in Poisson regression models that account for the evolution of disease risk on multiple time scales. The summary and plot methods for Lexis objects allow inspection of the follow-up times.
12615031	WOS:000289228300001	746FK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	tourr: An R Package for Exploring Multivariate Data with Projections	Journal	Article	2011	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000289228300001	This paper describes an R package which produces tours of multivariate data. The package includes functions for creating different types of tours, including grand, guided, and little tours, which project multivariate data (p-D) down to 1, 2, 3, or, more generally, d (<= p) dimensions. The projected data can be rendered as densities or histograms, scatterplots, anaglyphs, glyphs, scatterplot matrices, parallel coordinate plots, time series or images, and viewed using an R graphics device, passed to GGobi, or saved to disk. A tour path can be stored for visualisation or replay. With this package it is possible to quickly experiment with different, and new, approaches to tours of data.This paper contains animations that can be viewed using the Adobe Acrobat PDF viewer.
12617929	WOS:000208805700001	V30GY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fast Computation of Trimmed Means	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	6	1	WOS:000208805700001	We present two methods of calculating trimmed means without sorting the data in O(n) time. The existing method implemented in major statistical packages relies on sorting, which takes O (n log n) time. The proposed algorithm is based on the quickselect algorithm for calculating order statistics with O(n) expected running time. It is an order of magnitude faster than the existing method for large data sets.
12639452	WOS:000290527400001	763BG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fitting State Space Models with EViews	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000290527400001	This paper demonstrates how state space models can be fitted in EViews. We first briefly introduce EViews as an econometric software package. Next we fit a local level model to the Nile data. We then show how a multivariate "latent risk" model can be developed, making use of the EViews programming environment. We conclude by summarizing the possibilities and limitations of the software package when it comes to state space modeling.
12647192	WOS:000292097000001	783PS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	eco: R Package for Ecological Inference in 2 x 2 Tables	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	23	1	WOS:000292097000001	eco is a publicly available R package that implements the Bayesian and likelihood methods proposed in Imai, Lu, and Strauss (2008b) for ecological inference in 2 x 2 tables as well as the method of bounds introduced by (Duncan and Davis 1953). The package fits both parametric and nonparametric models using either the Expectation-Maximization algorithms (for likelihood models) or the Markov chain Monte Carlo algorithms (for Bayesian models). For all models, the individual-level data can be directly incorporated into the estimation whenever such data are available. Along with in-sample and out-of-sample predictions, the package also provides a functionality which allows one to quantify the effect of data aggregation on parameter estimation and hypothesis testing under the parametric likelihood models. This paper illustrates the usage of eco with several real data examples that are also part of the package.
12662383	WOS:000292096700001	783PP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Nineteen Ways of Looking at Statistical Software	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-12	12	1	WOS:000292096700001	We identify principles and practices for writing and publishing statistical software with maximum bene fit to the scholarly community.
12665455	WOS:000290526900001	763BC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	State Space Modeling Using SsfPack in S plus FinMetrics 3.0	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000290526900001	This paper presents two illustrations of state space modeling in S-PLUS using the Ssf-Pack 3.0 routines implemented in S+FinMetrics 3.0. The state space modeling functions in S+FinMetrics/SsfPack are extremely flexible and powerful and can be used for a wide variety of linear Gaussian state space models and for some non-linear and non-Gaussian state space models.
12680059	WOS:000293390700001	800TR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multilevel Fixed and Sequential Acceptance Sampling: The R Package MFSAS	Journal	Article	2011	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000293390700001	Multilevel acceptance sampling for attributes is used to decide whether a lot from an incoming shipment or outgoing production is accepted or rejected when the product has multiple levels of product quality or multiple types of (mutually exclusive) possible defects. This paper describes a package which provides the tools to create, evaluate, plot, and display the acceptance sampling plans for such lots for both fixed and sequential sampling. The functions for calculating cumulative probabilities for several common multivariate distributions (which are needed in the package) are provided as well.
12697012	WOS:000208590100012	V27CA	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Giving a useR! Talk	Journal	Editorial Material	2011	6	1	English	R JOURNAL	69-71	3	1	WOS:000208590100012	Giving a UseR! talk at the the international R user conference is a balancing act in which you have to try to impart some new ideas, provide sufficient background and keep the audience interested, all in a very short period of time.
12716877	WOS:000296719900001	844AQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	arf3DS4: An Integrated Framework for Localization and Connectivity Analysis of fMRI Data	Journal	Article	2011	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-33	33	1	WOS:000296719900001	In standard fMRI analysis all voxels are tested in a massive univariate approach, that is, each voxel is tested independently. This requires stringent corrections for multiple comparisons to control the number of false positive tests (i.e., marking voxels as active while they are actually not). As a result, fMRI analyses may suffer from low power to detect activation, especially in studies with high levels of noise in the data, for example developmental or single-subject studies. Activated region fitting (ARF) yields a solution by modeling fMRI data by multiple Gaussian shaped regions. ARF only requires a small number of parameters and therefore has increased power to detect activation. If required, the estimated regions can be directly used as regions of interest in a functional connectivity analysis. ARF is implemented in the R package arf3DS4. In this paper ARF and its implementation are described and illustrated with an example.
12720359	WOS:000292096800001	783PQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	anchors: Software for Anchoring Vignette Data	Journal	Article	2011	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000292096800001	When respondents use the ordinal response categories of standard survey questions in different ways, the validity of analyses based on the resulting data can be biased. Anchoring vignettes is a survey design technique intended to correct for some of these problems. The anchors package in R includes methods for evaluating and choosing anchoring vignettes, and for analyzing the resulting data.
12724727	WOS:000293391200001	800TW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Distributed Lag Linear and Non-Linear Models in R: The Package dlnm	Journal	Article	2011	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000293391200001	Distributed lag non-linear models (DLNMs) represent a modeling framework to flexibly describe associations showing potentially non-linear and delayed effects in time series data. This methodology rests on the definition of a crossbasis, a bi-dimensional functional space expressed by the combination of two sets of basis functions, which specify the relationships in the dimensions of predictor and lags, respectively. This framework is implemented in the R package dlnm, which provides functions to perform the broad range of models within the DLNM family and then to help interpret the results, with an emphasis on graphical representation. This paper offers an overview of the capabilities of the package, describing the conceptual and practical steps to specify and interpret DLNMs with an example of application to real data.
12755336	WOS:000298032900001	861PY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Amelia II: A Program for Missing Data	Journal	Article	2011	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-47	47	1	WOS:000298032900001	II is a complete R package for multiple imputation of missing data. The package implements a new expectation-maximization with bootstrapping algorithm that works faster, with larger numbers of variables, and is far easier to use, than various Markov chain Monte Carlo approaches, but gives essentially the same answers. The program also improves imputation models by allowing researchers to put Bayesian priors on individual cell values, thereby including a great deal of potentially valuable and extensive information. It also includes features to accurately impute cross-sectional datasets, individual time series, or sets of time series for different cross-sections. A full set of graphical diagnostics are also available. The program is easy to use, and the simplicity of the algorithm makes it far more robust; both a simple command line and extensive graphical user interface are included
12765577	WOS:000208590200011	V27CB	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Portable C plus plus for R Packages	Journal	Article	2011	12	1	English	R JOURNAL	60-63	4	1	WOS:000208590200011	Package checking errors are more common on Solaris than Linux. In many cases, these errors are due to non-portable C++ code. This article reviews some commonly recurring problems in C++ code found in R packages and suggests solutions.
12776465	WOS:000299494600002	881NY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Bayesian model averaging and weighted-average least squares: Equivariance, stability, and numerical issues	Journal	Article	2011	1	1	English	STATA JOURNAL	518-544	27	1	WOS:000299494600002	In this article, we describe the estimation of linear regression models with uncertainty about the choice of the explanatory variables. We introduce the Stata commands bma and wals, which implement, respectively, the exact Bayesian model-averaging estimator and the weighted-average least-squares estimator developed by Magnus, Powell, and Prufer (2010, Journal of Econometrics 154: 139-153). Unlike standard pretest estimators that are based on some preliminary diagnostic test, these model-averaging estimators provide a coherent way of making inference on the regression parameters of interest by taking into account the uncertainty due to both the estimation and the model selection steps. Special emphasis is given to several practical issues that users are likely to face in applied work: equivariance to certain transformations of the explanatory variables, stability, accuracy, computing speed, and out-of-memory problems. Performances of our bma and wals commands are illustrated using simulated data and empirical applications from the literature on model-averaging estimation.
12778527	WOS:000290513100001	762WP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Factor Analysis for Multiple Testing (FAMT): An R Package for Large-Scale Significance Testing under Dependence	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000290513100001	The R package FAMT (factor analysis for multiple testing) provides a powerful method for large-scale significance testing under dependence. It is especially designed to select differentially expressed genes in microarray data when the correlation structure among gene expressions is strong. Indeed, this method reduces the negative impact of dependence on the multiple testing procedures by modeling the common information shared by all the variables using a factor analysis structure. New test statistics for general linear contrasts are deduced, taking advantage of the common factor structure to reduce correlation and consequently the variance of error rates. Thus, the FAMT method shows improvements with respect to most of the usual methods regarding the non discovery rate and the control of the false discovery rate (FDR). The steps of this procedure, each of them corresponding to R functions, are illustrated in this paper by two microarray data analyses. We first present how to import the gene expression data, the covariates and gene annotations. The second step includes the choice of the optimal number of factors, the factor model fitting, and provides a list of selected gene according to a preset FDR control level. Finally, diagnostic plots are provided to help the user interpret the factors using a vailable external information on either genes or arrays.
12805300	WOS:000290526400001	763AX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The STAMP Software for State Space Models	Journal	Article	2011	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000290526400001	This paper reviews the use of STAMP (Structural Time Series Analyser, Modeler and Predictor) for modeling time series data using state-space methods with unobserved components. STAMP is a commercial, GUI-based program that runs on Windows, Linux and Macintosh computers as part of the larger OxMetrics System. STAMP can estimate a wide-variety of both univariate and multivariate state-space models, provides a wide array of diagnostics, and has a batch mode capability. The use of STAMP is illustrated for the Nile river data which is analyzed throughout this issue, as well as by modeling a variety of oceanographic and climate related data sets. The analyses of the oceanographic and climate data illustrate the breadth of models available in STAMP, and that state-space methods produce results that provide new insights into important scientific problems.
12807984	WOS:000288205900001	732RF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Deconvolution Estimation in Measurement Error Models: The R Package decon	Journal	Article	2011	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000288205900001	Data from many scientific areas often come with measurement error. Density or distribution function estimation from contaminated data and nonparametric regression with errors-in-variables are two important topics in measurement error models. In this paper, we present a new software package decon for R, which contains a collection of functions that use the deconvolution kernel methods to deal with the measurement error problems. The functions allow the errors to be either homoscedastic or heteroscedastic. To make the deconvolution estimators computationally more efficient in R, we adapt the fast Fourier transform algorithm for density estimation with error-free data to the deconvolution kernel estimation. We discuss the practical selection of the smoothing parameter in deconvolution methods and illustrate the use of the package through both simulated and real examples.
48796682	WOS:000232928700001	979FV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Monkeying with the goodness-of-fit test	Journal	Article	2005	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	4	1	WOS:000232928700001	The familiar Sigma(OBS - EXP)(2)/EXP goodness-of-fit measure is commonly used to test whether an observed sequence came from the realization of n independent identically distributed (iid) discrete random variables. It can be quite effective for testing for identical distribution, but is not suited for assessing independence, as it pays no attention to the order in which output values are received. This note reviews a way to adjust or tamper, that is, monkey-with the classical test to make it test for independence as well as identical distribution - in short, to test for both the i's in iid, using monkey tests similar to those in the Diehard Battery of Tests of Randomness (Marsaglia 1995).
48826189	WOS:000232807200001	977MF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	BClass: A Bayesian approach based on mixture models for clustering and classification of heterogeneous biological data	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000232807200001	Based on mixture models, we present a Bayesian method ( called BClass) to classify biological entities ( e. g. genes) when variables of quite heterogeneous nature are analyzed. Various statistical distributions are used to model the continuous/ categorical data commonly produced by genetic experiments and large-scale genomic projects. We calculate the posterior probability of each entry to belong to each element ( group) in the mixture. In this way, an original set of heterogeneous variables is transformed into a set of purely homogeneous characteristics represented by the probabilities of each entry to belong to the groups. The number of groups in the analysis is controlled dynamically by rendering the groups as ' alive' and 'dormant' depending upon the number of entities classified within them. Using standard Metropolis-Hastings and Gibbs sampling algorithms, we constructed a sampler to approximate posterior moments and grouping probabilities. Since this method does not require the definition of similarity measures, it is especially suitable for data mining and knowledge discovery in biological databases. We applied BClass to classify genes in RegulonDB, a database specialized in information about the transcriptional regulation of gene expression in the bacterium Escherichia coli. The classification obtained is consistent with current knowledge and allowed prediction of missing values for a number of genes. BClass is object-oriented and fully programmed in Lisp-Stat. The output grouping probabilities are analyzed and interpreted using graphical ( dynamically linked plots) and query-based approaches. We discuss the advantages of using Lisp-Stat as a programming language as well as the problems we faced when the data volume increased exponentially due to the ever-growing number of genomic projects.
48834185	WOS:000232891300001	978RS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An algorithm for clustered data generalized additive modelling with S-PLUS	Journal	Article	2005	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	24	1	WOS:000232891300001	We present a set of functions in S-PLUS to implement the clustered data generalized additive marginal modelling (CDGAM) strategy proposed by Berhane and Tibshirani (1998). A variety of working correlation structures are supported, and the regression basis may include components from the family of smoothing splines. Keywords: generalized estimating equations, clustered data analysis.
48836033	WOS:000232806400001	977LX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SimReg: A software includinge some new developments in multiple comparison and simultaneous confidence bands for linear regression models	Journal	Article	2005	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000232806400001	The problem of simultaneous inference and multiple comparison for comparing means of k(>= 3) populations has been long studied in the statistics literature and is widely available in statistics literature. However to-date, the problem of multiple comparison of regression models has not found its way to the software. It is only recently that the computational aspects of this problem have been resolved in a general setting. SimReg employs this new methodology and provides users with software for multiple regression of several regression models. The comparisons can be among any set of pairs, and moreover any number of predictors can be included in the model. More importantly predictors can be constrained to their natural boundaries, if known. Computational methods for the problem of simultaneous confidence bands when predictors are constrained to intervals has also recently been addressed. SimReg utilizes this recent development to offer simultaneous confidence bands for regression models with any number of predictor variables. Again, the predictors can be constrained to their natural boundaries which results in narrower bands, as compared to the case where no restriction is imposed. A by-product of these confidence bands is a new method for comparing two regression surfaces, that is more informative than the usual partial F test.
48842966	WOS:000232806700001	977MA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bioassay analysis using R	Journal	Article	2005	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000232806700001	We describe an add-on package for the language and environment R which allows simultaneous fitting of several non-linear regression models. The focus is on analysis of dose response curves, but the functionality is applicable to arbitrary non-linear regression models. Features of the package is illustrated in examples.
48862574	WOS:000232928800001	979FW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian analysis for penalized spline regression using WinBUGS	Journal	Article	2005	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	24	1	WOS:000232928800001	Penalized splines can be viewed as BLUPs in a mixed model framework, which allows the use of mixed model software for smoothing. Thus, software originally developed for Bayesian analysis of mixed models can be used for penalized spline regression. Bayesian inference for nonparametric models enjoys the flexibility of nonparametric models and the exact inference provided by the Bayesian inferential machinery. This paper provides a simple, yet comprehensive, set of programs for the implementation of nonparametric Bayesian analysis in WinBUGS. Good mixing properties of the MCMC chains are obtained by using low-rank thin-plate splines, while simulation times per iteration are reduced employing WinBUGS specific computational tricks.
48871341	WOS:000232891000001	978RP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	zoo: S3 infrastructure for regular and irregular time series	Journal	Article	2005	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	27	1	WOS:000232891000001	zoo is an R package providing an S3 class with methods for indexed totally ordered observations, such as discrete irregular time series. Its key design goals are independence of a particular index/time/date class and consistency with base R and the "ts" class for regular time series. This paper describes how these are achieved within zoo and provides several illustrations of the available methods for "zoo" objects which include plotting, merging and binding, several mathematical operations, extracting and replacing data and index, coercion and NA handling. A subclass "zooreg" embeds regular time series into the "zoo" framework and thus bridges the gap between regular and irregular time series classes in R.
48887577	WOS:000232890300001	978RI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	pinktoe: Semi-automatic traversal of trees	Journal	Article	2005	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	11	1	WOS:000232890300001	Tree based methods in S or R are extremely useful and popular. For simple trees and memorable variables it is easy to predict the outcome for a new case using only a standard decision tree diagram. However, for large trees or trees where the variable description is complex the decision tree diagram is often not enough. This article describes pinktoe: an R package containing two tools to assist with the semi-automatic traversal of trees. The PT tool creates a widget for each node to be visited in the tree that is needed to make a decision and permits the user to make decisions using radiobuttons. The pinktoe function generates a suite of HTML and Perl files that permit a CGI-enabled website to issue step-by-step questions to a user wishing to make a prediction using a tree.
48887785	WOS:000232890800001	978RN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	BugsXLA: Bayes for the common man	Journal	Article	2005	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	18	1	WOS:000232890800001	The absence of user-friendly software has long been a major obstacle to the routine application of Bayesian methods in business and industry. It will only be through widespread application of the Bayesian approach to real problems that issues, such as the use of prior distributions, can be practically resolved in the same way that the choice of significance levels has been in the classical approach; although most Bayesians would hope for a much more satisfactory resolution. It is only relatively recently that any general purpose Bayesian software has been available; by far the most widely used such package is WinBUGS. Although this software has been designed to enable an extremely wide variety of models to be coded relatively easily, it is unlikely that many will bother to learn the language and its nuances unless they are already highly motivated to try Bayesian methods. This paper describes a graphical user interface, programmed by the author, which facilitates the specification of a wide class of generalised linear mixed models for analysis using WinBUGS. The program, BugsXLA (v2.1), is an Excel Add-In that not only allows the user to specify a model as one would in a package such as SAS or S-PLUS, but also aids the specification of priors and control of the MCMC run itself. Inevitably, developing a program such as this forces one to think again about such issues as choice of default priors, parameterisation and assessing convergence. I have tried to adopt currently perceived good practices, but mainly share my approach so that others can apply it and, through constructive criticism, play a small part in the ultimate development of the first Bayesian software package truly useable by the average data analyst.
48902327	WOS:000231609700015	960RA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of generalized latent variable modeling by Skrondal and Rabe-Hesketh	Journal	Review	2005	1	1	English	STATA JOURNAL	130-133	4	1	WOS:000231609700015	The new book by Skrondal and Rabe-Hesketh (2004) is reviewed.
48902541	WOS:000232832000001	977VQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Some notes on the past and future of Lisp-Stat	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000232832000001	Lisp-Stat was originally developed as a framework for experimenting with dynamic graphics in statistics. To support this use, it evolved into a platform for more general statistical computing. The choice of the Lisp language as the basis of the system was in part coincidence and in part a very deliberate decision. This paper describes the background behind the choice of Lisp, as well as the advantages and disadvantages of this choice. The paper then discusses some lessons that can be drawn from experience with Lisp-Stat and with the R language to guide future development of Lisp-Stat, R, and similar systems.
48943715	WOS:000231710800007	962DI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Stata in space: Econometric analysis of spatially explicit raster data	Journal	Article	2005	1	1	English	STATA JOURNAL	224-238	15	1	WOS:000231710800007	Realizing the importance of location, economists are increasingly adopting spatial analytical and spatial econometric perspectives to study questions such as the geographical targeting of policy interventions, regional agglomeration effects, the diffusion of technologies across space, or causes and consequences of land-cover change. Explicitly accounting for location in econometric estimations can be of great benefit for researchers working at the interface of economics or environmental sciences and geography. The objective of this article is to demonstrate how spatially explicit raster data derived from standard geographical information system (GIS) software can be used within Stata. Three programs implemented as ado-files are presented. These import geographic raster data into Stata (ras2dta), draw systematic spatial samples within Stata (spatsam), and export data and estimation results in a form usable by standard GIS software (dta2ras). A numerical example is presented to estimate the determinants of forest cover with a spatially explicit logit model, calculate predicted probabilities, and map the predictions with GIS software.
48995403	WOS:000231710800001	962DI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Exploratory analysis of single nucleotide polymorphism (SNP) for quantitative traits	Journal	Article	2005	1	1	English	STATA JOURNAL	141-153	13	1	WOS:000231710800001	With the decreasing cost and the increasing ability to quickly genotype single nucleotide polymorphisms (SNP) across the human genome, large databases containing possibly hundreds of typed SNPs are becoming common in population-based studies of quantitative traits. Testing for association between individual SNPs and the quantitative trait is an important first step in the discovery of disease susceptibility SNPs. This task, however, could be time-consuming and tedious if a large number of SNPs is involved. In this article, I introduce two new commands designed to facilitate the screening and testing of multiple SNPs for possible association with quantitative traits.
49004883	WOS:000232831800001	977VO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	On abandoning XLISP-STAT	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-5	5	1	WOS:000232831800001	In 1998 the UCLA Department of Statistics, which had been one of the major users of Lisp-Stat, and one of the main producers of Lisp-Stat code, decided to switch to S/R. This paper discusses why this decision was made, and what the pros and the cons were.
49033377	WOS:000231710800006	962DI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Data inspection using biplots	Journal	Article	2005	1	1	English	STATA JOURNAL	208-223	16	1	WOS:000231710800006	Biplots display interunit distances, as well as variances and correlations of variables of large datasets. They can be used as a tool to reveal clustering, multicollinearity, and multivariate outliers, and to guide the interpretation of principal component analyses (PCA). This article describes the uses of biplots and its implementation in Stata.
49074487	WOS:000232831500001	977VL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Lisp-Stat to Java to R	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-10	10	1	WOS:000232831500001	This paper will describe my experiences in moving on from Lisp-Stat to Java to R. I was introduced to Lisp-Stat in 1989 and used it actively for teaching and research over the next 10 years. My use of Lisp-Stat culminated in a joint project with Hani Doss and I on Bayesian Sensitivity Analysis and it remains the largest piece of software I wrote using Lisp-Stat. At the time the project was completed, the only open statistical system that could deliver the goods was Lisp-Stat. In this article, I will describe how the power of Lisp, underlying statistical components and dynamic graphics were exploited in the project. When development on Lisp-Stat slowed down, Java was coming into its own as an important language and R became an open source collaborative project. Of course, I have moved on and I use R for most of my work today. I will touch upon with my experience with Java and R briefly.
49078559	WOS:000231609700011	960RA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Visualizing main effects and interactions for binary logit models	Journal	Article	2005	1	1	English	STATA JOURNAL	64-82	19	1	WOS:000231609700011	This paper considers the role of covariates when using predicted probabilities to interpret main effects and interactions in logit models. While predicted probabilities are very intuitive for interpreting main effects and interactions, the pattern of results depends on the contribution of covariates. We introduce a concept called the covariate contribution, which reflects the aggregate contribution of all of the remaining predictors (covariates) in the model and a family of tools to help visualize the relationship between predictors and the predicted probabilities across a variety of covariate contributions. We believe this strategy and the accompanying tools can help researchers who wish to use predicted probabilities as an interpretive framework for logit models acquire and present a more comprehensive interpretation of their results. These visualization tools could be extended to other models (such as binary probit, multinomial logistic, ordinal logistic models, and other nonlinear models).
49094618	WOS:000231609700013	960RA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Tabulation of multiple responses	Journal	Article	2005	1	1	English	STATA JOURNAL	92-122	31	1	WOS:000231609700013	Although multiple-response questions are quite common in survey research, Stata's official release does not provide much capability for an effective analysis of multiple-response variables. For example, in a study on drug addiction an interview question might be, "Which substances did you consume during the last four weeks?" The respondents just list all the drugs they took, if any; e.g., an answer could be "cannabis, cocaine, heroin" or "ecstasy, cannabis" or "none", etc. Usually, the responses to such questions are stored as a set of variables and, therefore, cannot be easily tabulated. I will address this issue here and present a new module to compute one- and two-way tables of multiple responses. The module supports several types of data structure, provides significance tests, and offers various options to control the computation and display of the results. In addition, tools to create graphs of multiple-response distributions are presented.
49140978	WOS:000232929200001	979GA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Introducing BACCO, an R bundle for Bayesian analysis of computer code output	Journal	Article	2005	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	21	1	WOS:000232929200001	This paper introduces the BACCO bundle of R routines for carrying out Bayesian analysis of computer code output. The bundle comprises packages emulator and calibrator, computerized implementations of the ideas of Oakley and O'Hagan ( 2002) and Kennedy and O'Hagan (2001a) respectively. The bundle is self-contained and fully documented R code, and includes a toy dataset that furnishes a working example of the functions. Package emulator carries out Bayesian emulation of computer code output; package calibrator allows the incorporation of observational data into model calibration using Bayesian techniques. The package is then applied to a dataset taken from climate science.
49164934	WOS:000231710800010	962DI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Density probability plots	Journal	Article	2005	1	1	English	STATA JOURNAL	259-273	15	1	WOS:000231710800010	Density probability plots show two guesses at the density function of a continuous variable, given a data sample. The first guess is the density function of a specified distribution (e.g., normal, exponential, gamma, etc.) with appropriate parameter values plugged in. The second guess is the same density function evaluated at quantiles corresponding to plotting positions associated with the sample's order statistics. If the specified distribution fits well, the two guesses will be close. Such plots, suggested by Jones and Daly in 1995, are explained and discussed with examples from simulated and real data. Comparisons are made with histograms, kernel density estimation, and quantile-quantile plots.
49173141	WOS:000231710800004	962DI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multiple imputation of missing values: update	Journal	Article	2005	1	1	English	STATA JOURNAL	188-201	14	1	WOS:000231710800004	This article describes a substantial update to mvis, which brings it more closely in line with the feature set of S. van Buuren and C. G. M. Oudshoorn's implementation of the MICE system in R and S-PLUS (for details, see http://www.multiple-imputation.com). To make a clear distinction from mvis, the principal program of the new Stata release is called ice. I will give details of how to use the new features and a practical illustrative example using real data. All the facilities of mvis are retained by ice. Some improvements to micombine for computing estimates from multiply imputed datasets are also described.
49198351	WOS:000232891100001	978RQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Rank-based analyses of linear models using R	Journal	Article	2005	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	26	1	WOS:000232891100001	It is well-known that Wilcoxon procedures out perform least squares procedures when the data deviate from normality and/or contain outliers. These procedures can be generalized by introducing weights; yielding so-called weighted Wilcoxon (WW) techniques. In this paper we demonstrate how WW-estimates can be calculated using an L-1 regression routine. More importantly, we present a collection of functions that can be used to implement a robust analysis of a linear model based on WW-estimates. For instance, estimation, tests of linear hypotheses, residual analyses, and diagnostics to detect differences in fits for various weighting schemes are discussed. We analyze a regression model, designed experiment, and autoregressive time series model for the sake of illustration. We have chosen to implement the suite of functions using the R statistical software package. Because R is freely available and runs on multiple platforms, WW-estimation and associated inference is now universally accessible.
49199704	WOS:000231710800005	962DI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation and testing of fixed-effect panel-data systems	Journal	Article	2005	1	1	English	STATA JOURNAL	202-207	6	1	WOS:000231710800005	This paper describes how to specify, estimate, and test multiple-equation, fixed-effect, panel-data equations in Stata. By specifying the system of equations as seemingly unrelated regressions, Stata panel-data procedures worked seamlessly for estimation and testing of individual variable coefficients, but additional routines using test were needed for testing of individual equations and differences between equations.
49254947	WOS:000231609700003	960RA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A conversation with William Gould	Journal	Editorial Material	2005	1	1	English	STATA JOURNAL	19-31	13	1	WOS:000231609700003	William Gould is President of StataCorp. He was born in Burbank, California, on January 21, 1952. He received a B.A. in economics from UCLA in 1974 and a C.Phil. in economics from UCLA in 1977, after initially majoring in physics and then engineering. He studied economics in the Ph.D. program at UCLA and was simultaneously a Research Fellow at The Rand Corporation. He did not turn in his dissertation in labor economics before becoming a Senior Research Associate at the National Bureau of Economic Research in Stanford, California, in 1977. In 1979, he become a Senior Economist at Unicon Research Corporation in Los Angeles, a company that he helped to found. He cofounded and served as Vice-President of Computing Resource Center in 1982, the company that went on to develop Stata. Bill became President of CRC in 1990 and, in 1993, CRC was renamed StataCorp.
49343615	WOS:000232662100001	975KT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bradley-Terry models in R	Journal	Article	2005	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-12	12	1	WOS:000232662100001	This paper describes the R add-on package BradleyTerry, which facilitates the specification and fitting of Bradley-Terry logit models to pair-comparison data. Included are the standard 'unstructured' Bradley-Terry model, structured versions in which the parameters are related through a linear predictor to explanatory variables, and the possibility of an order or 'home advantage' effect. Model fitting is either by maximum likelihood or by bias-reduced maximum likelihood in which the first-order asymptotic bias of parameter estimates is eliminated. Also provided are a simple and efficient approach to handling missing covariate data, and suitably-defined residuals for diagnostic checking of the linear predictor; these are new methodological contributions which will be discussed in greater detail elsewhere.
49401796	WOS:000232890700001	978RM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Calling the lp_solve linear program software from R,S-PLUS and Excel	Journal	Article	2005	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	13	1	WOS:000232890700001	We present a link that allows R, S-PLUS and Excel to call the functions in the 1p_solve system. 1p_solve is free software (licensed under the GNU Lesser GPL) that solves linear and mixed integer linear programs of moderate size (on the order of 10,000 variables and 50,000 constraints). R does not include this ability (though two add-on packages off er linear programs without integer variables), while S-PLUS users need to pay extra for the NuOPT library in order to solve these problems. Our link manages the interface between these statistical packages and 1p_solve. Excel has a built-in add-in named Solver that is capable of solving mixed integer programs, but only with fewer than 200 variables. This link allows Excel users to handle substantially larger problems at no extra cost. While our primary concern has been the Windows operating system, the package has been tested on some Unix-type systems as well.
49525208	WOS:000234226900001	997ED	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	FracSim: An R package to simulate multifractional Levy motions	Journal	Article	2005	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	19	1	WOS:000234226900001	In this article a procedure is proposed to simulate fractional fields, which are non Gaussian counterpart of the fractional Brownian motion. These fields, called real harmonizable ( multi) fractional Levy motions, allow fixing the Holder exponent at each point. FracSim is an R package developed in R and C language. Parallel computers have been also used.
49551832	WOS:000232831600001	977VM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Interactive biplot construction	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000232831600001	We analyze and discuss how a generic software to produce biplot graphs should be designed. We describe a data structure appropriate to include the biplot description and we specify the algorithm(s) to be used for several biplot types. We discuss the options the software should offer to the user in two different environments. In a highly interactive environment the user should be able to specify many graphical options and also to change them using the usual interactive tools. The resulting graph needs to be available in several formats, including high quality format for printing. In a web-based environment, the user submits a data file or listing together with some options specified either in a file or using a form. Then the graphic is sent back to the user in one of several possible formats according to the specifications. We review some of the already available software and we present an implementation based in XLISP-STAT. It can be run under Unix or Windows, and it is also part of a service that provides biplot graphs through the web.
49575575	WOS:000232831400001	977VK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Visualizing experimental designs for balanced ANOVA models using Lisp-Stat	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	18	1	WOS:000232831400001	The structure, or Hasse, diagram described by Taylor and Hilton ( 1981, American Statistician) provides a visual display of the relationships between factors for balanced complete experimental designs. Using the Hasse diagram, rules exist for determining the appropriate linear model, ANOVA table, expected means squares, and F-tests in the case of balanced designs. This procedure has been implemented in Lisp-Stat using a software representation of the experimental design. The user can interact with the Hasse diagram to add, change, or delete factors and see the effect on the proposed analysis. The system has potential uses in teaching and consulting.
49624401	WOS:000232891800001	978RX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bivariate Poisson and diagonal inflated bivariate Poisson regression models in R	Journal	Article	2005	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	36	1	WOS:000232891800001	In this paper we present an R package called bivpois for maximum likelihood estimation of the parameters of bivariate and diagonal inflated bivariate Poisson regression models. An Expectation-Maximization (EM) algorithm is implemented. Inflated models allow for modelling both over-dispersion (or under-dispersion) and negative correlation and thus they are appropriate for a wide range of applications. Extensions of the algorithms for several other models are also discussed. Detailed guidance and implementation on simulated and real data sets using bivpois package is provided.
49634330	WOS:000232831700001	977VN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Interactive geographical information system using Lisp-Stat: Prototypes and applications	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	34	1	WOS:000232831700001	We present a general framework of exploratory data analysis defining a set of concepts and prototypes developed within the Lisp-Stat programming environment for a M-to-M links multidimensional approach. We overview the main domains and fundamentals on which we lay the developed interactive GIS. In a second stage, we detail the different prototypes we implemented in a software called ARPEGE' and how they collaborate in providing an interactive spatial data exploration. Then we show four examples of concrete geographical applications and their underlaying data models. We end on a discussion about the contribution and the limitations of our conceptual framework and its associated software, and open to future research.
49667166	WOS:000232807000001	977MD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	EbayesThresh: R programs for empirical Bayes thresholding	Journal	Article	2005	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-38	38	1	WOS:000232807000001	Suppose that a sequence of unknown parameters is observed subject to independent Gaussian noise. The EbayesThresh package in the S language implements a class of Empirical Bayes thresholding methods that can take advantage of possible sparsity in the sequence, to improve the quality of estimation. The prior for each parameter in the sequence is a mixture of an atom of probability at zero and a heavy-tailed density. Within the package, this can be either a Laplace ( double exponential) density or else a mixture of normal distributions with tail behavior similar to the Cauchy distribution. The mixing weight, or sparsity parameter, is chosen automatically by marginal maximum likelihood. If estimation is carried out using the posterior median, this is a random thresholding procedure; the estimation can also be carried out using other thresholding rules with the same threshold, and the package provides the posterior mean, and hard and soft thresholding, as additional options. This paper reviews the method, and gives details ( far beyond those previously published) of the calculations needed for implementing the procedures. It explains and motivates both the general methodology, and the use of the EbayesThresh package, through simulated and real data examples. When estimating the wavelet transform of an unknown function, it is appropriate to apply the method level by level to the transform of the observed data. The package can carry out these calculations for wavelet transforms obtained using various packages in R and S-PLUS. Details, including a motivating example, are presented, and the application of the method to image estimation is also explored. The final topic considered is the estimation of a single sequence that may become progressively sparser along the sequence. An iterated least squares isotone regression method allows for the choice of a threshold that depends monotonically on the order in which the observations are made. An alternative possibility, also discussed in detail, is a particular parametric dependence of the sparsity parameter on the position in the sequence.
49676903	WOS:000232806800001	977MB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	spatstat: An R package for analyzing spatial point patterns	Journal	Article	2005	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-42	42	1	WOS:000232806800001	spatstat is a package for analyzing spatial point pattern data. Its functionality includes exploratory data analysis, model-fitting, and simulation. It is designed to handle realistic datasets, including inhomogeneous point patterns, spatial sampling regions of arbitrary shape, extra covariate data, and `marks' attached to the points of the point pattern. A unique feature of spatstat is its generic algorithm for fitting point process models to point pattern data. The interface to this algorithm is a function ppm that is strongly analogous to lm and glm. This paper is a general description of spatstat and an introduction for new users.
49684624	WOS:000232832100001	977VR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The health of Lisp-Stat	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-5	5	1	WOS:000232832100001	Lisp-Stat seems to be quite inert nowadays. However, there is no widespread agreement about what actually killed it, or even whether it has really passed away or not. Here we present the papers included in this special issue of the Journal of Statistical Software about it. Some of the included papers are about what Lisp-Stat was able to do in the past, other give testimony of the current state of health of Lisp-Stat, and other analyze whether there is any chance of its eventual recovery. We believe that the diagnosis performed here will appeal not only to people interested in Lisp-Stat but to all those involved in the development of statistical languages, as it provides hints as to what elements could make them succeed or fail.
49755292	WOS:000232928500001	979FT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A CLUE for CLUster ensembles	Journal	Article	2005	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	25	1	WOS:000232928500001	Cluster ensembles are collections of individual solutions to a given clustering problem which are useful or necessary to consider in a wide range of applications. The R package clue provides an extensible computational environment for creating and analyzing cluster ensembles, with basic data structures for representing partitions and hierarchies, and facilities for computing on these, including methods for measuring proximity and obtaining consensus and "secondary" clusterings.
49790724	WOS:000231710800003	962DI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multilingual datasets	Journal	Article	2005	1	1	English	STATA JOURNAL	162-187	26	1	WOS:000231710800003	This insert describes a new command mlanguage that facilitates the creation and maintenance of "comprehensive" multilingual datasets. These are datasets with many variables, many of which are value labeled, with labels in different languages, all contained within the dataset. The tools make it easy to add labels in a new language by translating an existing set of labels, to switch between the sets of labels, to verify the integrity of such labels, and to assist in keeping the labels complete.
49815686	WOS:000232806600001	977LZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A software tool for the exponential power distribution: The normalp package	Journal	Article	2005	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000232806600001	In this paper we present the normalp package, a package for the statistical environment R that has a set of tools for dealing with the exponential power distribution. In this package there are functions to compute the density function, the distribution function and the quantiles from an exponential power distribution and to generate pseudo-random numbers from the same distribution. Moreover, methods concerning the estimation of the distribution parameters are described and implemented. It is also possible to estimate linear regression models when we assume the random errors distributed according to an exponential power distribution. A set of functions is designed to perform simulation studies to see the suitability of the estimators used. Some examples of use of this package are provided.
49832672	WOS:000232831900001	977VP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A video tour through ViSta 6.4, a visual statistical system based on Lisp-Stat	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000232831900001	This paper offers a visual tour throughout ViSta 6.4, a freeware statistical program based on Lisp-Stat and focused on techniques for statistical visualization ( Young 2004). This travel around ViSta is based on screen recordings that illustrate the main features of the program in action. The following aspects of ViSta 6.4 are displayed: the program's interface (ViSta's desktop, menubar and pop-up menus, help system); its data management capabilities ( data input and editing, data transformations); features associated to data analysis (data description, statistical modeling); and the options for Lisp-Stat development in ViSta. The video recordings associated to this tour (. wmv files) can be visualized at http:// www. jstatsoft.org/v13/ i08/ using the Internet Explorer navigator, or by clicking on the figures in the paper.
49855061	WOS:000231710800008	962DI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Using the file command to produce formatted output for other applications	Journal	Article	2005	1	1	English	STATA JOURNAL	239-247	9	1	WOS:000231710800008	The file command provides a way to produce tables for use in other application software. It can be especially useful for combining descriptive results (such as means and percentages) and results from significance tests. Extracting and manipulating the results directly from Stata matrices gives more control over arrangement, while other Stata functions may be used to control numeric formats. This tutorial includes examples based on survey data of both plain text and HTML output.
49936592	WOS:000232807100001	977ME	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Lost opportunities: Why we need a variety of statistical languages	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-12	12	1	WOS:000232807100001	To the worker who only has a hammer, we are told, everything looks like a nail. Solutions to applied statistical problems are framed by the limitations imposed by statistical computing packages and languages. For better or worse, we can do what the packages do; we cannot do what the packages won't do. Statistical languages like R have basic tools that allow the analyst to design new hammers, but even in R we cannot build an arbitrary hammer, only ones within the limits imposed by the R language. XLISP-STAT imposes different limitations, so we can produce different hammers. In this article, I look at some of the tools in XLISP-STAT that allow the user to think about graphics in ways that cannot be easily replicated in other statistical languages. The interactive graphical methods available in XLISP-STAT lead to very different methodology than would be developed without the tools that XLISP-STAT provides. The general approach to graphics and indeed to data analysis in general is quite different in a package like Arc that is built on top of XLISP-STAT, than it is in other statistical packages. We discuss why that might be true, and why this depends on design options created by XLISP-STAT.
49986558	WOS:000231609700012	960RA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Further processing of estimation results: Basic programming with matrices	Journal	Article	2005	1	1	English	STATA JOURNAL	83-91	9	1	WOS:000231609700012	Rather than process estimation results in other applications, such as spreadsheets, this article shows how easy it is to process them inside Stata by undertaking some basic programming with matrices. Spreadsheet visualization helps define the task, but the steps are all core Stata: macros, loops, and matrices. The programming challenge is only a modest one for novices, while the benefits of converting do-files into ado programs can be considerable.
50005049	WOS:000232890400001	978RJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SAS macro BDM for fitting the Dale regression model to bivariate ordinal response data	Journal	Article	2005	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	12	1	WOS:000232890400001	A SAS macro for fitting an extension of the Dale (1986) regression model to bivariate ordinal data is provided. The macro is described in detail and examples from Dale (1986) and McMillan, Hanson, Bedrick, and Lapham (2005) are discussed.
50051914	WOS:000232890600001	978RL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MNP: R package for fitting the multinomial probit model	Journal	Article	2005	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	32	1	WOS:000232890600001	MNP is a publicly available R package that fits the Bayesian multinomial probit model via Markov chain Monte Carlo. The multinomial probit model is often used to analyze the discrete choices made by individuals recorded in survey data. Examples where the multinomial probit model may be useful include the analysis of product choice by consumers in market research and the analysis of candidate or party choice by voters in electoral studies. The MNP software can also fit the model with different choice sets for each individual, and complete or partial individual choice orderings of the available alternatives from the choice set. The estimation is based on the efficient marginal data augmentation algorithm that is developed by Imai and van Dyk (2005).
50162752	WOS:000232806900001	977MC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A comment on the implementation of the Ziggurat method	Journal	Article	2005	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-4	4	1	WOS:000232806900001	We show that the short period of the uniform random number generator in the published implementation of Marsaglia and Tsang's Ziggurat method for generating random deviates can lead to poor distributions. Changing the uniform random number generator used in its implementation fixes this issue.
50162829	WOS:000231609700010	960RA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Stata: The language of choice for time-series analysis?	Journal	Article	2005	1	1	English	STATA JOURNAL	46-63	18	1	WOS:000231609700010	This paper discusses the use of Stata for the analysis of time series and panel data. The evolution of time-series capabilities in Stata is reviewed. Facilities for data management, graphics, and econometric analysis from both official Stata and the user community are discussed. A new routine to provide moving-window regression estimates-rollreg-is described, and its use illustrated.
50163061	WOS:000232929000001	979FY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	arules - A computational environment for mining association rules and frequent item sets	Journal	Article	2005	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	25	1	WOS:000232929000001	Mining frequent itemsets and association rules is a popular and well researched approach for discovering interesting relationships between variables in large databases. The R package arules presented in this paper provides a basic infrastructure for creating and manipulating input data sets and for analyzing the resulting itemsets and rules. The package also includes interfaces to two fast mining algorithms, the popular C implementations of Apriori and Eclat by Christian Borgelt. These algorithms can be used to mine frequent itemsets, maximal frequent itemsets, closed frequent itemsets and association rules.
50220157	WOS:000231710800002	962DI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Value label utilities: labeldup and labelrename	Journal	Article	2005	1	1	English	STATA JOURNAL	154-161	8	1	WOS:000231710800002	I describe two utilities dealing with value labels. labeldup reports and optionally removes duplicate value labels. labelrename renames a value label. Both utilities, of course, preserve the links between variables and value labels and support multilingual datasets.
50235566	WOS:000231710800009	962DI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Teaching statistics to physicians using Stata	Journal	Article	2005	1	1	English	STATA JOURNAL	248-258	11	1	WOS:000231710800009	The Clinical Research Training Program (CRTP) at the Albert Einstein College of Medicine at Yeshiva University is a two-year program for physicians leading to a Master of Science degree in Clinical Research Methods. Beginning in July 2004, the program began teaching data analysis using Stata 8 in order to better meet the advanced statistical needs of the students. This paper details the structure and content of the course, how Stata was introduced, and the problems we encountered. Student comments and suggestions on future enhancements to Stata are included. Although challenging, our first semester teaching Stata was a success: the students all learned Stata and, more importantly, continued to use it for the analysis of their own research data after the course was complete.
50264168	WOS:000231609700014	960RA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A menu-driven facility for complex sample size calculation in randomized controlled trials with a survival or a binary outcome: Update	Journal	Article	2005	1	1	English	STATA JOURNAL	123-129	7	1	WOS:000231609700014	Royston and Babiker (2002) presented a menu-driven Stata program for the calculation of sample size or power for complex clinical trial designs under a survival time or binary outcome. In the present article, the package is updated to Stata 8 under the new name ART. Furthermore, the program has been extended to incorporate noninferiority designs and provides more detailed output. This package is the only realistic sample size tool for survival studies available in Stata.
50290773	WOS:000232891600001	978RV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The R commander: A basic-statistics graphical user interface to R	Journal	Article	2005	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	42	1	WOS:000232891600001	Unlike S-PLUS, R does not incorporate a statistical graphical user interface (GUI), but it does include tools for building GUIs. Based on the tcltk package (which furnishes an interface to the Tcl/Tk GUI toolkit), the Rcmdr package provides a basic-statistics graphical user interface to R called the "R Commander." The design objectives of the R Commander were as follows: to support, through an easy-to-use, extensible, cross-platform GUI, the statistical functionality required for a basic-statistics course (though its current functionality has grown to include support for linear and generalized-linear models, and other more advanced features); to make it relatively difficult to do unreasonable things; and to render visible the relationship between choices made in the GUI and the R commands that they generate. The R Commander uses a simple and familiar menu/dialog-box interface. Top-level menus include File, Edit, Data, Statistics, Graphs, Models, Distributions, Tools, and Help, with the complete menu tree given in the paper. Each dialog box includes a Help button, which leads to a relevant help page. Menu and dialog-box selections generate R commands, which are recorded in a script window and are echoed, along with output, to an output window. The script window also provides the ability to edit, enter, and re-execute commands. Error messages, warnings, and some other information appear in a separate messages window. Data sets in the R Commander are simply R data frames, and can be read from attached packages or imported from files. Although several data frames may reside in memory, only one is "active" at any given time. There may also be an active statistical model (e.g., an R 1m or g1m object). The purpose of this paper is to introduce and describe the use of the R Commander GUI; to describe the design and development of the R Commander; and to explain how the R Commander GUI can be extended. The second part of the paper (following a brief introduction) can serve as an introductory guide for students who will use the R Commander.
50300352	WOS:000232806500001	977LY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	R2WinBUGS: A package for running WinBUGS from R	Journal	Article	2005	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000232806500001	The R2WinBUGS package provides convenient functions to call WinBUGS from R. It automatically writes the data and scripts in a format readable by WinBUGS for processing in batch mode, which is possible since version 1.4. After the WinBUGS process has finished, it is possible either to read the resulting data into R by the package itself-which gives a compact graphical summary of inference and convergence diagnostics-or to use the facilities of the coda package for further analyses of the output. Examples are given to demonstrate the usage of this package.
50343066	WOS:000232927600001	979FK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	BayesX: Analyzing Bayesian structured additive regression models	Journal	Article	2005	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000232927600001	There has been much recent interest in Bayesian inference for generalized additive and related models. The increasing popularity of Bayesian methods for these and other model classes is mainly caused by the introduction of Markov chain Monte Carlo (MCMC) simulation techniques which allow realistic modeling of complex problems. This paper describes the capabilities of the free software package BayesX for estimating regression models with structured additive predictor based on MCMC inference. The program extends the capabilities of existing software for semiparametric regression included in S-PLUS, SAS, R or Stata. Many model classes well known from the literature are special cases of the models supported by BayesX. Examples are generalized additive ( mixed) models, dynamic models, varying coefficient models, geoadditive models, geographically weighted regression and models for space-time regression. BayesX supports the most common distributions for the response variable. For univariate responses these are Gaussian, Binomial, Poisson, Gamma, negative Binomial, zero inflated Poisson and zero inflated negative binomial. For multicategorical responses, both multinomial logit and probit models for unordered categories of the response as well as cumulative threshold models for ordered categories can be estimated. Moreover, BayesX allows the estimation of complex continuous time survival and hazard rate models.
50365592	WOS:000234226800001	997EC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A common platform for graphical models in R: The gRbase package	Journal	Article	2005	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	12	1	WOS:000234226800001	The gRbase package is intended to set the framework for computer packages for data analysis using graphical models. The gRbase package is developed for the open source language, R, and is available for several platforms. The package is intended to be widely extendible and flexible so that package developers may implement further types of graphical models using the available methods. The gRbase package consists of a set of S version 3 classes and associated methods for representing data and models. The package is linked to the dynamicGraph package (Badsberg 2005), an interactive graphical user interface for manipulating graphs. In this paper, we show how these building blocks can be combined and integrated with inference engines in the special cases of hierarchical log-linear models. We also illustrate how to extend the package to deal with other types of graphical models, in this case the graphical Gaussian models.
50413502	WOS:000240514400009	084EI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Graphs for all seasons	Journal	Article	2006	1	1	English	STATA JOURNAL	397-419	23	1	WOS:000240514400009	Time series showing seasonality-marked variation with time of year-are of interest to many scientists, including climatologists, other environmental scientists, epidemiologists, and economists. The usual graphs plotting response variables against time, or even time of year, are not always the most effective at showing the fine structure of seasonality. I survey various modifications of the usual graphs and other kinds of graphs with a range of examples. Although I introduce here two new Stata commands, cycleplot and sliceplot, I emphasize exploiting standard functions, data management commands, and graph options to get the graphs desired.
50435728	WOS:000240504200001	084AP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ITA 2.0: A program for classical and inductive item tree analysis	Journal	Article	2006	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	14	1	WOS:000240504200001	Item Tree Analysis ( ITA) is an explorative method of data analysis which can be used to establish a hierarchical structure on a set of dichotomous items from a questionnaire or test. There are currently two different algorithms available to perform an ITA. We describe a computer program called ITA 2.0 which implements both of these algorithms. In addition we show with a concrete data set how the program can be used for the analysis of questionnaire data.
50550543	WOS:000240514000008	084EE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Regression Models for Categorical Dependent Variables Using Stata, Second Edition, by Long and Freese	Journal	Review	2006	1	1	English	STATA JOURNAL	273-278	6	1	WOS:000240514000008	This article reviews Regression Models for Categorical Dependent Variables Using Stata, Second Edition, by Long and Freese.
50604977	WOS:000240514400010	084EI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of a gentle introduction to Stata by Acock	Journal	Review	2006	1	1	English	STATA JOURNAL	420-424	5	1	WOS:000240514400010	This article reviews A Gentle Introduction to Stata by Acock.
50607138	WOS:000239693800001	072SR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An accept-and-reject algorithm to sample from a set of permutations restricted by a time constraint	Journal	Article	2006	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	17	1	WOS:000239693800001	A modification of an accept-and-reject algorithm to sample from a set of restricted permutations is proposed. By concentrating on a special class of matrices obtained by restriction of the permutation in time, assuming the objects to be permuted to be events in time, the modified algorithm's running time can be shown to be linear instead of geometric in the number of elements. The implementation of the algorithm in the language R is presented in a Literate Programming style.
50639684	WOS:000242923600008	118DY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: In praise of trigonometric predictors	Journal	Article	2006	1	1	English	STATA JOURNAL	561-579	19	1	WOS:000242923600008	Using sine and cosine terms as predictors in modeling periodic time series and other kinds of periodic responses is a long-established technique, but it is often overlooked in many courses or textbooks. Such trigonometric regression is straightforward in Stata through applications of existing commands. I give various examples using classic periodic datasets on the motion of the asteroid Pallas and the daily rhythm of birth numbers. I make a brief connection to polynomial-trigonometric regression.
50654869	WOS:000237294600001	039GS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Hapassoc: Software for likelihood inference of trait associations with SNP haplotypes and other attributes	Journal	Article	2006	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000237294600001	Complex medical disorders, such as heart disease and diabetes, are thought to involve a number of genes which act in conjunction with lifestyle and environmental factors to increase disease susceptibility. Associations between complex traits and single nucleotide polymorphisms ( SNPs) in candidate genomic regions can provide a useful tool for identifying genetic risk factors. However, analysis of trait associations with single SNPs ignores the potential for extra information from haplotypes, combinations of variants at multiple SNPs along a chromosome inherited from a parent. When haplotype-trait associations are of interest and haplotypes of individuals can be determined, generalized linear models (GLMs) may be used to investigate haplotype associations while adjusting for the effects of non-genetic cofactors or attributes. Unfortunately, haplotypes cannot always be determined cost-effectively when data is collected on unrelated subjects. Uncertain haplotypes may be inferred on the basis of data from single SNPs. However, subsequent analyses of risk factors must account for the resulting uncertainty in haplotype assignment in order to avoid potential errors in interpretation. To account for such uncertainty, we have developed hapassoc, software for R implementing a likelihood approach to inference of haplotype and non-genetic effects in GLMs of trait associations. We provide a description of the underlying statistical method and illustrate the use of hapassoc with examples that highlight the flexibility to specify dominant and recessive effects of genetic risk factors, a feature not shared by other software that restricts users to additive effects only. Additionally, hapassoc can accommodate missing SNP genotypes for limited numbers of subjects.
50658390	WOS:000242923600001	118DY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Sequence analysis with Stata	Journal	Article	2006	1	1	English	STATA JOURNAL	435-460	26	1	WOS:000242923600001	We describe a general strategy to analyze sequence data and introduce SQ-Ados, a bundle of Stata programs implementing the proposed strategy. The programs include several tools for describing and visualizing sequences as well as a Mata library to perform optimal matching using the Needleman-Wunsch algorithm. With these programs Stata becomes the first statistical package to offer a complete set of tools for sequence analysis.
50673120	WOS:000239139800001	065DO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SAS/IML macros for a multivariate analysis of variance based on spatial signs	Journal	Article	2006	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	17	1	WOS:000239139800001	Recently, new nonparametric multivariate extensions of the univariate sign methods have been proposed. Randles (2000) introduced an affine invariant multivariate sign test for the multivariate location problem. Later on, Hettmansperger and Randles (2002) considered an affine equivariant multivariate median corresponding to this test. The new methods have promising efficiency and robustness properties. In this paper, we review these developments and compare them with the classical multivariate analysis of variance model. A new SAS/IML tool for performing a spatial sign based multivariate analysis of variance is introduced.
50693796	WOS:000240513700004	084EB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Generalized ordered logit/partial proportional odds models for ordinal dependent variables	Journal	Article	2006	1	1	English	STATA JOURNAL	58-82	25	1	WOS:000240513700004	This article describes the gologit2 program for generalized ordered logit models. gologit2 is inspired by Vincent Fu's gologit routine (Stata Technical Bulletin Reprints 8: 160-164) and is backward compatible with it but offers several additional powerful options. A major strength of gologit2 is that it can fit three special cases of the generalized model: the proportional odds/parallel-lines model, the partial proportional odds model, and the logistic regression model. Hence, gologit2 can fit models that are less restrictive than the parallel-lines models fitted by ologit (whose assumptions are often violated) but more parsimonious and interpretable than those fitted by a nonordinal method, such as multinomial logistic regression (i.e., mlogit). Other key advantages of gologit2 include support for linear constraints, survey data estimation, and the computation of estimated probabilities via the predict command.
50695771	WOS:000241808100001	102JX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The strucplot framework: visualizing multi-way contingency tables with VCD	Journal	Article	2006	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	48	1	WOS:000241808100001	This paper describes the "strucplot" framework for the visualization of multi-way contingency tables. Strucplot displays include hierarchical conditional plots such as mosaic, association, and sieve plots, and can be combined into more complex, specialized plots for visualizing conditional independence, GLMs, and the results of independence tests. The framework's modular design allows flexible customization of the plots' graphical appearance, including shading, labeling, spacing, and legend, by means of "graphical appearance control" functions. The framework is provided by the R package vcd.
50706643	WOS:000236151300001	023UI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Independencies induced from a graphical Markov model after marginalization and conditioning: the R package ggm	Journal	Article	2006	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	15	1	WOS:000236151300001	We describe some functions in the R package ggm to derive from a given Markov model, represented by a directed acyclic graph, different types of graphs induced after marginalizing over and conditioning on some of the variables. The package has a few basic functions that find the essential graph, the induced concentration and covariance graphs, and several types of chain graphs implied by the directed acyclic graph (DAG) after grouping and reordering the variables. These functions can be useful to explore the impact of latent variables or of selection effects on a chosen data generating model.
50736420	WOS:000241808000001	102JW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ada: an R package for stochastic boosting	Journal	Article	2006	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000241808000001	Boosting is an iterative algorithm that combines simple classification rules with 'mediocre' performance in terms of misclassification error rate to produce a highly accurate classification rule. Stochastic gradient boosting provides an enhancement which incorporates a random mechanism at each boosting step showing an improvement in performance and speed in generating the ensemble. ada is an R package that implements three popular variants of boosting, together with a version of stochastic gradient boosting. In addition, useful plots for data analytic purposes are provided along with an extension to the multi-class case. The algorithms are illustrated with synthetic and real data sets.
50758820	WOS:000240514000005	084EE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation of multinomial logit models with unobserved heterogeneity using maximum simulated likelihood	Journal	Article	2006	1	1	English	STATA JOURNAL	229-245	17	1	WOS:000240514000005	In this paper, we suggest a Stata routine for multinomial logit models with unobserved heterogeneity using maximum simulated likelihood based on Halton sequences. The purpose of this paper is twofold. First, we describe the technical implementation of the estimation routine and discuss its properties. Further, we compare our estimation routine with the Stata program gllamm, which solves integration by using Gauss-Hermite quadrature or adaptive quadrature. For the analysis, we draw on multilevel data about schooling. Our empirical findings show that the estimation techniques lead to approximately the same estimation. results. The advantage of simulation over Gauss-Hermite quadrature is a marked reduction in computational time for integrals with higher dimensions. Adaptive quadrature leads to more stable results relative to the other integration methods. However, simulation is more time efficient. We find that maximum simulated likelihood leads to estimation results with reasonable accuracy in roughly half the time required when using adaptive quadrature.
50777221	WOS:000240514000007	084EE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Maximum simulated likelihood estimation of random-effects dynamic probit models with autocorrelated errors	Journal	Article	2006	1	1	English	STATA JOURNAL	256-272	17	1	WOS:000240514000007	This paper investigates using maximum simulated likelihood (MSL) estimation for random-effects dynamic probit models with autocorrelated errors. It presents and illustrates a new Stata command, redpace, for this estimator. The paper also compares using pseudorandom numbers and Halton sequences of quasirandom numbers for MSL estimation of these models.
50876680	WOS:000240514400007	084EI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Importing federal reserve economic data	Journal	Article	2006	1	1	English	STATA JOURNAL	384-386	3	1	WOS:000240514400007	This note describes freduse, which imports datasets from the Federal Reserve economic data (FRED(R) repository.
50877832	WOS:000235180800001	010IH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Jackknife estimator of species richness with S-PLUS	Journal	Article	2006	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	12	1	WOS:000235180800001	An estimate of the number of species, S, usually called species richness by ecologists, in an area is one of the basic statistics used to ascertain biological diversity. Traditionally ecologists have used the number of species observed in a sample, S-0, to estimate S, realizing that S-0 is a lower bound for S. One alternative to S-0 is to use a nonparametric procedure such as jackknife resampling. For species richness, a closed form of the jackknife estimator is available. Typically statistical software contains only the traditional iterative form of the jackknife estimator. The purpose of this article is to propose an S-PLUS function for calculating the noniterative first order jackknife estimator of species richness and some associated plots and statistics.
50884575	WOS:000240514000003	084EE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A Mata Geweke-Hajivassiliou-Keane multivariate normal simulator	Journal	Article	2006	1	1	English	STATA JOURNAL	190-213	24	1	WOS:000240514000003	An accurate and efficient numerical approximation of the multivariate normal (MVN) distribution function is necessary for obtaining maximum likelihood estimates for models involving the MVN distribution. Numerical integration through simulation (Monte Carlo) or number-theoretic (quasi-Monte Carlo) techniques is one way to accomplish this task. One popular simulation technique is the Geweke-Hajivassiliou-Keane MVN simulator. This paper reviews this technique and introduces a Mata function that implements it. It also computes analytical first-order derivatives of the simulated probability with respect to the variables and the variance-covariance parameters.
50961046	WOS:000240514400003	084EI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Tests and confidence sets with correct size when instruments are potentially weak	Journal	Article	2006	1	1	English	STATA JOURNAL	335-347	13	1	WOS:000240514400003	We consider inference in the linear regression model with one endogenous variable and potentially weak instruments. We construct confidence sets for the coefficient on the endogenous variable by inverting the Anderson-Rubin, Lagrange multiplier, and conditional likelihood-ratio tests. Our confidence sets have correct coverage probabilities even when the instruments are weak. We propose a numerically simple algorithm for finding these confidence sets, and we present a Stata command that supersedes the one presented in Moreira and Poi (Stata Journal 3: 57-70).
50967497	WOS:000241807900001	102JV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Relative importance for linear regression in R: The package relaimpo	Journal	Article	2006	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	27	1	WOS:000241807900001	Relative importance is a topic that has seen a lot of interest in recent years, particularly in applied work. The R package relaimpo implements six different metrics for assessing relative importance of regressors in the linear model, two of which are recommended averaging over orderings of regressors and a newly proposed metric (Feldman 2005) called pmvd. Apart from delivering the metrics themselves, relaimpo also provides ( exploratory) bootstrap confidence intervals. This paper offers a brief tutorial introduction to the package. The methods and relaimpo's functionality are illustrated using the data set swiss that is generally available in R. The paper targets readers who have a basic understanding of multiple linear regression. For the background of more advanced aspects, references are provided.
50969502	WOS:000241808200001	102JY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Using R via PHP for teaching purposes: R-php	Journal	Article	2006	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	20	1	WOS:000241808200001	This paper deals with the R-php statistical software, that is an environment for statistical analysis, freely accessible and attainable through the World Wide Web, based on R. Indeed, this software uses, as "engine" for statistical analyses, R via PHP and its design has been inspired by a paper of de Leeuw (1997). R-php is based on two modules: a base module and a point-and-click module. R-php base allows the simple editing of R code in a form. R-php point-and-click allows some statistical analyses by means of a graphical user interface (GUI): then, to use this module it is not necessary for the user to know the R environment, but all the allowed analyses can be performed by using the computer mouse. We think that this tool could be particularly useful for teaching purposes: one possible use could be in a University computer laboratory to permit a smooth approach of students to R.
51060794	WOS:000242923600003	118DY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Testing for cross-sectional dependence in panel-data models	Journal	Article	2006	1	1	English	STATA JOURNAL	482-496	15	1	WOS:000242923600003	This article describes a new Stata routine, xtcsd. to test for the presence of cross-sectional dependence in panels with many cross-sectional units and few time-series observations. The command executes three different testing procedures-namely, Friedman's (Journal of the American Statistical Association 32: 675-701) (FR) test statistic, the statistic proposed by Frees (.Journal of Econometrics 69: 393-414), and the cross-sectional dependence (CD) test of Pesaran (General diagnostic tests for cross-section dependence in panels [University of Cambridge, Faculty of Economics, Cambridge Working Papers in Economics, Paper No. 0435]). We illustrate the command with an empirical example.
51083890	WOS:000240514400002	084EI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Confidence intervals for rank statistics: Somers' D and extensions	Journal	Article	2006	1	1	English	STATA JOURNAL	309-334	26	1	WOS:000240514400002	Somers' D is an asymmetric measure of association between two variables, which plays a central role as a parameter behind rank or nonparametric statistical methods. Given predictor variable X and outcome variable Y, we may estimate D-YX as a measure of the effect of X on Y, or we may estimate D-XY as a performance indicator of X as a predictor of Y. The somersd package allows the estimation of Somers' D and Kendall's tau(a) with confidence limits as well as p-values. The Stata 9 version of somersd can estimate extended versions of Somers' D not previously available, including the Gini index, the parameter tested by the sign test, and extensions to left- or right-censored data. It can also estimate stratified versions of Somers' D, restricted to pairs in the same stratum. Therefore, it is possible to define strata by grouping values of a confounder, or of a propensity score based on multiple confounders, and to estimate versions of Somers' D that measure the association between the outcome and the predictor, adjusted for the confounders. The Stata 9 version of somersd uses the Mata language for improved computational efficiency with large datasets.
51089812	WOS:000235180400001	010IE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Efficient calculation of jackknife confidence intervals for rank statistics	Journal	Article	2006	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	10	1	WOS:000235180400001	An algorithm is presented for calculating concordance-discordance totals in a time of order N log N, where N is the number of observations, using a balanced binary search tree. These totals can be used to calculate jackknife estimates and confidence limits in the same time order for a very wide range of rank statistics, including Kendall's tau, Somers' D, Harrell's c, the area under the receiver operating characteristic (ROC) curve, the Gini coefficient, and the parameters underlying the sign and rank-sum tests. A Stata package is introduced for calculating confidence intervals for these rank statistics using this algorithm, which has been implemented in the Mata compilable matrix programming language supplied with Stata.
51099267	WOS:000240513700007	084EB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Decomposing inequality and obtaining marginal effects	Journal	Article	2006	1	1	English	STATA JOURNAL	106-111	6	1	WOS:000240513700007	This article describes a user-written command, descogini, that decomposes the Gini coefficient by income source and allows the calculation of the impact that a marginal change in a particular income source will have on inequality. descogini can be used with bootstrap to obtain standard errors and confidence intervals.
51106476	WOS:000240514400006	084EI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Difference-based semiparametric estimation of partial linear regression models	Journal	Article	2006	1	1	English	STATA JOURNAL	377-383	7	1	WOS:000240514400006	This article describes the plreg command, which implements the difference-based algorithm for fitting partial linear regression models.
51108044	WOS:000240514000002	084EE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Calculation of multivariate normal probabilities by simulation, with applications to maximum simulated likelihood estimation	Journal	Article	2006	1	1	English	STATA JOURNAL	156-189	34	1	WOS:000240514000002	We discuss methods for calculating multivariate normal probabilities by simulation and two new Stata programs for this purpose: mdraws for deriving draws from the standard uniform density using either Halton or pseudorandom sequences, and an egen function, mvnp (), for calculating the probabilities themselves. Several illustrations show how the programs may be used for maximum simulated likelihood estimation.
51112850	WOS:000242923600007	118DY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Mata matters: Precision	Journal	Article	2006	1	1	English	STATA JOURNAL	550-560	11	1	WOS:000242923600007	Mata is Stata's matrix language. The Mata Matters column shows how Mata can be used interactively to solve problems and as a programming language to add new features to Stata. In this quarter's column, we look at the programming implications of the floating-point, base-2 encoding that modern computers use.
51117938	WOS:000242923600002	118DY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Practical fixed-effects estimation methods for the three-way error-components model	Journal	Article	2006	1	1	English	STATA JOURNAL	461-481	21	1	WOS:000242923600002	Methods for fixed-effects estimation of the three-way error-components model are not yet standard. Where possible, we make the fixed-effects methods originally developed by Abowd, Kramarz, and Margolis (Econometrica 67: 251333) for linked worker-firm data more accessible. We also show how these methods can be implemented in Stata. There is a caveat, however. If the researcher wants to recover estimates of the error components themselves, and the number of units at the higher level of aggregation is large, memory or matrix constraints may make using Stata to estimate the components themselves infeasible.
51157198	WOS:000236151400001	023UJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Introducing elliptic, an R package for elliptic and modular functions	Journal	Article	2006	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	22	1	WOS:000236151400001	This paper introduces the elliptic package of R routines, for numerical calculation of elliptic and related functions. Elliptic functions furnish interesting and instructive examples of many ideas of complex analysis, and the package illustrates these numerically and visually. A statistical application in fluid mechanics is presented.
51165347	WOS:000242923600009	118DY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of An Introduction to Stata for Health Researchers by Juul	Journal	Review	2006	1	1	English	STATA JOURNAL	580-583	4	1	WOS:000242923600009	This article reviews An Introduction to Stata for Health Researchers by Svend Juul.
51249705	WOS:000240513700008	084EB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Mata matters: creating new variables - sounds boring, isn't	Journal	Article	2006	1	1	English	STATA JOURNAL	112-123	12	1	WOS:000240513700008	Mata is Stata's matrix language. In the Mata Matters column, we show how Mata can be used interactively to solve problems and as a programming language to add new features to Stata. In this quarter's column, we continue to explore the handling of Stata datasets in Mata and focus on creating now variables.
51256063	WOS:000240514400001	084EI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Maximum likelihood estimation of endogenous switching and sample selection models for binary, ordinal, and count variables	Journal	Article	2006	1	1	English	STATA JOURNAL	285-308	24	1	WOS:000240514400001	Studying behavior in economics, sociology, and statistics often involves fitting models in which the response variable depends on a dummy variable-also known as a regime-switch variable-or in which the response variable is observed only if a particular selection condition is met. In either case, standard regression techniques deliver inconsistent estimators if unobserved factors that affect the response are correlated with unobserved factors that affect the switching or selection variable. Consistent estimators can be obtained by maximum likelihood estimation of a joint model of the outcome and switching or selection variable. This article describes a "wrapper" program, ssm, that calls gllarim (Rabe-Hesketh, Skrondal, and Pickles, GLLAMM Manual [University of California-Berkeley, Division of Biostatistics, Working Paper Series, Paper No. 160]) to fit such models. The wrapper accepts data in a simple structure, has a straightforward syntax, and reports output that is easily interpretable. One important feature of ssm is that the log likelihood can be evaluated using adaptive quadrature (Rabe-Hesketh, Skrondal, and Pickles, Stata Journal 2: 1-21; Journal of Econometrics 128: 301-323).
51319639	WOS:000242923600010	118DY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of A Stata Companion to Political Analysis by Pollock	Journal	Review	2006	1	1	English	STATA JOURNAL	584-587	4	1	WOS:000242923600010	This article reviews A Stata Companion to Political Analysis by Philip H. Pollock III.
51323310	WOS:000240206000001	079VM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A computational tool for testing dose-related trend using an age-adjusted bootstrap-based poly-k test	Journal	Article	2006	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	14	1	WOS:000240206000001	A computational tool for testing for a dose-related trend and/or a pairwise difference in the incidence of an occult tumor via an age-adjusted bootstrap-based poly-k test and the original poly-k test is presented in this paper. The poly-k test ( Bailer and Portier 1988) is a survival-adjusted Cochran-Armitage test, which achieves robustness to effects of differential mortality across dose groups. The original poly-k test is asymptotically standard normal under the null hypothesis. However, the asymptotic normality is not valid if there is a deviation from the tumor onset distribution that is assumed in this test. Our age-adjusted bootstrap-based poly-k test assesses the significance of assumed asymptotic normal tests and investigates an empirical distribution of the original poly-k test statistic using an age-adjusted bootstrap method. A tumor of interest is an occult tumor for which the time to onset is not directly observable. Since most of the animal carcinogenicity studies are designed with a single terminal sacrifice, the present tool is applicable to rodent tumorigenicity assays that have a single terminal sacrifice. The present tool takes input information simply from a user screen and reports testing results back to the screen through a user-interface. The computational tool is implemented in C/C++ and is applied to analyze a real data set as an example. Our tool enables the FDA and the pharmaceutical industry to implement a statistical analysis of tumorigenicity data from animal bioassays via our age-adjusted bootstrap-based poly-k test and the original poly-k test which has been adopted by the National Toxicology Program as its standard statistical test.
51340496	WOS:000236800500001	032UJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Support Vector Machines in R	Journal	Article	2006	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	28	1	WOS:000236800500001	Being among the most popular and efficient classification and regression methods currently available, implementations of support vector machines exist in almost every popular programming language. Currently four R packages contain SVM related software. The purpose of this paper is to present and compare these implementations.
51347899	WOS:000242546000001	112RZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Exact hypothesis tests for log-linear models with exactLoglinTest	Journal	Article	2006	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	17	1	WOS:000242546000001	This manuscript overviews exact testing of goodness of fit for log-linear models using the R package exactLoglinTest. This package evaluates model fit for Poisson log-linear models by conditioning on minimal sufficient statistics to remove nuisance parameters. A Monte Carlo algorithm is proposed to estimate P values from the resulting conditional distribution. In particular, this package implements a sequentially rounded normal approximation and importance sampling to approximate probabilities from the conditional distribution. Usually, this results in a high percentage of valid samples. However, in instances where this is not the case, a Metropolis Hastings algorithm can be implemented that makes more localized jumps within the reference set. The manuscript details how some conditional tests for binomial logit models can also be viewed as conditional Poisson log-linear models and hence can be performed via exactLoglinTest. A diverse battery of examples is considered to highlight use, features and extensions of the software. Notably, potential extensions to evaluating disclosure risk are also considered.
51350251	WOS:000242545800001	112RX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Data ellipses, HE plots and reduced-rank displays for multivariate linear models: SAS software and examples	Journal	Article	2006	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	43	1	WOS:000242545800001	This paper describes graphical methods for multiple-response data within the framework of the multivariate linear model (MLM), aimed at understanding what is being tested in a multivariate test, and how factor/predictor effects are expressed across multiple response measures. In particular, we describe and illustrate a collection of SAS macro programs for: ( a) Data ellipses and low-rank biplots for multivariate data, (b) HE plots, showing the hypothesis and error covariance matrices for a given pair of responses, and a given effect, ( c) HE plot matrices, showing all pairwise HE plots, and (d) low-rank analogs of HE plots, showing all observations, group means, and their relations to the response variables.
51356434	WOS:000242923600004	118DY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Confidence intervals for rank statistics: Percentile slopes, differences, and ratios	Journal	Article	2006	1	1	English	STATA JOURNAL	497-520	24	1	WOS:000242923600004	I present a program, censlope, for calculating confidence intervals for generalized Theil-Sen median (and other percentile) slopes (and per-unit ratios) of Y with respect to X. The confidence intervals are robust to the possibility that the conditional population distributions of Y, given different values of X, differ in ways other than location, such as having unequal variances. censlope uses the program somersd and is part of the somersd package. censlope can therefore estimate confounder-adjusted percentile slopes, limited to comparisons within strata defined by values of confounders, or by values of a propensity score representing multiple confounders. Iterative numerical methods have been implemented in the Mata language, enabling efficient calculation of percentile slopes and their confidence limits in large samples. I give example analyses from the auto dataset and from the Avon Longitudinal Study of Pregnancy and Childhood (ALSPAC).
51395190	WOS:000236151500001	023UK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Analyzing repeated measures marginal models on sample surveys with resampling methods	Journal	Article	2006	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	13	1	WOS:000236151500001	Packaged statistical software for analyzing categorical, repeated measures marginal models on sample survey data with binary covariates does not appear to be available. Consequently, this report describes a customized SAS program which accomplishes such an analysis on survey data with jackknifed replicate weights for which the primary sampling unit information has been suppressed for respondent confidentiality. First, the program employs the Macro Language and the Output Delivery System (ODS) to estimate the means and covariances of indicator variables for the response variables, taking the design into account. Then, it uses PROC CATMOD and ODS, ignoring the survey design, to obtain the design matrix and hypothesis test specifications. Finally, it enters these results into another run of CATMOD, which performs automated direct input of the survey design specifications and accomplishes the appropriate analysis. This customized SAS program can be employed, with minor editing, to analyze general categorical, repeated measures marginal models on sample surveys with replicate weights. Finally, the results of our analysis accounting for the survey design are compared to the results of two alternate analyses of the same data. This comparison confirms that such alternate analyses, which do not properly account for the design, do not produce useful results.
51400508	WOS:000235181200001	010IL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Proportional symbol mapping in R	Journal	Article	2006	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	7	1	WOS:000235181200001	Visualization of spatial data on a map aids not only in data exploration but also in communication to impart spatial conception or ideas to others. Although recent cartographic functions in R are rapidly becoming richer, proportional symbol mapping, which is one of the common mapping approaches, has not been packaged thus far. Based on the theories of proportional symbol mapping developed in cartography, the authors developed some functions for proportional symbol mapping using R, including mathematical and perceptual scaling. An example of these functions demonstrated the new expressive power and options available in R, particularly for the visualization of conceptual point data.
51401401	WOS:000241208100001	093ZF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SOCR: Statistics Online Computational Resource	Journal	Article	2006	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000241208100001	The need for hands-on computer laboratory experience in undergraduate and graduate statistics education has been firmly established in the past decade. As a result a number of attempts have been undertaken to develop novel approaches for problem-driven statistical thinking, data analysis and result interpretation. In this paper we describe an integrated educational web-based framework for: interactive distribution modeling, virtual online probability experimentation, statistical data analysis, visualization and integration. Following years of experience in statistical teaching at all college levels using established licensed statistical software packages, like STATA, S-PLUS, R, SPSS, SAS, Systat, etc., we have attempted to engineer a new statistics education environment, the Statistics Online Computational Resource (SOCR). This resource performs many of the standard types of statistical analysis, much like other classical tools. In addition, it is designed in a plug-in object-oriented architecture and is completely platform independent, web-based, interactive, extensible and secure. Over the past 4 years we have tested, fine-tuned and reanalyzed the SOCR framework in many of our undergraduate and graduate probability and statistics courses and have evidence that SOCR resources build student's intuition and enhance their learning.
51404889	WOS:000237294000001	039GM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Gompertz: A Scilab program for estimating Gompertz curve using Gauss-Newton method of least squares	Journal	Article	2006	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	12	1	WOS:000237294000001	A computer program for estimating Gompertz curve using Gauss-Newton method of least squares is described in detail. It is based on the estimation technique proposed in Reddy (1985). The program is developed using Scilab (version 3.1.1), a freely available scientific software package that can be downloaded from http://www.scilab.org/. Data is to be fed into the program from an external disk file which should be in Microsoft Excel format. The output will contain sample size, tolerance limit, a list of initial as well as the final estimate of the parameters, standard errors, value of Gauss-Normal equations namely GN(1) GN(2) and GN(3), No. of iterations, variance(sigma(2)), Durbin-Watson statistic, goodness of fit measures such as R-2, D value, covariance matrix and residuals. It also displays a graphical output of the estimated curve vis a vis the observed curve. It is an improved version of the program proposed in Dastidar (2005).
51459114	WOS:000240513700005	084EB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Explained variation for survival models	Journal	Article	2006	1	1	English	STATA JOURNAL	83-96	14	1	WOS:000240513700005	This article introduces a new measure of explained variation for use with censored survival data. It is a modified version of a measure previously described by John O'Quigley and colleagues, itself a modification of Nagelkerke's earlier proposal for a general index of determination. I describe Stata programs str2ph, which implements the new measure, and str2d, which implements a measure proposed in 2004 by Royston and Sauerbrei. I provide examples with real data.
51459443	WOS:000240513700002	084EB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Automatic generation of documents	Journal	Article	2006	1	1	English	STATA JOURNAL	22-39	18	1	WOS:000240513700002	This paper describes a natural interaction between Stata and markup languages. Stata's programming and analysis features, together with the flexibility in output formatting of the markup languages, allow generation and/or update of whole documents (e.g., reports, presentations on screen or web). We give examples for both LAT(E)X and HTML.
51523543	WOS:000242923600006	118DY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Bayesian analysis in Stata with WinBUGS	Journal	Article	2006	1	1	English	STATA JOURNAL	530-549	20	1	WOS:000242923600006	WinBUGS is a program for Bayesian model fitting by Gibbs sampling. WinBUGS has limited facilities for data handling, whereas Stata has no routines for Bayesian analysis; therefore. much can be gained by running Stata and WinBUGS together. We present a set of ado-files that enable data to be processed in Stata and then passed to WinBUGS for model fitting; finally, the results are read back into Stata for further processing.
51537360	WOS:000240513700010	084EB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of multilevel and longitudinal modeling using stata by Rabe-Hesketh and Skrondal	Journal	Article	2006	1	1	English	STATA JOURNAL	138-143	6	1	WOS:000240513700010	This article reviews Multilevel and Longitudinal Modeling Using Stata, by Rabe-Hesketh and Skrondal.
51556611	WOS:000240514400005	084EI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Jackknife instrumental variables estimation in stata	Journal	Article	2006	1	1	English	STATA JOURNAL	364-376	13	1	WOS:000240514400005	The two-stage least-squares (2SLS) instrumental variables estimator is commonly used to address endogeneity. However, the estimator suffers from bias that is exacerbated when the instruments are only weakly correlated with the endogenous variables and when many instruments are used. In this article, I discuss jackknife instrumental variables estimation as an alternative to 2SLS. Monte Carlo simulations comparing the jackknife instrument variables estimators to 2SLS and limited information maximum likelihood (LIML) show that two of the four variants perform remarkably well even when 2SLS does not. In a weak-instrument experiment, the two best performing jackknife estimators also outperform LIML.
51576763	WOS:000240514000004	084EE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Generating Halton sequences using Mata	Journal	Article	2006	1	1	English	STATA JOURNAL	214-228	15	1	WOS:000240514000004	This paper discusses the advantages of Halton sequences over pseudorandom uniform numbers when using simulation to approximate integrals numerically. We describe two types of sequences and give Mata examples. Finally, we document the Mata function halton(), currently in release 9.1 of Stata, which computes both a Halton sequence and its Hammersley variant. Options to use these point sets are available in the Stata 9 program asmprobit, a multinomial-probit estimator, and in the Stata 9.1 Mata function ghk(), the Geweke-Hajivassiliou-Keane multivariate-normal simulator.
51674981	WOS:000236800700001	032UK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	CVThresh: R package for level-dependent cross-validation thresholding	Journal	Article	2006	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000236800700001	The core of the wavelet approach to nonparametric regression is thresholding of wavelet coefficients. This paper reviews a cross-validation method for the selection of the thresholding value in wavelet shrinkage of Oh, Kim, and Lee (2006), and introduces the R package CVThresh implementing details of the calculations for the procedures. This procedure is implemented by coupling a conventional cross-validation with a fast imputation method, so that it overcomes a limitation of data length, a power of 2. It can be easily applied to the classical leave-one-out cross-validation and K-fold cross-validation. Since the procedure is computationally fast, a level-dependent cross-validation can be developed for wavelet shrinkage of data with various sparseness according to levels.
51677198	WOS:000235180600001	010IF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The R Package geepack for Generalized Estimating Equations	Journal	Article	2006	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-11	11	1	WOS:000235180600001	This paper describes the core features of the R package geepack, which implements the generalized estimating equations (GEE) approach for fitting marginal generalized linear models to clustered data. Clustered data arise in many applications such as longitudinal data and repeated measures. The GEE approach focuses on models for the mean of the correlated observations within clusters without fully specifying the joint distribution of the observations. It has been widely used in statistical practice. This paper illustrates the application of the GEE approach with geepack through an example of clustered binary data.
51710830	WOS:000240206200001	079VO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Some algorithms for the conditional mean vector and covariance matrix	Journal	Article	2006	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	9	1	WOS:000240206200001	We consider here the problem of computing the mean vector and covariance matrix for a conditional normal distribution, considering especially a sequence of problems where the conditioning variables are changing. The sweep operator provides one simple general approach that is easy to implement and update. A second, more goal-oriented general method avoids explicit computation of the vector and matrix, while enabling easy evaluation of the conditional density for likelihood computation or easy generation from the conditional distribution. The covariance structure that arises from the special case of an ARMA( p, q) time series can be exploited for substantial improvements in computational efficiency.
51740063	WOS:000239139500001	065DL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Programs to compute distribution functions and critical values for extreme value ratios for outlier detection	Journal	Article	2006	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	9	1	WOS:000239139500001	A set of FORTRAN subprograms is presented to compute density and cumulative distribution functions and critical values for the range ratio statistics of Dixon ( 1951, The Annals of Mathematical Statistics) These statistics are useful for detection of outliers in small samples.
51748340	WOS:000240514400008	084EI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Mata matters: Interactive use	Journal	Article	2006	1	1	English	STATA JOURNAL	387-396	10	1	WOS:000240514400008	Mata is Stata's matrix language. In the Mata Matters column, we show how Mata can be used interactively to solve problems and as a programming language to add new features to Stata. In this quarter's column, we look at interactive use of Mata.
51764840	WOS:000242923600005	118DY	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Calculating Murphy-Topel variance estimates in Stata: A simplified procedure	Journal	Article	2006	1	1	English	STATA JOURNAL	521-529	9	1	WOS:000242923600005	Building on the work by Hardin (Stata, Journal 2: 253-266), this note shows how the calculation of the Murphy-Topel variance estimator for two-step models call be simplified in Stata by using the scores option of predict.
51776995	WOS:000240513700003	084EB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Generalized least squares for trend estimation of summarized dose-response data	Journal	Article	2006	1	1	English	STATA JOURNAL	40-57	18	1	WOS:000240513700003	This paper presents a command, gist, for trend estimation across different exposure levels for either single or multiple summarized case-control, incidence-rate, and cumulative incidence data. This approach is based on constructing an approximate covariance estimate for the log relative risks and estimating a corrected linear trend using generalized least squares. For trend analysis of multiple studies, gist can estimate fixed- and random-effects metaregression models.
51785920	WOS:000240513700006	084EB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Goodness-of-fit test for a logistic regression model fitted using survey sample data	Journal	Article	2006	1	1	English	STATA JOURNAL	97-105	9	1	WOS:000240513700006	After a logistic regression model has been fitted, a global test of goodness of fit of the resulting model should be performed. A test that is commonly used to assess model fit is the Hosmer-Lemeshow test, which is available in Stata and most other statistical software programs. However, it is often of interest to fit a logistic regression model to sample survey data, such as data from the National Health Interview Survey or the National Health and Nutrition Examination Survey. Unfortunately, for such situations no goodness-of-fit testing procedures have been developed or implemented in available software. To address this problem, a Stata ado-command, svylogitgof, for estimating the F-adjusted mean residual test after svy: logit or svy: logistic estimation has been developed, and this paper describes its implementation.
51797391	WOS:000240513700001	084EB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating variance components in Stata	Journal	Article	2006	1	1	English	STATA JOURNAL	1-21	21	1	WOS:000240513700001	This article gives a brief overview of the popular methods for estimating variance components in linear models and describes several ways to obtain such estimates in Stata for various experimental designs. The article's emphasis is on using xtmixed to estimate variance components. Prior to Stata 9, loneway could be used to estimate variance components for one-way random-effects models. For other experimental designs, variance components could be computed manually using saved results after anova. The latter approach is viable but requires tedious computations for complicated experimental designs. Instead, as of Stata 9, variance components are easily obtained by using xtmixed.
51820329	WOS:000240514400004	084EI	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Graphical representation of interactions	Journal	Article	2006	1	1	English	STATA JOURNAL	348-363	16	1	WOS:000240514400004	We provide a program to illustrate interactions between treatment and covariates or between two covariates by using forest plots under either the Cox proportional hazards or the logistic regression model. The program is flexible in both the possibility of illustrating more than one interaction at a time and variable specifications of scale.
51824847	WOS:000240513700009	084EB	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Time of day	Journal	Article	2006	1	1	English	STATA JOURNAL	124-137	14	1	WOS:000240513700009	Many problems in statistical analysis include time-of-day variables, but Stata offers limited support for time-of-day calculations. Support is needed for dates with times, times alone, and durations or timings. This article presents two new programs as general utilities to convert back and forth between string and numeric representations.
51842254	WOS:000240206500001	079VR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Object-oriented computation of sandwich estimators	Journal	Article	2006	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000240206500001	Sandwich covariance matrix estimators are a popular tool in applied regression modeling for performing inference that is robust to certain types of model misspecification. Suitable implementations are available in the R system for statistical computing for certain model fitting functions only ( in particular lm()), but not for other standard regression functions, such as glm(), nls(), or survreg(). Therefore, conceptual tools and their translation to computational tools in the package sandwich are discussed, enabling the computation of sandwich estimators in general parametric models. Object orientation can be achieved by providing a few extractor functions - most importantly for the empirical estimating functions - from which various types of sandwich estimators can be computed.
51913580	WOS:000236800900001	032UL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A computer program to calculate two-stage short-run control chart factors for (X, MR) charts	Journal	Article	2006	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	13	1	WOS:000236800900001	This paper is the second in a series of two papers that fully develops two-stage short-run ( X, MR) control charts. This paper describes the development and execution of a computer program that accurately calculates first- and second-stage short-run control chart factors for ( X, MR) charts using the equations derived in the first paper. The software used is Mathcad. The program accepts values for number of subgroups, alpha for the X chart, and alpha for the MR chart both above the upper control limit and below the lower control limit. Tables are generated for specific values of these inputs and the implications of the results are discussed. A numerical example illustrates the use of the program.
51916119	WOS:000239139600001	065DM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Ratios of normal variables	Journal	Article	2006	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-10	10	1	WOS:000239139600001	This article extends and amplifies on results from a paper of over forty years ago. It provides software for evaluating the density and distribution functions of the ratio z/w for any two jointly normal variates z, w, and provides details on methods for transforming a general ratio z/w into a standard form, (a+x)/(b+y), with x and y independent standard normal and a, b non-negative constants. It discusses handling general ratios when, in theory, none of the moments exist yet practical considerations suggest there should be approximations whose adequacy can be verified by means of the included software. These approximations show that many of the ratios of normal variates encountered in practice can themselves be taken as normally distributed. A practical rule is developed: If a < 2.256 and 4 < b then the ratio (a+x)/(b+y) is itself approximately normally distributed with mean mu = a/(1.01b -.2713) and variance sigma(2) = (a(2) + 1)/(b(2) +.108b - 3.795) - mu(2).
51950154	WOS:000235181000001	010IJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	WhatIf: R software for evaluating counterfactuals	Journal	Article	2006	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	18	1	WOS:000235181000001	WhatIf is an R package that implements the methods for evaluating counterfactuals introduced in King and Zeng (2006a) and King and Zeng (2006b). It offers easy-to-use techniques for assessing a counterfactual's model dependence without having to conduct sensitivity testing over specified classes of models. These same methods can be used to approximate the common support of the treatment and control groups in causal inference.
51989121	WOS:000240514000006	084EE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Maximum simulated likelihood estimation of a negative binomial regression model with multinomial endogenous treatment	Journal	Article	2006	1	1	English	STATA JOURNAL	246-255	10	1	WOS:000240514000006	We describe specification and estimation of a multinomial treatment effects negative binomial regression model. A latent factor structure is used to accommodate selection into treatment, and a simulated likelihood method is used for estimation. We describe its implementation via the mtreatnb command.
52011001	WOS:000237294400001	039GQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Formulating state space models in R with focus on longitudinal regression models	Journal	Article	2006	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	15	1	WOS:000237294400001	We provide a language for formulating a range of state space models with response densities within the exponential family. The described methodology is implemented in the R-package sspir. A state space model is specified similarly to a generalized linear model in R, and then the time-varying terms are marked in the formula. Special functions for specifying polynomial time trends, harmonic seasonal patterns, unstructured seasonal patterns and time-varying covariates can be used in the formula. The model is fitted to data using iterated extended Kalman filtering, but the formulation of models does not depend on the implemented method of inference. The package is demonstrated on three datasets.
52069094	WOS:000242545700001	112RW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ltm: An R package for latent variable modeling and item response theory analyses	Journal	Article	2006	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	25	1	WOS:000242545700001	The R package ltm has been developed for the analysis of multivariate dichotomous and polytomous data using latent variable models, under the Item Response Theory approach. For dichotomous data the Rasch, the Two-Parameter Logistic, and Birnbaum's Three-Parameter models have been implemented, whereas for polytomous data Semejima's Graded Response model is available. Parameter estimates are obtained under marginal maximum likelihood using the Gauss-Hermite quadrature rule. The capabilities and features of the package are illustrated using two real data examples.
52101632	WOS:000244769500002	144BG	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Rasch analysis: Estimation and tests with raschtest	Journal	Article	2007	1	1	English	STATA JOURNAL	22-44	23	1	WOS:000244769500002	Analyzing latent variables is becoming more and more important in several fields, such as clinical research, psychology, educational sciences, ecology, and epidemiology. The item response theory allows analyzing latent variables measured by questionnaires of items with binary or ordinal responses. The Rasch model is the best known model of this theory for binary responses. Although one can estimate the parameters of the Rasch model with the clogit or xtlogit command (or with the unofficial gllamm command), these commands require special data preparation. The proposed raschtest command easily allows estimating the parameters of the Rasch model and fitting the resulting model.
52104401	WOS:000244769500004	144BG	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Sensitivity analysis for average treatment effects	Journal	Article	2007	1	1	English	STATA JOURNAL	71-83	13	1	WOS:000244769500004	Based on the conditional independence or unconfoundedness assumption, matching has become a popular approach to estimate average treatment effects. Checking the sensitivity of the estimated results with respect to deviations from this identifying assumption has become an increasingly important topic in the applied evaluation literature. If there are unobserved variables that affect assignment into treatment and the outcome variable simultaneously, a, hidden bias might arise to which matching estimators are not robust. We address this problem with the bounding approach proposed by Rosenbaum (Observational Studies, 2nd ed., New York: Springer), where mhbounds lets the researcher determine how strongly an unmeasured variable must influence the selection process to undermine the implications of the matching analysis.
52117394	WOS:000246391500001	166PP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	spBayes: An R package for univariate and multivariate hierarchical point-referenced spatial models	Journal	Article	2007	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	24	1	WOS:000246391500001	Scientists and investigators in such diverse fields as geological and environmental sciences, ecology, forestry, disease mapping, and economics often encounter spatially referenced data collected over a fixed set of locations with coordinates ( latitude - longitude, Easting - Northing etc.) in a region of study. Such point-referenced or geostatistical data are often best analyzed with Bayesian hierarchical models. Unfortunately, fitting such models involves computationally intensive Markov chain Monte Carlo (MCMC) methods whose efficiency depends upon the specific problem at hand. This requires extensive coding on the part of the user and the situation is not helped by the lack of available software for such algorithms. Here, we introduce a statistical software package, spBayes, built upon the R statistical computing platform that implements a generalized template encompassing a wide variety of Gaussian spatial process models for univariate as well as multivariate point-referenced data. We discuss the algorithms behind our package and illustrate its use with a synthetic and real data example.
52119248	WOS:000252431500001	252FD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Algorithms for linear time series analysis: With R package	Journal	Article	2007	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	26	1	WOS:000252431500001	Our ltsa package implements the Durbin-Levinson and Trench algorithms and provides a general approach to the problems of fitting, forecasting and simulating linear time series models as well as fitting regression models with linear time series errors. For computational efficiency both algorithms are implemented in C and interfaced to R. Examples are given which illustrate the efficiency and accuracy of the algorithms. We provide a second package FGN which illustrates the use of the ltsa package with fractional Gaussian noise (FGN). It is hoped that the ltsa will provide a base for further time series software.
52152266	WOS:000247011400001	175KB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Estimation of models in a Rasch family for polytomous items and multiple latent variables	Journal	Article	2007	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	36	1	WOS:000247011400001	The Rasch family of models considered in this paper includes models for polytomous items and multiple correlated latent traits, as well as for dichotomous items and a single latent variable. An R package is described that computes estimates of parameters and robust standard errors of a class of log-linear-by-linear association (LLLA) models, which are derived from a Rasch family of models. The LLLA models are special cases of log-linear models with bivariate interactions. Maximum likelihood estimation of LLLA models in this form is limited to relatively small problems; however, pseudo-likelihood estimation overcomes this limitation. Maximizing the pseudo-likelihood function is achieved by maximizing the likelihood of a single conditional multinomial logistic regression model. The parameter estimates are asymptotically normal and consistent. Based on our simulation studies, the pseudo-likelihood and maximum likelihood estimates of the parameters of LLLA models are nearly identical and the loss of efficiency is negligible. Recovery of parameters of Rasch models fit to simulated data is excellent.
52161070	WOS:000252428300001	252ED	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	elrm: Software implementing exact-like inference for logistic regression models	Journal	Article	2007	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000252428300001	Exact inference is based on the conditional distribution of the sufficient statistics for the parameters of interest given the observed values for the remaining sufficient statistics. Exact inference for logistic regression can be problematic when data sets are large and the support of the conditional distribution cannot be represented in memory. Additionally, these methods are not widely implemented except in commercial software packages such as LogXact and SAS. Therefore, we have developed elrm, software for R implementing ( approximate) exact inference for binomial regression models from large data sets. We provide a description of the underlying statistical methods and illustrate the use of elrm with examples. We also evaluate elrm by comparing results with those obtained using other methods.
52161079	WOS:000252430900001	252EZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	On fractional Gaussian random fields simulations	Journal	Article	2007	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000252430900001	To simulate Gaussian fields poses serious numerical problems: storage and computing time. The midpoint displacement method is often used for simulating the fractional Brownian fields because it is fast. We propose an effective and fast method, valid not only for fractional Brownian fields, but for any Gaussian fields. First, our method is compared with midpoint for fractional Brownian fields. Second, the performance of our method is illustrated by simulating several Gaussian fields. The software FieldSim is an R package developed in R and C and that implements the procedures on which this paper focuses.
52181046	WOS:000247012200001	175KJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Mokken scale analysis in R	Journal	Article	2007	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	19	1	WOS:000247012200001	Mokken scale analysis (MSA) is a scaling procedure for both dichotomous and polytomous items. It consists of an item selection algorithm to partition a set of items into Mokken scales and several methods to check the assumptions of two nonparametric item response theory models: the monotone homogeneity model and the double monotonicity model. First, we present an R package mokken for MSA and explain the procedures. Second, we show how to perform MSA in R using test data obtained with the Adjective Checklist.
52198053	WOS:000254886000006	286ZQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Counting groups, especially panels	Journal	Article	2007	1	1	English	STATA JOURNAL	571-581	11	1	WOS:000254886000006	Counting panels, and more generally groups, is sometimes possible in Stata through a reduction command (e.g., collapse, contract, statsby) that produces a smaller dataset or through a tabulation command. Yet there are also many problems, especially with irregular sets of observations for varying times, that do not yield easily to this approach. This column focuses on techniques for answering such questions while maintaining the same data structure. Especially useful are the Stata commands by: and egen and indicator variables constructed for the purpose. With by: we often exploit the fact that subscripts are defined within group, not within dataset. egen functions are often used to produce group-level statistics. Tagging each group just once ensures that summaries, including counts, are of groups, not individual observations.
52217924	WOS:000244769500006	144BG	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	File filtering in Stata: Handling complex data formats and navigating log files efficiently	Journal	Article	2007	1	1	English	STATA JOURNAL	98-105	8	1	WOS:000244769500006	A text file filter is a program that converts one text file into another on the basis of a set of rules. For statistical applications, a text file filter can convert data embedded in a complicated text file so that Stata can read and analyze it. A text file filter can also automate the production of more user-friendly output from long Stata log files. The file command lets you use text file filters in Stata. This article reviews some key programming points for successful implementation of such filters.
52219272	WOS:000244769500009	144BG	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of An Introduction to Modern Econometrics Using Stata by Baum	Journal	Review	2007	1	1	English	STATA JOURNAL	131-136	6	1	WOS:000244769500009	This article reviews An Introduction to Modern Econometrics Using Stata by Christopher F. Baum.
52219954	WOS:000244067900001	134FC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	FluxSimulator: An R package to simulate isotopomer distributions in metabolic networks	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	17	1	WOS:000244067900001	The representation of biochemical knowledge in terms of fluxes ( transformation rates) in a metabolic network is often a crucial step in the development of new drugs and efficient bioreactors. Mass spectroscopy ( MS) and nuclear magnetic resonance spectroscopy ( NMRS) in combination with C-13 labeled substrates are experimental techniques resulting in data that may be used to quantify fluxes in the metabolic network underlying a process. The massive amount of data generated by spectroscopic experiments increasingly requires software which models the dynamics of the underlying biological system. In this work we present an approach to handle isotopomer distributions in metabolic networks using an object-oriented programming approach, implemented using S4 classes in R. The developed package is called FluxSimulator and provides a user friendly interface to specify the topological information of the metabolic network as well as carbon atom transitions in plain text files. The package automatically derives the mathematical representation of the formulated network, and assembles a set of ordinary differential equations ( ODEs) describing the change of each isotopomer pool over time. These ODEs are subsequently solved numerically. In a case study FluxSimulator was applied to an example network. Our results indicate that the package is able to reproduce exact changes in isotopomer compositions of the metabolite pools over time at given flux rates.
52242354	WOS:000252429100001	252EK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	NonbinROC: Software for evaluating accuracies with non-binary gold diagnostic standards	Journal	Article	2007	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-10	10	1	WOS:000252429100001	ROC analysis is a standard method for estimating and comparing diagnostic tests' accuracies when the gold standard is binary. However, there are many situations when the gold standard is not binary. In these situations, traditional ROC methods applied have lead to biased and uninformative outcomes. This article introduces nonbinROC, software for R that implements nonparametric estimators proposed by Obuchowski (2005) for estimating and comparing diagnostic tests' accuracies when the gold standard is measured on a continuous, ordinal or nominal scale. The results produced from these estimators are interpreted in the same manner as in ROC analysis but are not associated with any ROC curve.
52243692	WOS:000252429300001	252EL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	boa: An R package for MCMC output convergence assessment and posterior inference	Journal	Article	2007	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-37	37	1	WOS:000252429300001	Markov chain Monte Carlo (MCMC) is the most widely used method of estimating joint posterior distributions in Bayesian analysis. The idea of MCMC is to iteratively produce parameter values that are representative samples from the joint posterior. Unlike frequentist analysis where iterative model fitting routines are monitored for convergence to a single point, MCMC output is monitored for convergence to a distribution. Thus, specialized diagnostic tools are needed in the Bayesian setting. To this end, the R package boa was created. This manuscript presents the user's manual for boa, which out lines the use of and methodology upon which the software is based. Included is a description of the menu system, data management capabilities, and statistical/graphical methods for convergence assessment and posterior inference. Throughout the manual, a linear regression example is used to ilustrate the software.
52271019	WOS:000244068000001	134FD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fluorescence Lifetime Imaging Microscopy (FLIM) data analysis with TIMP	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	20	1	WOS:000244068000001	Fluorescence Lifetime Imaging Microscopy ( FLIM) allows fluorescence lifetime images of biological objects to be collected at 250 nm spatial resolution and at ( sub-) nanosecond temporal resolution. Often n(comp) kinetic processes underlie the observed fluorescence at all locations, but the intensity of the fluorescence associated with each process varies per-location, i. e., per-pixel imaged. Then the statistical challenge is global analysis of the image: use of the fluorescence decay in time at all locations to estimate the ncomp lifetimes associated with the kinetic processes, as well as the amplitude of each kinetic process at each location. Given that typical FLIM images represent on the order of 10(2) timepoints and 10(3) locations, meeting this challenge is computationally intensive. Here the utility of the TIMP package for R to solve parameter estimation problems arising in FLIM image analysis is demonstrated. Case studies on simulated and real data evidence the applicability of the partitioned variable projection algorithm implemented in TIMP to the problem domain, and showcase options included in the package for the visual validation of models for FLIM data.
52305377	WOS:000248072200008	190PV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of A Handbook of Statistical Analyses Using Stata, Fourth Edition, by Rabe-Hesketh and Everitt	Journal	Review	2007	1	1	English	STATA JOURNAL	245-248	4	1	WOS:000248072200008	This article reviews A Handbook of Statistical Analyses Using Stata, Fourth Edition; by Sophia Rabe-Hesketh and Brian S. Everitt.
52337935	WOS:000252431800001	252FG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	gap: Genetic analysis package	Journal	Article	2007	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	18	1	WOS:000252431800001	A preliminary attempt at collecting tools and utilities for genetic data as an R package called gap is described. Genomewide association is then described as a specific example, linking the work of Risch and Merikangas (1996), Long and Langley (1997) for family-based and population-based studies, and the counterpart for case-cohort design established by Cai and Zeng (2004). Analysis of staged design as outlined by Skol et al. (2006) and associate methods are discussed. The package is flexible, customizable, and should prove useful to researchers especially in its application to genomewide association studies.
52347266	WOS:000252429800001	252EQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The ade4 package: Implementing the duality diagram for ecologists	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000252429800001	Multivariate analyses are well known and widely used to identify and understand structures of ecological communities. The ade4 package for the R statistical environment proposes a great number of multivariate methods. Its implementation follows the tradition of the French school of "Analyse des Donnees" and is based on the use of the duality diagram. We present the theory of the duality diagram and discuss its implementation in ade4. Classes and main functions are presented. An example is given to illustrate the ade4 philosophy.
52362401	WOS:000254886000002	286ZQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Enhanced routines for instrumental variables/generalized method of moments estimation and testing	Journal	Article	2007	1	1	English	STATA JOURNAL	465-506	42	1	WOS:000254886000002	We extend our 2003 paper on instrumental variables and generalized method of moments estimation, and we test and describe enhanced routines that address heteroskedasticity- and autocorrelation-consistent standard errors, weak instruments, limited-information maximum likelihood and k-class estimation, tests for endogeneity and Ramsey's regression specification-error test, and autocorrelation tests for instrumental variable estimates and panel-data instrumental variable estimates.
52382163	WOS:000249929700006	217DX	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Fitting mixed logit models by using maximum simulated likelihood	Journal	Article	2007	1	1	English	STATA JOURNAL	388-401	14	1	WOS:000249929700006	This article describes the mixlogit Stata command for fitting mixed logit models by using maximum simulated likelihood.
52383841	WOS:000247011600001	175KD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Modeling of responses and response times with the package cirt	Journal	Article	2007	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	14	1	WOS:000247011600001	In computerized testing, the test takers' responses as well as their response times on the items are recorded. The relationship between response times and response accuracies is complex and varies over levels of observation. For example, it takes the form of a tradeoff between speed and accuracy at the level of a fixed person but may become a positive correlation for a population of test takers. In order to explore such relationships and test hypotheses about them, a conjoint model is proposed. Item responses are modeled by a two-parameter normal-ogive IRT model and response times by a lognormal model. The two models are combined using a hierarchical framework based on the fact that response times and responses are nested within individuals. All parameters can be estimated simultaneously using an MCMC estimation approach. A R package for the MCMC algorithm is presented and explained.
52423247	WOS:000244067400001	134EY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	amsrpm: Robust point matching for retention time alignment of LC/MS data with R	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	12	1	WOS:000244067400001	Proteomics is the study of the abundance, function and dynamics of all proteins present in a living organism, and mass spectrometry ( MS) has become its most important tool due to its unmatched sensitivity, resolution and potential for high-throughput experimentation. A frequently used variant of mass spectrometry is coupled with liquid chromatography ( LC) and is denoted as "LC/MS". It produces two-dimensional raw data, where significant distortions along one of the dimensions can occur between different runs on the same instrument, and between instruments. A compensation of these distortions is required to allow for comparisons between and inference based on different experiments. This article introduces the amsrpm software package. It implements a variant of the Robust Point Matching ( RPM) algorithm that is tailored for the alignment of LC and LC/MS experiments. Problem-specific enhancements include a specialized dissimilarity measure, and means to enforce smoothness and monotonicity of the estimated transformation function. The algorithm does not rely on pre-specified landmarks, it is insensitive towards outliers and capable of modeling nonlinear distortions. Its usefulness is demonstrated using both simulated and experimental data. The software is available as an open source package for the statistical programming language R.
52430038	WOS:000248072200002	190PV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Maximum likelihood and two-step estimation of an ordered-probit selection model	Journal	Article	2007	1	1	English	STATA JOURNAL	167-182	16	1	WOS:000248072200002	We discuss the estimation of a regression model with an ordered-probit selection rule. We have written a Stata command, oheckman, that computes two-step and full-information rnaximum-likelihood estimates of this model. Using Monte Carlo simulations, we compare the performances of these estimators under various conditions.
52433132	WOS:000252431600001	252FE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Inference in graphical Gaussian models with edge and vertex symmetries with the gRc package for R	Journal	Article	2007	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	26	1	WOS:000252431600001	In this paper we present the R package gRc for statistical inference in graphical Gausian models in which symmetry restrictions have been imposed on the concentration or partial correlation matrix. The models are represented by coloured graphs where parameters associated with edges or vertices of same colour are restricted tobeing identical. We describe algorithms for maximum likelihood estimate and discuss model selection issues. The paper illustrates the practical use of the gRc package.
52466628	WOS:000249929700003	217DX	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Simulation-based sensitivity analysis for matching estimators	Journal	Article	2007	1	1	English	STATA JOURNAL	334-350	17	1	WOS:000249929700003	This article presents a Stata program (sensatt) that implements the sensitivity analysis for matching estimators proposed by Ichino, Mealli, and Nannicini (Journal of Applied Econometrics, forthcoming). The analysis simulates a potential confounder to assess the robustness of the estimated treatment effects with respect to deviations from the conditional independence assumption. The program uses the commands for propensity-score matching (att*) developed by Becker and Ichino (Stata Journal 2: 358-377). I give an example by using the National Supported Work demonstration, widely known in the program evaluation literature.
52510336	WOS:000252428800001	252EH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ks: Kernel density estimation and kernel discriminant analysis for multivariate data in R	Journal	Article	2007	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000252428800001	Kernel smoothing is one of the most widely used non-parametric data smoothing techniques. We introduce a new R package ks for multivariate kernel smoothing. Currently it contains functionality for kernel density estimation and kernel discriminant analysis. It is a comprehensive package for bandwidth matrix selection, implementing a wide range of data-driven diagonal and unconstrained bandwidth selectors.
52510757	WOS:000244068500001	134FI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Using R-based VOStat as a low-resolution spectrum analysis tool	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	12	1	WOS:000244068500001	We describe here an online software suite VOStat written mainly for the Virtual Observatory, a novel structure in which astronomers share terabyte scale data. Written mostly in the public-domain statistical computing language and environment R, it can do a variety of statistical analysis on multidimensional, multi-epoch data with errors. Included are techniques which allow astronomers to start with multi-color data in the form of low-resolution spectra and select special kinds of sources in a variety of ways including color outliers. Here we describe the tool and demonstrate it with an example from Palomar-QUEST, a synoptic sky survey.
52546410	WOS:000247011300001	175KA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multilevel IRT modeling in practice with the package mlirt	Journal	Article	2007	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000247011300001	Variance component models are generally accepted for the analysis of hierarchical structured data. A shortcoming is that outcome variables are still treated as measured without an error. Unreliable variables produce biases in the estimates of the other model parameters. The variability of the relationships across groups and the group-effects on individuals' outcomes differ substantially when taking the measurement error in the dependent variable of the model into account. The multilevel model can be extended to handle measurement error using an item response theory (IRT) model, leading to a multilevel IRT model. This extended multilevel model is in particular suitable for the analysis of educational response data where students are nested in schools and schools are nested within cities/countries.
52579847	WOS:000252430700001	252EY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Introducing untb, an R package for simulating ecological drift under the unified neutral theory of Biodiversity	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000252430700001	The distribution of abundance amongst species with similar ways of life is a classical problem in ecology. The unified neutral theory of biodiversity, due to Hubbell, states that observed population dynamics may be explained on the assumption of per capita equivalence amongst individuals. One can thus dispense with differences between species, and differences between abundant and rare species: all individuals behave alike in respect of their probabilities of reproducing and death. It is a striking fact that such a parsimonious theory results in a non-trivial dominance-diversity curve ( that is, the simultaneous existence of both abundant and rare species) and even more striking that the theory predicts abundance curves that match observations across a wide range of ecologies. This paper introduces the untb package of R routines, for numerical simulation of ecological drift under the unified neutral theory. A range of visualization, analytical, and simulation tools are provided in the package and these are presented with examples in the paper.
52596865	WOS:000244067800001	134FB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Model-based methods of classification: Using the mclust software in chemometrics	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	13	1	WOS:000244067800001	Due to recent advances in methods and software for model-based clustering, and to the interpretability of the results, clustering procedures based on probability models are increasingly preferred over heuristic methods. The clustering process estimates a model for the data that allows for overlapping clusters, producing a probabilistic clustering that quantifies the uncertainty of observations belonging to components of the mixture. The resulting clustering model can also be used for some other important problems in multivariate analysis, including density estimation and discriminant analysis. Examples of the use of model-based clustering and classification techniques in chemometric studies include multivariate image analysis, magnetic resonance imaging, microarray image segmentation, statistical process control, and food authenticity. We review model-based clustering and related methods for density estimation and discriminant analysis, and show how the R package mclust can be applied in each instance.
52599463	WOS:000246391300001	166PN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Numerical computing and graphics for the power method transformation using Mathematica	Journal	Article	2007	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	17	1	WOS:000246391300001	This paper provides the requisite information and description of software that perform numerical computations and graphics for the power method polynomial transformation. The software developed is written in the Mathematica 5.2 package PowerMethod. m and is associated with fifth-order polynomials that are used for simulating univariate and multivariate non-normal distributions. The package is flexible enough to allow a user the choice to model theoretical pdfs, empirical data, or a user's own selected distribution(s). The primary functions perform the following ( a) compute standardized cumulants and polynomial coefficients, (b) ensure that polynomial transformations yield valid pdfs, and ( c) graph power method pdfs and cdfs. Other functions compute cumulative probabilities, modes, trimmed means, intermediate correlations, or perform the graphics associated with fitting power method pdfs to either empirical or theoretical distributions. Numerical examples and Monte Carlo results are provided to demonstrate and validate the use of the software package. The notebook Demo. nb is also provided as a guide for user of the power method.
52648494	WOS:000254886000004	286ZQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A generic function evaluator implemented in Mata	Journal	Article	2007	1	1	English	STATA JOURNAL	542-555	14	1	WOS:000254886000004	Background: When implementing new statistical procedures, there is often a need for simple-and yet computationally efficient-ways of numerically evaluating composite distribution functions. If the statistical procedure must support calculations for censored and noncensored cases, those calculations should be carried out using efficient computational implementations of both definite and indefinite integrals (e.g., calculation of tail areas of distribution functions). Method: We developed a generic function evaluator such that users may specify a function using reverse Polish notation. As its argument the function evaluator takes a matrix of pointers and then applies the rows of this matrix to its internally defined stack of pointers. Accordingly, each row of the argument matrix defines a single operation such as evaluating a function on the current element of the stack, applying an algebraic operation to the two top elements of the stack, or manipulating the stack itself. Defining new composite distribution functions from other (atomic) distribution functions then corresponds to joining two or more function-defining matrices vertically. This approach can further be used to obtain integrals of any defined function. As an example we show how the density and distribution function for the minimum of two Weibull distributed random variables can be numerically evaluated and integrated. Results: The procedure provides a flexible and extensible framework for implementing numerical evaluation of general, composite distributions. The procedure is numerically relatively efficient, although not optimal.
52660717	WOS:000244769500008	144BG	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Making it count	Journal	Article	2007	1	1	English	STATA JOURNAL	117-130	14	1	WOS:000244769500008	The count command has one simple role, to count observations in general or that satisfy some condition(s). This task can be useful when some larger problem pivots on counting, especially if count is used with a loop over observations or variables. I use various problems, mostly of data management, as examples. I also make comparisons with the use of _N, summarize, and egen for the same or similar problems.
52669436	WOS:000247011700001	175KE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Confidence intervals for standardized effect sizes: Theory, application, and implementation	Journal	Article	2007	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000247011700001	The behavioral, educational, and social sciences are undergoing a paradigmatic shift in methodology, from disciplines that focus on the dichotomous outcome of null hypothesis significance tests to disciplines that report and interpret effect sizes and their corresponding confidence intervals. Due to the arbitrariness of many measurement instruments used in the behavioral, educational, and social sciences, some of the most widely reported effect sizes are standardized. Although forming confidence intervals for standardized effect sizes can be very beneficial, such confidence interval procedures are generally difficult to implement because they depend on noncentral t, F, and chi(2) distributions. At present, no main-stream statistical package provides exact confidence intervals for standardized effects without the use of specialized programming scripts. Methods for the Behavioral, Educational, and Social Sciences (MBESS) is an R package that has routines for calculating confidence intervals for noncentral t, F, and chi(2) distributions, which are then used in the calculation of exact confidence intervals for standardized effect sizes by using the confidence interval transformation and inversion principles. The present article discusses the way in which confidence intervals are formed for standardized effect sizes and illustrates how such confidence intervals can be easily formed using MBESS in R.
52690686	WOS:000249929700001	217DX	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Robust standard errors for panel regressions with cross-sectional dependence	Journal	Article	2007	1	1	English	STATA JOURNAL	281-312	32	1	WOS:000249929700001	I present a new Stata program, xtscc, that estimates pooled ordinary least-squares/weighted least-squares regression and fixed-effects (within) regression models with Driscoll and Kraay (Review of Economics and Statistics 80: 549-560) standard errors. By running Monte Carlo simulations, I compare the finite-sample properties of the cross-sectional dependence-consistent Driscoll-Kraay estimator with the properties of other, more commonly used covariance matrix estimators that do not account for cross-sectional dependence. The results indicate that Driscoll-Kraay standard errors are well calibrated when cross-sectional dependence is present. However, erroneously ignoring cross-sectional correlation in the estimation of panel models can lead to severely biased statistical results. I illustrate the xtscc program by considering an application from empirical finance. Thereby, I also propose a Hausman-type test for fixed effects that is robust to general forms of cross-sectional and temporal dependence.
52758341	WOS:000247010700001	175JU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Estimating the multilevel Rasch model: with the lme4 package	Journal	Article	2007	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	18	1	WOS:000247010700001	Traditional Rasch estimation of the item and student parameters via marginal maximum likelihood, joint maximum likelihood or conditional maximum likelihood, assume individuals in clustered settings are uncorrelated and items within a test that share a grouping structure are also uncorrelated. These assumptions are often violated, particularly in educational testing situations, in which students are grouped into classrooms and many test items share a common grouping structure, such as a content strand or a reading passage. Consequently, one possible approach is to explicitly recognize the clustered nature of the data and directly incorporate random effects to account for the various dependencies. This article demonstrates how the multilevel Rasch model can be estimated using the functions in R for mixed-effects models with crossed or partially crossed random effects. We demonstrate how to model the following hierarchical data structures: a) individuals clustered in similar settings ( e. g., classrooms, schools), b) items nested within a particular group ( such as a content strand or a reading passage), and c) how to estimate a teacher x content strand interaction.
52762718	WOS:000252429500001	252EN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Introduction to the special volume on "ecology and ecological modelling in R"	Journal	Editorial Material	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-7	7	1	WOS:000252429500001	The third special volume in the "Foometrics in R" series of the Journal of Statistical Software collects a number of contributions describing statistical methodology and corresponding implementations related to ecology and ecological modelling. The scope of the papers ranges from theoretical ecology and ecological modelling to statistical methodology relevant for data analyses in ecological applications.
52789918	WOS:000249734500001	214JT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	accuracy: Tools for accurate and reliable statistical computing	Journal	Article	2007	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	30	1	WOS:000249734500001	Most empirical social scientists are surprised that low-level numerical issues in software can have deleterious effects on the estimation process. Statistical analyses that appear to be perfectly successful can be invalidated by concealed numerical problems. We have developed a set of tools, contained in accuracy, a package for R and S-PLUS, to diagnose problems stemming from numerical and measurement error and to improve the accuracy of inferences. The tools included in accuracy include a framework for gauging the computational stability of model results, tools for comparing model results, optimization diagnostics, and tools for collecting entropy for true random numbers generation.
52816435	WOS:000252430200001	252EU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Statistical methods for the qualitative assessment of dynamic models with time delay (R package qualV)	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000252430200001	Results of ecological models differ, to some extent, more from measured data than from empirical knowledge. Existing techniques for validation based on quantitative assessments sometimes cause an underestimation of the performance of models due to time shifts, accelerations and delays or systematic differences between measurement and simulation. However, for the application of such models it is often more important to reproduce essential patterns instead of seemingly exact numerical values. This paper presents techniques to identify patterns and numerical methods to measure the consistency of patterns between observations and model results. An orthogonal set of deviance measures for absolute, relative and ordinal scale was compiled to provide informations about the type of difference. Furthermore, two different approaches accounting for time shifts were presented. The first one transforms the time to take time delays and speed differences into account. The second one describes known qualitative criteria dividing time series into interval units in accordance to their main features. The methods differ in their basic concepts and in the form of the resulting criteria. Both approaches and the deviance measures discussed are implemented in an R package. All methods are demonstrated by means of water quality measurements and simulation data. The proposed quality criteria allow to recognize systematic differences and time shifts between time series and to conclude about the quantitative and qualitative similarity of patterns.
52864444	WOS:000244769500001	144BG	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A survey on survey statistics: What is done and can be done in Stata	Journal	Article	2007	1	1	English	STATA JOURNAL	1-21	21	1	WOS:000244769500001	This article will survey issues in analyzing complex survey data and describe some of the capabilities of Stata for such analyses. We will briefly review key elements of survey design and explain the effects of different design features on bias and variance. We compare different methods of variance estimation for stratified and clustered samples and discuss the handling of survey weights. We will also give examples for the practical importance of Stata's survey capabilities.
52867613	WOS:000246391600001	166PQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Rcapture: Loglinear models for capture-recapture in R	Journal	Article	2007	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	31	1	WOS:000246391600001	This article introduces Rcapture, an R package for capture-recapture experiments. The data for analysis consists of the frequencies of the observable capture histories over the t capture occasions of the experiment. A capture history is a vector of zeros and ones where one stands for a capture and zero for a miss. Rcapture can fit three types of models. With a closed population model, the goal of the analysis is to estimate the size N of the population which is assumed to be constant throughout the experiment. The estimator depends on the way in which the capture probabilities of the animals vary. Rcapture features several models for these capture probabilities that lead to different estimators for N. In an open population model, immigration and death occur between sampling periods. The estimation of survival rates is of primary interest. Rcapture can fit the basic Cormack-Jolly-Seber and Jolly-Seber model to such data. The third type of models fitted by Rcapture are robust design models. It features two levels of sampling; closed population models apply within primary periods and an open population model applies between periods. Most models in Rcapture have a loglinear form; they are fitted by carrying out a Poisson regression with the R function glm. Estimates of the demographic parameters of interest are derived from the loglinear parameter estimates; their variances are obtained by linearization. The novel feature of this package is the provision of several new options for modeling capture probabilities heterogeneity between animals in both closed population models and the primary periods of a robust design. It also implements many of the techniques developed by R. M. Cormack for open population models.
52879507	WOS:000248072200009	190PV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Identifying spells	Journal	Article	2007	1	1	English	STATA JOURNAL	249-265	17	1	WOS:000248072200009	Spells in time series (and more generally in any kind of one-dimensional series) may be defined as sequences of observations that are homogeneous in some sense. For example, a categorical variable may remain in the same state, or values of a measured variable may satisfy the same true-false condition. Devices for working with spells in Stata include marking the start of each spell with indicator variables and tagging spells with integer codes. Panel data are easy to handle with the by: prefix. Some kinds of spell identification require two passes through the data; as when only spells of some minimum length are of interest or short gaps are tolerable within spells. Many questions concerning spells are easy to answer given careful use of by: and appropriate sort order, selection of just 1 observation from each panel or spell; and appreciation of the many functions written for egen. Caps before; between; and after spells can also be important, and I suggest a convention for handling them.
52883500	WOS:000254886000003	286ZQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Causal inference with observational data	Journal	Article	2007	1	1	English	STATA JOURNAL	507-541	35	1	WOS:000254886000003	Problems with inferring causal relationships from nonexperimental data are briefly reviewed, and four broad classes of methods designed to allow estimation of and inference about causal parameters are described: panel regression, matching or reweighting, instrumental variables, and regression discontinuity. Practical examples are offered, and discussion focuses on checking required assumptions to the extent possible.
52897687	WOS:000249929700005	217DX	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Profile likelihood for estimation and confidence intervals	Journal	Article	2007	1	1	English	STATA JOURNAL	376-387	12	1	WOS:000249929700005	Normal-based confidence intervals for a parameter of interest axe inaccurate when the sampling distribution of the estimate is nonnormal. The technique known as profile likelihood can produce confidence intervals with better coverage. It may be used when the model includes only the variable of interest or several other variables in addition. Profile-likelihood confidence intervals are particularly, useful in nonlinear models. The command pllf computes and plots the maximum likelihood estimate and profile likelihood-based confidence interval for one parameter in a wide variety of regression models.
52920695	WOS:000254886000001	286ZQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multiple imputation of missing values: further update of ice, with an emphasis on interval censoring	Journal	Article	2007	1	1	English	STATA JOURNAL	445-464	20	1	WOS:000254886000001	Multiple imputation of missing data continues to be a topic of considerable interest and importance to applied researchers. In this article, the ice package for multiple imputation is further updated. Special attention in this article is paid to imputing interval-censored observations, and a suggestion to use imputation of right-censored survival data to elucidate covariate effects graphically.
52948015	WOS:000244769500007	144BG	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Mata Matters: Subscripting	Journal	Article	2007	1	1	English	STATA JOURNAL	106-116	11	1	WOS:000244769500007	Mata is Stata's matrix language. In the Mata Matters column, we show how Mata can be used interactively to solve problems and as a programming language to add new features to Stata. Subscripting is the subject of this column. Stata has three subscripting modes, and two of them are about more than accessing an element of a vector or matrix. The advanced forms of subscripting can, by themselves, be the solution to some problems.
52982918	WOS:000249929700007	217DX	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	An exact and a Monte Carlo proposal to the Fisher-Pitman permutation tests for paired replicates and for independent samples	Journal	Article	2007	1	1	English	STATA JOURNAL	402-412	11	1	WOS:000249929700007	This article concerns the nonparametric Fisher-Pitman tests for paired replicates mid independent samples. After outlining the theory of exact tests. I derive Monte Carlo simulations for both of them. Simulations can be useful if one deals with many observations because of the complexity of the algorithms in regard to sample sizes. The tests are designed to be a, more powerful alternative to the Wilcoxon signed-rank test and the Wilcoxon-Mann-Whitney rank-sum test if the observations are given on at least an interval scale. The results gained by Monte Carlo versions of the tests are accurate enough in comparison to the exact versions. Finally, I give examples for using both supplemented tests.
52991467	WOS:000252429700001	252EP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Maximum likelihood method for predicting environmental conditions from assemblage composition: The R package bio.infer	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000252429700001	This paper provides a brief introduction to the R package bio. infer, a set of scripts that facilitates the use of maximum likelihood (ML) methods for predicting environmental conditions from assemblage composition. Environmental conditions can often be inferred from only biological data, and these inferences are useful when other sources of data are unavailable. ML prediction methods are statistically rigorous and applicable to a broader set of problems than more commonly used weighted averaging techniques. However, ML methods require a substantially greater investment of time to program algorithms and to perform computations. This package is designed to reduce the effort required to apply ML prediction methods.
52997906	WOS:000252431900001	252FH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	WinBUGSio: A SAS macro for the remote execution of WinBUGS	Journal	Article	2007	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	10	1	WOS:000252431900001	This is a macro which facilitates remote execution of WinBUGS from within SAS. The macro pre-processes data for WinBUGS, writes the WinBUGS batch-script, executes this script and reads in output statistics from the WinBUGS log-file back into SAS native format. The user specifies the input and output file names and directory path as well as the statistics to be monitored in WinBUGS. The code works best for a model that has already been set up and checked for convergence diagnostics within WinBUGS. An obvious extension of the use of this macro is for running simulations where the input and output files all have the same name but all that differs between simulation iterations is the input dataset. The functionality and syntax of the macro call are described in this paper and illustrated using a simple linear regression model.
53032698	WOS:000244067100001	134EV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The pls package: Principal component and partial least squares regression in R	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000244067100001	The pls package implements principal component regression ( PCR) and partial least squares regression ( PLSR) in R ( R Development Core Team 2006b), and is freely available from the Comprehensive R Archive Network ( CRAN), licensed under the GNU General Public License ( GPL). The user interface is modelled after the traditional formula interface, as exemplified by 1m. This was done so that people used to R would not have to learn yet another interface, and also because we believe the formula interface is a good way of working interactively with models. It thus has methods for generic functions like predict, update and coef. It also has more specialised functions like scores, loadings and RMSEP, and a flexible cross-validation system. Visual inspection and assessment is important in chemometrics, and the pls package has a number of plot functions for plotting scores, loadings, predictions, coefficients and RMSEP estimates. The package implements PCR and several algorithms for PLSR. The design is modular, so that it should be easy to use the underlying algorithms in other functions. It is our hope that the package will serve well both for interactive data analysis and as a building block for other functions or packages using PLSR or PCR. We will here describe the package and how it is used for data analysis, as well as how it can be used as a part of other packages. Also included is a section about formulas and data frames, for people not used to the R modelling idioms.
53040271	WOS:000252431400001	252FC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	systemfit: A package for estimating systems of simultaneous equations in R	Journal	Article	2007	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	40	1	WOS:000252431400001	Many statistical analyses (e. g., in econometrics, biostatistics and experimental design) are based on models containing systems of structurally related equations. The systemfit package provides the capability to estimate systems of linear equations within the R programming environment. For instance, this package can be used for "ordinary least squares" (OLS), "seemingly unrelated regression" (SUR), and the instrumental variable (IV) methods "two-stage least squares" (2SLS) and "three-stage least squares" (3SLS), where SUR and 3SLS estimations can optionally be iterated. Furthermore, the systemfit package provides tools for several statistical tests. It has been tested on a variety of datasets and its reliability is demonstrated.
53042423	WOS:000252431200001	252FB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	bcp: An R package for performing a Bayesian analysis of change point problems	Journal	Article	2007	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000252431200001	Barry and Hartigan (1993) propose a Bayesian analysis for change point problems. We provide a brief summary of selected work on change point problems, both preceding and following Barry and Hartigan. We outline Barry and Hartigan's approach and off er a new R package, pkgbcp (Erdman and Emerson 2007), implementing their analysis. We discuss two frequentist alternatives to the Bayesian analysis, the recursive circular binary segmentation algorithm (Olshen and Venkatraman 2004) and the dynamic programming algorithm of (Bai and Perron 2003). We illustrate the application of bcp with economic and microarray data from the literature.
53076835	WOS:000245822500001	158VF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Adaptive smoothing of digital images: the R package adimpro	Journal	Article	2007	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	17	1	WOS:000245822500001	Digital imaging has become omnipresent in the past years with a bulk of applications ranging from medical imaging to photography. When pushing the limits of resolution and sensitivity noise has ever been a major issue. However, commonly used non-adaptive filters can do noise reduction at the cost of a reduced effective spatial resolution only. Here we present a new package adimpro for R, which implements the propagation-separation approach by (Polzehl and Spokoiny 2006) for smoothing digital images. This method naturally adapts to different structures of different size in the image and thus avoids oversmoothing edges and fine structures. We extend the method for imaging data with spatial correlation. Furthermore we show how the estimation of the dependence between variance and mean value can be included. We illustrate the use of the package through some examples.
53080325	WOS:000244061900001	134DD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	RAGE: A Java-implemented visual random generator	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	10	1	WOS:000244061900001	Carefully designed Java applications turn out to be efficient and platform independent tools that can compete well with classical implementations of statistical software. The project presented here is an example underlining this statement for random variate generation. An end-user application called RAGE ( Random Variate Generator) is developed to generate random variates from probability distributions. A Java class library called JDiscreteLib has been designed and implemented for the simulation of random variables from the most usual discrete distributions inside RAGE. For each distribution, specific and general algorithms are available for this purpose. RAGE can also be used as an interactive simulation tool for data and data summary visualization.
53081378	WOS:000247011900001	175KG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Extended Rasch modeling: The eRm package for the application of IRT models in R	Journal	Article	2007	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	20	1	WOS:000247011900001	Item response theory models (IRT) are increasingly becoming established in social science research, particularly in the analysis of performance or attitudinal data in psychology, education, medicine, marketing and other fields where testing is relevant. We propose the R package eRm ( extended Rasch modeling) for computing Rasch models and several extensions. A main characteristic of some IRT models, the Rasch model being the most prominent, concerns the separation of two kinds of parameters, one that describes qualities of the subject under investigation, and the other relates to qualities of the situation under which the response of a subject is observed. Using conditional maximum likelihood (CML) estimation both types of parameters may be estimated independently from each other. IRT models are well suited to cope with dichotomous and polytomous responses, where the response categories may be unordered as well as ordered. The incorporation of linear structures allows for modeling the effects of covariates and enables the analysis of repeated categorical measurements. The eRm package fits the following models: the Rasch model, the rating scale model (RSM), and the partial credit model (PCM) as well as linear reparameterizations through covariate structures like the linear logistic test model (LLTM), the linear rating scale model (LRSM), and the linear partial credit model (LPCM). We use an unitary, efficient CML approach to estimate the item parameters and their standard errors. Graphical and numeric tools for assessing goodness-of-fit are provided.
53104918	WOS:000243591100001	127MT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	rpanel: Simple interactive controls for R functions using the tcltk package	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	18	1	WOS:000243591100001	In a variety of settings it is extremely helpful to be able to apply R functions through buttons, sliders and other types of graphical control. This is particularly true in plotting activities where immediate communication between such controls and a graphical display allows the user to interact with a plot in a very effective manner. The tcltk package provides extensive tools for this and the aim of the rpanel package is to provide simple and well documented functions which make these facilities as accessible as possible. In addition, the operations which form the basis of communication within tcltk are managed in a way which allows users to write functions with a more standard form of parameter passing. This paper describes the basic design of the software and illustrates it on a variety of examples of interactive control of graphics. The tkrplot system is used to allow plots to be integrated with controls into a single panel. An example of the use of a graphical image, and the ability to interact with this, is also discussed.
53113227	WOS:000247010800001	175JV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Correspondence analysis in R, with two- and three-dimensional graphics: The ca package	Journal	Article	2007	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	13	1	WOS:000247010800001	We describe an implementation of simple, multiple and joint correspondence analysis in R. The resulting package comprises two parts, one for simple correspondence analysis and one for multiple and joint correspondence analysis. Within each part, functions for computation, summaries and visualization in two and three dimensions are provided, including options to display supplementary points and perform subset analyses. Special emphasis has been put on the visualization functions that offer features such as different scaling options for biplots and three-dimensional maps using the rgl package. Graphical options include shading and sizing plot symbols for the points according to their contributions to the map and masses respectively.
53133157	WOS:000248072200005	190PV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	QIC program and model selection in GEE analyses	Journal	Article	2007	1	1	English	STATA JOURNAL	209-220	12	1	WOS:000248072200005	The generalized estimating equation (GEE) approach is a widely used statistical method in the analysis of longitudinal data in clinical and epidemiological studies. It is an extension of the generalized linear model (GLM) method to correlated data such that valid standard errors of the parameter estimates can be drawn. Unlike the GLM method, which is based on the maximum likelihood theory for independent observations, the GEE method is based on the quasilikelihood theory and no assumption is made about the distribution of response observations. Therefore. Akaike's information criterion, a widely used method for model selection in GLM; is not applicable to GEE directly. However, Pan (Biometrics 2001; 57: 120-125) proposed a model-selection method for GEE and termed it quasilikelihood under the independence model criterion. This criterion can also be used to select the best-working correlation structure. From Pan's methods, I developed a general Stata program, qic, that accommodates all the distribution and link functions and correlation structures available in Stata version 9. In this paper, I introduce this program and demonstrate how to use it to select the best working correlation structure and the best subset of covariates through two examples in longitudinal studies.
53140111	WOS:000248072200003	190PV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Two postestimation commands for assessing confounding effects in epidemiological studies	Journal	Article	2007	1	1	English	STATA JOURNAL	183-196	14	1	WOS:000248072200003	Confounding is a major issue in observational epidemiological studies. This paper describes two postestimation commands for assessing confounding effects. One command (confall) displays and plots all possible effect estimates against one of p-value, Akaike information criterion, or Bayesian information criterion. This computing-intensive procedure allows researchers to inspect the variability of the effect estimates from various possible models. Another command (chest) uses a step-wise approach to identify variables that have substantially changed the effect estimate. Both commands can be used after most common estimation commands in epidemiological studies, such as logistic regression, conditional logistic regression, Poisson regression, linear regression, and Cox proportional hazards models.
53147471	WOS:000248072200004	190PV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation of nonstationary heterogeneous panels	Journal	Article	2007	1	1	English	STATA JOURNAL	197-208	12	1	WOS:000248072200004	We introduce a new Stata command, xtpmg, for estimating nonstationary heterogeneous panels in which the number of groups and number of time-series observations are both large. Based on recent advances in the nonstationary panel literature, xtpmg provides three alternative estimators: a traditional fixed-effects estimator; the mean-group estimator of Pesaran and Smith (Estimating long-run relationships from dynamic heterogeneous panels, Journal of Econometrics 68: 79-113); and the pooled mean-group estimator of Pesaran, Shin, and Smith (Estimating long-run relationships in dynamic heterogeneous panels, DAE Working Papers Amalgamated Series 9721; Pooled mean group estimation of dynamic heterogeneous panels, Journal of the American Statistical Association 94: 621-634).
53170225	WOS:000244769500003	144BG	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multivariable modeling with cubic regression splines: A principled approach	Journal	Article	2007	1	1	English	STATA JOURNAL	45-70	26	1	WOS:000244769500003	Spline functions provide a useful and flexible basis for modeling relationships with continuous predictors. However, to limit instability and provide sensible regression models in the multivariable setting, a principled approach to model selection and function estimation is important. Here the multivariable fractional polynomials approach to model building is transferred to regression splines. The essential features are specifying a maximum acceptable complexity for each continuous function and applying a closed-test approach to each continuous predictor to simplify the model where possible. Important adjuncts are an initial choice of scale for continuous predictors (linear or logarithmic), which often helps one to generate realistic, parsimonious final models; a goodness-of-fit test for a parametric function of a predictor; and a preliminary predictor transformation to improve robustness.
53193079	WOS:000252429400001	252EM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Reshaping data with the reshape package	Journal	Article	2007	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000252429400001	This paper presents the reshape package for R, which provides a common framework for many types of data reshaping and aggregation. It uses a paradigm of 'melting' and 'casting', where the data are 'melted' into a form which distinguishes measured and identifying variables, and then 'cast' into a new shape, whether it be a data frame, list, or high dimensional array. The paper includes an introduction to the conceptual framework, practical advice for melting and casting, and a case study.
53225067	WOS:000248072200001	190PV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Improved generalized estimating equation analysis via xtqls for quasi-least squares in Stata	Journal	Article	2007	1	1	English	STATA JOURNAL	147-166	20	1	WOS:000248072200001	Quasi-least squares (QLS) is an alternative method for estimating the correlation parameters within the framework of the generalized estimating equation (GEE) approach for analyzing correlated cross-sectional and longitudinal data. This article summarizes the development of QLS that occurred in several reports and describes its use with the user-written program xtqls in Stata. Also, it demonstrates the following advantages of QLS: (1) QLS allows some correlation structures that have not yet been implemented in the framework of GEE, (2) QLS can be applied as an alternative to GEE if the GEE estimate is infeasible, and (3) QLS uses the same estimating equation for estimation of beta as GEE; as a result, QLS can involve programs already available for GEE. In particular, xtqls calls the Stata program xtgee within an iterative approach that alternates between updating estimates of the correlation parameter a and then using xtgee to solve the GEE for beta at the current estimate of a. The benefit of this approach is that after xtqls, all the usual postregression estimation commands are readily available to the user.
53227597	WOS:000244067600001	134EZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Chemical Informatics functionality in R	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000244067600001	The flexibility and scope of the R programming environment has made it a popular choice for statistical modeling and scientific prototyping in a number of fields. In the field of chemistry, R provides several tools for a variety of problems related to statistical modeling of chemical information. However, one aspect common to these tools is that they do not have direct access to the information that is available from chemical structures, such as contained in molecular descriptors. We describe the rcdk package that provides the R user with access to the CDK, a Java framework for cheminformatics. As a result, it is possible to read in a variety of molecular formats, calculate molecular descriptors and evaluate fingerprints. In addition, we describe the rpubchem that will allow access to the data in PubChem, a public repository of molecular structures and associated assay data for approximately 8 million compounds. Currently, the package allows access to structural information as well as some simple molecular properties from PubChem. In addition the package allows access to bio-assay data from the PubChem FTP servers.
53246836	WOS:000249929700004	217DX	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Modeling of the cure fraction in survival studies	Journal	Article	2007	1	1	English	STATA JOURNAL	351-375	25	1	WOS:000249929700004	Cure models are a special type of survival analysis model where it is assumed that there are a proportion of subjects who will never experience the event and thus the survival curve will eventually reach a plateau. In population-based cancer studies, cure is said to occur when the mortality (hazard) rate in the diseased group of individuals returns to the same level as that expected in the general population. The cure fraction is of interest to patients and a useful measure to monitor trends and differences in survival of curable disease. I will describe the strsmix and strsnmix commands, which fit the two main types of cure fraction model, namely, the mixture and nonmixture cure fraction models. These models allow incorporation of the expected background mortality rate and thus enable the modeling of relative survival when cure is a possibility. I give an example to illustrate the commands.
53252730	WOS:000244769500005	144BG	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Stata and the WeeW information system	Journal	Article	2007	1	1	English	STATA JOURNAL	84-97	14	1	WOS:000244769500005	The need for timely collection and analysis of epidemiological data is becoming of primary importance, e.g., for bioterrorism detection or epidemiological surveillance. Web-based information systems (WISs) may provide the needed technological support. Thus we present the WeeW (workflow-enabled epidemiological WIS) system-i.e., a WIS that helps epidemiologists, through workflow management, to effectively select remote centers, collect, and process the received data to produce conclusive technical reports. In detail, we show the functionalities of the WeeW system, its architecture, and particularly Stata's role in executing statistical analyses and producing graphs. Furthermore, we discuss the performance of the WeeW-Stata interface. Finally, we outline short conclusions regarding the advantages and drawbacks connected with the proposed solution.
53280539	WOS:000252430500001	252EX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Estimating and analyzing demographic models using the popbio package in R	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000252430500001	A complete assessment of population growth and viability from field census data often requires complex data manipulations, statistical routines, mathematical tools, programming environments, and graphical capabilities. We therefore designed an R package called popbio to facilitate both the construction and analysis of projection matrix models. The package consists primarily of the R translation of MATLAB code found in Caswell (2001) and Morris and Doak (2002) for the analysis of projection matrix models. The package also includes methods to estimate vital rates and construct projection matrix models from census data typically collected in plant demography studies. In these studies, vital rates can often be estimated directly from annual censuses of tagged individuals using transition frequency tables. Because the construction of projection matrix models requires careful management of census data, we describe the steps to construct a projection matrix in detail.
53294836	WOS:000252430000001	252ES	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Exploring habitat selection by wildlife with adehabitat	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000252430000001	Knowledge of the environmental features affecting habitat selection by animals is important for designing wildlife management and conservation policies. The package adehabitat for the R software is designed to provide a computing environment for the analysis and modelling of such relationships. This paper focuses on the preliminary steps of data exploration and analysis, performed prior to a more formal modelling of habitat selection. In this context, I illustrate the use of a factorial analysis, the K-select analysis. This method is a factorial decomposition of marginality, one measure of habitat selection. This method was chosen to present the package because it illustrates clearly many of its features ( home range estimation, spatial analyses, graphical possibilities, etc.). I strongly stress the powerful capabilities of factorial methods for data analysis, using as an example the analysis of habitat selection by the wild boar ( Sus scrofa L.) in a Mediterranean environment.
53320365	WOS:000249734800001	214JW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The WaveD transform in R: Performs fast translation-invariant wavelet deconvolution	Journal	Article	2007	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	27	1	WOS:000249734800001	This paper provides an introduction to a software package called waved making available all code necessary for reproducing the figures in the recently published articles on the WaveD transform for wavelet deconvolution of noisy signals. The forward WaveD transforms and their inverses can be computed using any wavelet from the Meyer family. The WaveD coefficients can be depicted according to time and resolution in several ways for data analysis. The algorithm which implements the translation invariant WaveD transform takes full advantage of the fast Fourier transform (FFT) and runs in O(n(log n)(2)) steps only. The waved package includes functions to perform thresholding and fine resolution tuning according to methods in the literature as well as newly designed visual and statistical tools for assessing WaveD fits. We give a waved tutorial session and review benchmark examples of noisy convolutions to illustrate the non-linear adaptive properties of wavelet deconvolution.
53338859	WOS:000252428700001	252EG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Calculation of critical values for Somerville's FDR procedures	Journal	Article	2007	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000252428700001	A Fortran 95 program has been written to calculate critical values for the step-up and step-down FDR procedures developed by Somerville (2004). The program allows for arbitrary selection of number of hypotheses, FDR rate, one- or two-sided hypotheses, common correlation coefficient of the test statistics and degrees of freedom. An MCV (minimum critical value) may be specified, or the program will calculate a specified number of critical values or steps in an FDR procedure. The program can also be used to efficiently ascertain an upper bound to the number of hypotheses which the procedure will reject, given either the values of the test statistics, or their p values. Limiting the number of steps in an FDR procedure can be used to control the number or proportion of false discoveries (Somerville and Hemmelmann 2007). Using the program to calculate the largest critical values makes possible efficient use of the FDR procedures for very large numbers of hypotheses.
53365809	WOS:000249929700002	217DX	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimating parameters of dichotomous and ordinal item response models with gllamm	Journal	Article	2007	1	1	English	STATA JOURNAL	313-333	21	1	WOS:000249929700002	Item response theory models are measurement models for categorical responses. Traditionally, the models are used in educational testing, where responses to test items can be viewed as indirect measures of latent ability. The test items are scored either dichotomously (correct-incorrect) or by using an ordinal scale (a grade from poor to excellent). Item response models also apply equally for measurement of other latent traits. Here we describe the one- and two-parameter logit models for dichotomous items, the partial-credit and rating scale models for ordinal items, and an extension of these models where the latent variable is regressed on explanatory variables. We show how these models can be expressed as generalized linear latent and mixed models and fitted by using the user-written command gllamm.
53381124	WOS:000248072200006	190PV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	predict and adjust with logistic regression	Journal	Article	2007	1	1	English	STATA JOURNAL	221-226	6	1	WOS:000248072200006	Within Stata there are two ways of getting average predicted values for different groups after an estimation command: adjust and predict. After OLS regression (regress), these two ways give the same answer. However, after logistic regression, the average predicted probabilities differ. This article discusses where that difference comes from and the consequent subtle difference in interpretation.
53386146	WOS:000252428900001	252EI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian age-period-cohort modeling and prediction - BAMP	Journal	Article	2007	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000252428900001	The software package B A M P provides a method of analyzing incidence or mortality data on the Lexis diagram, using a Bayesian version of an age-period-cohort model. A hierarchical model is assumed with a binomial model in the first-stage. As smoothing priors for the age, period and cohort parameters random walks of first and second order, with and without an additional unstructured component are available. Unstructured heterogeneity can also be included in the model. In order to evaluate the model fit, posterior deviance, DIC and predictive deviances are computed. By projecting the random walk prior into the future, future death rates can be predicted.
53399218	WOS:000252428600001	252EF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Self- and super-organizing maps in R: The kohonen package	Journal	Article	2007	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000252428600001	In this age of ever-increasing data set sizes, especially in the natural sciences, visualisation becomes more and more important. Self-organizing maps have many features that make them attractive in this respect: they do not rely on distributional assumptions, can handle huge data sets with ease, and have shown their worth in a large number of applications. In this paper, we highlight the kohonen package for R, which implements self-organizing maps as well as some extensions for supervised pattern recognition and data fusion.
53453534	WOS:000252430300001	252EV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	simecol : An object-oriented framework for ecological modeling in R	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000252430300001	The simecol package provides an open structure to implement, simulate and share ecological models. A generalized object-oriented architecture improves readability and potential code re-use of models and makes simecol-models freely extendable and simple to use. The simecol package was implemented in the S4 class system of the programming language R. Reference applications, e. g. predator-prey models or grid models are provided which can be used as a starting point for own developments. Compact example applications and the complete code of an individual-based model of the water flea Daphnia document the efficient usage of simecol for various purposes in ecological modeling, e. g. scenario analysis, stochastic simulations and individual based population dynamics. Ecologists are encouraged to exploit the abilities of simecol to structure their work and to use R and object-oriented programming as a suitable medium for the distribution and share of ecological modeling code.
53553353	WOS:000254886000005	286ZQ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Mata Matters: Structures	Journal	Article	2007	1	1	English	STATA JOURNAL	556-570	15	1	WOS:000254886000005	Mata is Stata's matrix language. In the Mata Matters column, we show how Mata can be used interactively to solve problems and as a programming language to add new features to Stata. Structures are the subject of this column. Structures are an advanced programming technique that can greatly simplify complicated code.
53554623	WOS:000244068200001	134FF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ImpuR: A collection of diagnostic tools developed in R in the context of peak impurity detection in HPLC-DAD but potentially useful with other types of time-intensity matrices	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	18	1	WOS:000244068200001	HPLC-DAD systems generate time intensity ( absorbance) matrices called spectrochromatograms. Under good experimental conditions, spectro-chromatograms of elution peaks of pure analytes are bilinear products of a time peak and an absorbance spectrum. Co-eluting impurities create deviations from this pure bilinear structure. Unfortunately, other imperfections, such as scan averaging, large optical windows, imperfect lamp alignment, mobile phase fluctuations, etc. also create departures from the pure bilinear structure. This makes it hard to distinguish low concentration impurities from artifacts and hampers safe detection of contaminants. There are two main ways to deal with such artifacts: removal and simulation, and ImpuR provides R functions to do both and to integrate both approaches. More specifically, ImpuR provides a set of tools to explore time-intensity matrices with respect to their bilinear structure and departures from it. It includes exploratory graphs for bilinear matrices ( bilinear residual graphs and singular value decompositions), spectral dissimilarity curves via window-evolving factor analysis with heteroscedasticity correction and the sine method, methods for removal of artifacts, and a comprehensive simulation tool to assess the impact of potential artifacts and to allow for the construction of guide curves for use with the sine method.
53556068	WOS:000245823000001	158VK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Bayesian smoothing with Gaussian processes using Fourier basis functions in the spectralGP package	Journal	Article	2007	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-38	38	1	WOS:000245823000001	The spectral representation of stationary Gaussian processes via the Fourier basis provides a computationally efficient specification of spatial surfaces and nonparametric regression functions for use in various statistical models. I describe the representation in detail and introduce the spectralGP package in R for computations. Because of the large number of basis coefficients, some form of shrinkage is necessary; I focus on a natural Bayesian approach via a particular parameterized prior structure that approximates stationary Gaussian processes on a regular grid. I review several models from the literature for data that do not lie on a grid, suggest a simple model modification, and provide example code demonstrating MCMC sampling using the spectralGP package. I describe reasons that mixing can be slow in certain situations and provide some suggestions for MCMC techniques to improve mixing, also with example code, and some general recommendations grounded in experience.
53578821	WOS:000249929700008	217DX	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking stata: Turning over a new leaf	Journal	Article	2007	1	1	English	STATA JOURNAL	413-433	21	1	WOS:000249929700008	Stem-and-leaf displays have been widely taught since John W. Tukey publicized them energetically in the 1970s. They remain useful for many distributions of small or modest size, especially for showing fine structure such as digit preference. Stata's implementation stem produces typed text displays and has some inevitable limitations, especially for comparison of two or more displays. One can re-create stem-and-leaf displays with a few basic Stata commands as scatterplots of stem variable versus position on line with leaves shown as marker labels. Comparison of displays then becomes easy and natural using scatter, by(). Back-to-back presentation of paired displays is also possible. I discuss variants on standard stem-and-leaf displays in which each distinct value is a stem, each distinct value is its own leaf, or axes are swapped. The problem shows how one can, with a few lines of Stata, often produce standard graph forms from first principles, allowing in turn new variants. I also present a new program, stemplot, as a convenience tool.
53595122	WOS:000252430400001	252EW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	demogR: A package for the construction and analysis of age-structured demographic models in R	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000252430400001	The analysis of matrix population models has become a fundamental tool in ecology, conservation biology, and life history theory. In this paper, I present demogR, a package for analyzing age-structured population models in R. The package includes tools for the construction and analysis of matrix population models. In addition to the standard analyses commonly used in evolutionary demography and conservation biology, demogR contains a variety of tools from classical demography. This includes the construction of period life tables, and the generation of model mortality and fertility schedules for human populations. The tools in demogR are generally applicable to age-structured populations but are particularly useful for analyzing problems in human ecology. I illustrate some of the capabilities of the package by doing an evolutionary demographic analysis of several human populations.
53625073	WOS:000252431000001	252FA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	TSP - Infrastructure for the traveling salesperson problem	Journal	Article	2007	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000252431000001	The traveling salesperson (or, salesman) problem (TSP) is a well known and important combinatorial optimization problem. The goal is to find the shortest tour that visits each city in a given list exactly once and then returns to the starting city. Despite this simple problem statement, solving the TSP is difficult since it belongs to the class of NP-complete problems. The importance of the TSP arises besides from its theoretical appeal from the variety of its applications. Typical applications in operations research include vehicle routing, computer wiring, cutting wallpaper and job sequencing. The main application in statistics is combinatorial data analysis, e.g., reordering rows and columns of data matrices or identifying clusters. In this paper, we introduce the R package TSP which provides a basic infrastructure for handling and solving the traveling salesperson problem. The package features S3 classes for specifying a TSP and its (possibly optimal) solution as well as several heuristics to find good solutions. In addition, it provides an interface to Concorde, one of the best exact TSP solvers currently available.
53647661	WOS:000248072200007	190PV	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Making regression tables simplified	Journal	Article	2007	1	1	English	STATA JOURNAL	227-244	18	1	WOS:000248072200007	estout, introduced by Jann (Stata Journal 5: 288-308), is a useful tool for producing regression tables from stored estimates. However, its syntax is relatively complex and commands may turn out long even for simple tables. Furthermore, having to store the estimates beforehand can be cumbersome. To facilitate the production of regression tables, I therefore present here two new commands called eststo and esttab. eststo is a wrapper for official Stata's estimates store and simplifies the storing of estimation results for tabulation. esttab, on the other hand, is a wrapper for estout and simplifies compiling nice-looking tables frond the stored estimates without much typing. I also provide updates to estout and estadd.
53650728	WOS:000252429900001	252ER	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Interactive multivariate data analysis in R with the ade4 and ade4TkGUI packages	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000252429900001	ade4 is a multivariate data analysis package for the R statistical environment, and ade4TkGUI is a Tcl/Tk graphical user interface for the most essential methods of ade4. Both packages are available on CRAN. An overview of ade4TkGUI is presented, and the pros and cons of this approach are discussed. We conclude that command line interfaces (CLI) and graphical user interfaces (GUI) are complementary. ade4TkGUI can be valuable for biologists and particularly for ecologists who are often occasional users of R. It can spare them having to acquire an in-depth knowledge of R, and it can help first time users in a first approach.
53651636	WOS:000252430100001	252ET	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The ecodist package for dissimilarity-based analysis of ecological data	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000252430100001	Ecologists are concerned with the relationships between species composition and environmental factors, and with spatial structure within those relationships. A dissimilarity-based framework incorporating space explicitly is an extremely flexible tool for answering these questions. The R package ecodist brings together methods for working with dissimilarities, including some not available in other R packages. We present some of the features of ecodist, particularly simple and partial Mantel tests, and make recommendations for their effective use. Although the partial Mantel test is often used to account for the effects of space, the assumption of linearity greatly reduces its effectiveness for complex spatial patterns. We introduce a modification of the Mantel correlogram designed to overcome this restriction and allow consideration of complex nonlinear structures. This extension of the method allows the use of partial multivariate correlograms and tests of relationship between variables at different spatial scales. Some of the possibilities are demonstrated using both artificial data and data from an ongoing study of plant community composition in grazinglands of the northeastern United States.
53673421	WOS:000244067000001	134EU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An introduction to the special volume "Spectroscopy and Chemometrics in R"	Journal	Editorial Material	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	5	1	WOS:000244067000001	This special volume collates ten issues under the rubric "Spectroscopy and Chemometrics in R". In so doing, it provides an overview of the breadth, depth and state of the art of R-based software projects for spectroscopy and chemometrics applications. Just as the authors have contributed to R their documentation and source code, so has R contributed to the quality, standardization and dissemination of their software, as this volume attests. We hope that the volume is inspiring to both computational statisticians interested in applications of their methodologies and to spectroscopists or chemometricians in need of solutions to their data analysis problems.
53691182	WOS:000252428400001	252EE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Enjoy the joy of copulas: With a package copula	Journal	Article	2007	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000252428400001	Copulas have become a popular tool in multivariate modeling successfully applied in many fields. A good open-source implementation of copulas is much needed for more practitioners to enjoy the joy of copulas. This article presents the design, features, and some implementation details of the R package copula. The package provides a carefully designed and easily extensible platform for multivariate modeling with copulas in R. S4 classes for most frequently used elliptical copulas and Archimedean copulas are implemented, with methods for density/distribution evaluation, random number generation, and graphical display. Fitting copula-based models with maximum likelihood method is provided as template examples. With the classes and methods in the package, the package can be easily extended by user-defined copulas and margins to solve problems.
53734068	WOS:000252429600001	252EO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Analogue methods in palaeoecology: Using the analogue package	Journal	Article	2007	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000252429600001	Palaeoecology is an important branch of ecology that uses the subfossil remains of organisms preserved in lake, ocean and bog sediments to inform on changes in ecosystems and the environment through time. The analogue package contains functions to perform modern analogue technique ( MAT) transfer functions, which can be used to predict past changes in the environment, such as climate or lake-water pH from species data. A related technique is that of analogue matching, which is concerned with identifying modern sites that are floristically and faunistically similar to fossil samples. These techniques, and others, are increasingly being used to inform public policy on environmental pollution and conservation practices. These methods and other functionality in analogue are illustrated using the Surface Waters Acidification Project diatom: pH training set and diatom counts on samples of a sediment core from the Round Loch of Glenhead, Galloway, Scotland. The paper is aimed at palaeoecologists who are familiar with the techniques described but not with R.
53752699	WOS:000247011100001	175JY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	The Rasch sampler	Journal	Article	2007	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	14	1	WOS:000247011100001	The Rasch sampler is an efficient algorithm to sample binary matrices with given marginal sums. It is a Markov chain Monte Carlo (MCMC) algorithm. The program can handle matrices of up to 1024 rows and 64 columns. A special option allows to sample square matrices with given marginals and fixed main diagonal, a problem prominent in social network analysis. In all cases the stationary distribution is uniform. The user has control on the serial dependency.
53756649	WOS:000243591000001	127MS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A C++ program for the Cramer-von Mises two-sample test	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	15	1	WOS:000243591000001	As larger sets of high-throughput data in genomics and proteomics become more readily available, there is a growing need for fast algorithms designed to compute exact p values of distribution-free statistical tests. We present a program for computing the exact distribution of the two-sample Cramer-von Mises test statistic under the null hypothesis that the two samples are drawn from the same continuous distribution. The program makes it possible to handle substantially larger sample sizes than earlier proposed computational tools. The C++ source code for the program is published with this paper, and an R package is under development.
53786758	WOS:000244068300001	134FG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	spectrino software: Spectra visualization and preparation for R	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000244068300001	spectrino is a spectra preparation software utility for the R language and environment for statistical computing. It is an operating-system specific tool, for use under Microsoft Windows, with specialized visualization, organization and preprocessing features for spectra. The software accepts spectral data from analytical instruments and then prepares a data structure to be introduced in R. spectrino has a rich set of features to create data structures and visually manipulate/compare spectra. The application is accessible by a library of functions from within R. These commands allow for the creation and manipulation of data structures in spectrino and the selective extraction of spectral data. Before exporting, the spectra are preprocessed according the requirements of consecutive discriminant analysis. This preprocessing is adjustable by a series of options.
53799128	WOS:000247012000001	175KH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Marginal maximum likelihood estimation of item response models in R	Journal	Article	2007	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	24	1	WOS:000247012000001	Item response theory (IRT) models are a class of statistical models used by researchers to describe the response behaviors of individuals to a set of categorically scored items. The most common IRT models can be classified as generalized linear fixed-and/or mixed-effect models. Although IRT models appear most often in the psychological testing literature, researchers in other fields have successfully utilized IRT-like models in a wide variety of applications. This paper discusses the three major methods of estimation in IRT and develops R functions utilizing the built-in capabilities of the R environment to find the marginal maximum likelihood estimates of the generalized partial credit model. The currently available R packages ltm is also discussed.
53824630	WOS:000252429000001	252EJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fitting single and mixture of generalized lambda distributions to data via discretized and miximum likelihood methods: GLDEX in R	Journal	Article	2007	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000252429000001	This paper describes the use of GLDEX in R to fit distributions to empirical data using the discretized and maximum likelihood methods. The GLDEX package also provides diagnostic tests to examine the quality of fit through the resample Kolmogorov-Smirnoff test, quantile plots and comparison of the mean, variance, skewness and kurtosis between the empirical data and the fitted distribution.
53827529	WOS:000252431700001	252FF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Generalized additive models for location scale and shape (GAMLSS) in R	Journal	Article	2007	12	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	46	1	WOS:000252431700001	GAMLSS is a general framework for fitting regression type models where the distribution of the response variable does not have to belong to the exponential family and includes highly skew and kurtotic continuous and discrete distribution. GAMLSS allows all the parameters of the distribution of the response variable to be modelled as linear/non-linear or smooth functions of the explanatory variables. This paper starts by defining the statistical framework of GAMLSS, then describes the current implementation of GAMLSS in R and finally gives four different data examples to demonstrate how GAMLSS can be used for statistical modelling.
53860992	WOS:000244067300001	134EX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	TIMP: An R package for modeling multi-way spectroscopic measurements	Journal	Article	2007	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	46	1	WOS:000244067300001	TIMP is an R package for modeling multiway spectroscopic measurements. The package allows for the simultaneous analysis of datasets collected under different experimental conditions in terms of a wide variety of parametric models. Models arising in spectroscopy data analysis often have some parameters that are intrinstically nonlinear, and some parameters that are conditionally linear on estimates of the nonlinear parameters. TIMP fits such separable nonlinear models using partitioned variable projection, a variant of the variable projection algorithm that is described here for the first time. The of the partitioned variable projection algorithm allows fitting many models for spectroscopy datasets using much less memory as compared to under the standard variable projection algorithm that is implemented in nonlinear optimization routines ( e. g., the plinear option of the R function nls), as is shown here. An overview of modeling with TIMP is also given that includes several case studies in the application of the package.
53939006	WOS:000260799800001	370YU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Tools for Exploring Multivariate Data: The Package ICS	Journal	Article	2008	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000260799800001	Invariant coordinate selection (ICS) has recently been introduced as a method for exploring multivariate data. It includes as a special case a method for recovering the unmixing matrix in independent components analysis (ICA). It also serves as a basis for classes of multivariate nonparametric tests, and as a tool in cluster analysis or blind discrimination. The aim of this paper is to briefly explain the (ICS) method and to illustrate how various applications can be implemented using the R package ICS. Several examples are used to show how the ICS method and ICS package can be used in analyzing a multivariate data set.
53984605	WOS:000261526300001	381HK	1548-7660	1548766	NULL	NULL	NULL	ARTN 8	NULL	Implementing a Class of Permutation Tests: The coin Package	Journal	Article	2008	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000261526300001	The R package coin implements a unified approach to permutation tests providing a huge class of independence tests for nominal, ordered, numeric, and censored data as well as multivariate data at mixed scales. Based on a rich and flexible conceptual framework that embeds different permutation test procedures into a common theory, a computational framework is established in coin that likewise embeds the corresponding R functionality in a common S 4 class structure with associated generic functions. As a consequence, the computational tools in coin inherit the flexibility of the underlying theory and conditional inference functions for important special cases can be set up easily. Conditional versions of classical tests-such as tests for location and scale problems in two or more samples, independence in two- or three- way contingency tables, or association problems for censored, ordered categorical or multivariate data-can easily be implemented as special cases using this computational toolbox by choosing appropriate transformations of the observations. The paper gives a detailed exposition of both the internal structure of the package and the provided user interfaces along with examples on how to extend the implemented functionality.
53999987	WOS:000258206600001	334FB	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Sample selection models in R: Package sampleSelection	Journal	Article	2008	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000258206600001	This paper describes the implementation of Heckman-type sample selection models in R. We discuss the sample selection problem as well as the Heckman solution to it, and argue that although modern econometrics has non- and semiparametric estimation methods in its toolbox, Heckman models are an integral part of the modern applied analysis and econometrics syllabus. We describe the implementation of these models in the package sampleSelection and illustrate the usage of the package on several simulation and real data examples. Our examples demonstrate the effect of exclusion restrictions, identification at infinity and misspecification. We argue that the package can be used both in applied research and teaching.
54102441	WOS:000261526700001	381HO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	A MATLAB Package for Markov Chain Monte Carlo with a Multi-Unidimensional IRT Model	Journal	Article	2008	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000261526700001	Unidimensional item response theory(IRT) models are useful when each item is designed to measure some facet of a unified latent trait. In practical applications, items are not necessarily measuring the same underlying trait, and hence the more general multiunidimensional model should be considered. This paper provides the requisite information and description of software that implements the Gibbs sampler for such models with two item parameters and a normal ogive form. The software developed is written in the MATLAB package IRTmu2no. The package is flexible enough to allow a user the choice to simulate binary response data with multiple dimensions, set the number of total or burn-in iterations, specify starting values or prior distributions for model parameters, check convergence of the Markov chain, as well as obtain Bayesian fit statistics. Illustrative examples are provided to demonstrate and validate the use of the software package.
54104317	WOS:000259635100005	354JE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	kountry: A Stata utility for merging cross-country data from multiple sources	Journal	Article	2008	1	1	English	STATA JOURNAL	390-400	11	1	WOS:000259635100005	This article describes kountry, a data-management command that can be used to translate one country-coding scheme into amother, to recode country names into a "standardized form" and to generate geographic-region variables. uusers can build a custom dictionary through a helper command, kountryadd, that "teaches" kountry new name variations. The dictionary can be protected form an accedental overwriting though two helper commands: kountrybackup and kountryrestore.
54164139	WOS:000255794800001	299ZT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Unified geostatistical modeling for data fusion and spatial heteroskedasticity with R package ramps	Journal	Article	2008	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000255794800001	This article illustrates usage of the ramps R package, which implements the reparameterized and marginalized posterior sampling (RAMPS) algorithm for complex Bayesian geostatistical models. The RAMPS methodology allows joint modeling of areal and point-source data arising from the same underlying spatial process. A reparametrization of variance parameters facilitates slice sampling based on simplexes, which can be useful in general when multiple variances are present. Prediction at arbitrary points can be made, which is critical in applications where maps are needed. Our implementation takes advantage of sparse matrix operations in the Matrix package and can provide substantial savings in computing time for large datasets. A user-friendly interface, similar to the nlme mixed effects models package, enables users to analyze datasets with little programming effort. Support is provided for numerous spatial and spatiotemporal correlation structures, user-defined correlation structures, and non-spatial random effects. The package features are illustrated via a synthetic dataset of spatially correlated observation distributed across the state of Iowa, USA.
54166306	WOS:000258206300001	334EY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Censored quantile regression redux	Journal	Article	2008	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000258206300001	Quantile regression for censored survival (duration) data offers a more flexible alternative to the Cox proportional hazard model for some applications. We describe three estimation methods for such applications that have been recently incorporated into the R package quantreg: the Powell (1986) estimator for fixed censoring, and two methods for random censoring, one introduced by Portnoy (2003), and the other by Peng and Huang (2008). The Portnoy and Peng-Huang estimators can be viewed, respectively, as generalizations to regression of the Kaplan-Meier and Nelson-Aalen estimators of univariate quantiles for censored observations. Some asymptotic and simulation comparisons are made to highlight advantages and disadvantages of the three methods.
54198821	WOS:000255794600001	299ZR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	explorase: Multivariate exploratory analysis and visualization for systems biology	Journal	Article	2008	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000255794600001	The datasets being produced by high-throughput biological experiments, such as microarrays, have forced biologists to turn to sophisticated statistical analysis and visualization tools in order to understand their data. We address the particular need for an open-source exploratory data analysis tool that applies numerical methods in coordination with interactive graphics to the analysis of experimental data. The software package, known as explorase, provides a graphical user interface (GUI) on top of the R platform for statistical computing and the GGobi software for multivariate interactive graphics. The GUI is designed for use by biologists, many of whom are unfamiliar with the R language. It displays metadata about experimental design and biological entities in tables that are sortable and filterable. There are menu shortcuts to the analysis methods implemented in R, including graphical interfaces to linear modeling tools. The GUI is linked to data plots in GGobi through a brush tool that simultaneously colors rows in the entity information table and points in the GGobi plots. explorase is an R package publicly available from Bioconductor and is a tool in the MetNet platform for the analysis of systems biology data.
54201231	WOS:000258207100001	334FG	1548-7660	1548766	NULL	NULL	NULL	ARTN 8	10.18637/jss.v027.i08	Regression models for count data in R	Journal	Article	2008	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000258207100001	The classical Poisson, geometric and negative binomial regression models for count data belong to the family of generalized linear models and are available at the core of the statistics toolbox in the R system for statistical computing. After reviewing the conceptual and computational features of these methods, a new implementation of hurdle and zero-inflated regression models in the functions hurdle () and zeroinfl () from the package pscl is introduced. It re-uses design and functionality of the basic R functions just as the underlying conceptual tools extend the classical models. Both hurdle and zero-in inflated model, are able to incorporate over-dispersion and excess zeros-two problems that typically occur in count data sets in economics and the social sciences-better than their classical counterparts. Using cross-section data on the demand for medical care, it is illustrated how the classical as well as the zero-augmented models can be fitted, inspected and tested in practice.
54206357	WOS:000254886200006	286ZS	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Spineplots and their kin	Journal	Article	2008	1	1	English	STATA JOURNAL	105-121	17	1	WOS:000254886200006	The term spineplot has been applied over the last decade or so to a type of bar chart used particularly for showing frequencies, proportions, or percentages of two cross-classified categorical variables. The principle is that the areas of rectangular tiles are proportional to the frequencies in the cells of a contingency table. Often both coarse and fine structure are easy to see, including departures from independence. The main idea has, in fact, been rediscovered repeatedly over at least the last 130 years. In its most general form, it has been widely publicized under the name mosaic plots. This column introduces, discusses, and exemplifies a Stata implementation of spineplots. It is noted that a restriction to two variables is more apparent than real, as either axis of a spineplot can show a composite variable defined by cross combinations of two or more variables.
54207597	WOS:000268680300003	479SD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Meta-regression in Stata	Journal	Article	2008	1	1	English	STATA JOURNAL	493-519	27	1	WOS:000268680300003	We present a revised version of the metareg command, which performs meta-analysis regression (meta-regression) on study-level Summary data. The major revisions involve improvements to the estimation methods and the addition of an option to use a permutation test; to estimate p-values, including an adjustment for multiple testing. We have also made additions to the output, added an option to produce a graph, and included support for the predict command. Stata 8.0 or above is required.
54249669	WOS:000257343800005	321ZA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Error-correction-based cointegration tests for panel data	Journal	Article	2008	1	1	English	STATA JOURNAL	232-241	10	1	WOS:000257343800005	This article describes a new Stata command called xtwest, which implements the four error-correction-based panel cointegration tests developed by Westerlund (2007). The tests are general enough to allow for a large degree of heterogeneity, both in the long-run cointegrating relationship and in the short-run dynamics, and dependence within as well as across the cross-sectional units.
54267100	WOS:000254619600001	283EN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Getting things in order: An introduction to the R package seriation	Journal	Article	2008	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-34	34	1	WOS:000254619600001	Seriation, i.e., finding a suitable linear order for a set of objects given data and a loss or merit function, is a basic problem in data analysis. Caused by the problem's combinatorial nature, it is hard to solve for all but very small sets. Nevertheless, both exact solution methods and heuristics are available. In this paper we present the package seriation which provides an infrastructure for seriation with R. The infrastructure comprises data structures to represent linear orders as permutation vectors, a wide array of seriation methods using a consistent interface, a method to calculate the value of various loss and merit functions, and several visualization techniques which build on seriation. To illustrate how easily the package can be applied for a variety of applications, a comprehensive collection of examples is presented.
54273410	WOS:000257322300001	321QT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An implementation of Bayesian adaptive regression splines (BARS) in C with S and R wrappers	Journal	Article	2008	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000257322300001	BARS (DiMatteo, Genovese, and Kass 2001) uses the powerful reversible-jump MCMC engine to perform spline-based generalized nonparametric regression. It has been shown to work well in terms of having small mean-squared error in many examples (smaller than known competitors), as well as producing visually-appealing fits that are smooth (filtering out high-frequency noise) while adapting to sudden changes (retaining high-frequency signal). However, BARS is computationally intensive. The original implementation in S was too slow to be practical in certain situations, and was found to handle some data sets incorrectly. We have implemented BARS in C for the normal and Poisson cases, the latter being important in neurophysiological and other point-process applications. The C implementation includes all needed subroutines for fitting Poisson regression, manipulating B-splines (using code created by Bates and Venables), and finding starting values for Poisson regression (using code for density estimation created by Kooperberg). The code utilizes only freely-available external libraries (LAPACK and BLAS) and is otherwise self-contained. We have also provided wrappers so that BARS can be used easily within S or R.
54290251	WOS:000257343800002	321ZA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	The Stata command felsdvreg to fit a linear model with two high-dimensional fixed effects	Journal	Article	2008	1	1	English	STATA JOURNAL	170-189	20	1	WOS:000257343800002	This article Proposes a memory-saving decomposition of the design matrix to facilitate the estimation of a linear model with two high-dimensional fixed effects. A common way to fit such a model is to take into account one of the effects by including dummy variables and to sweep out the other effect by the within transformation (fixed-effects transformation). If the number of panel units is high, creating and storing the dummy variables can involve prohibitively large computer-memory requirements. The memory-saving procedure to set up the moment matrices for estimation presented in this article can reduce the memory requirements considerably. The companion Stata ado-file felsdvreg implements the estimation method, takes care of identification issues, and provides useful summary statistics.
54325833	WOS:000259946800001	358VY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	lawstat: An R Package for Law, Public Policy and Biostatistics	Journal	Article	2008	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000259946800001	We present a new R software package lawstat that contains statistical tests and procedures that are utilized in various litigations on securities law, antitrust law, equal employment and discrimination as well as in public policy and biostatistics. Along with the wellknown tests such as the Bartels test, runs test, tests of homogeneity of several sample proportions, the Brunner-Munzel tests, the Lorenz curve, the Cochran-Mantel-Haenszel test and others, the package contains new distribution-free robust tests for symmetry, robust tests for normality that are more sensitive to heavy-tailed departures, measures of relative variability, Levene-type tests against trends in variances etc. All implemented tests and methods are illustrated by simulations and real-life examples from legal cases, economics and biostatistics. Although the packageis called lawstat, it presents implementation and discussion of statistical procedures and tests that are also employed in a variety of other applications, e.g., biostatistics, environmental studies, social sciences and others, in other words, all applications utilizing statistical dataanalysis. Hence, name of the package should not be considered as a restriction to legal statistics. The package will be useful to applied statisticians and "quantitatively alert practitioners " of other subjects as well as an asset in teaching statistical courses.
54368370	WOS:000257343800006	321ZA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Contour-enhanced funnel plots for meta-analysis	Journal	Article	2008	1	1	English	STATA JOURNAL	242-254	13	1	WOS:000257343800006	Funnel plots are commonly used to investigate publication and related biases in meta-analysis. Although asymmetry in the appearance of a funnel plot is often interpreted as being caused by publication bias, in reality the asymmetry could be due to other factors that cause systematic differences in the results of large and small studies, for example, confounding factors such as differential study quality. Funnel plots can be enhanced by adding contours of statistical significance to aid in interpreting the funnel plot. If studies appear to be missing in areas of low statistical significance, then it is possible that the asymmetry is due to publication bias. If studies appear to be missing in areas of high statistical significance, then publication bias is a less likely cause of the funnel asymmetry. It is proposed that this enhancement to funnel plots should be used routinely for meta-analyses where it is possible that results could be suppressed on the basis of their statistical significance.
54375386	WOS:000259946300001	358VT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Improved Subset Autoregression: With R Package	Journal	Article	2008	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000259946300001	The FitAR R (R Development Core Team 2008) package that is available on the Comprehensive R Archive Network is described. This package provides a comprehensive approach to fitting autoregressive and subset autoregressive time series. For long time series with complicated autocorrelation behavior, such as the monthly sunspot numbers, subset autoregression may prove more feasible and/or parsimonious than using AR or ARMA models. The two principal functions in this package are SelectModel and FitAR for automatic model selection and model fitting respectively. In addition to the regular autoregressive model and the usual subset autoregressive models (Tong 1977), these functions implement a new family of models. This new family of subset autoregressive models is obtained by using the partial autocorrelations as parameters and then selecting a subset of these parameters. Further properties and results for these models are discussed in McLeod and Zhang (2006). The advantages of this approach are that not only is an efficient algorithm for exact maximum likelihood implemented but that efficient methods are derived for selecting high-order subset models that may occur in massive datasets containing long time series. A new improved extended BIC criterion, UBIC, developed by Chen and Chen (2008) is implemented for subset model selection. A complete suite of model building functions for each of the three types of autoregressive models described above are included in the package. The package includes functions for time series plots, diagnostic testing and plotting, bootstrapping, simulation, forecasting, Box-Cox analysis, spectral density estimation and other useful time series procedures. As well as methods for standard generic functions including print, plot, predict and others, some new generic functions and methods are supplied that make it easier to work with the output from FitAR for bootstrapping, simulation, spectral density estimation and Box-Cox analysis.
54375545	WOS:000259616700001	354CO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Computing and displaying isosurfaces in R	Journal	Article	2008	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000259616700001	This paper presents R utilities for computing and displaying isosurfaces, or three-dimensional contour surfaces, from a three-dimensional array of function values. A version of the marching cubes algorithm that takes into account face and internal ambiguities is used to compute the isosurfaces. Vectorization is used to ensure adequate performance using only R code. Examples are presented showing contours of theoretical densities, density estimatates,and medical imaging data. Rendering can use the rgl package or standard or grid graphics, and a set of tools for representing and rendering surfaces using standard or grid graphics is presented.
54376625	WOS:000261527200001	381HT	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Exact Tests for Two-Way Contingency Tables with Structural Zeros	Journal	Article	2008	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000261527200001	Fisher's exact test, named for Sir Ronald Aylmer Fisher, tests contingency tables for homogeneity of proportion. This paper discusses a generalization of Fisher's exact test for the case where some of the table entries are constrained to be zero. The resulting test is useful for assessing cases where the null hypothesis of conditional multinomial distribution is suspected to be false. The test is implemented in the form of a new R package, aylmer.
54386122	WOS:000259635100003	354JE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A Stata package for the estimation of the dose-response function through adjustment for the generalized propensity score	Journal	Article	2008	1	1	English	STATA JOURNAL	354-373	20	1	WOS:000259635100003	In this article we briefly review the role of the propensity socre in estimating dose response functions as described in Hirano and imbens (2001. Applied baysian Modeling and Cansal Inference form Incomplete-date Perspectives. 73. S.1). Then we present a set of Stata programs that estimate the propensity score in a setting with a continuous treatment test the balancing property of the generalized propensity score and estimate the dose response fimetion. We illustrate these programs by using a datasel collected by IImbens. Rubin and Saeerdose (2001. American Economic Review 91, 778 794).
54397730	WOS:000257343800007	321ZA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Mata Matters: Overflow, underflow and the IEEE floating-point format	Journal	Article	2008	1	1	English	STATA JOURNAL	255-268	14	1	WOS:000257343800007	Mata is Stata's matrix language. The Mata Matters column shows how Mata can be used interactively to solve problems and as a programming language to add new features to Stata. In this quarter's column, we investigate underflow and overflow and then delve into the details of how floating-point numbers are stored in the IEEE 754 floating-point standard. We show how to test for overflow and underflow. We demonstrate how to use the %21x format to see underflow and the %16H, %16L, %8H, and %8L formats for displaying the byte content of doubles and floats.
54403279	WOS:000255794900001	299ZU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Invariant and metric free proximities for data matching: An R package	Journal	Article	2008	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000255794900001	Data matching is a typical statistical problem in non experimental and/or observational studies or, more generally, in cross-sectional studies in which one or more data sets are to be compared. Several methods are available in the literature, most of which based on a particular metric or on statistical models, either parametric or nonparametric. In this paper we present two methods to calculate aproximity which have the property of being invariant under monotonic transformations. These methods require at most the notion of ordering. An open-source software in the form of a R package is also presented.
54428104	WOS:000268680300001	479SD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	The Blinder-Oaxaca decomposition for linear regression models	Journal	Article	2008	1	1	English	STATA JOURNAL	453-479	27	1	WOS:000268680300001	The counterfactual decomposition technique popularized by Blinder (1973, Journal of Human Resources, 436-455) and Oaxaca (1973, International Economic Review, 693-709) is widely used to study mean outcome differences between groups. For example, the technique is often used to analyze wage gaps by sex or race. This article summarizes the technique and addresses several complications, such as the identification of effects of categorical predictors in the detailed decomposition or the estimation of standard errors. A new command called oaxaca is introduced, and examples illustrating its usage are given.
54435944	WOS:000259635100007	354JE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Correlation with confidence, or Fisher's z revisited	Journal	Article	2008	1	1	English	STATA JOURNAL	413-439	27	1	WOS:000259635100007	Ronald Aylmer Fisher suggested transforming correlations by usign the inverse hyperbolic tangent, or atanh function a device often called fisher's z transformation. This article reviews that function and its iverse, the hyperbolic tangent or tanh function with discussions of their definitions and behavior, their use in statistical inference with correlations, and how to apply them in Stata. Examples show the use of Stata and Mata in calculator style. new commands corrci and corrcii are also presented for correlation confidenence intervals. The results of using bootstrapping to produce confidence intervals for corralations are also compared. Various historical commants are sprinkled throughout.
54491430	WOS:000254620000001	283ER	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	actuar: An R package for actuarial science	Journal	Article	2008	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-37	37	1	WOS:000254620000001	actuar is a package providing additional Actuarial Science functionality to the R statistical system. The project was launched in 2005 and the package is available on the Comprehensive R Archive Network since February 2006. The current version of the package contains functions for use in the fields of loss distributions modeling, risk theory ( including ruin theory), simulation of compound hierarchical models and credibility theory. This paper presents in detail but with few technical terms the most recent version of the package.
54540158	WOS:000254886200001	286ZS	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	metan: fixed- and random-effects meta-analysis	Journal	Article	2008	1	1	English	STATA JOURNAL	3-28	26	1	WOS:000254886200001	This article describes updates of the meta-analysis command metan and options that have been added since the command's original publication (Bradburn, Deeks, and Altman, metan - an alternative meta-analysis command, Stata Technical Bulletin Reprints, vol. 8, pp. 86-100). These include version 9 graphics with flexible display options, the ability to meta-analyze precalculated effect estimates, and the ability to analyze subgroups by using the by() option. Changes to the output, saved variables, and saved results are also described.
54611213	WOS:000258205500001	334EQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	VAR, SVAR and SVEC models: Implementation within R package vars	Journal	Article	2008	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000258205500001	The structure of the package vars and its implementation of vector autoregressive, structural vector autoregressive and structural vector error correction models are explained in this paper. In addition to the three cornerstone functions VAR (), SVAR () and SVEC () for estimating such models, functions for diagnostic testing, estimation of a restricted models, prediction, causality analysis, impulse response analysis and forecast error variance decomposition are provided too. It is further possible to convert vector error correction models into their level VAR representation. The different methods and functions are elucidated by employing a macroeconomic data set for Canada. However, the focus in this writing is on the implementation part rather than the usage of the tools at hand.
54645296	WOS:000259635100008	354JE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of A Visual Guide to Stata Graphics, Second Edition by Michael N. Mitchell	Journal	Review	2008	1	1	English	STATA JOURNAL	440-443	4	1	WOS:000259635100008	This article reviews A Visnal Guide to Stata Graphics, Second edition by Michael N. Mitchell.
54693251	WOS:000254619700001	283EO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	clValid: An R package for cluster validation	Journal	Article	2008	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000254619700001	The R package clValid contains functions for validating the results of a clustering analysis. There are three main types of cluster validation measures available, "internal", "stability", and "biological". The user can choose from nine clustering algorithms in existing R packages, including hierarchial, K-means, self-organizing maps (SOM), and model-based clustering. In addition, we provide a function to perform the self-organizing maps (SOM), and model-based clustering. In addition, we provide a function to perform the self-organizing tree algorithm (SOTA) method of clustering. Any combination of validation measures and clustering methods can be requested in a single function call. This allows the user to simultaneouly evaluate several clustering algorithms while varying the number of clusters, to help determine the most appropriate method and number of clusters for the dataset of interest. Additionally, the package can automatically make use of the biological information contained in the Gene Ontology (GO) database to calculate the biological validation measures, via the annotation packages available in Bioconductor. The function returns an object of S4 class "clValid", which has summary, plot, print, and additional methods which allow the user to display the optimal validation scores and extract clustering results.
54727019	WOS:000268680300007	479SD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Demand-system estimation: Update	Journal	Article	2008	1	1	English	STATA JOURNAL	554-556	3	1	WOS:000268680300007	The nlsur command is better suited to demand-system estimation than the suite of ado-files provided in Poi (2002, Stata Journal 2: 403-410) because it is faster and requires only one ancillary ado-file. This article replicates the results presented in Poi (2002) by using nlsur instead of ml.
54747415	WOS:000257343800001	321ZA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multinomial goodness-of-fit: Large-sample tests with survey design correction and exact tests for small samples	Journal	Article	2008	1	1	English	STATA JOURNAL	147-169	23	1	WOS:000257343800001	I introduce the new mgof command to compute distributional tests for discrete (categorical, multinomial) variables. The command supports large-sample tests for complex survey designs and exact tests for small samples as well as classic large-sample chi(2)-approximation tests based on Pearson's X-2, the likelihood ratio, or any other statistic from the power-divergence family (Cressie and Read, 1984, Journal of the Royal Statistical Society, Series B (Methodological) 46: 440-464). The complex survey correction is based on the approach by Rao and Scott (1981, Journal of the American Statistical Association 76: 221-230) and parallels the survey design correction used for independence tests in svy: tabulate. mgof computes the exact tests by using Monte Carlo methods or exhaustive enumeration. mgof also provides an exact one-sample Kolmogorov-Smirnov test for discrete data.
54806754	WOS:000254886200004	286ZS	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Tests for unbalanced error-components models under local misspecification	Journal	Article	2008	1	1	English	STATA JOURNAL	68-78	11	1	WOS:000254886200004	This paper derives unbalanced versions of the test statistics for first-order serial correlation and random individual effects summarized in Sosa-Escudero and Bera (2001, Stata Technical Bulletin Reprints, vol. 10, pp. 307-311), and updates their xttest1 routine. The derived test statistics should be useful for applied researchers faced with the increasing availability of panel information where not every individual or country is observed for the full time span. The test statistics proposed here are based on ordinary least-squares residuals and hence are computationally very simple.
54816561	WOS:000258204500001	334EG	1548-7660	1548766	NULL	NULL	NULL	ARTN 1	NULL	Econometrics in R: Past, present and future	Journal	Article	2008	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	5	1	WOS:000258204500001	Recently, computational methods and software have been receiving more attention in the econometrics literature, emphasizing that they are integral components of modern econometric research. This has also promoted the development of many new econometrics software packages written in R and made available on the Comprehensive R Archive Network. This special volume on "Econometrics in R" features a selection of these recent activities that includes packages for econometric analysis of cross-section, time series and panel data. This introduction to the special volume highlights the contents of the contributions and embeds them into a brief overview of other past, present, and future projects for econometrics in R.
54825346	WOS:000254886200008	286ZS	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Event History Analysis with Stata by Blossfeld, Goisch, and Rohwer	Journal	Review	2008	1	1	English	STATA JOURNAL	129-133	5	1	WOS:000254886200008	This article reviews Event History Analysis with Stata by Blossfeld, Golsch, and Rohwer.
54846068	WOS:000257343800004	321ZA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Production function estimation in Stata using the Olley and Pakes method	Journal	Article	2008	1	1	English	STATA JOURNAL	221-231	11	1	WOS:000257343800004	Productivity is often computed by approximating the weighted sum of the inputs from the estimation of the Cobb-Douglas production function. Such estimates, however, may suffer from simultaneity and selection biases. Olley and Pakes (1996, Econometrica 64: 1263-1297) introduced a semiparametric method that allows us to estimate the production function parameters consistently and thus obtain reliable productivity measures by controlling for such biases. This study first reviews this method and then introduces a Stata command to implement it. We show that when simultaneity and selection biases are not controlled for, the coefficients for the variable inputs are biased upward and the coefficients for the fixed inputs are biased downward.
54911557	WOS:000258204800001	334EJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Panel data econometrics in R: The plum package	Journal	Article	2008	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-43	43	1	WOS:000258204800001	Panel data econometrics is obviously one of the main fields in the profession, but most of the models used are difficult to estimate with R. plm is a package for R which intends to make the estimation of linear panel models straightforward. plm provides functions to estimate a wide variety of models and to make (robust) inference.
54920999	WOS:000257343800008	321ZA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking stata: Between tables and graphs	Journal	Article	2008	1	1	English	STATA JOURNAL	269-289	21	1	WOS:000257343800008	Table-like graphs can be interesting, useful, and even mildly innovative. This column outlines some Stata techniques for producing such graphs. graph dot is likely to be the most under-appreciated command among all existing commands. Using by ( ) with various choices is a good way to mimic a categorical axis in many graph commands. When graph bar or graph dot is not flexible enough to do what you want, moving to the more flexible two way is usually advisable. lab mask and seqvar are introduced as new commands useful for preparing axis labels and axis positions for categorical variables. Applications of these ideas to, e.g., confidence interval plots lies ahead.
54963565	WOS:000254886200002	286ZS	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A tool for deterministic and probabilistic sensitivity analysis of epidemiologic studies	Journal	Article	2008	1	1	English	STATA JOURNAL	29-48	20	1	WOS:000254886200002	Classification errors, selection bias, and uncontrolled confounders are likely to be present in most epidemiologic studies, but the uncertainty introduced by these types of biases is seldom quantified. The authors present a simple yet easy-to-use Stata command to adjust the relative risk for exposure misclassification, selection bias, and an unmeasured confounder. This command implements both deterministic and probabilistic sensitivity analysis. It allows the user to specify a variety of probability distributions for the bias parameters, which are used to simulate distributions for the bias-adjusted exposure-disease relative risk. We illustrate the command by applying it to a case-control study of occupational resin exposure and lung-cancer deaths. By using plausible probability distributions for the bias parameters, investigators can report results that incorporate their uncertainties regarding systematic errors and thus avoid overstating their certainty about the effect under study. These results can supplement conventional results and can help pinpoint major sources of conflict in study interpretations.
55082570	WOS:000257322400001	321QU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Rational arithmetic mathematica functions to evaluate the two-sided one sample K-S cumulative sampling distribution	Journal	Article	2008	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-40	40	1	WOS:000257322400001	One of the most widely used goodness-of-fit tests is the two-sided one sample Kolmogorov-Smirnov (K-S) test which has been implemented by many computer statistical software packages. To calculate a two-sided p value (evaluate the cumulative sampling distribution), these packages use various methods including recursion formulae, limiting distributions, and approximations of unknown accuracy developed over thirty years ago. Based on an extensive literature search for the two-sided one sample K-S test, this paper identifies an exact formula for sample sizes up to 31, six recursion formulae, and one matrix formula that can be used to calculate a p value. To ensure accurate calculation by avoiding catastrophic cancelation and eliminating rounding error, each of these formulae is implemented in rational arithmetic. For the six recursion formulae and the matrix formula, computational experience for sample sizes up to 500 shows that computational times are increasing functions of both the sample size and the number of digits in the numerator and denominator integers of the rational number test statistic. The computational times of the seven formulae vary immensely but the Durbin recursion formula is almost always the fastest. Linear search is used to calculate the inverse of the cumulative sampling distribution (find the confidence interval half-width) and tables of calculated half-widths are presented for sample size up to 500. Using calculated half-widths as input, computational times for the fastest formula, the Durbin recursion formula, are given for sample sizes up to two thousand.
55082681	WOS:000268680300006	479SD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A shortcut through long loops: An illustration of two alternatives to looping over observations	Journal	Article	2008	1	1	English	STATA JOURNAL	540-553	14	1	WOS:000268680300006	It is well known that looping over observations can be slow and should be avoided. The objective of this article is to discuss two alternative solutions to looping over observations that can be used to overcome a particular data-management problem of merging datasets in which unique key identifiers changed over time. The first alternative, mapch, which is introduced in this article, uses a combination of appending, indexing, and merging to solve the problem, while the second alternative uses repeated merging. Both solutions are much quicker than looping over observations. However, depending on the nature of the problem, one solution may work better than the other. It is argued that the use of such dataset-type manipulations may be suitable to overcome other data-management problems. More generally speaking, the issue that is addressed-searching for an alternative to looping over observations-may be common and illustrates the importance of balancing the costs of developing an efficient solution with the benefits accruing from that solution.
55113147	WOS:000260799600001	370YS	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Building Predictive Models in R Using the caret Package	Journal	Article	2008	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000260799600001	The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the bene fits of parallel processing with several types of models.
55120306	WOS:000255794500001	299ZQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Markov chain Monte Carlo estimation of normal ogive IRT models in MATLAB	Journal	Article	2008	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000255794500001	Modeling the interaction between persons and items at the item level for binary response data, item response theory (IRT) models have been found useful in a wide variety of applications in various fields. This paper provides the requisite information and description of software that implements the Gibbs sampling procedures for the one-, two- and three-parameter normal ogive models. The software developed is written in the MATLAB package IRTuno. The package is flexible enough to allow a use the choice to simulate binary response data, set the number of total or burn-in iterations, specify starting values or prior distributions for model parameters, check convergence of the Markov chain, and obtain Bayesian fit statistics. Illustrative examples are provided to demonstrate and validate the use of the software package. The m-file v25i08. m is also provided as a guide for the use of the MCMC algorithms with the three dichotomous IRT models.
55125219	WOS:000259635100004	354JE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Creating print-ready tables in Stata	Journal	Article	2008	1	1	English	STATA JOURNAL	374-389	16	1	WOS:000259635100004	This article describes the new Stata command xml_tab, which outputs the results of estimation commands and Stata matrices directly into tables in XML format. The XML files can be opened with Microsoft Excel or OpenOffice Calc, or they can be linked with Microsoft Word files. By using XML xml_tab allows Stata users to apply a rich set of formatting options to the elements of output tables.
55133285	WOS:000252432000001	252FI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	yaImpute: An R package for kNN imputation	Journal	Article	2008	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	NULL	16	1	WOS:000252432000001	This article introduces yaImpute, an R package for nearest neighbor search and imputation. Although nearest neighbor imputation is used in a host of disciplines, the methods implemented in the yaImpute package are tailored to imputation-based forest attribute estimation and mapping. The impetus to writing the yaImpute is a growing interest in nearest neighbor imputation methods for spatially explicit forest inventory, and a need within this research community for software that facilitates comparison among different nearest neighbor search algorithms and subsequent imputation techniques. yaImpute provides directives for defining the search space, subsequent distance calculation, and imputation rules for a given number of nearest neighbors. Further, the package offers a suite of diagnostics for comparison among results generated from different imputation analyses and a set of functions for mapping imputation results.
55276846	WOS:000254886200007	286ZS	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Applied Health Economics by Jones, Rice, Bago d'Uva, and Balia	Journal	Review	2008	1	1	English	STATA JOURNAL	122-128	7	1	WOS:000254886200007	This article reviews Applied Health Economics by Jones, Rice, Bago d'Uva, and Balia.
55282318	WOS:000258204100001	334EC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Caching and distributing statistical analyses in R	Journal	Article	2008	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000258204100001	We present the cacher package for R, which provides tools for caching statistical analyses and for distributing these analyses to others in an efficient manner. The cacher package takes objects created by evaluating R expressions and stores them in key-value databases. These databases of cached objects can subsequently be assembled into packages for distribution over the web. The cacher package also provides tools to help readers examine the data and code in a statistical analysis and reproduce, modify, or improve upon the results. In addition, readers can easily conduct alternate analyses of the data. We describe the design and implementation of the cacher package and provide two examples of how the package can be used for reproducible research.
55289524	WOS:000259635100002	354JE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Implementing double-robust estimators of causal effects	Journal	Article	2008	1	1	English	STATA JOURNAL	334-353	20	1	WOS:000259635100002	This article describes the implementation of a double-robust estimator for pretest-posttest studies (Lunceford and Davidian, 2004. Statistics in Medicine 23; 2937-2960) and presents a new Stata command (dr) that carries out the procedure. A double-robust estimator gives the analyst two opportunities for obtaining unbiased inference when adjusting for selection effects such as confounding by allowing for different forms of model misspecification; a double-robust estimator also can offer increased efficiency when all the models are correctly specified. We demostrate the results with a Monte Carlo simulation study, and we show how to implement the double-robust estimator on a single simulated dataset, both manually and by using the dr command.
55299402	WOS:000268680300002	479SD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	The Blinder-Oaxaca decomposition for nonlinear regression models	Journal	Article	2008	1	1	English	STATA JOURNAL	480-492	13	1	WOS:000268680300002	In this article, a general Blinder-Oaxaca decomposition for nonlinear models is derived, which allows the difference in an outcome variable between two groups to be decomposed into several components. We show how, using nldecompose, this general decomposition can be applied to different models with discrete and limited dependent variables. We further demonstrate how the standard errors of the estimated components can be calculated by using Stata's bootstrap command as a prefix.
55302106	WOS:000268680300011	479SD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of Multilevel and Longitudinal Modeling Using Stata, Second Edition, by Sophia Rabe-Hesketh and Anders Skrondal	Journal	Review	2008	1	1	English	STATA JOURNAL	579-582	4	1	WOS:000268680300011	This article reviews Multilevel and Longitudinal Modeling Using Stata, Second Edition, by Sophia Rabe-Hesketh and Anders Skrondal.
55323033	WOS:000268680300004	479SD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A closer examination of subpopulation analysis of complex-sample survey data	Journal	Article	2008	1	1	English	STATA JOURNAL	520-531	12	1	WOS:000268680300004	In recent years, general-purpose statistical software packages have incorporated new procedures that feature several useful options for design-based analysis of complex-sample Survey data. A common and frequently desired technique for analysis of survey data in practice is the restriction of estimation to a subpopulation of interest. These subpopulations are often referred to interchangeably in a variety of fields as subclasses, subgroups, and domains. In this article, we consider two approaches that analysts of complex-sample survey data call follow when analyzing subpopulations; we also consider the implications of each approach for estimation and inference. We then present examples of both approaches, using selected procedures in Stata to analyze data from the National Hospital Ambulatory Medical Care Survey (NHAMCS). We conclude with important considerations for subpopulation analyses and a summary of suggestions for practice.
55342319	WOS:000254619800001	283EP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Text mining infrastructure in R	Journal	Article	2008	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-54	54	1	WOS:000254619800001	During the last decade text mining has become a widely used discipline utilizing statistical and machine learning methods. We present the tm package which provides a framework for text mining applications within R. We give a survey on text mining facilities in R and explain how typical application tasks can be carried out using our framework. We present techniques for count- based analysis methods, text clustering, text classification and string kernels.
55347917	WOS:000252620100001	254XA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	PresenceAbsence: An R package for presence absence analysis	Journal	Article	2008	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-31	31	1	WOS:000252620100001	The PresenceAbsence package for R provides a set of functions useful when evaluating the results of presence-absence analysis, for example, models of species distribution or the analysis of diagnostic tests. The package provides a toolkit for selecting the optimal threshold for translating a probability surface into presence-absence maps specifically tailored to their intended use. The package includes functions for calculating threshold dependent measures such as confusion matrices, percent correctly classified (PCC), sensitivity, specificity, and Kappa, and produces plots of each measure as the threshold is varied. It also includes functions to plot the Receiver Operator Characteristic (ROC) curve and calculates the associated area under the curve (AUC), a threshold independent measure of model quality. Finally, the package computes optimal thresholds by multiple criteria, and plots these optimized thresholds on the graphs.
55443871	WOS:000254886200003	286ZS	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A new framework for managing and analyzing multiply imputed data in Stata	Journal	Article	2008	1	1	English	STATA JOURNAL	49-67	19	1	WOS:000254886200003	A new set of tools is described for performing analyses of an ensemble of datasets that includes multiple copies of the original data with imputations of missing values, as required for the method of multiple imputation. The tools replace those originally developed by the authors. They are based on a simple data management paradigm in which the imputed datasets are all stored along with the original data in a single dataset with a vertically stacked format, as proposed by Royston in his ice and micombine commands. Stacking into a single dataset simplifies the management of the imputed datasets compared with storing them individually. Analysis and manipulation of the stacked datasets is performed with a new prefix command, mim, which can accommodate data imputed by any method as long as a few simple rules are followed in creating the imputed data. mim can validly fit most of the regression models available in Stata to multiply imputed datasets, giving parameter estimates and confidence intervals computed according to Rubin's results for multiple imputation inference. Particular attention is paid to limiting the available postestimation commands to those that are known to be valid within the multiple imputation context. However, the user has flexibility to override these defaults. Features of these new tools are illustrated using two previously published examples.
55493843	WOS:000258180900001	333VM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Visualizing and assessing acceptance sampling plans: The R package acceptance sampling	Journal	Article	2008	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000258180900001	Manufacturers and government agencies frequently use acceptance sampling to decide whether a lot from a supplier or exporting country should be accepted or rejected. International standards on acceptance sampling provide sampling plans for specific circumstances. The aim of this package is to provide an easy-to-use interface to visualize single, double or multiple sampling plans. In addition, methods have been provided to enable the user to assess sampling plans against pre-specified levels of performance, as measured by the probability of acceptance for a given level of quality in the lot.
55501014	WOS:000255795200001	299ZX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	GEEQBOX: A MATLAB toolbox for generalized estimating equations and quasi-least squares	Journal	Article	2008	5	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000255795200001	The GEEQBOX toolbox analyzes correlated data via the method of generalized estimating (GEE) and quasi-least squares (QLS), an approach based on GEE that overcomes some limitations of GEE that have been noted in the literature. GEEQBOX is currently able to handle correlated data that follows a normal, Bernoulii or Poisson distribution, and that is assumed to have an AR(1), Markov, tri-diagonal, equicorrelated, unstructured or working independence correlation structure. This toolbox is for use with MATLAB.
55515433	WOS:000257322600001	321QW	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An R package for a general class of inverse Gaussian distributions	Journal	Article	2008	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000257322600001	The inverse Gaussian distribution is a positively skewed probability model that has received great attention in the last 20 years. Recently, a family that generalizes this model called inverse Gaussian type distributions has been developed. The new R package named ig has been designed to analyze data from inverse Gaussian type distributions. This package contains basic probabilistic functions, lifetime indicators and a random number generator from this model. Also, parameter estimates and diagnostics analysis can be obtained using likelihood methods by means of this package. In addition, goodness-of-fit methods are implemented in order to detect the suitability of the model to the data. The capabilities and features of the ig package are illustrated using simulated and real data sets. Furthermore, some new results related to the inverse Gaussian type distribution are also obtained. Moreover, a simulation study is conducted for evaluating the estimation method implemented in the ig package.
55516887	WOS:000254619400001	283EL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	FactoMineR: An R package for multivariate analysis	Journal	Article	2008	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000254619400001	In this article, we present FactoMineR an R package dedicated to multivariate data analysis. The main features of this package is the possibility to take into account different types of variables (quantitative or categorical), different types of structure on the data (a partition on the variables, a hierarchy on the variables, a partition on the individuals) and finally supplementary information (supplementary individuals and variables). Moreover, the dimensions issued from the different exploratory data analyses can be automatically described by quantitative and/or categorical variables. Numerous graphics are also available with various options. Finally, a graphical user interface is implemented within the Rcmdr environment in order to propose an user friendly package.
55554260	WOS:000257322500001	321QV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Arbitrary precision mathematica functions to evaluate the one-sided one sample K-S cumulative sampling distribution	Journal	Article	2008	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-55	55	1	WOS:000257322500001	Efficient rational arithmetic methods that can exactly evaluate the cumulative sampling distribution of the one-sided one sample Kolmogorov-Smirnov (K-S) test have been developed by Brown and Harvey (2007) for sample sizes n up to fifty thou sand. This paper implements in arbitrary precision the same 13 formulae to evaluate the one-sided one sample K-S cumulative sampling distribution. Computational experience identifies the fastest implementation which is then used to calculate confidence interval bandwidths and p values for sample sizes up to ten million.
55572339	WOS:000257322700001	321QX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Model averaging software for dichotomous dose response risk estimation	Journal	Article	2008	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-15	15	1	WOS:000257322700001	Model averaging has been shown to be a useful method for incorporating model uncertainty in quantitative risk estimation. In certain circumstances this technique is computationally complex, requiring sophisticated software to carry out the computation. We introduce software that implements model averaging for risk assessment based upon dichotomous dose-response data. This software, which we call Model Averaging for Dichotomous Response Benchmark Dose (MADr-BMD), fits the quantal response models, which are also used in the US Environmental Protection Agency benchmark dose software suite, and generates a model-averaged dose response model to generate benchmark dose and benchmark dose lower bound estimates. The software fulfills a need for risk assessors, allowing them to go beyond one single model in their risk assessments based on quantal data by focusing on a set of models that describes the experimental data.
55603857	WOS:000255795000001	299ZV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	GillespieSSA: Implementing the stochastic simulation algorithm in R	Journal	Article	2008	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000255795000001	The deterministic dynamics of populations in continuous time are traditionally described using coupled, first-order ordinary differential equations. While this approach is accurate for large systems, it is often inadequate for small systems where key species may be present in small numbers or where key reactions occur at a low rate. The Gillespie stochastic simulation algorithm (SSA) is a procedure for generating time-evolution trajectories of finite populations in continuous time and has become the standard algorithm for these types of stochastic models. This article presents a simple-to-use and flexible framework for implementing the SSA using the high-level statistical computing language R and the package GillespieSSA. Using three ecological models as examples (logistic growth, Rosenzweig-MacArthur predator-prey model, and Kermack-McKendrick SIRS metapopulation model), this paper shows how a deterministic model can be formulated as a finite-population stochastic model within the framework of SSA theory and how it can be implemented in R. Simulations of the stochastic models are performed using four different SSA Monte Carlo methods: one exact method (Gillespie's direct method); and three approximate methods (explicit, binomial, and optimized tau-leap methods). Comparison of simulation results confirms that while the time-evolution trajectories obtained from the different SSA methods are indistinguishable, the approximate methods are up to four orders of magnitude faster than the exact methods.
55612054	WOS:000259947000001	358WA	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	FlexMix Version 2: Finite Mixtures with Concomitant Variables and Varying and Constant Parameters	Journal	Article	2008	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-35	35	1	WOS:000259947000001	flexmix provides infrastructure for fexible fitting of finite mixture models in R using the expectation-maximization (EM) algorithm or one of its variants. The functionality of the package was enhanced. Now concomitant variable models as well as varying and constant parameters for the component specific generalized linear regression models can be fitted. The application of the package is demonstrated on several examples, the implementation described and examples given to illustrate how new drivers for the component specific models and the concomitant variable models can be de fined.
55648255	WOS:000268680300008	479SD	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Distinct observations	Journal	Article	2008	1	1	English	STATA JOURNAL	557-568	12	1	WOS:000268680300008	Distinct observations are those different with respect to one or more variables, considered either individually or jointly. Distinctness is thus a key aspect of the similarity or difference of observations. It is sometimes Confounded With uniqueness. Counting the number of distinct observations may be required at and point from initial data cleaning or checking to subsequent statistical analysis. We review how far existing commands in official Stata Offer solutions to this issue, and we show how to answer questions about distinct observations from first principles by using the by prefix and the egen command. The new distinct command is offered as a convenience tool.
55661441	WOS:000261526600001	381HN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	ofw: An R Package to Select Continuous Variables for Multiclass Classification with a Stochastic Wrapper Method	Journal	Article	2008	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000261526600001	When dealing with high dimensional and low sample size data, feature selection is often needed to help reduce the dimension of the variable space while optimizing the classification task. Few tools exist for selecting variables in such data sets, especially when classes are numerous ( > 2). We have developed ofw, an R package that implements, in the context of classification, the meta algorithm "optimal feature weighting". We focus on microarray data, although the method can be applied to any p >> n problems with continuous variables. The aim is to select relevant variables and to numerically evaluate the resulting variable selection. Two versions are proposed with the application of supervised multiclass classifiers such as classification and regression trees and support vector machines. Furthermore, a weighted approach can be chosen to deal with unbalanced multiclasses, a common characteristic in microarray data sets.
55663962	WOS:000254886200005	286ZS	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	fuzzy: A program for performing qualitative comparative analyses (QCA) in Stata	Journal	Article	2008	1	1	English	STATA JOURNAL	79-104	26	1	WOS:000254886200005	Qualitative comparative analysis (QCA) is an increasingly popular analytic strategy, with applications to numerous empirical fields. This article briefly discusses the substantive motivation and technical details of QCA, as well as fuzzy-set QCA, followed by an in-depth discussion of how the new program fuzzy performs these techniques in Stata. An empirical example is presented that demonstrates the full suite of tools contained within fuzzy, including creating configurations, performing a series of statistical tests of the configurations, and reducing the identified configurations.
55722746	WOS:000259635100006	354JE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Mata matters: Macros	Journal	Article	2008	1	1	English	STATA JOURNAL	401-412	12	1	WOS:000259635100006	Mata is Stata's matrix language. In the Mata Matters colunm, we show how Mata can be used interactively to solve problems and as a programming language to add new features to Stata. Macros are the subject of this column.
55737601	WOS:000258205200001	334EN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Automatic time series forecasting: The forecast package for R	Journal	Article	2008	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	22	1	WOS:000258205200001	Automatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovations state space models that underly exponential smoothing methods. The second is a step-wise algorithm for forecasting with ARIMA models. The algorithms are applicable to both seasonal and non-seasonal data, and are compared and illustrated using four real time series. We also briefly describe some of the other functionality available in the forecast package.
55740734	WOS:000252620500001	254XE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	CCA: An R package to extend canonical correlation analysis	Journal	Article	2008	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000252620500001	Canonical correlations analysis (CCA) is an exploratory statistical method to highlight correlations between two datasets acquired on the same experimental units. The cancor () function in R (R Development Core Team 2007) performs the core of computations but further work was required to provide the user with additional tools to facilitate the interpretation of the results. We implemented an R package, CCA, freely available from the Comprehensive R Archive Network (CRAN, http://CRAN.R-project.org/), to develop numerical and graphical outputs and to enable the user to handle missing values. The CCA package also includes a regularized version of CCA to deal with datasets with more variables than units. Illustrations are given through the analysis of a dataset coming from a nutrigenomic study in the mouse.
55742175	WOS:000261526100001	381HI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	GLUMIP 2.0: SAS/IML Software for Planning Internal Pilots	Journal	Article	2008	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000261526100001	Internal pilot designs involve conducting interim power analysis (without interim data analysis) to modify the final sample size. Recently developed techniques have been described to avoid the type I error rate inflation inherent to unadjusted hypothesis tests, while still providing the advantages of an internal pilot design. We present GLUMIP 2.0, the latest version of our free SAS/IML software for planning internal pilot studies in the general linear univariate model (GLUM) framework. The new analytic forms incorporated into the updated software solve many problems inherent to current internal pilot techniques for linear models with Gaussian errors. Hence, the GLUMIP 2.0 software makes it easy to perform exact power analysis for internal pilots under the GLUM framework with independent Gaussian errors and fixed predictors.
55780418	WOS:000257343800003	321ZA	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	SNP and SML estimation of univariate and bivariate binary-choice models	Journal	Article	2008	1	1	English	STATA JOURNAL	190-220	31	1	WOS:000257343800003	We discuss the semi-nonparametric approach of Gallant and Nychka (1987, Econometrica 55: 363-390), the serniparametric maximum likelihood approach of Klein and Spady (1993, Econometrica 61: 387-421), and a set of new Stata commands for serniparametric estimation of three binary-choice models. The first is a univariate model, while the second and the third are bivariate models without and with sample selection, respectively. The proposed estimators are root n consistent and asymptotically normal for the model parameters of interest under weak assumptions on the distribution of the underlying error terms. Our Monte Carlo simulations suggest that the efficiency losses of the semi-non parametric and the semiparametric maximum likelihood estimators relative to a maximum likelihood correctly specified estimator of a parametric probit are rather small. On the other hand, a comparison of these estimators in non-Gaussian designs suggests that semi-nonparametric and serniparametric maximum likelihood estimators substantially dominate the parametric probit maximum likelihood estimator.
55792398	WOS:000258205800001	334ET	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Nonparametric econometrics: The np package	Journal	Article	2008	7	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000258205800001	We describe the R np package via a series of applications that may be of interest to applied econometricians. The np package implements a variety of nonparametric and semiparametric kernel-based estimators that are popular among econometricians. There are also procedures for nonparametric tests of significance and consistent model specification tests for parametric mean regression models and parametric quantile regression models, among others. The np package focuses on kernel methods appropriate for the mix of continuous, discrete, and categorical data often found in applied settings. Data-driven methods of bandwidth selection are emphasized throughout, though we caution the user that data-driven bandwidth selection methods can be computationally demanding.
55800417	WOS:000254619900001	283EQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Software for Implementing the Sequential Elimination of Level Combinations Algorithm	Journal	Article	2008	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000254619900001	Genetic algorithms (GAs) are a popular technology to search for an optimum in a large search space. Using new concepts of forbidden array and weighted mutation, Mandal, Wu, and Johnson (2006) used elements of GAs to introduce a new global optimization technique called sequential elimination of level combinations (SELC), that efficiently finds optimums. A SAS macro, and MATLAB and R functions are developed to implement the SELC algorithm.
55816943	WOS:000259635100001	354JE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Semiparametric analysis of case-control genetic data in the presence of environmental factors	Journal	Article	2008	1	1	English	STATA JOURNAL	305-333	29	1	WOS:000259635100001	In the past decade, many statistical methods have been proposed for the analysis of case control genetic data with an emphasis on haplotype-based disease association studies. Most of the methodology has concentrated on the estimation of genetic (haplotype) main effects. Most methods accounted for estimation and gene-environment interaction effects by using prospective-type analyses that may lead to biased estimates when used with case control data. Several recent publications addressed the issue of retrospective sampling in the analysis of case-control genetic data in the presence of environmental factors by developing efficient semiparametric statistical methods. This article describes the new Stata command haplologit, which implements efficient profile-likelihood semiparametric methods for fitting gene environment models in the very important special cases of a rare disease, a single candidate gene in Hardy-Weinberg equilibrium, and independence of genetic and environmental factors.
55842418	WOS:000254619500001	283EM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MLDS: Maximum likelihood difference scaling in R	Journal	Article	2008	3	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000254619500001	The MLDS package in the R programming language can be used to estimate perceptual scales based on the results of psychophysical experiments using the method of difference scaling. In a difference scaling experiment, observers compare two 'supra-threshold differences (a, b) and (c, d) on each trial. The approach is based on a stochastic model of how the observer decides which perceptual difference (or interval) (a;b) or (c;d) is greater, and the parameters of the model are estimated using a maximum likelihood criterion. We also propose a method to test the model by evaluating the self-consistency of the estimated scale. The package includes an example in which an observer judges the differences in correlation between scatter plots. The example may be readily adapted to estimate perceptual scales for arbitrary physical continua.
55872579	WOS:000268680400003	479SE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multivariate random-effects meta-analysis	Journal	Article	2009	1	1	English	STATA JOURNAL	40-56	17	1	WOS:000268680400003	Multivariate meta-analysis combines estimates of several related parameters over several studies. These parameters can, for example, refer to multiple outcomes or comparisons between more than two groups. A new Stata command, mvmeta, performs maximum likelihood, restricted maximum likelihood, or method-of-moments estimation of random-effects multivariate meta-analysis models. A utility command, mvmeta_make, facilitates the preparation of summary datasets from more detailed data. The commands are illustrated with data from the Fibrinogen Studies Collaboration, a meta-analysis of observational studies; I estimate the shape of the association between a quantitative exposure and disease events by grouping the quantitative exposure into several categories.
55889851	WOS:000270999700006	509FJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Robust regression in Stata	Journal	Article	2009	1	1	English	STATA JOURNAL	439-453	15	1	WOS:000270999700006	In regression analysis, the presence of outliers in the dataset can strongly distort the classical least-squares estimator and lad to unreachable results To deal with this, several robust-to-outliers methods have been proposed in the statiscal literature. In Stata, some of these methods are available through the rreg and qreg commands. Unfortunately, these methods resist only some specific types of outhers and turn out to be ineffective under alternative scenarios. In this article, we present more effective robust estimators that we implemented in Stata We also present a graphical tool that recognizes the type of detected outtliers.
55934319	WOS:000268973400006	483NJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Further development of flexible parametric models for survival analysis	Journal	Article	2009	1	1	English	STATA JOURNAL	265-290	26	1	WOS:000268973400006	Royston and Parmar (2002, Statistics in Medicine 21: 2175-2197) developed a class of flexible parametric survival models that were programmed in Stata with the stpm command (Royston, 2001, Stata Journal 1: 1-28). In this article, we introduce a new command, stpm2, that extends the methodology. New features for stpm2 include improvement in the way time-dependent covariates are modeled, with these effects far less likely to be over parameterized; the ability to incorporate expected mortality and thus fit relative survival models; and a superior predict command that enables simple quantification of differences between any two covariate patterns through calculation of time-dependent hazard ratios, hazard differences, and survival differences. The ideas are illustrated through a study of breast cancer survival and incidence of hip fracture in prostate cancer patients.
55988840	WOS:000267708500001	467AP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	BiplotGUI: Interactive Biplots in R	Journal	Article	2009	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000267708500001	Biplots simultaneously provide information on both the samples and the variables of a data matrix in two- or three-dimensional representations. The BiplotGUI package provides a graphical user interface for the construction of, interaction with, and manipulation of biplots in R. The samples are represented as points, with coordinates determined either by the choice of biplot, principal coordinate analysis or multidimensional scaling. Various transformations and dissimilarity metrics are available. Information on the original variables is incorporated by linear or non-linear calibrated axes. Goodness-of-fit measures are provided. Additional descriptors can be superimposed, including convex hulls, alpha-bags, point densities and classification regions. Amongst the interactive features are dynamic variable value prediction, zooming and point and axis drag-and-drop. Output can easily be exported to the R workspace for further manipulation. Three-dimensional biplots are incorporated via the rgl package. The user requires almost no knowledge of R syntax.
56016859	WOS:000268680400002	479SE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Accommodating covariates in receiver operating characteristic analysis	Journal	Article	2009	1	1	English	STATA JOURNAL	17-39	23	1	WOS:000268680400002	Classification accuracy is the ability of a marker or diagnostic test to discriminate between two groups of individuals, cases and controls, and is commonly summarized by using the receiver operating characteristic (ROC) curve. In studies of classification accuracy, there are often covariates that should be incorporated into the ROC analysis. We describe three ways of using covariate information. For factors that affect; marker observations among controls, we present a method for covariate adjustment. For factors that affect discrimination (i.e., the ROC curve), we describe methods for modeling the ROC curve as a function of covariates. Finally, for factors that contribute to discrimination, we propose combining the marker and covariate information, and we ask how much discriminatory accuracy improves (in incremental value) with the addition of the marker to the covariates. These methods follow naturally when representing the ROC curve as a summary of the distribution of case marker observations, standardized with respect to the control distribution.
56021005	WOS:000270999700004	509FJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Implementing weak-instrument robust tests for a general class of instrumental-variables models	Journal	Article	2009	1	1	English	STATA JOURNAL	398-421	24	1	WOS:000270999700004	We present a minimum distance approach for conducting hypothesis testing in the presence of potentially weak instruments. Under tlus approach, We propose size-correct tests for limited dependent variable models with endogenous explanatory variables such as endogenous tobit and probit models. Additionally, we extend weak-instrument tests for the linear instrumental-variables model by allowing for variance-covariance estimation that is robust to arbitrary heteroskedasticity or intracluster dependence. We invert these tests to construct confidence intervals on the coefficient of the endogenous variable. We also provide a postestimation command for Stata, called r1vtest for computing the tests and estimating confidence intervals.
56027037	WOS:000208589800005	V27BX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	asympTest: A Simple R Package for Classical Parametric Statistical Tests and Confidence Intervals in Large Samples	Journal	Article	2009	12	1	English	R JOURNAL	26-30	5	1	WOS:000208589800005	asympTest is an R package implementing large sample tests and confidence intervals. One and two sample mean and variance tests (differences and ratios) are considered. The test statistics are all expressed in the same form as the Student t-test, which facilitates their presentation in the classroom. This contribution also fills the gap of a robust (to non-normality) alternative to the chi-square single variance test for large samples, since no such procedure is implemented in standard statistical software.
56056741	WOS:000266310100001	449DD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Quality Control for Statistical Graphics: The graphicsQC Package for R	Journal	Article	2009	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000266310100001	An important component of quality control for statistical graphics software is the ability not only to test that code runs without errors, but also to test that code produces the right result. The simple way to test for the correct result in graphical output is to test whether a test file differs from a control file; this is effective in determining whether a difference exists. However, the test can be significantly enhanced by also producing a graphical image of any difference; this makes it much easier to determine how and why two files differ. This article describes the graphicsQC package for R, which provides functions for producing and comparing files of graphical output and for generating are port of the results, including images of any differences.
56104696	WOS:000271534000001	516HX	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Isotone Optimization in R: Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods	Journal	Article	2009	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000271534000001	In this paper we give a general framework for isotone optimization. First we discuss a generalized version of the pool-adjacent-violators algorithm (PAVA) to minimize a separable convex function with simple chain constraints. Besides of general convex functions we extend existing PAVA implementations in terms of observation weights, approaches for tie handling, and responses from repeated measurement designs. Since isotone optimization problems can be formulated as convex programming problems with linear constraints we then develop a primal active set method to solve such problem. This methodology is applied on specific loss functions relevant in statistics. Both approaches are implemented in the R package isotone.
56108670	WOS:000273272200007	539QL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Mata Matters: File processing	Journal	Article	2009	1	1	English	STATA JOURNAL	599-620	22	1	WOS:000273272200007	Mata is Stata's matrix language. In the Mata Matters column, we show bow Mata can be used interactively to solve problems and as a programming language to add new features to Stata. The subject of this column is using Mata to read into Stata datasets that are formatted difficultly, which involves Mata's file processing and string processing capabilities.
56152145	WOS:000267708700001	467AQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An Excel Add-In for Statistical Process Control Charts	Journal	Article	2009	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-12	12	1	WOS:000267708700001	Statistical process control (SPC) descibes a widely-used set of approaches used to detect shifts in processes in, for example, manufacturing. Among these are "control charts". Control charts and other SPC techniques have been in use since at least the 1950s, and, because they are comparatively unsophisticated, are often used by management or operations personnel without formal statistical training. These personnel will often have experience with the popular spreadsheet program Excel, but may have less training on a mainstream statistical package. Base Excel does not provide the ability to draw control charts directly, although add-ins for that purpose are available for purchase. We present a free add-in for Excel that draws the most common sorts of control charts. It follows the development of the textbook of Montgomery (2005), so it may be well-suited for instructional purposes.
56160505	WOS:000208589800007	V27BX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Transitioning to R: Replicating SAS, Stata, and SUDAAN Analysis Techniques in Health Policy Data	Journal	Article	2009	12	1	English	R JOURNAL	37-44	8	1	WOS:000208589800007	Statistical, data manipulation, and presentation tools make R an ideal integrated package for research in the fields of health policy and healthcare management and evaluation. However, the technical documentation accompanying most data sets used by researchers in these fields does not include syntax examples for analysts to make the transition from another statistical package to R. This paper describes the steps required to import health policy data into R, to prepare that data for analysis using the two most common complex survey variance calculation techniques, and to produce the principal set of statistical estimates sought by health policy researchers. Using data from the Medical Expenditure Panel Survey Household Component (MEPS-HC), this paper outlines complex survey data analysis techniques in R, with side-by-side comparisons to the SAS, Stata, and SUDAAN statistical software packages.
56172239	WOS:000270821500001	507AL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Effect Displays in R for Multinomial and Proportional-Odds Logit Models: Extensions to the effects Package	Journal	Article	2009	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000270821500001	Based on recent work by Fox and Anderson (2006), this paper describes substantial extensions to the effects package for R to construct effect displays for multinomial and proportional-odds logit models. The package previously was limited to linear and generalized linear models. Effect displays are tabular and graphical representations of terms - typically high-order terms - in a statistical model. For polytomous logit models, effect displays depict fitted category probabilities under the model, and can include point-wise confidence envelopes for the effects. The construction of effect displays by functions in the effects package is essentially automatic. The package provides several kinds of displays for polytomous logit models.
56186398	WOS:000266311100001	449DN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	From Spider-Man to Hero - Archetypal Analysis in R	Journal	Article	2009	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000266311100001	Archetypal analysis has the aim to represent observations in a multivariate data set as convex combinations of extremal points. This approach was introduced by Cutler and Breiman (1994); they defined the concrete problem, laid out the theoretical foundations and presented an algorithm written in Fortran. In this paper we present the R package archetypes which is available on the Comprehensive R Archive Network. The package provides an implementation of the archetypal analysis algorithm within R and different exploratory tools to analyze the algorithm during its execution and its final result. The application of the package is demonstrated on two examples.
56210479	WOS:000269819400001	494NE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	CAR: A MATLAB Package to Compute Correspondence Analysis with Rotations	Journal	Article	2009	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000269819400001	Correspondence analysis (CA) is a popular method that can be used to analyse relationships between categorical variables. Like principal component analysis, CA solutions can be rotated both orthogonally and obliquely to simple structure without affecting the total amount of explained inertia. We describe a MATLAB package for computing CA. The package includes orthogonal and oblique rotation of axes. It is designed not only for advanced users of MATLAB but also for beginners. Analysis can be done using a user-friendly interface, or by using command lines. We illustrate the use of CAR with one example.
56249935	WOS:000271534100001	516HY	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	mixtools: An R Package for Analyzing Finite Mixture Models	Journal	Article	2009	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-29	29	1	WOS:000271534100001	The mixtools package for R provides a set of functions for analyzing a variety of finite mixture models. These functions include both traditional methods, such as EM algorithms for univariate and multivariate normal mixtures, and newer methods that reflect some recent research in finite mixture models. In the latter category, mixtools provides algorithms for estimating parameters in a wide range of different mixture-of-regression contexts, in multinomial mixtures such as those arising from discretizing continuous multivariate data, in nonparametric situations where the multivariate component densities are completely unspecified, and in semiparametric situations such as a univariate location mixture of symmetric but otherwise unspecified densities. Many of the algorithms of the mixtools package are EM algorithms or are based on EM-like ideas, so this article includes an overview of EM algorithms for finite mixture models.
56308711	WOS:000269819700001	494NH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Structural Adaptive Smoothing in Diffusion Tensor Imaging: The R Package dti	Journal	Article	2009	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-22	23	1	WOS:000269819700001	Diffusion weighted imaging has become and will certainly continue to be all important tool in medical research and diagnostics. Data obtained with diffusion weighted imaging are characterized by a high noise level. Thus, estimation of quantities like anisotropy indices or the main diffusion direction may be significantly compromised by noise in clinical or neuroscience applications. Here, we present a new package dti for R, which provides functions for the analysis of diffusion weighted data within the diffusion tensor model. This includes smoothing by a recently proposed structural adaptive smoothing procedure based oil the propagation-separation approach in the context of the widely used diffusion tensor model. We extend the procedure and show, how a correction for Rician bias can be incorporated We use a heteroscedastic nonlinear regression model to estimate the diffusion tensor. The smoothing procedure naturally adapts to different structures of different size and thus avoids oversmoothing edges and fine structures. We illustrate the usage and capabilities of the package through some examples.
56321938	WOS:000270821700001	507AN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An Object-Oriented Framework for Robust Multivariate Analysis	Journal	Article	2009	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-47	47	1	WOS:000270821700001	Taking advantage of the S4 class system of the programming environment R, which facilitates the creation and maintenance of reusable and modular components, an object-oriented framework for robust multivariate analysis was developed. The framework resides in the packages robust base and rrcov and includes an almost complete set of algorithms for computing robust multivariate location and scatter, various robust methods for principal component analysis as well as robust linear and quadratic discriminant analysis. The design of these methods follows common patterns which we call statistical design patterns in analogy to the design patterns widely used in software engineering. The application of the framework to data analysis as well as possible extensions by the development of new methods is demonstrated on examples which themselves are part of the package rrcov
56355938	WOS:000208589800006	V27BX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	copas: An R package for Fitting the Copas Selection Model	Journal	Article	2009	12	1	English	R JOURNAL	31-36	6	1	WOS:000208589800006	This article describes the R package copas which is an add-on package to the R package meta. The R package copas can be used to fit the Copas selection model to adjust for bias in meta-analysis. A clinical example is used to illustrate fitting and interpreting the Copas selection model.
56376304	WOS:000263105100001	403TO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	LogConcDEAD: An R Package for Maximum Likelihood Estimation of a Multivariate Log-Concave Density	Journal	Article	2009	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000263105100001	In this article we introduce the R package LogConcDEAD (Log-concave density estimation in arbitrary dimensions). Its main function is to compute the nonparametric maximum likelihood estimator of a log-concave density. Functions for plotting, sampling from the density estimate and evaluating the density estimate are provided. All of the functions available in the package are illustrated using simple, reproducible examples with simulated data.
56432537	WOS:000268700300001	479ZF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	State of the Art in Parallel Computing with R	Journal	Article	2009	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000268700300001	R is a mature open-source programming language for statistical computing and graphics. Many areas of statistical research are experiencing rapid growth in the size of data sets. Methodological advances drive increased use of simulations. A common approach is to use parallel computing. This paper presents an overview of techniques for parallel computing with R on computer clusters, on multi-core systems, and in grid computing. It reviews sixteen different packages, comparing them on their state of development, the parallel technology used, as well as on usability, acceptance, and performance. Two packages (snow, Rmpi) stand out as particularly suited to general use on computer clusters. Packages for grid computing are still in development, with only one package currently available to the end user. For multi-core systems five different packages exist, but a number of issues pose challenges to early adopters. The paper concludes with ideas for further developments in high performance computing with R. Example code is available in the appendix.
56469706	WOS:000263825100001	413XD	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MCPMod: An R Package for the Design and Analysis of Dose-Finding Studies	Journal	Article	2009	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000263825100001	In this article the MCPMod package for the R programming environment will be introduced. It implements a recently developed methodology for the design and analysis of dose-response studies that combines aspects of multiple comparison procedures and modeling approaches (Bretz et al. 2005). The MCPMod package provides tools for the analysis of dose finding trials, as well as a variety of tools necessary to plan an experiment to be analyzed using the MCP-Mod methodology.
56486386	WOS:000268680400007	479SE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Rowwise	Journal	Article	2009	1	1	English	STATA JOURNAL	137-157	21	1	WOS:000268680400007	Stata's main data model treats observations in rows and variables in columns quite differently, but rowwise problems also arise that require working against the grain. This column shows how to exploit; existing functions and egen functions when they exist and apply to such problems. It offers advice on how to build your own loops, egen functions, or programs when needed. Mata provides especially convenient tools for constructing many such functions and programs, centered on putting selected data into matrices and then processing each observation as a separate vector. Two programs, rowsort and rowranks, are formally published with this column.
56500264	WOS:000266311000001	449DM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MetaEasy: A Meta-Analysis Add-In for Microsoft Excel	Journal	Article	2009	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-25	25	1	WOS:000266311000001	Meta-analysis is a statistical methodology that combines or integrates the results of several independent clinical trials considered by the analyst to be 'combinable' (Huque 1988). However, completeness and user-friendliness are uncommon both in specialised meta-analysis software packages and in mainstream statistical packages that have to rely on user-written commands. We implemented the meta-analysis methodology in a Microsoft Excel add-in which is freely available and incorporates more meta-analysis models (including the iterative maximum likelihood and profile likelihood) than are usually available, while paying particular attention to the user-friendliness of the package.
56515927	WOS:000268680400006	479SE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	How to do xtabond2: An introduction to difference and system GMM in Stata	Journal	Article	2009	1	1	English	STATA JOURNAL	86-136	51	1	WOS:000268680400006	The difference and system generalized method-of-moments estimators, developed by Holtz-Eakin, Newey, and Rosen (1988, Econometrica 56: 13711395); Arellano and Bond (1991, Review of Economic Studies 58: 277-297); Arellano and Bover (1995, Journal of Econometrics 68: 29-51); and Blundell and Bond (1998, Journal of Econometrics 87: 115-143), are increasingly popular. Both are general estimators designed for situations with "small T, large N" panels, meaning few time periods and many individuals; independent variables that are not strictly exogenous, meaning they are correlated with past and possibly current realizations of the error; fixed effects; and heteroskedasticity and autocorrelation within individuals. This pedagogic article first introduces linear generalized method of moments. Then it describes how limited time span and potential for fixed effects and endogenous regressors drive the design of the estimators of interest, offering Stata-based examples along the way. Next it describes how to apply these estimators with xtabond2. It also explains how to perform the Arellano-Bond test for autocorrelation in a panel after other Stata commands, using abar. The article concludes with some tips for proper use.
56525911	WOS:000268973400001	483NJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Methods for estimating adjusted risk ratios	Journal	Article	2009	1	1	English	STATA JOURNAL	175-196	22	1	WOS:000268973400001	The risk ratio can be a useful statistic for summarizing the results of cross-sectional, cohort, and randomized trial studies. I discuss several methods for estimating adjusted risk ratios and show how they can be executed in Stata, including 1) Mantel-Haenszel and inverse-variance stratified methods; 2) generalized linear regression with a log link and binomial distribution; 3) generalized linear regression with a log link, normal distribution, and robust variance estimator, 4) Poisson regression with a robust variance estimator; 5) Cox proportional hazards regression with a robust variance estimator; 6) standardized risk ratios from logistic, probit, complementary log-log, and log-log regression; and 7) a substitution method. Advantages and drawbacks are noted for some methods.
56537427	WOS:000270999700007	509FJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Nonparametric testing of distributions-the Epps-Singleton two-sample test using the empirical characteristic function	Journal	Article	2009	1	1	English	STATA JOURNAL	454-465	12	1	WOS:000270999700007	In statistics, two-sample tests are used to determine whether two samples have been drawn from the same population. An example of such a test is the widely used Kolinogorov Smirnov two-sample test There are other distribution-free tests that might, be applied ill similar occasions. lit this article, we describe a two-sample onmibus test introduced by Epps and Singleton, which usually has a greater power than the Kolmogorov Smirnov test although it is distribution free The superiority of the Epps - singleton characteristic function test is illustrated in two examples We compare the two tests and supplement this contribution With a Stata implementation of the omnibus test.
56560814	WOS:000267708100001	467AM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	An Interactive Java Statistical Image Segmentation System: GemIdent	Journal	Article	2009	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-20	20	1	WOS:000267708100001	Supervised learning can be used to segment/identify regions of interest in images using both color and morphological information. A novel object identification algorithm was developed in Java to locate immune and cancer cells in images of immunohistochemically-stained lymph node tissue from a recent study published by Kohrt et al. (2005). The algorithms are also showing promise in other domains. The success of the method depends heavily on the use of color, the relative homogeneity of object appearance and on interactivity. As is often the case in segmentation, an algorithm specifically tailored to the application works bette than using broader methods that work passably well on any problem. Our main innovation is the interactive feature extraction from color images. We also enable the user to improve the classification with an interactive visualization system. This is then coupled with the statistical learning algorithms and intensive feedback from the user over many classification-correction iterations, resulting in highly accurate and user-friendly solution. The system ultimately provides the locations of every cell recognized in the entire tissue in a text file tailored to be easily imported into R (Ihaka and Gentleman 1996; R Development Core Team 2009) for further statistical analyses. This data is invaluable in the study of spatial and multidimentional relationships between cell populations and tumor structure. This system is available at http://www.GemIdent.com/ together with three demonstration videos and a manual.
56586329	WOS:000263105300001	403TQ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	RinRuby: Accessing the R Interpreter from Pure Ruby	Journal	Article	2009	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000263105300001	RinRuby is a Ruby library that integrates the R interpreter in Ruby, making R's statistical routines and graphics available within Ruby. The library consists of a single Ruby script that is simple to install and does not require any special compilation or installation of R. Since the library is 100% pure Ruby, it works on a variety of operating systems, Ruby implementations, and versions of R. RinRuby's methods are simple, making for readable code. This paper describes RinRuby usage, provides comprehensive documentation, gives several examples, and discusses RinRuby's implementation. The latest version of RinRuby can be found at the project website:http://rinruby.ddahl.org/.
56597699	WOS:000263105200001	403TP	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Adaptive Mixture of Student-t Distributions as a Flexible Candidate Distribution for Efficient Simulation: The R Package AdMit	Journal	Article	2009	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-32	32	1	WOS:000263105200001	This paper presents the R package AdMit which provides flexible functions to approximate a certain target distribution and to efficiently generate a sample of random draws from it, given only a kernel of the target density function. The core algorithm consists of the function AdMit which fits an adaptive mixture of Student-t distributions to the density of interest. Then, importance sampling or the independence chain Metropolis-Hastings algorithm is used to obtain quantities of interest for the target density, using the fitted mixture as the importance or candidate density. The estimation procedure is fully automatic and thus avoids the time-consuming and difficult task of tuning a sampling algorithm. The relevance of the package is shown in two examples. The first aims at illustrating in detail the use of the functions provided by the package in a bivariate bimodal distribution. The second shows the relevance of the adaptive mixture procedure through the Bayesian estimation of a mixture of ARCH model fitted to foreign exchange log-returns data. The methodology is compared to standard cases of importance sampling and the Metropolis-Hastings algorithm using a naive candidate and with the Griddy-Gibbs approach.
56604616	WOS:000270513700001	503DV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	CircStat: A MATLAB Toolbox for Circular Statistics	Journal	Article	2009	9	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000270513700001	Directional data is ubiquitious in science. Due to its circular nature such data cannot be analyzed with commonly used statistical techniques. Despite the rapid development of specialized methods for directional statistics over the last fifty years, there is only little software available that makes such methods easy to use for practioners. Most importantly, one of the most commonly used programming languages in biosciences, MATLAB, is currently not supporting directional statistics. To remedy this situation, we have implemented the CircStat toolbox for MATLAB which provides methods for the descriptive and inferential statistical analysis of directional data. We cover the statistical background of the available methods and describe how to apply them to data. Finally, we analyze a dataset from neurophysiology to demonstrate the capabilities of the CircStat toolbox.
56613082	WOS:000268700700001	479ZJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Gifi Methods for Optimal Scaling in R: The Package homals	Journal	Article	2009	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-21	21	1	WOS:000268700700001	Homogeneity analysis combines the idea of maximizing the correlations between variables of a multivariate data set with that of optimal scaling. In this article we present methodological and practical issues of the R package homals which performs homogeneity analysis and various extensions. By setting rank constraints nonlinear principal component analysis can be performed. The variables can be partitioned into sets such that homogeneity analysis is extended to nonlinear canonical correlation analysis or to predictive models which emulate discriminant analysis and regression models. For each model the scale level of the variables can be taken into account by setting level constraints. All algorithms allow for missing values.
56620343	WOS:000263105000001	403TN	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Robust Likelihood-Based Survival Modeling with Microarray Data	Journal	Article	2009	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-16	16	1	WOS:000263105000001	Gene expression data can be associated with various clinical outcomes. In particular, these data can be of importance in discovering survival-associated genes for medical applications. As alternatives to traditional statistical methods, sophisticated methods and software programs have been developed to overcome the high-dimensional difficulty of microarray data. Nevertheless, new algorithms and software programs are needed to include practical functions such as the discovery of multiple sets of survival-associated genes and the incorporation of risk factors, and to use in the R environment which many statisticians are familiar with. For survival modeling with microarray data, we have developed a software program (called rbsurv) which can be used conveniently and interactively in the R environment. This program selects survival-associated genes based on the partial likelihood of the Cox model and separates training and validation sets of samples for robustness. It can discover multiple sets of genes by iterative forward selection rather than one large set of genes. It can also allow adjustment for risk factors in microarray survival modeling. This software package, the rbsurv package, can be used to discover survival-associated genes with microarray data conveniently.
56636148	WOS:000273272200002	539QL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	cem: Coarsened exact matching in Stata	Journal	Article	2009	1	1	English	STATA JOURNAL	524-546	23	1	WOS:000273272200002	In this article, we introduce a Stata implementation of coarsened exact matching, a new method for improving the estimation of causal effects by reducing imbalance in covariates between treated and control groups. Coarsened exact matching is faster, is easier to use and understand, requires fewer assumptions, is more easily automated, and possesses more attractive statistical properties for many applications than do existing matching methods. In coarsened exact matching, users temporarily coarsen their data, exact match on these coarsened data, and then run their analysis on the uncoarsened, matched data. Coarsened exact matching bounds the degree of model dependence and causal effect estimation error by ex ante user choice, is monotonic imbalance bounding (so that reducing the maximum imbalance on one variable has no effect on others), does not require a separate procedure to restrict data to common support, meets the congruence principle., is approximately invariant to measurement error, balances all nonlinearities and interactions in sample (i.e., not merely in expectation), and works with multiply imputed datasets. Other matching methods inherit many of the coarsened exact matching method's properties when applied to further match data preprocessed by coarsened exact matching. The cem command implements the coarsened exact matching algorithm in Stata.
56655489	WOS:000208589800002	V27BX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Aspects of the Social Organization and Trajectory of the R Project	Journal	Article	2009	12	1	English	R JOURNAL	5-13	9	1	WOS:000208589800002	Based partly on interviews with members of the R Core team, this paper considers the development of the R Project in the context of open-source software development and, more generally, voluntary activities. The paper describes aspects of the social organization of the R Project, including the organization of the R Core team; describes the trajectory of the R Project; seeks to identify factors crucial to the success of R; and speculates about the prospects for R.
56657013	WOS:000266310600001	449DI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Importing Vector Graphics: The grImport Package for R	Journal	Article	2009	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-37	37	1	WOS:000266310600001	This article describes an approach to importing vector-based graphical images into statistical software as implemented in a package called grImport for the R statistical computing environment. This approach assumes that an original image can be transformed into a PostScript format (i.e., the opriginal image is in a standard vector graphics format such as PostScript, PDF, or SVG). The grImport pack-age consists of three components: a function for converting PostScript files to an R-specific XML format; a function for reading the XML format into special Picture objects in R; and functions for manipulating and drawing Picture objects. Several examples and applications are presented, including annotating a statistical plot with an imported logo and using imported images as plotting symbols.
56747760	WOS:000266310300001	449DF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	fgui: A Method for Automatically Creating Graphical User Interfaces for Command-Line R Packages	Journal	Article	2009	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000266310300001	The fgui R package is designed for developers of R packages, to help rapidly, and sometimes fully automatically, create a graphical user interface for command line R package. The interface is built upon the Tcl/Tk graphical interface include in R. The package further facilitates the developer by loading in the help files from the command line functions to provide context sensitive help to the user with no additional effort from the developer. Passing a function as argument to the routines in the fgui package creates a graphical interface for the function, and further options are available to tweak this interface for those who want more flexibility.
56754955	WOS:000268680400004	479SE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Meta-analysis with missing data	Journal	Article	2009	1	1	English	STATA JOURNAL	57-69	13	1	WOS:000268680400004	A new command, metamiss, performs meta-analysis with binary outcomes when some or all studies have missing data. Missing values can be imputed as successes, as failures, according to observed event rates, or by a combination of these according to reported reasons for the data being missing. Alternatively, the user can specify the value of, or a prior distribution for, the informative missingness odds ratio.
56815820	WOS:000273272200008	539QL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Paired, parallel, or profile plots for changes, correlations, and other comparisons	Journal	Article	2009	1	1	English	STATA JOURNAL	621-639	19	1	WOS:000273272200008	Paired, parallel, or profile plots showing the values of two variables may be constructed readily using a combination of graph twoway commands. This column explores the principles and practice of such plot-making, considering both wide and long (panel or longitudinal) data structures in which such data may appear. Applications include analysis of change over time or space and indeed ally kind of correlation or comparison between variables. Such plots may be extended to show numeric values and associated name information.
56860791	WOS:000273272200003	539QL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Bootstrap assessment of the stability of multivariable models	Journal	Article	2009	1	1	English	STATA JOURNAL	547-570	24	1	WOS:000273272200003	Assessing the instability of a multivariable model is important but is rarely done in practice. Model instability occurs when selected predictors-and for multivariable fractional polynomial modeling, selected functions of continuous predictors-are sensitive to small changes in the data. Bootstrap analysis is a useful technique for investigating variations among selected models in samples drawn at random with replacement. Such samples mimic datasets that are structurally similar to that under study and that could plausibly have arisen instead. The bootstrap inclusion fraction of a candidate variable usefully indicates the importance of the variable. We describe Stata tools for stability analysis in the context, of the mfp command for multivariable model building. We offer practical guidance and illustrate the application of the tools to a study in prostate cancer.
56890181	WOS:000266310800001	449DK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SMCTC: Sequential Monte Carlo in C plus	Journal	Article	2009	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-41	41	1	WOS:000266310800001	Sequential Monte Carlo methods are a very general class of Monte Carlo methods for sampling from sequences of distributions. Simple examples of these algorithms are used very widely in the tracking and signal processing literature. Recent developments illustrate that these techniques have much more general applicability, and can be applied very effectively to statistical inference problems. Unfortunately, these methods are often perceived as being computationally expensive and difficult to implement. This article seeks to address both of these problems. A C++ template class library for the efficient and convenient implementation of very general Sequential Monte Carlo algorithms is presented. Two example applications are provided: a simple particle filter for illustrative purposes and a state-of-the-art algorithm for rare event estimation.
56896363	WOS:000270821600001	507AM	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Unit Root CADF Testing with R	Journal	Article	2009	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000270821600001	This paper describes CADFtest, an R package for testing for the presence of a unit root in a time series using the covariate-augmented Dickey-Fuller (CADF) test proposed in Hansen (1995b). The procedures presented here are user friendly, allow fully automatic model specification, and allow computation of the asymptotic p values of the test.
56909414	WOS:000263825200001	413XE	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Processing Ecological Data in R with the mefa Package	Journal	Article	2009	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-28	28	1	WOS:000263825200001	mefa is an R package for multivariate data handling in ecology and biogeography. It provides object classes to represent the data coded by samples, taxa and segments (i.e., subpopulations, repeated measures). It supports easy processing of the data along with relational data tables for samples and taxa. An object of class 'mefa' is a project specific compendium of the dataset and can be easily used in further analyses. Methods are provided for extraction, aggregation, conversion, plotting, summary and reporting of 'mefa' objects. Reports can be generated in plain text or LATEX format. This paper presents worked examples on a variety of ecological analyses.
56955064	WOS:000268973400004	483NJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Two techniques for investigating interactions between treatment and continuous covariates in clinical trials	Journal	Article	2009	1	1	English	STATA JOURNAL	230-251	22	1	WOS:000268973400004	There is increasing interest in the medical world in the possibility of tailoring treatment to the individual patient. Statistically, the relevant task is to identify interactions between covariates and treatments, such that the patient's value of a given covariate influences how strongly (or even whether) they are likely to respond to a treatment. The most valuable data are obtained in randomized controlled clinical trials of novel treatments in comparison with a control treatment. We describe two techniques to detect and model such interactions. The first technique, multivariable fractional polynomials interaction, is based on fractional polynomials methodology, and provides a method of testing for continuous-by-binary interactions and by modeling the treatment effect as a function of a continuous covariate. The second technique, subpopulation treatment-effect pattern plot, aims to do something similar but is focused on producing a nonparametric estimate of the treatment effect, expressed graphically. Stata programs for both of these techniques are described. Real data for brain and breast cancer are used as examples.
56996461	WOS:000273272200006	539QL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Implementation of a new solution to the multivariate Behrens-Fisher problem	Journal	Article	2009	1	1	English	STATA JOURNAL	593-598	6	1	WOS:000273272200006	Krishnamoorthy and Yu (2004, Statistics and Probability Letters 66: 161-169) published a new approximate solution to the multivariate Behrens-Fisher problem. It is a modification of Nel and Van der Merwe's (1986, Communications in Statistics, Theory and Methods 15: 3719-3735) test. The test is invariant and identical to Welch's test for one-dimensional data. In this article, I describe all implementation of the test in Stata. The hotelmnm command allows you to perform the test easily and returns computed values for possible further computations.
57011225	WOS:000208589800003	V27BX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Party on!	Journal	Article	2009	12	1	English	R JOURNAL	14-17	4	1	WOS:000208589800003	Random forests are one of the most popular statistical learning algorithms, and a variety of methods for fitting random forests and related recursive partitioning approaches is available in R. This paper points out two important features of the random forest implementation cforest available in the party package: The resulting forests are unbiased and thus preferable to the randomForest implementation available in randomForest if predictor variables are of different types. Moreover, a conditional permutation importance measure has recently been added to the party package, which can help evaluate the importance of correlated predictor variables. The rationale of this new measure is illustrated and hands-on advice is given for the usage of recursive partitioning tools in R.
57026308	WOS:000263825000001	413XC	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	PSAgraphics: An R Package to Support Propensity Score Analysis	Journal	Article	2009	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-23	23	1	WOS:000263825000001	Propensity score analysis is a technique for adjusting for selection bias in observational data. Estimated propensity scores (probability of treatment given observed covariates) are used for stratification of observations. Within strata covariates should be more balanced between the two treatments than without the stratification. PSA graphics is an R package that provides flexible graphical tools to assess within strata balance between treatment groups, as well as how covariate distributions differ across strata. Additional graphical tools facilitate estimation of treatment effects having adjusted for covariate differences. Several new and conventional numerical measures of balance are also provided.
57046212	WOS:000268973400002	483NJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Updated tests for small-study effects in meta-analyses	Journal	Article	2009	1	1	English	STATA JOURNAL	197-210	14	1	WOS:000268973400002	This article describes an updated version of the metabias command, which provides statistical tests for funnel plot asymmetry. In addition to the previously implemented tests, metabias implements two new tests that are recommended in the recently updated Cochrane Handbook for Systematic Reviews of Interventions (Higgins and Green 2008). The first new test, proposed by Harbord, Egger, and Sterne (2006, Statistics in Medicine 25: 3443-3457), is a modified version of the commonly used test proposed by Egger et al. (1997, British Medical Journal 315: 629-634). It regresses Z/root V against root V, where Z is the efficient score and V is Fisher's information (the variance of Z under the null hypothesis). The second new test is Peters' test, which is based on a weighted linear regression of the intervention effect estimate on the reciprocal of the sample size. Both of the-se tests maintain better control of the false-positive rate than the test proposed by Egger at al., while retaining similar power.
57061970	WOS:000270821800001	507AO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	BB: An R Package for Solving a Large System of Nonlinear Equations and for Optimizing a High-Dimensional Nonlinear Objective Function	Journal	Article	2009	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-26	26	1	WOS:000270821800001	We discuss R package BB, in particular, its capabilities for solving a nonlinear system of equations. The function BBsolve in BB can be used for this purpose. We demonstrate the utility of these functions for solving: (a) large systems of nonlinear equations, (b) smooth, nonlinear estimating equations in statistical modeling, and (c) non-smooth estimating equations arising in rank-based regression modeling of censored failure time data. The function BBoptim can be used to solve smooth, box-constrained optimization problems. A main strength of BB is that, due to its low memory and storage requirements, it is ideally suited for solving high-dimensional problems with thousands of variables.
57065002	WOS:000270999700005	509FJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A seasonal unit-root test with Stata	Journal	Article	2009	1	1	English	STATA JOURNAL	422-438	17	1	WOS:000270999700005	Many economic time series exhibit important systematic fluctations within the year, i.e., seasonality In contrast to usual practice, I argue that using original data should always be considered, although the process is more complicated than that of using seasonally adjusted data Motivations to use unadjusted data come from the information contained in their peaks and troughs and from economic theory. One major complication is the possible unit root at seasonal frequencies. In this article, I tackle the issue of implementing a test to identify the source of seasonality. In particular, I follow Hylleberg et al. (1990, Journal of Econometrics 44, 215-238) for quarterly data.
57065908	WOS:000270999700003	509FJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Improved degrees of freedom for multivariate significance tests obtained from multiply imputed, small-sample data	Journal	Article	2009	1	1	English	STATA JOURNAL	388-397	10	1	WOS:000270999700003	We propose improvements to existing degrees of freedom used for significance testing of multivariate hypotheses in small samples when missing data are handled using multiple imputation. The improvements are for 1) tests based on unrestricted fractions of missing information and 2) tests based on equal fractions of missing information with M(p - 1) <= 4, where M is the number of imputations and p is the number of tested parameters. Using the m1 command avialable as of Stata 11, we demonstrate via simulation that using these adjustments can result in a more sensible degrees of freedom (and hence closer-to-nominal rejection rates) than existing degrees of freedom.
57073911	WOS:000208589800009	V27BX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	sos: Searching Help Pages of R Packages	Journal	Article	2009	12	1	English	R JOURNAL	56-59	4	1	WOS:000208589800009	The sos package provides a means to quickly and flexibly search the help pages of contributed packages, finding functions and datasets in seconds or minutes that could not be found in hours or days by any other means we know. Its findFn function accesses Jonathan Baron's R Site Search database and returns the matches in a data frame of class "findFn", which can be further manipulated by other sos functions to produce, for example, an Excel file that starts with a summary sheet that makes it relatively easy to prioritize alternative packages for further study. As such, it provides a very powerful way to do a literature search for functions and packages relevant to a particular topic of interest and could become virtually mandatory for authors of new packages or papers in publications such as The R Journal and the Journal of Statistical Software.
57145691	WOS:000266310500001	449DH	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	SOCR Analyses: Implementation and Demonstration of a New Graphical Statistics Educational Toolkit	Journal	Article	2009	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000266310500001	The web-based, Java-written SOCR (Statistical Online Computational Resource) tools have been utilized in amy undergraduate and graduate level statistics courses for seven years now (Dinov 2006; Dinov et al. 2008b). It has been proven that these resources can successfully improve students' learning (Dinov et al. 2008b). Being first published online in 2005, SOCR Analyses is a somewhat new component and it concentrate on data modeling for both parametric and non-parametric data analyses with graphical model diagnostics. One of the main purposes of SOCR Analyses is to facilitate statistical learning for high school and undergraduate students. As we have already implemented SOCR Distributions and Experiments, SOCR Analyses and Charts fulfill the rest of a standard statistics curricula. Currently, there are four core components of SOCR Analyses. Linear models included in SOCR Analyses are simple linear regression, multiple linear regression, one-way and two-way ANOVA. Tests for sample comparisons include t-test in the parametric category. Some examples of SOCR Analyses' in the non-parametric category are Wilcoxon rank sum test, Kruskal-Wallis test, Friedman's test, Kolmogorov-Smirnoff test and Fligner-Killeen test. Hypothesis testing models include contingency table, Friedan's test and Fisher's exact test. The last component of Analysis is a utility for computing sample sizes for normal distribution. In this article, we present the design framework, computational implementation and the utilization of SOCR Analyses.
57187145	WOS:000268973400003	483NJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	metandi: Meta-analysis of diagnostic accuracy using hierarchical logistic regression	Journal	Article	2009	1	1	English	STATA JOURNAL	211-229	19	1	WOS:000268973400003	Meta-analysis of diagnostic test accuracy presents many challenges. Even in the simplest case, when the data are summarized by a 2 x 2 table from each study, a statistically rigorous analysis requires hierarchical (multilevel) models that respect the binomial data structure, such as hierarchical logistic regression. We present a Stata package, metandi, to facilitate the fitting of such models in Stata. The commands display the results in two alternative parameterizations and produce a customizable plot. metandi requires either Stata 10 or above (which has the new command xtmelogit), or Stata 8.2 or above with gllamm installed.
57198034	WOS:000273272200005	539QL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Fitting and interpreting Cragg's tobit alternative using Stata	Journal	Article	2009	1	1	English	STATA JOURNAL	584-592	9	1	WOS:000273272200005	In this article, I introduce the user-written command craggit, which simultaneously fits both tiers of Cragg's (1971, Econometrica 39: 829-844) "two-tier" (sometimes called "two-stage" or "double-hurdle") alternative to tobit for corner-solution models. A key limitation to the tobit model is that the probability of a positive value and the actual value, given that it is positive, are determined by the same underlying process (i.e., the same parameters). Cragg proposed a more flexible alternative that allows these outcomes to be determined by separate processes through the incorporation of a probit model in the first tier and a truncated normal model in the second. Also, tobit is nested in craggit, making the latter a popular choice among "two-tier" models. In the article, I also present postestimation syntax to facilitate the understanding and interpretation of results.
57252807	WOS:000266310700001	449DJ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	POWERLIB: SAS/IML Software for Computing Power in Multivariate Linear Models	Journal	Article	2009	4	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000266310700001	The POWERLIB SAS/IML software provides convenient power calculations for a wide range of multivariate linear models with Gaussian errors. The software includes the Box, Geisser-Greenhouse, Huynh-Feldt, and uncorrected tests in the "univariate" approach to repeated measures (UNIREP), the Hotelling Lawley Trace, Pillai-Bartlett Trace, and Wilks Lambda tests in "multivariate" approach (MULTIREP), as well as a limited but useful range of mixed models. The familiar univariate linear model with Gaussian errors is an important special case. For estimated covariance, the software provides confidence limits for the resulting estimated power. All power and confidence limits values can be output to a SAS dataset, which can be used to easily produce plots and tables for manuscripts.
57261500	WOS:000208589800010	V27BX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	The New R Help System	Journal	Article	2009	12	1	English	R JOURNAL	60-65	6	1	WOS:000208589800010	Version 2.10.0 of R includes new code for processing '.Rd' help files. There are some changes to what is allowed, and some new capabilities and opportunities.
57268848	WOS:000208589800008	V27BX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	Rattle: A Data Mining GUI for R	Journal	Article	2009	12	1	English	R JOURNAL	45-55	11	1	WOS:000208589800008	Data mining delivers insights, patterns, and descriptive and predictive models from the large amounts of data available today in many organisations. The data miner draws heavily on methodologies, techniques and algorithms from statistics, machine learning, and computer science. R increasingly provides a powerful platform for data mining. However, scripting and programming is sometimes a challenge for data analysts moving into data mining. The Rattle package provides a graphical user interface specifically for data mining using R. It also provides a stepping stone toward using R as a programming language for data analysis.
57307407	WOS:000267708300001	467AO	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multi-Objective Optimal Experimental Designs for ER-fMRI Using MATLAB	Journal	Article	2009	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-13	13	1	WOS:000267708300001	Designs for event-related functional magnetic resonance imaging (ER-fMRI) that help to efficiently achieve the statistical goals while taking into account the psychological constraints and customized requirements are in great demand. This is not only because of the popularity of ER-fMRI but also because of the high cost of ER-fMRI experiments; being able to collect highly informative data is crucial. In this paper, we develop a MATLAB program which can accommodate many user-specified experimental conditions to efficiently find ER-fMRI optimal designs.
57318487	WOS:000273272200001	539QL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A menu-driven facility for sample-size calculation in novel multiarm, multistage randomized controlled trials with a time-to-event outcome	Journal	Article	2009	1	1	English	STATA JOURNAL	505-523	19	1	WOS:000273272200001	We present menu- and command-driven Stata programs for the calculation of sample size, number of events, and trial duration for a novel type of clinical trial design with a time-to-event outcome and two or more experimental arms. The approach is based oil terminating accrual of patients to inferior experimental treatment; arms at all early stage in the trial, allowing through to the next stage only treatments that show a predefined degree of advantage against the control treatment. The first stage of testing uses an intermediate outcome measure for the definitive (primary) outcome rather than with the primary outcome itself. The experimental arms are compared pairwise with the control arm according to the intermediate outcome measure. Arms that survive the comparison enter the next stage of patient accrual, culminating in comparisons against control on the primary outcome measure. The features supported include unequal patient allocation, target hazard ratios that may differ from 1 under the mull hypothesis, and the ability to stop patient recruitment at a specified time after trial initiation. The computations of sample size and power are based on the asymptotic mean and variance of the log hazard-ratio under the null and alternative hypotheses. The overall operating characteristics are computed from the intermediate and final stage significance levels and power, and the correlation between the log hazard-ratios oil the intermediate and primary outcome measures at the different stages. We illustrate the approach with the design of a United Kingdom Medical Research Council six-arm trial ill prostate cancer in which the intermediate outcome is failure-free survival and the primary outcome is overall survival.
57346832	WOS:000268992000001	483TV	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package	Journal	Article	2009	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000268992000001	Dynamic time warping is a popular technique for comparing time series, providing both a distance measure that is insensitive to local compression and stretches and the warping which optimally deforms one of the two input series onto the other. A variety of algorithms and constraints have been discussed in the literature. The dtw package provides an unification of them; it allows R users to compute time series alignments mixing freely a variety of continuity constraints, restriction windows, endpoints, local distance definitions, and so on. The package also provides functions for visualizing alignments and constraints using several classic diagram types.
57448079	WOS:000263825400001	413XG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Automatic Generation of Exams in R	Journal	Article	2009	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-14	14	1	WOS:000263825400001	Package exams provides a framework for automatic generation of standardized statistical exams which is especially useful for large-scale exams. To employ the tools, users just need to supply a pool of exercises and a master file controlling the layout of the final PDF document. The exercises are specified in separate Sweave files (containing R code for data generation and LATEX code for problem and solution description) and the master file is a LATEX document with some additional control commands. This paper gives an overview of the main design aims and principles as well as strategies for adaptation and extension. Hands-on illustrations-based on example exercises and control files provided in the package-are presented to get new users started easily.
57448155	WOS:000271534200001	516HZ	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Response-Surface Methods in R, Using rsm	Journal	Article	2009	10	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-17	17	1	WOS:000271534200001	This article describes the recent package rsm, which was designed to provide R support for standard response-surface methods. Functions are provided to generate central-composite and Box-Behnken designs. For analysis of the resulting data, the package provides for estimating the response surface, testing its lack of fit, displaying an ensemble of contour plots of the fitted surface, and doing follow-up analyses such as steepest ascent, canonical analysis, and ridge analysis. It also implements a coded-data structure to aid in this essential aspect of the methodology. The functions are designed in hopes of providing an intuitive and effective user interface. Potential exists for expanding the package in a variety of ways.
57465759	WOS:000268700800001	479ZK	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Simple and Canonical Correspondence Analysis Using the R Package anacor	Journal	Article	2009	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000268700800001	This paper presents the R package anacor for the computation of simple and canonical correspondence analysis with missing values. The canonical correspondence analysis is specified in a rather general way by imposing covariates on the rows and/or the columns of the two-dimensional frequency table. The package allows for scaling methods such as standard, Benzecri, centroid, and Goodman scaling. In addition, along with well-known two- and three-dimensional joint plots including confidence ellipsoids, it offers alternative plotting possibilities in terms of transformation plots, Benzecri plots, and regression plots.
57508934	WOS:000268700600001	479ZI	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Multidimensional Scaling Using Majorization: SMACOF in R	Journal	Article	2009	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-30	30	1	WOS:000268700600001	In this paper we present the methodology of multidimensional scaling problems (MDS) solved by means of the majorization algorithm. The objective function to be minimized is known as stress and functions which majorize stress are elaborated. This strategy to solve MDS problems is called SMACOF and it is implemented in an R package of the same name which is presented in this article. We extend the basic SMACOF theory in terms of configuration constraints, three-way data, unfolding models, and projection of the resulting configurations onto spheres and other quadratic surfaces. Various examples are presented to show the possibilities of the SMACOF approach offered by the corresponding package.
57514628	WOS:000208589800004	V27BX	2073-4859	2073485	NULL	NULL	NULL	NULL	NULL	ConvergenceConcepts: An R Package to Investigate Various Modes of Convergence	Journal	Article	2009	12	1	English	R JOURNAL	18-25	8	1	WOS:000208589800004	ConvergenceConcepts is an R package, built upon the tkrplot, tcltk and lattice packages, designed to investigate the convergence of simulated sequences of random variables. Four classical modes of convergence may be studied, namely: almost sure convergence (a.s.), convergence in probability (P), convergence in law (L) and convergence in r-th mean (r). This investigation is performed through accurate graphical representations. This package may be used as a pedagogical tool. It may give students a better understanding of these notions and help them to visualize these difficult theoretical concepts. Moreover, some scholars could gain some insight into the behaviour of some random sequences they are interested in.
57586522	WOS:000268973400007	483NJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Implementing Horn's parallel analysis for principal component analysis and factor analysis	Journal	Article	2009	1	1	English	STATA JOURNAL	291-298	8	1	WOS:000268973400007	I present paran, an implementation of Horn's parallel analysis criteria for factor or component retention in common factor analysis or principal component analysis in Stata. The command permits classical parallel analysis and more recent extensions to it for the pca and factor commands. paran provides a needed extension to Stata's built-in factor- and component-retention criteria.
57588333	WOS:000270999700009	509FJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: Creating and varying box plots	Journal	Article	2009	1	1	English	STATA JOURNAL	478-496	19	1	WOS:000270999700009	Box plots have been a standard statistical graph since John W. Tukey and his colleagues and students publicized them energetically in the 1970s. In Stata, graph box and graph hbox are commands available to draw box plots, but sometimes neither is sufficiently flexible for drawing some variations on standard box plot designs. This column explains how to use egen to calculate the statistical ingredients needed for box plots and twoway to re-create the plots themselves. That then allows variations such as adding means, connecting medians, or showing all data points beyond cortain quantiles.
57607816	WOS:000268973400005	483NJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multiple imputation of missing values: New features for mim	Journal	Article	2009	1	1	English	STATA JOURNAL	252-264	13	1	WOS:000268973400005	We present an update of mim, a program for managing multiply imputed datasets and performing inference (estimating parameters) using Rubin's rules for combining estimates from imputed datasets. The new features of particular importance are an option for estimating the Monte Carlo error (due to the sampling variability of the imputation process) in parameter estimates and in related quantities, and a general routine for combining any scalar estimate across imputations.
57611033	WOS:000272259100001	526AF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fitting the Cusp Catastrophein R: A cusp Package Primer	Journal	Article	2009	11	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000272259100001	Of the seven elementary catastrophes in catastrophe theory, the "cusp" model is the most widely applied. Most applications are however qualitative. Quantitative techniques for catastrophe modeling have been developed, but so far the limited availability of flexible software has hindered quantitative assessment. We present a package that implements and extends the method of Cobb (Cobb and Watson 1980; Cobb, Koppstein, and Chen 1983), and makes it easy to quantitatively fit and compare different cusp catastrophe models in a statistically principled way. After a short introduction to the cusp catastrophe, we demonstrate the package with two instructive examples.
57621246	WOS:000270999700008	509FJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Multiple imputation of missing values: Further update of ice, with an emphasis an categorical variables	Journal	Article	2009	1	1	English	STATA JOURNAL	466-477	12	1	WOS:000270999700008	Multiple inputation of missing data continues to be a topic of considerable interest and importance to applied researchers. In this article, the ice package for multiple imputation by chained equations (also known as fully conditional specification) is further updated. Special attention is paid to categorical variables. The relationship between ice and the new multiple-imputation system in Stata 11 is clarified.
57646888	WOS:000263825300001	413XF	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	MIDAS: A SAS Macro for Multiple Imputation Using Distance-Aided Selection of Donors	Journal	Article	2009	2	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-18	18	1	WOS:000263825300001	In this paper we describe MIDAS: a SAS macro for multiple imputation using distance aided selection of donors which implements an iterative predictive mean matching hot deck for imputing missing data. This is a flexible multiple imputation approach that can handle data in a variety of formats: continuous, ordinal, and scaled. Because the imputation models are implicit, it is not necessary to specify a parametric distribution for each variable to be imputed. MIDAS also allows the user to address the sensitivity of their inferences to different assumptions concerning the missing data mechanism. An example using MIDAS to impute missing data is presented and MIDAS is compared to existing missing data software.
57648868	WOS:000268700400001	479ZG	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Generalized and Customizable Sets in R	Journal	Article	2009	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000268700400001	We present data structures and algorithms for sets and some generalizations thereof (fuzzy sets, multisets, and fuzzy multisets) available for R through the sets package. Fuzzy (multi-)sets are based on dynamically bound fuzzy logic families. Further extensions include user-definable iterators and matching functions.
57683553	WOS:000268973400009	483NJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Speaking Stata: I. J. Good and quasi-Bayes smoothing of categorical frequencies	Journal	Article	2009	1	1	English	STATA JOURNAL	306-314	9	1	WOS:000268973400009	I. J. Good (1916-2009) was a prolific scientist who contributed to many fields, mostly from a Bayesian standpoint. This column explains his idea of quasi-Bayes (a.k.a. pseudo-Bayes) estimation or smoothing of categorical frequencies in a contingency table, which is especially useful as a way of dealing with awkward sampling or random zeros. It shows how the method can be implemented, almost calculator-style, using a combination of Stata and Mata. Convenience commands qsbayesi and qsbayes are also introduced.
57691085	WOS:000267708000001	467AL	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	cem: Software for Coarsened Exact Matching	Journal	Article	2009	6	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-27	27	1	WOS:000267708000001	This program is designed to improve causal inference via a method of matching that is widely applicable in observational data and easy to understand and use (if you understand how to draw a histogram, you will understand this method). The program implements the coarsened exact matching (CEM) algorithm, described below. CEM may be used alone or in combination with any existing matching method. This algorithm, and its statistical properties, are described in Iacus, King, and Porro (2008).
57702550	WOS:000268680400008	479SE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Review of The Workflow of Data Analysis Using Stata, by J. Scott Long	Journal	Review	2009	1	1	English	STATA JOURNAL	158-160	3	1	WOS:000268680400008	This article reviews The Workflow of Data Analysis Using Stata, by J. Scott Long.
57718252	WOS:000263105400001	403TR	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Maximum Entropy Bootstrap for Time Series: The meboot R Package	Journal	Article	2009	1	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-19	19	1	WOS:000263105400001	The maximum entropy bootstrap is an algorithm that creates an ensemble for time series inference. Stationarity is not required and the ensemble satisfies the ergodic theorem and the central limit theorem. The meboot R package implements such algorithm. This document introduces the procedure and illustrates its scope by means of several guided applications.
57745245	WOS:000268680400005	479SE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A general-purpose method for two-group randomization tests	Journal	Article	2009	1	1	English	STATA JOURNAL	70-85	16	1	WOS:000268680400005	We outline a novel approach to calculate exact p-levels for two-sample randomization tests. The approach closely resembles permute in its applications; with the main difference being that the results are approximated only if the execution time needed to calculate exact p-levels would exceed a specified maximum. We demonstrate its use by deriving p-levels for the significance of Somers' D, the coefficient of variation; the difference in means and in medians, and the difference in two multinomials.
57768762	WOS:000270999700002	509FJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Graphical representation of multivariate data using Chernoff faces	Journal	Article	2009	1	1	English	STATA JOURNAL	374-387	14	1	WOS:000270999700002	Chernoff (1971, Technical Report 71, Department of Statistics, Stanford University; 1973, Journal of the American Statistical Association 68; 361-368) proposed the use of cartoon-like faces to represent points m k dimensions. This article desribes a State implementation of a face-generating algorithm using the method proposed by Fluty (1980, Technical Report 3, Institute of Mathematical Statistics and Actuarial Science, Bern University), Schupbach (1987, Technical Report 25, Institute of Mathematical Statistics and Acturial Science, Bern University), and Friendly (1991, http://www.math.yorku.ca/SCS/sasmac/faces.html). I present examples of applying Chernoff faces to data clustering and outher detection.
57791446	WOS:000268973400008	483NJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	The Skillings-Mack test (Friedman test when there are missing data)	Journal	Article	2009	1	1	English	STATA JOURNAL	299-305	7	1	WOS:000268973400008	The Skillings-Mack statistic (Skillings and Mack, 1981, Technometrics 23: 171-177) is a general Friedman-type statistic that call be used ill almost any block design with an arbitrary missing-data structure. The missing data call be either missing by design, for example, all incomplete block design, or missing completely at random. The Skillings-Mack test is equivalent to the Friedman test when there are no missing data ill a balanced complete block design, and the Skillings-Mack test is equivalent to the test suggested in Durbin (1951, British Journal of Psychology, Statistical Section 4: 85-90) for a balanced incomplete block design. The Friedman test was implemented in Stata by Goldstein (19911 Stata Technical Bulletin 3: 26-27) and further developed in Goldstein (2005, Stata, Journal 5: 285). This article introduces the skilmack command, which performs the Skillings-Mack test. The skilmack command is also useful when there are many ties or equal ranks (N.B. the Friedman statistic compared with the chi(2) distribution will give a conservative result), as well as for small samples, appropriate results call be obtained by simulating the distribution of the test statistic under the mill hypothesis.
57822556	WOS:000268680400001	479SE	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Estimation and comparison of receiver operating characteristic curves	Journal	Article	2009	1	1	English	STATA JOURNAL	1-16	16	1	WOS:000268680400001	The receiver operating characteristic (ROC) curve displays the capacity of a marker or diagnostic test to discriminate between two groups of subjects, cases versus controls. We present a comprehensive suite of Stata commands for performing ROC analysis. Nonparametric, semiparametric, and parametric estimators are calculated. Comparisons between curves are based on the area or partial area under the ROC curve. Alternatively, pointwise comparisons between ROC curves or inverse ROC curves can be made. We describe options to adjust these analyses for covariates and to perform ROC regression in a companion article. We use a unified framework by representing the ROC curve as the distribution of the marker in cases where we have standardized it to the control reference distribution.
57829222	WOS:000270999700001	509FJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Confirmatory factor analysis using confa	Journal	Article	2009	1	1	English	STATA JOURNAL	329-373	45	1	WOS:000270999700001	This article describes the confa command, which fits confirmatory factor analysis models by maximum likelihood and provides diagnostics for the fitted models Descriptions of the command and it's options are given: and some illustrative examples are provided
57830826	WOS:000273272200004	539QL	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	Partial effects in probit and logit models with a triple dummy-variable interaction term	Journal	Article	2009	1	1	English	STATA JOURNAL	571-583	13	1	WOS:000273272200004	In nonlinear regression models, such as probit or logit models, coefficients cannot be interpreted as partial effects. The partial effects are usually nonlinear combinations of all regressors and regression coefficients of the model. We derive the partial effects in such models with a triple dummy-variable interaction term. The formulas derived here are implemented in the Stata inteff3 command. The command also applies the delta method to compute the standard errors of the partial effects. We illustrate the use of the command with an empirical application, analyzing how the gender gap in labor-market participation is affected by the presence of children and a university degree. We find that the presence of children increases the gender gap in labor-market participation but that this increase is smaller for more highly educated individuals.
57845798	WOS:000268991900001	483TU	1548-7660	1548766	NULL	NULL	NULL	NULL	NULL	Fechnerian Scaling in R: The Package fechner	Journal	Article	2009	8	1	English	JOURNAL OF STATISTICAL SOFTWARE	1-24	24	1	WOS:000268991900001	Fechnerian scaling is a procedure for constructing a metric on a set of objects (e.g., colors, symbols, X-ray films, or even statistical models) to represent dissimilarities among the objects "from the point of view" of a system (e.g., person, technical device, or even computational algorithm) "perceiving" these objects. This metric, called Fechnerian, is computed from a data matrix of pairwise discrimination probabilities or any other pairwise measure which can be interpreted as the degree with which two objects within the set are discriminated from each other. This paper presents the package fechner for performing Fechnerian scaling of object sets in R. We describe the functions of the package. Fechnerian scaling then is demonstrated on real and artificial data sets accompanying the package.
57858572	WOS:000268973400010	483NJ	1536-867X	1536867	NULL	NULL	NULL	NULL	NULL	A statistician's perspective on "Mostly Harmless Econometrics: An Empiricist's Companion", by Joshua D. Angrist and Jorn-Steffen Pischke	Journal	Article	2009	1	1	English	STATA JOURNAL	315-320	6	1	WOS:000268973400010	This article reviews Mostly Harmless Econometrics: An Empiricist's Companion, by Joshua D. Angrist and Jorn-Steffen Pischke.
